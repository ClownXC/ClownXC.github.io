<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>Flink 保证Exactly Once[2] | 周晓晨</title><meta name="author" content="周晓晨"><meta name="copyright" content="周晓晨"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><meta name="description" content="FlinkKafkaConsumer 做为 Source，从 Kafka 读取数据到 Flink 中，首先想一下设计 FlinkKafkaConsumer，需要考虑哪些？">
<meta property="og:type" content="article">
<meta property="og:title" content="Flink 保证Exactly Once[2]">
<meta property="og:url" content="http://joccer.gitee.io/2019/12/10/Flink-%E4%BF%9D%E8%AF%81Exactly-Once-2/index.html">
<meta property="og:site_name" content="周晓晨">
<meta property="og:description" content="FlinkKafkaConsumer 做为 Source，从 Kafka 读取数据到 Flink 中，首先想一下设计 FlinkKafkaConsumer，需要考虑哪些？">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg">
<meta property="article:published_time" content="2019-12-10T07:16:09.000Z">
<meta property="article:modified_time" content="2021-01-06T02:31:28.091Z">
<meta property="article:author" content="周晓晨">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://joccer.gitee.io/2019/12/10/Flink-%E4%BF%9D%E8%AF%81Exactly-Once-2/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css"><script>var GLOBAL_CONFIG = { 
  root: '/',
  hexoversion: '5.2.0',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: {"limitDay":1,"position":"top","messagePrev":"这篇文章已经发表","messageNext":"天了，其中某些内容可能已经过时～"},
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  ClickShowText: undefined,
  lightbox: 'mediumZoom',
  Snackbar: {"chs_to_cht":"你已切换为繁体","cht_to_chs":"你已切换为简体","day_to_night":"你已切换为深色模式","night_to_day":"你已切换为浅色模式","bgLight":"#49b1f5","bgDark":"#121212","position":"bottom-left"},
  justifiedGallery: {
    js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
    css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false
};

var saveToLocal = {
  set: function setWithExpiry(key, value, ttl) {
    const now = new Date()
    const expiryDay = ttl * 86400000
    const item = {
      value: value,
      expiry: now.getTime() + expiryDay,
    }
    localStorage.setItem(key, JSON.stringify(item))
  },

  get: function getWithExpiry(key) {
    const itemStr = localStorage.getItem(key)

    if (!itemStr) {
      return undefined
    }
    const item = JSON.parse(itemStr)
    const now = new Date()

    if (now.getTime() > item.expiry) {
      localStorage.removeItem(key)
      return undefined
    }
    return item.value
  }
}</script><script id="config_change">var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isSidebar: true,
  postUpdate: '2021-01-06 10:31:28'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(function () {
  window.activateDarkMode = function () {
    document.documentElement.setAttribute('data-theme', 'dark')
    if (document.querySelector('meta[name="theme-color"]') !== null) {
      document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
    }
  }
  window.activateLightMode = function () {
    document.documentElement.setAttribute('data-theme', 'light')
    if (document.querySelector('meta[name="theme-color"]') !== null) {
      document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
    }
  }

  const autoChangeMode = 'false'
  const t = saveToLocal.get('theme')
  if (autoChangeMode === '1') {
    const isDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches
    const isLightMode = window.matchMedia('(prefers-color-scheme: light)').matches
    const isNotSpecified = window.matchMedia('(prefers-color-scheme: no-preference)').matches
    const hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

    if (t === undefined) {
      if (isLightMode) activateLightMode()
      else if (isDarkMode) activateDarkMode()
      else if (isNotSpecified || hasNoSupport) {
        const now = new Date()
        const hour = now.getHours()
        const isNight = hour <= 6 || hour >= 18
        isNight ? activateDarkMode() : activateLightMode()
      }
      window.matchMedia('(prefers-color-scheme: dark)').addListener(function (e) {
        if (saveToLocal.get('theme') === undefined) {
          e.matches ? activateDarkMode() : activateLightMode()
        }
      })
    } else if (t === 'light') activateLightMode()
    else activateDarkMode()
  } else if (autoChangeMode === '2') {
    const now = new Date()
    const hour = now.getHours()
    const isNight = hour <= 6 || hour >= 18
    if (t === undefined) isNight ? activateDarkMode() : activateLightMode()
    else if (t === 'light') activateLightMode()
    else activateDarkMode()
  } else {
    if (t === 'dark') activateDarkMode()
    else if (t === 'light') activateLightMode()
  }
})()</script><link rel="stylesheet" href="/css/iconfont.css"><meta name="generator" content="Hexo 5.2.0"></head><body><div id="mobile-sidebar"><div id="menu_mask"></div><div id="mobile-sidebar-menus"><div class="mobile_author_icon"><img class="avatar-img" src="/img/chen.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="mobile_post_data"><div class="mobile_data_item is-center"><div class="mobile_data_link"><a href="/archives/"><div class="headline">文章</div><div class="length_num">101</div></a></div></div><div class="mobile_data_item is-center">      <div class="mobile_data_link"><a href="/tags/"><div class="headline">标签</div><div class="length_num">10</div></a></div></div><div class="mobile_data_item is-center">     <div class="mobile_data_link"><a href="/categories/"><div class="headline">分类</div><div class="length_num">14</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 项目实战</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fa fa-heartbeat"></i><span> 娱乐</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/playlist/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page" href="/gallery/"><i class="fa-fw fas fa-images"></i><span> 照片</span></a></li><li><a class="site-page" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/books/"><i class="fa-fw fas fa-book"></i><span> 书籍</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-folder-open"></i><span> 视频资料</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/teach/"><i class="fa-fw fas fa-video"></i><span> 数值分析</span></a></li><li><a class="site-page" href="/teach/"><i class="fa-fw fas fa-video"></i><span> 编译原理</span></a></li><li><a class="site-page" href="/teach/"><i class="fa-fw fas fa-video"></i><span> 操作系统</span></a></li><li><a class="site-page" href="/teach/"><i class="fa-fw fas fa-video"></i><span> 计算机组成</span></a></li><li><a class="site-page" href="/teach/"><i class="fa-fw fas fa-video"></i><span> 计算机网络</span></a></li><li><a class="site-page" href="/teach/jvm/"><i class="fa-fw fas fa-video"></i><span> JVM</span></a></li><li><a class="site-page" href="/teach/"><i class="fa-fw fas fa-video"></i><span> MySQL数据库</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于我</span></a></div></div></div></div><div id="body-wrap"><div id="sidebar"><i class="fas fa-arrow-right on" id="toggle-sidebar"></i><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar">     </div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%86%E6%9E%90-FlinkKafkaConsumer-%E7%9A%84%E8%AE%BE%E8%AE%A1%E6%80%9D%E6%83%B3"><span class="toc-number">1.</span> <span class="toc-text">分析 FlinkKafkaConsumer 的设计思想</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Kafka-offset-%E5%AD%98%E5%82%A8%E5%8F%8A%E5%A6%82%E4%BD%95%E5%AE%9E%E7%8E%B0-Consumer-%E5%AE%9E%E4%BE%8B%E6%B6%88%E8%B4%B9-partition-%E7%9A%84%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1"><span class="toc-number">1.1.</span> <span class="toc-text">Kafka offset 存储及如何实现 Consumer 实例消费 partition 的负载均衡</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Source-%E7%AB%AF%E5%B9%B6%E8%A1%8C%E5%BA%A6%E6%94%B9%E5%8F%98%E4%BA%86%EF%BC%8C%E5%A6%82%E4%BD%95%E6%9D%A5%E6%81%A2%E5%A4%8D-offset"><span class="toc-number">1.2.</span> <span class="toc-text">Source 端并行度改变了，如何来恢复 offset</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%A6%82%E4%BD%95%E5%AE%9E%E7%8E%B0%E8%87%AA%E5%8A%A8%E5%8F%91%E7%8E%B0%E5%BD%93%E5%89%8D%E6%B6%88%E8%B4%B9-topic-%E4%B8%8B%E6%96%B0%E5%A2%9E%E7%9A%84-partition"><span class="toc-number">1.3.</span> <span class="toc-text">如何实现自动发现当前消费 topic 下新增的 partition</span></a></li></ol></li></ol></div></div></div><header class="post-bg" id="page-header" style="background-image: url(https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg)"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">周晓晨</a></span><span id="menus"><div id="search_button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 项目实战</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fa fa-heartbeat"></i><span> 娱乐</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/playlist/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page" href="/gallery/"><i class="fa-fw fas fa-images"></i><span> 照片</span></a></li><li><a class="site-page" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/books/"><i class="fa-fw fas fa-book"></i><span> 书籍</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-folder-open"></i><span> 视频资料</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/teach/"><i class="fa-fw fas fa-video"></i><span> 数值分析</span></a></li><li><a class="site-page" href="/teach/"><i class="fa-fw fas fa-video"></i><span> 编译原理</span></a></li><li><a class="site-page" href="/teach/"><i class="fa-fw fas fa-video"></i><span> 操作系统</span></a></li><li><a class="site-page" href="/teach/"><i class="fa-fw fas fa-video"></i><span> 计算机组成</span></a></li><li><a class="site-page" href="/teach/"><i class="fa-fw fas fa-video"></i><span> 计算机网络</span></a></li><li><a class="site-page" href="/teach/jvm/"><i class="fa-fw fas fa-video"></i><span> JVM</span></a></li><li><a class="site-page" href="/teach/"><i class="fa-fw fas fa-video"></i><span> MySQL数据库</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于我</span></a></div></div><span class="close" id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></span></span></nav><div id="post-info"><div id="post-title"><div class="posttitle">Flink 保证Exactly Once[2]</div></div><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2019-12-10T07:16:09.000Z" title="发表于 2019-12-10 15:16:09">2019-12-10</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2021-01-06T02:31:28.091Z" title="更新于 2021-01-06 10:31:28">2021-01-06</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Flink/">Flink</a></span></div><div class="meta-secondline"> <span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">3.6k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>13分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout_post" id="content-inner"><article id="post"><div class="post-content" id="article-container"><p>FlinkKafkaConsumer 做为 Source，从 Kafka 读取数据到 Flink 中，首先想一下设计 FlinkKafkaConsumer，需要考虑哪些？</p>
<a id="more"></a>

<h3 id="分析-FlinkKafkaConsumer-的设计思想"><a href="#分析-FlinkKafkaConsumer-的设计思想" class="headerlink" title="分析 FlinkKafkaConsumer 的设计思想"></a>分析 FlinkKafkaConsumer 的设计思想</h3><p>FlinkKafkaConsumer 做为 Source，从 Kafka 读取数据到 Flink 中，首先想一下设计 FlinkKafkaConsumer，需要考虑哪些？</p>
<ul>
<li>Flink 中 kafka 的 offset 保存在哪里，具体如何保存呢？任务重启恢复时，如何读取之前消费的 offset？</li>
<li>如果 Source 端并行度改变了，如何来恢复 offset？</li>
<li>如何保证每个 FlinkKafkaConsumer 实例消费的 partition 负载均衡？如何保证不出现有的实例消费 5 个 kafka partition，有的实例仅消费 1 个 kafka partition？</li>
<li>当前消费的 topic 如果动态增加了 partition，Flink 如何实现自动发现并消费？</li>
</ul>
<p>带着这些问题来看一看 FlinkKafkaConsumer 是怎么解决上述问题的。</p>
<h4 id="Kafka-offset-存储及如何实现-Consumer-实例消费-partition-的负载均衡"><a href="#Kafka-offset-存储及如何实现-Consumer-实例消费-partition-的负载均衡" class="headerlink" title="Kafka offset 存储及如何实现 Consumer 实例消费 partition 的负载均衡"></a>Kafka offset 存储及如何实现 Consumer 实例消费 partition 的负载均衡</h4><p>Flink 将任务恢复需要的信息都保存在状态中，当然 Kafka 的 offset 信息也保存在 Flink 的状态中，当任务从状态中恢复时会从状态中读取相应的 offset，并从 offset 位置开始消费。</p>
<p>在 Flink 中有两个基本的 State：Keyed State 和 Operator State。</p>
<ul>
<li>Keyed State 只能用于 KeyedStream 的 function 和 Operator 中，一个 Key 对应一个 State；</li>
<li>而 Operator State 可以用于所有类型的 function 和 Operator 中，一个 Operator 实例对应一个 State，假如一个算子并行度是 5 且使用 Operator State，那么这个算子的每个并行度都对应一个 State，总共 5 个 State。</li>
</ul>
<p>FlinkKafkaConsumer 做为 Source 只能使用 Operator State，Operator State 只支持一种数据结构 ListState，可以当做 List 类型的 State。所以 FlinkKafkaConsumer 中，将状态保存在 Operator State 对应的 ListState 中。具体如何保存呢？需要先了解每个 FlinkKafkaConsumer 具体怎么消费 Kafka。</p>
<p>对于同一个消费者组，Kafka 要求 topic 的每个 partition 只能被一个 Consumer 实例消费，相反一个 Consumer 实例可以去消费多个 partition。当 Flink 消费 Kafka 时，出现了以下三种情况：</p>
<table>
<thead>
<tr>
<th align="left">情况</th>
<th align="left">现象</th>
</tr>
</thead>
<tbody><tr>
<td align="left">FlinkKafkaConsumer 并行度大于 topic 的 partition 数</td>
<td align="left">有些 FlinkKafkaConsumer 不会消费 Kafka</td>
</tr>
<tr>
<td align="left">FlinkKafkaConsumer 并行度等于 topic 的 partition 数</td>
<td align="left">每个 FlinkKafkaConsumer 消费 1 个 partition</td>
</tr>
<tr>
<td align="left">FlinkKafkaConsumer 并行度小于 topic 的 partition 数</td>
<td align="left">每个 FlinkKafkaConsumer 至少消费 1 个 partition，可能会消费多个 partition</td>
</tr>
</tbody></table>
<p>Flink 是如何为每个 Consumer 实例合理地分配去消费哪些 partition 呢？源码中 KafkaTopicPartitionAssigner 类的 assign 方法，负责分配 partition 给 Consumer 实例。assign 方法的输入参数为 KafkaTopicPartition 和 Consumer 的并行度，KafkaTopicPartition 主要包含两个字段：String 类型的 topic 和 int 类型的 partition。assign 方法返回该 KafkaTopicPartition 应该分配给哪个 Consumer 实例去消费。假如 Consumer 的并行度为 5，表示包含了 5 个 subtask，assign 方法的返回值范围为 0~4，分别表示该 partition 分配给 subtask0-subtask4。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> partition Kafka 中 topic 和 partition 信息</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> numParallelSubtasks subtask 的数量</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span> 该 KafkaTopicPartition 分配给哪个 subtask 去消费</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">int</span> <span class="title">assign</span><span class="params">(KafkaTopicPartition partition, <span class="keyword">int</span> numParallelSubtasks)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> startIndex = ((partition.getTopic().hashCode() * <span class="number">31</span>) &amp; <span class="number">0x7FFFFFFF</span>) % numParallelSubtasks;</span><br><span class="line">    <span class="keyword">return</span> (startIndex + partition.getPartition()) % numParallelSubtasks;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>assign 方法是如何给 KafkaTopicPartition 分配 Consumer 实例的呢？</p>
<p>第一行代码根据 topic name 的 hashCode 运算后对 subtask 的数量求余生成一个 startIndex，第二行代码用 startIndex + partition 编号对 subtask 的数量求余，可以保证该 topic 的 0 号 partition 分配给 startIndex 对应的 subtask，后续的 partition 依次分配给后续的 subtask。</p>
<p>例如，名为 “test-topic” 的 topic 有 11 个 partition 分别为 partition0-partition10，Consumer 有 5 个并行度分别为 subtask0-subtask4。计算后的 startIndex 为 1，表示 partition0 分配给 subtask1，partition1 分配给 subtask2 以此类推，subtask 与 partition 的对应关系如下图所示。</p>
<p>assign 方法给 partition 分配 subtask 实际上是轮循的策略，首先计算一个起点 startIndex 分配给 partition0，后续的 partition 轮循地分配给 subtask，从而使得每个 subtask 消费的 partition 得以均衡。</p>
<p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-19-122937.jpg" alt="img"></p>
<p>每个 subtask 只负责一部分 partition，所以在维护 partition 的 offset 信息时，每个 subtask 只需要将自己消费的 partition 的 offset 信息保存到状态中即可。</p>
<p>保存的格式理论来讲应该是 kv 键值对，key 为 KafkaTopicPartition，value 为 Long 类型的 offset 值。但 Flink 的 Operator State 只支持 ListState 一种数据结构，不支持 kv 格式，可以将 KafkaTopicPartition 和 Long 封装为 Tuple2&lt;KafkaTopicPartition, Long&gt; 存储到 ListState 中。如下所示，Flink 源码中确实如此，使用 ListState&lt;Tuple2&lt;KafkaTopicPartition, Long&gt;&gt; 类型的 unionOffsetStates 来保存 Kafka 的 offset 信息。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/** Accessor for state in the operator state backend. */</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">transient</span> ListState&lt;Tuple2&lt;KafkaTopicPartition, Long&gt;&gt; unionOffsetStates;</span><br></pre></td></tr></table></figure>

<p>当 Flink 应用从 Checkpoint 恢复任务时，会从 unionOffsetStates 中读取上一次 Checkpoint 保存的 offset 信息，并从 offset 的位置开始继续消费，从而实现 Flink 任务的故障容错。例如，任务重启后，Operator State 是一个 Operator 实例对应一个 State，subtask0 依然消费 partition4 和 partition9，subtask0 从自己的 State 中可以读取到 partition4 和 partition9 消费的 offset，从 offset 位置接着往后消费即可。问题来了，若 FlinkKafkaConsumer 的并行度改变后，offset 信息如何恢复呢？</p>
<h4 id="Source-端并行度改变了，如何来恢复-offset"><a href="#Source-端并行度改变了，如何来恢复-offset" class="headerlink" title="Source 端并行度改变了，如何来恢复 offset"></a>Source 端并行度改变了，如何来恢复 offset</h4><p>subtask1 当前消费了 3 个 partition，而其他 subtask 仅消费 2 个 partition，当发现 subtask1 读取 Kafka 成为瓶颈后，需要调大 Consumer 的并行度，使得每个 subtask 最多仅消费 2 个 partition。将 Consumer 实例的并行度增大到 6 以后，分配器对 partition 重新分配给 6 个 subtask，计算后的 startIndex 为 0，表示 partition0 分配给 subtask0，后续的 partition 采用轮循策略，partition 与 subtask 的对应关系如下。</p>
<p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-19-122939.jpg" alt="img"></p>
<p>之前 subtask0 消费 partition 4 和 9，并行度调大以后，subtask0 被分配消费 partition 0 和 6。但是 Flink 任务从 Checkpoint 恢复后，能保证 subtask0 读取到 partition 0 和 6 的 offset 吗？这个就需要深入了解当 Flink 算子并行度改变后，Operator State 的 ListState 两种恢复策略。两种策略如下所示，在 initializeState 方法中执行相应 API 来恢复。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">OperatorStateStore stateStore = context.getOperatorStateStore();</span><br><span class="line"></span><br><span class="line"><span class="comment">// 通过 getListState 获取 ListState</span></span><br><span class="line">stateStore.getListState(ListStateDescriptor&lt;S&gt; var1);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 通过 getUnionListState 获取 ListState</span></span><br><span class="line">stateStore.getUnionListState(ListStateDescriptor&lt;S&gt; var1);</span><br></pre></td></tr></table></figure>

<p>当并行度改变后，getListState 恢复策略是均匀分配，将 ListState 中保存的所有元素均匀地分配到所有并行度中，每个 subtask 获取到其中一部分状态信息。</p>
<p>getUnionListState 策略是将所有的状态信息合并后，每个 subtask 都获取到全量的状态信息。在 FlinkKafkaConsumer 中，假如使用 getListState 来获取 ListState，采用均匀分配状态信息的策略，Flink 可能给 subtask0 分配了 partition0 和 partition1 的 offset 信息，但实际上分配器让 subtask0 去消费 partition0 和 partition6，此时 subtask0 并拿不到 partition 6 的 offset 信息，不知道该从 partition 6 哪个位置消费，所以均匀分配状态信息的策略并不能满足需求。</p>
<p>这里应该使用 getUnionListState 来获取 ListState，也就是说每个 subtask 都可以获取到所有 partition 的 offset 信息，然后根据分配器让 subtask 0 去消费 partition0 和 partition6 时，subtask0 只需要从全量的 offset 中拿到 partition0 和 partition6 的状态信息即可。</p>
<p>这么做会使得每个 subtask 获取到一些无用的 offset 的信息，但实际上这些 offset 信息占用的空间会比较小，所以该方案成本比较低。关于 OperatorState 的 ListState 两种获取方式请参考代码：</p>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://github.com/zhisheng17/flink-learning/blob/master/flink-learning-state/src/main/java/com/zhisheng/state/operator/state/UnionListStateExample.java">https://github.com/zhisheng17/flink-learning/blob/master/flink-learning-state/src/main/java/com/zhisheng/state/operator/state/UnionListStateExample.java</a></p>
</blockquote>
<p>FlinkKafkaConsumer 初始化时，恢复 offset 相关的源码如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// initializeState  方法中用于恢复 offset 状态信息</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">void</span> <span class="title">initializeState</span><span class="params">(FunctionInitializationContext context)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    OperatorStateStore stateStore = context.getOperatorStateStore();</span><br><span class="line">    <span class="comment">// 此处省略了兼容 Flink 1.2 之前状态 API 的场景</span></span><br><span class="line">    ...</span><br><span class="line">    <span class="comment">// 此处使用的 getUnionListState，而不是 getListState。因为重启后，可能并行度被改变了</span></span><br><span class="line">    <span class="keyword">this</span>.unionOffsetStates = stateStore.getUnionListState(<span class="keyword">new</span> ListStateDescriptor&lt;&gt;(</span><br><span class="line">            OFFSETS_STATE_NAME,</span><br><span class="line">            TypeInformation.of(<span class="keyword">new</span> TypeHint&lt;Tuple2&lt;KafkaTopicPartition, Long&gt;&gt;() &#123;&#125;)));</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (context.isRestored() &amp;&amp; !restoredFromOldState) &#123;</span><br><span class="line">        restoredState = <span class="keyword">new</span> TreeMap&lt;&gt;(<span class="keyword">new</span> KafkaTopicPartition.Comparator());</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 将状态中恢复的 offset 信息 put 到 TreeMap 类型的 restoredState 中，方便查询</span></span><br><span class="line">        <span class="keyword">for</span> (Tuple2&lt;KafkaTopicPartition, Long&gt; kafkaOffset : unionOffsetStates.get()) &#123;</span><br><span class="line">            restoredState.put(kafkaOffset.f0, kafkaOffset.f1);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// open 方法对 FlinkKafkaConsumer 做初始化</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(Configuration configuration)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    <span class="comment">// 创建 Kafka partition 的发现器，用于检测该 subtask 应该去消费哪些 partition</span></span><br><span class="line">    <span class="keyword">this</span>.partitionDiscoverer = createPartitionDiscoverer(</span><br><span class="line">            topicsDescriptor,</span><br><span class="line">            getRuntimeContext().getIndexOfThisSubtask(),</span><br><span class="line">            getRuntimeContext().getNumberOfParallelSubtasks());</span><br><span class="line">    <span class="keyword">this</span>.partitionDiscoverer.open();</span><br><span class="line">    <span class="comment">// subscribedPartitionsToStartOffsets 存储当前 subtask 需要消费的 partition 及对应的 offset 初始信息</span></span><br><span class="line">    subscribedPartitionsToStartOffsets = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line">    <span class="comment">//用 partition 发现器获取该 subtask 应该消费且新发现的 partition</span></span><br><span class="line">    <span class="keyword">final</span> List&lt;KafkaTopicPartition&gt; allPartitions = partitionDiscoverer.discoverPartitions();</span><br><span class="line">    <span class="comment">// restoredState 在 initializeState 时初始化，所以 != null 表示任务从 Checkpoint 处恢复</span></span><br><span class="line">    <span class="keyword">if</span> (restoredState != <span class="keyword">null</span>) &#123;</span><br><span class="line">        <span class="keyword">for</span> (KafkaTopicPartition partition : allPartitions) &#123;</span><br><span class="line">            <span class="comment">// 若分配给该 subtask 的 partition 在 restoredState 中不包含</span></span><br><span class="line">            <span class="comment">// 说明该 partition 是新创建的 partition，默认从 earliest 开始消费</span></span><br><span class="line">              <span class="comment">// 并添加到 restoredState 中</span></span><br><span class="line">            <span class="keyword">if</span> (!restoredState.containsKey(partition)) &#123;</span><br><span class="line">                restoredState.put(partition, KafkaTopicPartitionStateSentinel.EARLIEST_OFFSET);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (Map.Entry&lt;KafkaTopicPartition, Long&gt; restoredStateEntry : restoredState.entrySet()) &#123;</span><br><span class="line">            <span class="comment">// 遍历 restoredState，使用分配器检测当前的 partition 是否分配给当前的 subtask</span></span><br><span class="line">            <span class="comment">// assign 方法返回当前 partition 应该分配的 subtask index 编号</span></span><br><span class="line">            <span class="comment">// getRuntimeContext().getIndexOfThisSubtask()  返回当前 subtask 的 index 编号</span></span><br><span class="line">            <span class="keyword">if</span> (KafkaTopicPartitionAssigner.assign(</span><br><span class="line">                restoredStateEntry.getKey(), getRuntimeContext().getNumberOfParallelSubtasks())</span><br><span class="line">                    == getRuntimeContext().getIndexOfThisSubtask())&#123;</span><br><span class="line">                <span class="comment">// 如果当前遍历的 partition 分配给当前 subtask 来消费，则将 partition 信息加到  subscribedPartitionsToStartOffsets 中</span></span><br><span class="line">                subscribedPartitionsToStartOffsets.put(restoredStateEntry.getKey(), restoredStateEntry.getValue());</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="comment">// else 表示任务不是从 Checkpoint 处恢复，本次源码主要分析状态恢复，不考虑该情况</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>对 offset 信息快照相关的源码如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">void</span> <span class="title">snapshotState</span><span class="params">(FunctionSnapshotContext context)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    <span class="comment">// 把旧的 offset 信息从 unionOffsetStates 清除掉</span></span><br><span class="line">    unionOffsetStates.clear();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">final</span> AbstractFetcher&lt;?, ?&gt; fetcher = <span class="keyword">this</span>.kafkaFetcher;</span><br><span class="line">    <span class="comment">// 通过提取器从 Kafka 读取数据，若 fetcher == null 表示提取器还未初始化</span></span><br><span class="line">    <span class="keyword">if</span> (fetcher == <span class="keyword">null</span>) &#123;</span><br><span class="line">        <span class="comment">// Kafka 提取器还未初始化，说明还未从 Kafka 中读取数据</span></span><br><span class="line">                <span class="comment">// 所以遍历 subscribedPartitionsToStartOffsets，将 offset 的初始信息写入到状态中</span></span><br><span class="line">        <span class="keyword">for</span> (Map.Entry&lt;KafkaTopicPartition, Long&gt; subscribedPartition : subscribedPartitionsToStartOffsets.entrySet()) &#123;</span><br><span class="line">            unionOffsetStates.add(Tuple2.of(subscribedPartition.getKey(), subscribedPartition.getValue()));</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (offsetCommitMode == OffsetCommitMode.ON_CHECKPOINTS) &#123;</span><br><span class="line">            <span class="comment">// 将 offset put 到 pendingOffsetsToCommit，后续 Commit 到 Kafka </span></span><br><span class="line">            pendingOffsetsToCommit.put(context.getCheckpointId(), restoredState);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="comment">// 从 Kafka 提取器中获取该 subtask 订阅的 partition 当前消费的 offset 信息</span></span><br><span class="line">        HashMap&lt;KafkaTopicPartition, Long&gt; currentOffsets = fetcher.snapshotCurrentState();</span><br><span class="line">        <span class="keyword">if</span> (offsetCommitMode == OffsetCommitMode.ON_CHECKPOINTS) &#123;</span><br><span class="line">            <span class="comment">// 将 offset put 到 pendingOffsetsToCommit，后续 Commit 到 Kafka </span></span><br><span class="line">            pendingOffsetsToCommit.put(context.getCheckpointId(), currentOffsets);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (Map.Entry&lt;KafkaTopicPartition, Long&gt; kafkaTopicPartitionLongEntry : currentOffsets.entrySet()) &#123;</span><br><span class="line">            <span class="comment">// 将该 subtask 订阅的 partition 以及当前 partition 消费到的 offset 写入到状态中</span></span><br><span class="line">            unionOffsetStates.add(</span><br><span class="line">                    Tuple2.of(kafkaTopicPartitionLongEntry.getKey(), kafkaTopicPartitionLongEntry.getValue()));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>上述源码分析描述了，当 Checkpoint 时 FlinkKafkaConsumer 如何将 offset 信息保存到状态中，当任务从 Checkpoint 处恢复时 FlinkKafkaConsumer 如何从状态中获取相应的 offset 信息，并解答了当 Source 并行度改变时 FlinkKafkaConsumer 如何来恢复 offset 信息。</p>
<h4 id="如何实现自动发现当前消费-topic-下新增的-partition"><a href="#如何实现自动发现当前消费-topic-下新增的-partition" class="headerlink" title="如何实现自动发现当前消费 topic 下新增的 partition"></a>如何实现自动发现当前消费 topic 下新增的 partition</h4><p>当 FlinkKafkaConsumer 初始化时，每个 subtask 会订阅一批 partition，但是当 Flink 任务运行过程中，如果被订阅的 topic 创建了新的 partition，FlinkKafkaConsumer 如何实现动态发现新创建的 partition 并消费呢？</p>
<p>在使用 FlinkKafkaConsumer 时，可以通过 Properties 传递一些配置参数，当配置了参数FlinkKafkaConsumerBase.KEY_PARTITION<em>DISCOVERY_INTERVAL</em>MILLIS 时，就会开启 partition 的动态发现，该参数表示间隔多久检测一次是否有新创建的 partition。那具体实现原理呢？相关源码的 UML 图如下所示：</p>
<p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-11-15-132311.png" alt="img"></p>
<p>笔者生产环境使用的 FlinkKafkaConsumer011，FlinkKafkaConsumer011 继承 FlinkKafkaConsumer09，FlinkKafkaConsumer09 继承 FlinkKafkaConsumerBase。将参数 KEY_PARTITION<em>DISCOVERY_INTERVAL_MILLIS 传递给 FlinkKafkaConsumer011 时，在 FlinkKafkaConsumer09 的构造器中会调用 getLong(checkNotNull(props, “props”), KEY_PARTITION_DISCOVERY_INTERVAL</em>MILLIS, PARTITION_DISCOVERY_DISABLED) 解析该参数，并最终赋值给 FlinkKafkaConsumerBase 的 discoveryIntervalMillis 属性。后续相关源码如下所示：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// FlinkKafkaConsumerBase 的 run 方法</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">(SourceContext&lt;T&gt; sourceContext)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">      ...</span><br><span class="line">    <span class="keyword">if</span> (discoveryIntervalMillis == PARTITION_DISCOVERY_DISABLED) &#123;</span><br><span class="line">            kafkaFetcher.runFetchLoop();</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="comment">// discoveryIntervalMillis 被设置了，则开启 PartitionDiscovery</span></span><br><span class="line">            runWithPartitionDiscovery();</span><br><span class="line">        &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// runWithPartitionDiscovery 方法会调用 createAndStartDiscoveryLoop 方法</span></span><br><span class="line"><span class="comment">// createAndStartDiscoveryLoop 方法内创建了一个线程去循环检测发现新分区</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">createAndStartDiscoveryLoop</span><span class="params">(AtomicReference&lt;Exception&gt; discoveryLoopErrorRef)</span> </span>&#123;</span><br><span class="line">    <span class="comment">//  创建一个线程去循环检测发现新分区</span></span><br><span class="line">    discoveryLoopThread = <span class="keyword">new</span> Thread(() -&gt; &#123;</span><br><span class="line">        <span class="keyword">while</span> (running) &#123;</span><br><span class="line">            <span class="keyword">final</span> List&lt;KafkaTopicPartition&gt; discoveredPartitions;</span><br><span class="line">            <span class="comment">//  用 partition 发现器获取该 subtask 应该消费且新发现的 partition</span></span><br><span class="line">            discoveredPartitions = partitionDiscoverer.discoverPartitions();</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 发现了新的 partition，则添加到 Kafka 提取器</span></span><br><span class="line">            <span class="keyword">if</span> (running &amp;&amp; !discoveredPartitions.isEmpty()) &#123;</span><br><span class="line">                <span class="comment">//  kafkaFetcher 添加 新发现的 partition</span></span><br><span class="line">                kafkaFetcher.addDiscoveredPartitions(discoveredPartitions);</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (running &amp;&amp; discoveryIntervalMillis != <span class="number">0</span>) &#123;</span><br><span class="line">                <span class="comment">//  sleep 设置的间隔时间</span></span><br><span class="line">                Thread.sleep(discoveryIntervalMillis);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;, <span class="string">&quot;Kafka Partition Discovery for &quot;</span> + getRuntimeContext().getTaskNameWithSubtasks());</span><br><span class="line"></span><br><span class="line">    discoveryLoopThread.start();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>discoveryLoopThread 线程中每间隔 discoveryIntervalMillis 时间会调用 partition 发现器获取该 subtask 应该消费且新发现的 partition，在 open 方法初始化时，同样也调用 partitionDiscoverer.discoverPartitions() 方法来获取新发现的 partition，partition 发现器的 discoverPartitions 方法第一次调用时，会返回该 subtask 所有的 partition，后续调用只会返回新发现的且应该被当前 subtask 消费的 partition。discoverPartitions 方法源码如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> List&lt;KafkaTopicPartition&gt; <span class="title">discoverPartitions</span><span class="params">()</span> <span class="keyword">throws</span> WakeupException, ClosedException </span>&#123;</span><br><span class="line">    List&lt;KafkaTopicPartition&gt; newDiscoveredPartitions;</span><br><span class="line">    <span class="comment">// 获取订阅的 Topic 的所有 partition </span></span><br><span class="line">    newDiscoveredPartitions = getAllPartitionsForTopics(topicsDescriptor.getFixedTopics());</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 剔除 旧的 partition 和 不应该被该 subtask 去消费的 partition</span></span><br><span class="line">    Iterator&lt;KafkaTopicPartition&gt; iter = newDiscoveredPartitions.iterator();</span><br><span class="line">    KafkaTopicPartition nextPartition;</span><br><span class="line">    <span class="keyword">while</span> (iter.hasNext()) &#123;</span><br><span class="line">        nextPartition = iter.next();</span><br><span class="line">        <span class="comment">// setAndCheckDiscoveredPartition 方法设计比较巧妙，</span></span><br><span class="line">          <span class="comment">// 将旧的 partition 和 不应该被该 subtask 消费的 partition，返回 false</span></span><br><span class="line">        <span class="comment">// 将这些partition 剔除，就是新发现的 partition</span></span><br><span class="line">        <span class="keyword">if</span> (!setAndCheckDiscoveredPartition(nextPartition)) &#123;</span><br><span class="line">            iter.remove();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> newDiscoveredPartitions;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// discoveredPartitions 中存放着所有发现的 partition</span></span><br><span class="line"><span class="keyword">private</span> Set&lt;KafkaTopicPartition&gt; discoveredPartitions = <span class="keyword">new</span> HashSet&lt;&gt;();</span><br><span class="line"></span><br><span class="line"><span class="comment">// setAndCheckDiscoveredPartition 方法实现</span></span><br><span class="line"><span class="comment">// 当参数的 partition 是新发现的 partition 且应该被当前 subtask 消费时，返回 true</span></span><br><span class="line"><span class="comment">// 旧的 partition 和 不应该被该 subtask 消费的 partition，返回 false</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">setAndCheckDiscoveredPartition</span><span class="params">(KafkaTopicPartition partition)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// discoveredPartitions 中不存在，表示发现了新的 partition，将其加入到 discoveredPartitions  </span></span><br><span class="line">    <span class="keyword">if</span> (!discoveredPartitions.contains(partition)) &#123;</span><br><span class="line">        discoveredPartitions.add(partition);</span><br><span class="line">        <span class="comment">// 再通过分配器来判断该 partition 是否应该被当前 subtask 去消费</span></span><br><span class="line">        <span class="keyword">return</span> KafkaTopicPartitionAssigner.assign(partition, numParallelSubtasks) == indexOfThisSubtask;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>上述代码中依赖 Set 类型的 discoveredPartitions 来判断 partition 是否是新的 partition，刚开始 discoveredPartitions 是一个空的 Set，所以任务初始化第一次调用发现器的 discoverPartitions 方法时，会把所有属于当前 subtask 的 partition 都返回，来保证所有属于当前 subtask 的 partition 都能被消费到。之后任务运行过程中，若创建了新的 partition，则新 partition 对应的那一个 subtask 会自动发现并从 earliest 位置开始消费，新创建的 partition 对其他 subtask 并不会产生影响。</p>
</div><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">周晓晨</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://joccer.gitee.io/2019/12/10/Flink-%E4%BF%9D%E8%AF%81Exactly-Once-2/">http://joccer.gitee.io/2019/12/10/Flink-%E4%BF%9D%E8%AF%81Exactly-Once-2/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://joccer.gitee.io" target="_blank">周晓晨</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"></div><div class="post_share"><div class="social-share" data-image="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" data-sites="wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i> 打赏<div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="/image/wechat.jpg" target="_blank"><img class="post-qr-code-img" src="/image/wechat.jpg" alt="微信"/></a><div class="post-qr-code-desc">微信</div></li><li class="reward-item"><a href="/image/alipay.jpg" target="_blank"><img class="post-qr-code-img" src="/image/alipay.jpg" alt="支付宝"/></a><div class="post-qr-code-desc">支付宝</div></li></ul></div></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2019/12/10/Flink%E6%A6%82%E8%BF%B0/"><img class="prev-cover" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" onerror="onerror=null;src='/img/404.jpg'"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">Hello Flink</div></div></a></div><div class="next-post pull-right"><a href="/2019/12/10/Flink-%E4%BF%9D%E8%AF%81Exactly-Once-1/"><img class="next-cover" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" onerror="onerror=null;src='/img/404.jpg'"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">Flink 保证Exactly Once[1]</div></div></a></div></nav><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div class="vcomment" id="vcomment"></div></div></div></div></article></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2018 - 2021 By 周晓晨</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><section id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="chat_btn" type="button" title="rightside.chat_btn"><i class="fas fa-sms"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></section><div class="search-dialog" id="local-search"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div></div><hr/><div id="local-search-results"><div id="local-hits"></div><div id="local-stats"><div class="local-search-stats__hr" id="hr"><span>由</span> <a target="_blank" rel="noopener" href="https://github.com/wzpan/hexo-generator-search" style="color:#49B1F5;">hexo-generator-search</a>
 <span>提供支持</span></div></div></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div id="search-mask"></div><div><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/medium-zoom/dist/medium-zoom.min.js"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><script src="/js/search/local-search.js"></script><div class="js-pjax"><script>function loadValine () {
  function initValine () {
    const initData = {
      el: '#vcomment',
      appId: 'os3tR4qQtIEkoK1S8XDHyBwY-gzGzoHsz',
      appKey: 'obbNIfixVI0uj7WoAqKy0hUv',
      placeholder: 'ヾﾉ≧∀≦)o来啊，快活啊!',
      avatar: 'monsterid',
      meta: 'nick,mail'.split(','),
      pageSize: '10',
      lang: 'en',
      recordIP: false,
      serverURLs: '',
      emojiCDN: '',
      emojiMaps: "",
      enableQQ: false,
      path: window.location.pathname,
    }

    if (true) { 
      initData.requiredFields= ('nick,mail'.split(','))
    }

    const valine = new Valine(initData)
  }

  if (typeof Valine === 'function') initValine() 
  else $.getScript('https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js', initValine)
}

if ('Valine' === 'Valine' || !true) {
  if (true) btf.loadComment(document.querySelector('#vcomment'),loadValine)
  else setTimeout(() => loadValine(), 0)
} else {
  function loadOtherComment () {
    loadValine()
  }
}</script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><script id="canvas_nest" defer="defer" color="0,0,255" opacity="0.7" zIndex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/canvas-nest.min.js"></script><script>(function(i,s,o,g,r,a,m){i["DaoVoiceObject"]=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;a.charset="utf-8";m.parentNode.insertBefore(a,m)})(window,document,"script",('https:' == document.location.protocol ? 'https:' : 'http:') + "//widget.daovoice.io/widget/6d85e03d.js","daovoice")
</script><script>var isChatBtn = true
daovoice('init', {
  app_id: '6d85e03d',},{
  launcher: { 
     disableLauncherIcon: isChatBtn // 悬浮 ICON 是否显示
  },
});
daovoice('update');

if (isChatBtn) {
  var chatBtnFn = () => {
    var chatBtn = document.getElementById("chat_btn")
    chatBtn.addEventListener("click", function(){
      daovoice('show')
    });
  }
  chatBtnFn()
} else {
  if (false) {
    function chatBtnHide () {
      daovoice('update', {},{
        launcher: { 
        disableLauncherIcon: true // 悬浮 ICON 是否显示
        },
      });
    }
    function chatBtnShow () {
      daovoice('update', {},{
        launcher: { 
        disableLauncherIcon: false // 悬浮 ICON 是否显示
        },
      });
    }
  }
}</script></div><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"model":{"jsonPath":"/live2dw/assets/haruto.model.json"},"display":{"position":"right","width":150,"height":300},"mobile":{"show":false},"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body></html>