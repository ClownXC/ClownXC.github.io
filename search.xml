<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Cloudera平台搭建</title>
      <link href="2020/05/25/Cloudera%E5%B9%B3%E5%8F%B0%E6%90%AD%E5%BB%BA/"/>
      <url>2020/05/25/Cloudera%E5%B9%B3%E5%8F%B0%E6%90%AD%E5%BB%BA/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><h1 id="1-服务器环境准备"><a href="#1-服务器环境准备" class="headerlink" title="1.服务器环境准备"></a>1.服务器环境准备</h1><h2 id="1-1-服务器环境概述"><a href="#1-1-服务器环境概述" class="headerlink" title="1.1 服务器环境概述"></a>1.1 服务器环境概述</h2><p>数据集群包含两台专业服务器，通过XenServer服务器虚拟化软件把两台专业服务器虚拟化为五台虚拟服务器(1个master和4个slave)；五台虚拟服务器都安装了<code>CentOS7(Linux)</code>操作系统，在此基础上安装了 <code>Java</code>、<code>C/C++</code>、<code>Scala</code>等基本开发工具，以及<code>Hadoop(HDFS,YARN)</code>、<code>MySQL</code>、<code>ZooKeeper</code>、<code>Kafka</code> 、<code>Spark2</code></p><a id="more"></a><p><code>Hbase</code>、<code>Spark</code>等数据集群必须的大数据存储及处理软件。数据集群需要安装的软件及其层次关系如表1.1所示。数据集群除了安装Hadoop、Spark、Hbase等组件外，在Master节点和data1节点安装了MySQL数据库。</p><table><thead><tr><th>主机</th><th>所在服务器</th><th>密码</th></tr></thead><tbody><tr><td>192.168.10.96</td><td>192.168.10.90</td><td>123456</td></tr><tr><td>192.168.10.98</td><td>192.168.10.90</td><td>123456</td></tr><tr><td>192.168.10.100</td><td>192.168.10.90</td><td>123456</td></tr><tr><td>192.168.10.102</td><td>192.168.10.120</td><td>123456</td></tr><tr><td>192.168.10.104</td><td>192.168.10.120</td><td>123456</td></tr></tbody></table><h2 id="1-2-关闭防火墙"><a href="#1-2-关闭防火墙" class="headerlink" title="1.2 关闭防火墙"></a>1.2 关闭防火墙</h2><p>关闭五台虚拟主机防火墙，分别在主机上执行以下命令</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">systemctl stop firewalld</span><br><span class="line">systemctl <span class="built_in">disable</span> firewalld</span><br></pre></td></tr></table></figure><h2 id="1-3-关闭-selinux"><a href="#1-3-关闭-selinux" class="headerlink" title="1.3 关闭 selinux"></a>1.3 关闭 selinux</h2><p>三台机器在root用户下执行以下命令关闭selinux</p><p>三台机器执行以下命令，关闭</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">selinuxvim /etc/selinux/config </span><br><span class="line"></span><br><span class="line">SELINUX=disabled</span><br></pre></td></tr></table></figure><h2 id="1-4-时间服务器"><a href="#1-4-时间服务器" class="headerlink" title="1.4 时间服务器"></a>1.4 时间服务器</h2><p>网络时间协议 <code>NTP(Network Time Protocol)</code>，可以用来同步网络中各个计算机的时间。<code>CentOS7</code>自带了<code>ntp</code>服务，这个服务不仅可以设置让本机和其他计算机做时间同步，还可以让本机扮演一个<code>time server</code>的角色，让局域网其他计算机和本机同步时间。修改配置文件<code>/etc/ntp.conf</code>可以把一台计算机设置为时间服务器，或与其他服务器同步时间。</p><p>以下设置<code>master</code> 为时间服务器，其他计算机 <code>(data1~data4)</code> 和 <code>master</code> 实现时间同步。以下以 <code>data1</code> (IP地址<code>192.168.10.98</code>)为例，介绍 <code>slave</code>  与 <code>master</code> 做时间同步。</p><h3 id="1-4-1-设置-master-基准时间"><a href="#1-4-1-设置-master-基准时间" class="headerlink" title="1.4.1 设置 master 基准时间"></a>1.4.1 设置 <code>master</code> 基准时间</h3><p>配置 <code>master</code> 做 <code>time server</code> ,<code>master</code> 本身不和其他机器时间同步，而是取本地硬件时间。所以，需要先把<code>master</code> 机器的时间调整准确。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@master~] date -s 09/13/2017         //设定日期 </span><br><span class="line">[root@master~] date -s 11:12:00          //设定时间</span><br><span class="line">[root@master~] clock -w</span><br><span class="line">[root@master~] hwclock -w</span><br></pre></td></tr></table></figure><h3 id="1-4-2-设置-master-为时间服务器"><a href="#1-4-2-设置-master-为时间服务器" class="headerlink" title="1.4.2 设置 master 为时间服务器"></a>1.4.2 <strong>设置</strong> <code>master</code> 为时间服务器</h3><p>将master配置成一个time server，需要修改/etc/ntp.conf。</p><p>如果master连接Internet，则master可以与以下“上级时间服务器”进行同步。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">server 0.centos.pool.ntp.org iburst     </span><br><span class="line">server 1.centos.pool.ntp.org iburst</span><br><span class="line">server 2.centos.pool.ntp.org iburst</span><br><span class="line">server 3.centos.pool.ntp.org </span><br></pre></td></tr></table></figure><h3 id="1-4-3-允许本机-ntpd-和本地硬件时间同步"><a href="#1-4-3-允许本机-ntpd-和本地硬件时间同步" class="headerlink" title="1.4.3. 允许本机 ntpd 和本地硬件时间同步"></a>1.4.3. 允许本机 <code>ntpd</code> 和本地硬件时间同步</h3><p>如果master与上级服务器同步失败，或master没有连接Internet，则和本地硬件时间同步。把以下带下划线内容添加到“上级服务器”的后面。如果master与上级服务器同步失败，和本地硬件时间同步。如图7-1所示。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">server 127.127.1.0</span><br><span class="line">fudge 127.127.1.0 stratum 10</span><br></pre></td></tr></table></figure><h2 id="7-5-关闭chronyd"><a href="#7-5-关闭chronyd" class="headerlink" title="7.5 关闭chronyd"></a>7.5 关闭chronyd</h2><p><code>chronyd</code>也是与时间相关的服务，设置为开机自启动，这个服务会导致<code>ntp</code>无法开启开机自启动，所以需要关闭该进程。时间服务器和客户机都要关闭。</p><ul><li>查看<code>chronyd</code>状态</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@master ~] systemctl status chronyd </span><br></pre></td></tr></table></figure><ul><li>关闭<code>chronyd</code>服务</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@master ~] systemctl <span class="built_in">disable</span> chronyd.service  </span><br></pre></td></tr></table></figure><h2 id="7-6-CentOS7-设置开机自启动-ntp-服务"><a href="#7-6-CentOS7-设置开机自启动-ntp-服务" class="headerlink" title="7.6  CentOS7 设置开机自启动 ntp 服务"></a>7.6  <code>CentOS7</code> 设置开机自启动 <code>ntp</code> 服务</h2><p>局域网时间服务器和客户机都应启动开机自启动<code>ntp</code>服务。</p><ul><li>设置<code>master</code>开机自启动<code>ntp</code>服务</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@master ~] systemctl enable ntpd.service </span><br></pre></td></tr></table></figure><ul><li>查看<code>ntpd</code>状态</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@master ~] systemctl status ntpd  </span><br></pre></td></tr></table></figure><ul><li>设置<code>data1</code>开机自启动<code>ntp</code>服务</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@data1 ~] systemctl enable ntpd.service </span><br></pre></td></tr></table></figure><ul><li>查看<code>ntpd</code>状态</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@data1 ~] systemctl status ntpd</span><br></pre></td></tr></table></figure><h2 id="1-4-更改主机名"><a href="#1-4-更改主机名" class="headerlink" title="1.4 更改主机名"></a>1.4 更改主机名</h2><p>五台机器分别更改主机名</p><table><thead><tr><th>主机</th><th>主机名</th></tr></thead><tbody><tr><td>192.168.10.96</td><td>master</td></tr><tr><td>192.168.10.98</td><td>data1</td></tr><tr><td>192.168.10.100</td><td>data2</td></tr><tr><td>192.168.10.102</td><td>data3</td></tr><tr><td>192.168.10.104</td><td>data4</td></tr></tbody></table><h2 id="1-6-安装-JDK"><a href="#1-6-安装-JDK" class="headerlink" title="1.6 安装 JDK"></a>1.6 安装 JDK</h2><h3 id="1-6-1-卸载-JDK1-8"><a href="#1-6-1-卸载-JDK1-8" class="headerlink" title="1.6.1 卸载 JDK1.8"></a>1.6.1 卸载 JDK1.8</h3><blockquote><p>Centos 默认安装了  JDK1.8 ,但 JDK1.8 的可执行文件和众多库文件分布在不同目录下。给hadoop环境变量设置造成很多不便。因此，建议卸载系统默认安装的 JDK1.8 </p></blockquote><ul><li>查看 jdk 版本信息</li><li>查看已安装 jdk 组件</li><li>卸载 jdk 及其组件</li></ul><h3 id="1-6-2-重新安装-JDK1-8"><a href="#1-6-2-重新安装-JDK1-8" class="headerlink" title="1.6.2 重新安装 JDK1.8"></a>1.6.2 重新安装 JDK1.8</h3><ul><li><p>解压 jdk1.8</p></li><li><p>安装 jdk1.8</p></li><li><p>修改配置文件</p><ul><li><p>配置环境变量</p></li><li><p>使环境变量生效</p></li></ul></li></ul><h1 id="2-准备cloudera安装包"><a href="#2-准备cloudera安装包" class="headerlink" title="2.准备cloudera安装包"></a>2.准备cloudera安装包</h1><h2 id="2-1-Cloudera-Manager-5"><a href="#2-1-Cloudera-Manager-5" class="headerlink" title="2.1 Cloudera Manager 5"></a>2.1 Cloudera Manager 5</h2><blockquote><p>文件名: cloudera-manager-centos7-cm5.14.0_x86_64.tar.gz</p><p>下载地址: <a href="https://archive.cloudera.com/cm5/cm/5/">https://archive.cloudera.com/cm5/cm/5/</a></p></blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">由于是离线部署，因此需要预先下载好需要的文件。</span><br><span class="line">需要准备的文件有:</span><br><span class="line"></span><br><span class="line">Cloudera Manager 5</span><br><span class="line">文件名: cloudera-manager-centos7-cm5.14.0_x86_64.tar.gz</span><br><span class="line">下载地址: https://archive.cloudera.com/cm5/cm/5/</span><br><span class="line">CDH安装包（Parecls包）</span><br><span class="line">版本号必须与Cloudera Manager相对应</span><br><span class="line">下载地址: https://archive.cloudera.com/cdh5/parcels/5.14.0/</span><br><span class="line">需要下载下面3个文件：</span><br><span class="line">CDH-5.14.0-1.cdh5.14.0.p0.23-el7.parcel</span><br><span class="line">CDH-5.14.0-1.cdh5.14.0.p0.23-el7.parcel.sha1</span><br><span class="line">manifest.json</span><br><span class="line">MySQL jdbc驱动</span><br><span class="line">文件名: mysql-connector-java-.tar.gz</span><br><span class="line">下载地址: https://dev.mysql.com/downloads/connector/j/</span><br><span class="line">解压出: mysql-connector-java-bin.jar</span><br></pre></td></tr></table></figure><h1 id="4-所有机器安装依赖包"><a href="#4-所有机器安装依赖包" class="headerlink" title="4.所有机器安装依赖包"></a>4.所有机器安装依赖包</h1><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum -y install chkconfig python bind-utils psmisc libxslt zlib sqlite cyrus-sasl-plain cyrus-sasl-gssapi fuse portmap fuse-libs redhat-lsb</span><br></pre></td></tr></table></figure><h1 id="5-安装mysql数据库"><a href="#5-安装mysql数据库" class="headerlink" title="5.安装mysql数据库"></a>5.安装mysql数据库</h1><p>在第二台机器上(随机选择的机器，计划在第一台机器上安装cloudera管理服务比较耗费资源,所以在第二台机器上安装mysql数据库)安装mysql数据库.</p><p>参考【MySQL安装之yum安装教程】</p><h1 id="6-安装cloudera服务端"><a href="#6-安装cloudera服务端" class="headerlink" title="6.安装cloudera服务端"></a>6.安装cloudera服务端</h1><h2 id="6-1-解压服务端管理安装包"><a href="#6-1-解压服务端管理安装包" class="headerlink" title="6.1 解压服务端管理安装包"></a>6.1 解压服务端管理安装包</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">所有节点上传cloudera-manager-centos7-cm5.14.0_x86_64.tar.gz文件并解压</span></span><br><span class="line">[root@node01 ~]# tar -zxvf cloudera-manager-centos7-cm5.14.2_x86_64.tar.gz -C /opt</span><br><span class="line">[root@node02 ~]# tar -zxvf cloudera-manager-centos7-cm5.14.2_x86_64.tar.gz -C /opt</span><br><span class="line">[root@node03 ~]# tar -zxvf cloudera-manager-centos7-cm5.14.2_x86_64.tar.gz -C /opt</span><br></pre></td></tr></table></figure><p>解压完可以在/opt目录下看到文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[root@node01 ~]# cd /opt/</span><br><span class="line">[root@node01 opt]# ll</span><br><span class="line">total 0</span><br><span class="line">drwxr-xr-x. 4 1106 4001 36 Apr  3  2018 cloudera</span><br><span class="line">drwxr-xr-x. 9 1106 4001 88 Apr  3  2018 cm-5.14.2</span><br><span class="line">[root@node01 opt]# cd cloudera/</span><br><span class="line">[root@node01 cloudera]# ll</span><br><span class="line">total 0</span><br><span class="line">drwxr-xr-x. 2 1106 4001 6 Apr  3  2018 csd</span><br><span class="line">drwxr-xr-x. 2 1106 4001 6 Apr  3  2018 parcel-repo</span><br><span class="line">[root@node01 cloudera]# </span><br></pre></td></tr></table></figure><h2 id="6-2-创建客户端运行目录"><a href="#6-2-创建客户端运行目录" class="headerlink" title="6.2 创建客户端运行目录"></a>6.2 创建客户端运行目录</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">所有节点手动创建文件夹</span></span><br><span class="line">[root@node01 ~]# mkdir /opt/cm-5.14.2/run/cloudera-scm-agent</span><br><span class="line">[root@node02 ~]# mkdir /opt/cm-5.14.2/run/cloudera-scm-agent</span><br><span class="line">[root@node03 ~]# mkdir /opt/cm-5.14.2/run/cloudera-scm-agent</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="6-3-创建cloudera-scm用户"><a href="#6-3-创建cloudera-scm用户" class="headerlink" title="6.3 创建cloudera-scm用户"></a>6.3 创建cloudera-scm用户</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">所有节点创建cloudera-scm用户</span></span><br><span class="line">useradd --system --home=/opt/cm-5.14.0/run/cloudera-scm-server --no-create-home --shell=/bin/false --comment &quot;Cloudera SCM User&quot; cloudera-scm</span><br></pre></td></tr></table></figure><h2 id="6-4-初始化数据库"><a href="#6-4-初始化数据库" class="headerlink" title="6.4 初始化数据库"></a>6.4 初始化数据库</h2><p>初始化数据库（只需要在Cloudera Manager Server节点执行）</p><p>将提供的msyql驱动包上传到第一台机器的root home目录下，然后将mysql jdbc驱动放入相应位置:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[root@node01 ~]# cp mysql-connector-java.jar /opt/cm-5.14.2/share/cmf/lib/</span><br><span class="line">[root@node01 ~]#  /opt/cm-5.14.2/share/cmf/schema/scm_prepare_database.sh mysql -h node02 -uroot -p&#x27;!Qaz123456&#x27; --scm-host node01 scm scm &#x27;!Qaz123456&#x27;    </span><br><span class="line">JAVA_HOME=/usr/java/jdk1.8.0_211-amd64</span><br><span class="line">Verifying that we can write to /opt/cm-5.14.2/etc/cloudera-scm-server</span><br><span class="line">Creating SCM configuration file in /opt/cm-5.14.2/etc/cloudera-scm-server</span><br><span class="line">Executing:  /usr/java/jdk1.8.0_211-amd64/bin/java -cp /usr/share/java/mysql-connector-java.jar:/usr/share/java/oracle-connector-java.jar:/opt/cm-5.14.2/share/cmf/schema/../lib/* com.cloudera.enterprise.dbutil.DbCommandExecutor /opt/cm-5.14.2/etc/cloudera-scm-server/db.properties com.cloudera.cmf.db.</span><br><span class="line">[                          main] DbCommandExecutor              INFO  Successfully connected to database.</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">显示初始化成功</span></span><br><span class="line">All done, your SCM database is configured correctly!</span><br><span class="line">[root@node01 ~]# </span><br></pre></td></tr></table></figure><p>脚本参数说明:<br>${数据库类型} -h ${数据库所在节点ip/hostname} -u${数据库用户名} -p${数据库密码} –scm-host ${Cloudera Manager Server节点ip/hostname} scm(数据库)  scm(用户名) scm(密码)</p><h2 id="6-5-修改所有节点客户端配置"><a href="#6-5-修改所有节点客户端配置" class="headerlink" title="6.5 修改所有节点客户端配置"></a>6.5 修改<strong>所有节点</strong>客户端配置</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">将其中的server_host参数修改为Cloudera Manager Server节点的主机名</span></span><br><span class="line">[root@node01 ~]# vi /opt/cm-5.14.2/etc/cloudera-scm-agent/config.ini</span><br><span class="line">[root@node01 ~]# vi /opt/cm-5.14.2/etc/cloudera-scm-agent/config.ini </span><br><span class="line">[General]</span><br><span class="line"><span class="meta">#</span><span class="bash"> 将默认的server_host=localhost 修改成node01</span></span><br><span class="line">server_host=node01</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="6-6-上传CDH安装包"><a href="#6-6-上传CDH安装包" class="headerlink" title="6.6 上传CDH安装包"></a>6.6 上传CDH安装包</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">将如下文件放到Server节点的/opt/cloudera/parcel-repo/目录中:</span></span><br><span class="line"><span class="meta">#</span><span class="bash">CDH-5.14.2-1.cdh5.14.2.p0.3-el7.parcel</span></span><br><span class="line"><span class="meta">#</span><span class="bash">CDH-5.14.2-1.cdh5.14.2.p0.3-el7.parcel.sha1</span></span><br><span class="line"><span class="meta">#</span><span class="bash">manifest.json</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 重命名sha1文件</span></span><br><span class="line">[root@node01 parcel-repo]# mv CDH-5.14.2-1.cdh5.14.2.p0.3-el7.parcel.sha1 CDH-5.14.2-1.cdh5.14.2.p0.3-el7.parcel.sha</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="6-7-更改安装目录用户组权限"><a href="#6-7-更改安装目录用户组权限" class="headerlink" title="6.7 更改安装目录用户组权限"></a>6.7 更改安装目录用户组权限</h2><p><strong>所有节点</strong>更改cm相关文件夹的用户及用户组</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@node01 ~]# chown -R cloudera-scm:cloudera-scm /opt/cloudera</span><br><span class="line">[root@node01 ~]# chown -R cloudera-scm:cloudera-scm /opt/cm-5.14.2</span><br><span class="line">[root@node01 ~]# </span><br></pre></td></tr></table></figure><h2 id="6-8-启动Cloudera-Manager和agent"><a href="#6-8-启动Cloudera-Manager和agent" class="headerlink" title="6.8 启动Cloudera Manager和agent"></a>6.8 启动Cloudera Manager和agent</h2><p>Server(node01)节点</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@node01 ~]# /opt/cm-5.14.2/etc/init.d/cloudera-scm-server start</span><br><span class="line">Starting cloudera-scm-server:                              [  OK  ]</span><br><span class="line">[root@node01 ~]# /opt/cm-5.14.2/etc/init.d/cloudera-scm-agent start </span><br><span class="line">Starting cloudera-scm-agent:                               [  OK  ]</span><br><span class="line">[root@node01 ~]# </span><br><span class="line"></span><br></pre></td></tr></table></figure><h1 id="7-服务安装"><a href="#7-服务安装" class="headerlink" title="7.服务安装"></a>7.服务安装</h1><p>使用浏览器登录cloudera-manager的web界面,用户名和密码都是admin</p><p><img src="assets/1570789521954.png" alt="1570789521954"></p><p>登陆之后，在协议页面勾选接受协议,点击继续</p><p><img src="assets/1570790224972.png" alt="1570790224972"></p><p>选择免费版本，免费版本已经能够满足我们日常业务需求,选择免费版即可.点击继续</p><p><img src="assets/1570790460382.png" alt="1570790460382"></p><p>如下图，点击继续</p><p><img src="assets/1570790639728.png" alt="1570790639728"></p><p>如下图，点击当前管理的机器，然后选择机器，点击继续</p><p><img src="assets/1570790871033.png" alt="1570790871033"></p><p>如下图，然后选择你的parcel对应版本的包</p><p><img src="assets/1570790984431.png" alt="1570790984431"></p><p>点击后，进入安装页面，稍等片刻</p><p>如下图，集群安装中 </p><p><img src="assets/1570791090600.png" alt="1570791090600"></p><p>如下图，安装包分配成功，点击继续</p><p><img src="assets/1570793156892.png" alt="1570793156892"></p><p><img src="assets/1570793245342.png" alt="1570793245342"></p><p>针对这样的警告，需要在每一台机器输入如下命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">echo never &gt; &#x2F;sys&#x2F;kernel&#x2F;mm&#x2F;transparent_hugepage&#x2F;defrag</span><br><span class="line">echo never &gt; &#x2F;sys&#x2F;kernel&#x2F;mm&#x2F;transparent_hugepage&#x2F;enabled</span><br><span class="line">echo &#39;vm.swappiness&#x3D;10&#39;&gt;&gt; &#x2F;etc&#x2F;sysctl.conf</span><br><span class="line">sysctl vm.swappiness&#x3D;10</span><br></pre></td></tr></table></figure><p>如下图，然后点击重新运行，不出以为，就不会在出现警告了，点击完成,进入hadoop生态圈服务组件的安装</p><p><img src="assets/1570793435494.png" alt="1570793435494"></p><p>如下图，选择自定义服务，我们先安装好最基础的服务组合。那么在安装之前，如果涉及到hive和oozie的安装，那么先去mysql中，自己创建数据库，并赋予权限；</p><p>因此：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">database</span> hive;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">database</span> oozie;</span><br><span class="line"></span><br><span class="line"><span class="keyword">grant</span> <span class="keyword">all</span> <span class="keyword">on</span> *.* <span class="keyword">to</span> hive <span class="keyword">identified</span> <span class="keyword">by</span> <span class="string">&#x27;!Qaz123456&#x27;</span>;</span><br><span class="line"><span class="keyword">grant</span> <span class="keyword">all</span> <span class="keyword">on</span> *.* <span class="keyword">to</span> oozie <span class="keyword">identified</span> <span class="keyword">by</span> <span class="string">&#x27;!Qaz123456&#x27;</span>;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>如果出现如下错误:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">mysql&gt;</span><span class="bash"> grant all on *.* to oozie identified by <span class="string">&#x27;!Qaz123456&#x27;</span>;</span></span><br><span class="line">ERROR 1045 (28000): Access denied for user &#x27;root&#x27;@&#x27;localhost&#x27; (using password: YES)</span><br><span class="line"><span class="meta">mysql&gt;</span><span class="bash"> update mysql.user <span class="built_in">set</span> Grant_priv=<span class="string">&#x27;Y&#x27;</span>,Super_priv=<span class="string">&#x27;Y&#x27;</span> <span class="built_in">where</span> user = <span class="string">&#x27;root&#x27;</span> and host = <span class="string">&#x27;localhost&#x27;</span>;</span></span><br><span class="line">Query OK, 1 row affected (0.00 sec)</span><br><span class="line">Rows matched: 1  Changed: 1  Warnings: 0</span><br><span class="line"></span><br><span class="line"><span class="meta">mysql&gt;</span><span class="bash"> flush privileges;</span></span><br><span class="line"><span class="meta">mysql&gt;</span><span class="bash"> quit</span></span><br><span class="line">Bye</span><br><span class="line">You have new mail in /var/spool/mail/root</span><br><span class="line">[root@node02 ~]# systemctl restart mysqld.service</span><br><span class="line">[root@node02 ~]# mysql -u root -p                </span><br><span class="line">Enter password: </span><br><span class="line">Welcome to the MySQL monitor.  Commands end with ; or \g.</span><br><span class="line">Your MySQL connection id is 3</span><br><span class="line">Server version: 5.7.27 MySQL Community Server (GPL)</span><br><span class="line"></span><br><span class="line">Copyright (c) 2000, 2019, Oracle and/or its affiliates. All rights reserved.</span><br><span class="line"></span><br><span class="line">Oracle is a registered trademark of Oracle Corporation and/or its</span><br><span class="line">affiliates. Other names may be trademarks of their respective</span><br><span class="line">owners.</span><br><span class="line"></span><br><span class="line">Type &#x27;help;&#x27; or &#x27;\h&#x27; for help. Type &#x27;\c&#x27; to clear the current input statement.</span><br><span class="line"></span><br><span class="line"><span class="meta">mysql&gt;</span><span class="bash"> grant all on *.* to hive identified by <span class="string">&#x27;!Qaz123456&#x27;</span>;</span></span><br><span class="line">Query OK, 0 rows affected, 1 warning (0.00 sec)</span><br><span class="line"></span><br><span class="line"><span class="meta">mysql&gt;</span><span class="bash"> grant all on *.* to oozie identified by <span class="string">&#x27;!Qaz123456&#x27;</span>;</span></span><br><span class="line">Query OK, 0 rows affected, 1 warning (0.00 sec)</span><br><span class="line"></span><br><span class="line"><span class="meta">mysql&gt;</span><span class="bash"> </span></span><br></pre></td></tr></table></figure><p>这样再安装软件！</p><p>那么，选择自定义服务,如果我们后续需要其他服务时我们在进行添加</p><p><img src="assets/1570795590845.png" alt="1570795590845"></p><p>然后点击继续，进入选择服务添加分配页面，分配即可</p><p><img src="assets/1570795837600.png" alt="1570795837600"></p><p>选择完成后服务，如下图,可以点击按照主机查看服务分部情况</p><p><img src="assets/1570795867075.png" alt="1570795867075"></p><p><img src="assets/1570795808683.png" alt="1570795808683"></p><p>点击继续后，如下图，输入mysql数据库中数数据库scm，用户名scm，密码!Qaz123456,点击测试连接，大概等30s，显示成功，点击继续</p><p><img src="assets/1570796013065.png" alt="1570796013065"></p><p>一路点击继续,剩下的就是等待</p><p><img src="assets/1570796339035.png" alt="1570796339035"></p><p>如上图，如果等待时间过长，我们可以将manager所在机器(也就是node01)停止后把内存调整的大一些建议如果是笔记本4g以上，如果是云环境8g以上，我们这里先调整为4g以上，重新启node01机器后重新启动cloudera的server和agent</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@node01 ~]# cd /opt/cm-5.14.2/etc/init.d</span><br><span class="line"><span class="meta">#</span><span class="bash">启动server</span></span><br><span class="line">[root@node01 init.d]# ./cloudera-scm-server start</span><br><span class="line"><span class="meta">#</span><span class="bash">启动agent</span></span><br><span class="line">[root@node01 init.d]# ./cloudera-scm-agent start</span><br></pre></td></tr></table></figure><h1 id="8-重新登录cloudera-manager"><a href="#8-重新登录cloudera-manager" class="headerlink" title="8.重新登录cloudera manager"></a>8.重新登录cloudera manager</h1><p>登录成功后，如下图，重新启动集群,接下来就是等待.</p><p><img src="assets/1570799640007.png" alt="1570799640007"></p><h1 id="9-集群测试"><a href="#9-集群测试" class="headerlink" title="9.集群测试"></a>9.集群测试</h1><h2 id="9-1-文件系统测试"><a href="#9-1-文件系统测试" class="headerlink" title="9.1 文件系统测试"></a>9.1 文件系统测试</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">切换hdfs用户对hdfs文件系统进行测试是否能够进行正常读写</span></span><br><span class="line">[root@node01 ~]# su hdfs</span><br><span class="line">[hdfs@node01 ~]# hadoop dfs -ls /</span><br><span class="line">DEPRECATED: Use of this script to execute hdfs command is deprecated.</span><br><span class="line">Instead use the hdfs command for it.</span><br><span class="line"></span><br><span class="line">Found 1 items</span><br><span class="line">d-wx------   - hdfs supergroup          0 2019-10-11 08:21 /tmp</span><br><span class="line">[hdfs@node01 ~]# touch test</span><br><span class="line">[hdfs@node01 ~]# vi test </span><br><span class="line">hello world</span><br><span class="line"></span><br><span class="line">[hdfs@node01 ~]$ hadoop dfs -put words /test</span><br><span class="line">[hdfs@node01 ~]$ hadoop dfs -ls /</span><br><span class="line">DEPRECATED: Use of this script to execute hdfs command is deprecated.</span><br><span class="line">Instead use the hdfs command for it.</span><br><span class="line"></span><br><span class="line">Found 2 items</span><br><span class="line">drwxr-xr-x   - hdfs supergroup          0 2019-10-11 09:09 /test</span><br><span class="line">d-wx------   - hdfs supergroup          0 2019-10-11 08:21 /tmp</span><br><span class="line">[hdfs@node01 ~]$ hadoop dfs -ls /test</span><br><span class="line">DEPRECATED: Use of this script to execute hdfs command is deprecated.</span><br><span class="line">Instead use the hdfs command for it.</span><br><span class="line"></span><br><span class="line">Found 1 items</span><br><span class="line">-rw-r--r--   3 hdfs supergroup         12 2019-10-11 09:09 /test/words</span><br><span class="line">[hdfs@node01 ~]$ hadoop dfs -text /test/words</span><br><span class="line">DEPRECATED: Use of this script to execute hdfs command is deprecated.</span><br><span class="line">Instead use the hdfs command for it.</span><br><span class="line"></span><br><span class="line">hello world</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="9-2-yarn集群测试"><a href="#9-2-yarn集群测试" class="headerlink" title="9.2 yarn集群测试"></a>9.2 yarn集群测试</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br></pre></td><td class="code"><pre><span class="line">[hdfs@node01 ~]$ hadoop jar /opt/cloudera/parcels/CDH-5.14.2-1.cdh5.14.2.p0.3/jars/hadoop-mapreduce-examples-2.6.0-cdh5.14.2.jar wordcount /test/words /test/output</span><br><span class="line">19/10/11 22:47:59 INFO client.RMProxy: Connecting to ResourceManager at node03.kaikeba.com/192.168.52.120:8032</span><br><span class="line">19/10/11 22:47:59 INFO mapreduce.JobSubmissionFiles: Permissions on staging directory /user/hdfs/.staging are incorrect: rwx---rwx. Fixing permissions to correct value rwx------</span><br><span class="line">19/10/11 22:48:00 INFO input.FileInputFormat: Total input paths to process : 1</span><br><span class="line">19/10/11 22:48:00 INFO mapreduce.JobSubmitter: number of splits:1</span><br><span class="line">19/10/11 22:48:00 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1570847238197_0001</span><br><span class="line">19/10/11 22:48:01 INFO impl.YarnClientImpl: Submitted application application_1570847238197_0001</span><br><span class="line">19/10/11 22:48:01 INFO mapreduce.Job: The url to track the job: http://node03.kaikeba.com:8088/proxy/application_1570847238197_0001/</span><br><span class="line">19/10/11 22:48:01 INFO mapreduce.Job: Running job: job_1570847238197_0001</span><br><span class="line">19/10/11 22:48:28 INFO mapreduce.Job: Job job_1570847238197_0001 running in uber mode : false</span><br><span class="line">19/10/11 22:48:28 INFO mapreduce.Job:  map 0% reduce 0%</span><br><span class="line">19/10/11 22:50:10 INFO mapreduce.Job:  map 100% reduce 0%</span><br><span class="line">19/10/11 22:50:17 INFO mapreduce.Job:  map 100% reduce 17%</span><br><span class="line">19/10/11 22:50:19 INFO mapreduce.Job:  map 100% reduce 33%</span><br><span class="line">19/10/11 22:50:21 INFO mapreduce.Job:  map 100% reduce 50%</span><br><span class="line">19/10/11 22:50:24 INFO mapreduce.Job:  map 100% reduce 67%</span><br><span class="line">19/10/11 22:50:25 INFO mapreduce.Job:  map 100% reduce 83%</span><br><span class="line">19/10/11 22:50:29 INFO mapreduce.Job:  map 100% reduce 100%</span><br><span class="line">19/10/11 22:50:29 INFO mapreduce.Job: Job job_1570847238197_0001 completed successfully</span><br><span class="line">19/10/11 22:50:30 INFO mapreduce.Job: Counters: 49</span><br><span class="line">        File System Counters</span><br><span class="line">                FILE: Number of bytes read=144</span><br><span class="line">                FILE: Number of bytes written=1044048</span><br><span class="line">                FILE: Number of read operations=0</span><br><span class="line">                FILE: Number of large read operations=0</span><br><span class="line">                FILE: Number of write operations=0</span><br><span class="line">                HDFS: Number of bytes read=118</span><br><span class="line">                HDFS: Number of bytes written=16</span><br><span class="line">                HDFS: Number of read operations=21</span><br><span class="line">                HDFS: Number of large read operations=0</span><br><span class="line">                HDFS: Number of write operations=12</span><br><span class="line">        Job Counters </span><br><span class="line">                Launched map tasks=1</span><br><span class="line">                Launched reduce tasks=6</span><br><span class="line">                Data-local map tasks=1</span><br><span class="line">                Total time spent by all maps in occupied slots (ms)=100007</span><br><span class="line">                Total time spent by all reduces in occupied slots (ms)=24269</span><br><span class="line">                Total time spent by all map tasks (ms)=100007</span><br><span class="line">                Total time spent by all reduce tasks (ms)=24269</span><br><span class="line">                Total vcore-milliseconds taken by all map tasks=100007</span><br><span class="line">                Total vcore-milliseconds taken by all reduce tasks=24269</span><br><span class="line">                Total megabyte-milliseconds taken by all map tasks=102407168</span><br><span class="line">                Total megabyte-milliseconds taken by all reduce tasks=24851456</span><br><span class="line">        Map-Reduce Framework</span><br><span class="line">                Map input records=1</span><br><span class="line">                Map output records=2</span><br><span class="line">                Map output bytes=20</span><br><span class="line">                Map output materialized bytes=120</span><br><span class="line">                Input split bytes=106</span><br><span class="line">                Combine input records=2</span><br><span class="line">                Combine output records=2</span><br><span class="line">                Reduce input groups=2</span><br><span class="line">                Reduce shuffle bytes=120</span><br><span class="line">                Reduce input records=2</span><br><span class="line">                Reduce output records=2</span><br><span class="line">                Spilled Records=4</span><br><span class="line">                Shuffled Maps =6</span><br><span class="line">                Failed Shuffles=0</span><br><span class="line">                Merged Map outputs=6</span><br><span class="line">                GC time elapsed (ms)=581</span><br><span class="line">                CPU time spent (ms)=11830</span><br><span class="line">                Physical memory (bytes) snapshot=1466945536</span><br><span class="line">                Virtual memory (bytes) snapshot=19622957056</span><br><span class="line">                Total committed heap usage (bytes)=1150287872</span><br><span class="line">        Shuffle Errors</span><br><span class="line">                BAD_ID=0</span><br><span class="line">                CONNECTION=0</span><br><span class="line">                IO_ERROR=0</span><br><span class="line">                WRONG_LENGTH=0</span><br><span class="line">                WRONG_MAP=0</span><br><span class="line">                WRONG_REDUCE=0</span><br><span class="line">        File Input Format Counters </span><br><span class="line">                Bytes Read=12</span><br><span class="line">        File Output Format Counters </span><br><span class="line">                Bytes Written=16</span><br><span class="line">You have new mail in /var/spool/mail/root</span><br><span class="line">[hdfs@node01 ~]$ hdfs dfs -ls /test/output</span><br><span class="line">Found 7 items</span><br><span class="line">-rw-r--r--   3 hdfs supergroup          0 2019-10-11 22:50 /test/output/_SUCCESS</span><br><span class="line">-rw-r--r--   3 hdfs supergroup          0 2019-10-11 22:50 /test/output/part-r-00000</span><br><span class="line">-rw-r--r--   3 hdfs supergroup          8 2019-10-11 22:50 /test/output/part-r-00001</span><br><span class="line">-rw-r--r--   3 hdfs supergroup          0 2019-10-11 22:50 /test/output/part-r-00002</span><br><span class="line">-rw-r--r--   3 hdfs supergroup          0 2019-10-11 22:50 /test/output/part-r-00003</span><br><span class="line">-rw-r--r--   3 hdfs supergroup          0 2019-10-11 22:50 /test/output/part-r-00004</span><br><span class="line">-rw-r--r--   3 hdfs supergroup          8 2019-10-11 22:50 /test/output/part-r-00005</span><br><span class="line">[hdfs@node01 ~]$  hdfs dfs -text /test/output/part-r-00001</span><br><span class="line">world   1</span><br><span class="line">[hdfs@node01 ~]$  hdfs dfs -text /test/output/part-r-00005</span><br><span class="line">hello   1</span><br><span class="line">You have new mail in /var/spool/mail/root</span><br><span class="line">[hdfs@node01 ~]$ </span><br></pre></td></tr></table></figure><h1 id="10-手动添加Kafka服务"><a href="#10-手动添加Kafka服务" class="headerlink" title="10.手动添加Kafka服务"></a>10.手动添加Kafka服务</h1><p>我们以安装kafka为例进行演示</p><h2 id="10-1-检查kafka安装包"><a href="#10-1-检查kafka安装包" class="headerlink" title="10.1 检查kafka安装包"></a>10.1 检查kafka安装包</h2><p>首先检查是否已经存在Kafka的parcel安装包，如下图提示远程提供，说明我们下载的parcel安装包中不包含Kafka的parcel安装包，这时需要我们手动到官网上下载</p><p><img src="assets/1570850847758.png" alt="1570850847758"></p><h2 id="10-2-检查Kafka安装包版本"><a href="#10-2-检查Kafka安装包版本" class="headerlink" title="10.2 检查Kafka安装包版本"></a>10.2 检查Kafka安装包版本</h2><p>首先查看搭建cdh版本 和kafka版本，是否是支持的：</p><p>登录如下网址：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">https:&#x2F;&#x2F;www.cloudera.com&#x2F;documentation&#x2F;enterprise&#x2F;release-notes&#x2F;topics&#x2F;rn_consolidated_pcm.html#pcm_kafka</span><br></pre></td></tr></table></figure><p>我的CDH版本是cdh5.14.0 ，我想要的kafka版本是1.0.1</p><p>因此选择：</p><p><img src="assets/1633376-20190508130456986-1658024926.png" alt="img"></p><h2 id="10-3-下载Kafka-parcel安装包"><a href="#10-3-下载Kafka-parcel安装包" class="headerlink" title="10.3 下载Kafka parcel安装包"></a>10.3 下载Kafka parcel安装包</h2><p>然后下载：<a href="http://archive.cloudera.com/kafka/parcels/3.1.0/">http://archive.cloudera.com/kafka/parcels/3.1.0/</a></p><p><img src="assets/1633376-20190508130549214-56463812.png" alt="img"></p><p>需要将下载的KAFKA-3.1.0-1.3.1.0.p0.35-el7.parcel.sha1 改成 KAFKA-3.1.0-1.3.1.0.p0.35-el7.parcel.sha</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@node01 ~]# mv KAFKA-3.1.0-1.3.1.0.p0.35-el7.parcel.sha1 KAFKA-3.1.0-1.3.1.0.p0.35-el7.parcel.sha</span><br><span class="line">You have new mail in /var/spool/mail/root</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>然后将这三个文件，拷贝到parcel-repo目录下。如果有相同的文件，即manifest.json，只需将之前的重命名备份即可。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[root@node01 ~] cd /opt/cloudera/parcel-repo/</span><br><span class="line">[root@node01 parcel-repo]# mv manifest.json bak_manifest.json </span><br><span class="line"><span class="meta">#</span><span class="bash">拷贝到parcel-repo目录下</span></span><br><span class="line">[root@node01 ~]# mv KAFKA-3.1.0-1.3.1.0.p0.35-el7.parcel* manifest.json /opt/cloudera/parcel-repo/</span><br><span class="line">[root@node01 ~]# ll</span><br><span class="line">total 989036</span><br><span class="line">-rw-------. 1 root root      1260 Apr 16 01:35 anaconda-ks.cfg</span><br><span class="line">-rw-r--r--. 1 root root 832469335 Oct 11 13:23 cloudera-manager-centos7-cm5.14.2_x86_64.tar.gz</span><br><span class="line">-rw-r--r--. 1 root root 179439263 Oct 10 20:14 jdk-8u211-linux-x64.rpm</span><br><span class="line">-rw-r--r--. 1 root root    848399 Oct 11 17:02 mysql-connector-java.jar</span><br><span class="line">-rw-r--r--  1 root root        12 Oct 11 21:01 words</span><br><span class="line">You have new mail in /var/spool/mail/root</span><br><span class="line">[root@node01 ~]# ll</span><br></pre></td></tr></table></figure><h2 id="10-4-分配激活Kafka"><a href="#10-4-分配激活Kafka" class="headerlink" title="10.4 分配激活Kafka"></a>10.4 分配激活Kafka</h2><p>如下图，在管理首页选择parcel</p><p><img src="assets/1570859081770.png" alt="1570859081770"></p><p>如下图，检查更新多点击几次，就会出现分配按钮</p><p><img src="assets/1570859361922.png" alt="1570859361922"></p><p>点击分配，等待分配按钮激活</p><p><img src="assets/1570859491074.png" alt="1570859491074"></p><p>如下图，正在分配中…</p><p><img src="assets/1570859524848.png" alt="1570859524848"></p><p>如下图按钮已经激活</p><p><img src="assets/1570859672818.png" alt="1570859672818"></p><p><img src="assets/1570859816399.png" alt="1570859816399"></p><p>如上两张图图，点击激活和确定，然后等待激活    </p><p>正在激活…</p><p><img src="assets/1570859868869.png" alt="1570859868869"></p><p>如下图，分配并激活成功</p><p><img src="assets/1570859891912.png" alt="1570859891912"></p><h2 id="10-5-添加Kafka服务"><a href="#10-5-添加Kafka服务" class="headerlink" title="10.5 添加Kafka服务"></a>10.5 添加Kafka服务</h2><p>点击cloudera manager回到主页</p><p><img src="assets/1570860017451.png" alt="1570860017451"></p><p>页面中点击下拉操作按钮，点击添加服务</p><p><img src="assets/1570849216563.png" alt="1570849216563"></p><p>如下图，点击选择kafka，点击继续</p><p><img src="assets/1570849280747.png" alt="1570849280747"></p><p>如下图，选择Kakka Broker在三个节点上安装，Kafka MirrorMaker安装在node03上，Gateway安装在node02上（服务选择安装，需要自己根据每台机器上健康状态而定,这里只是作为参考）</p><p><img src="assets/1570849433668.png" alt="1570849433668"></p><p>如下图，填写Destination Broker List和Source Broker List后点击继续</p><p><strong>注意:这里和上一步中选择的角色分配有关联,Kafka Broker选择的是三台机器Destination Broker List中就填写三台机器的主机名，中间使用逗号分开，如果选择的是一台机器那么久选择一台，一次类推.Source Broker List和Destination Broker List填写一样.</strong></p><p><img src="assets/1570861737283.png" alt="1570861737283"></p><p>如下图，添加服务，最终状态为已完成，启动过程中会出现错误不用管，这时因为CDH给默认将kafka的内存设置为50M,太小了， 后续需要我们手动调整,点击继续</p><p><img src="assets/1570862070794.png" alt="1570862070794"></p><p>如下图,点击完成.</p><p><img src="assets/1570862239690.png" alt="1570862239690"></p><p>如下图，添加成功的Kafka服务</p><p><img src="assets/1570862272925.png" alt="1570862272925"></p><h2 id="10-6-配置Kafka的内存"><a href="#10-6-配置Kafka的内存" class="headerlink" title="10.6 配置Kafka的内存"></a>10.6 配置Kafka的内存</h2><p>如下图，点击Kafka服务</p><p><img src="assets/1570862705339.png" alt="1570862705339"></p><p>如下图，点击实例，点击Kafka Broker（<strong>我们先配置node01节点的内存大小,node02和node03内存配置方式相同，需要按照此方式进行修改</strong>）</p><p><img src="assets/1570862765551.png" alt="1570862765551"></p><p>如上图，点击Kafka Broker之后，如下图所示，点击配置</p><p><img src="assets/1570862896070.png" alt="1570862896070"></p><p>右侧浏览器垂直滚动条往下找到broker_max_heap_size，修改值为256,点击保存更改</p><p><img src="assets/1570863035798.png" alt="1570863035798"></p><p><strong>node02和node03按照上述步骤进行同样修改.</strong></p><h2 id="10-7-重新启动kafka集群"><a href="#10-7-重新启动kafka集群" class="headerlink" title="10.7 重新启动kafka集群"></a>10.7 重新启动kafka集群</h2><p>点击启动</p><p><img src="assets/1570863234060.png" alt="1570863234060"></p><p>点击启动</p><p><img src="assets/1570863253382.png" alt="1570863253382"></p><p>启动成功</p><p><img src="assets/1570863458643.png" alt="1570863458643"></p><h1 id="11-手动添加服务"><a href="#11-手动添加服务" class="headerlink" title="11.手动添加服务"></a>11.手动添加服务</h1><p>请参考【10.手动添加Kafka服务】操作步骤.</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Cloudera </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="2020/04/22/Flink-%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C/Kafka%20%E6%B6%88%E6%81%AF%E4%B8%8D%E4%B8%A2%E5%A4%B1/"/>
      <url>2020/04/22/Flink-%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C/Kafka%20%E6%B6%88%E6%81%AF%E4%B8%8D%E4%B8%A2%E5%A4%B1/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p><strong>生产者，Broker，消费者都是有可能丢数据的</strong></p><h3 id="生产端"><a href="#生产端" class="headerlink" title="生产端"></a><strong>生产端</strong></h3><p>生产者丢数据，即发送的数据根本没有保存到 Broker 端。出现这个情况的原因可能是，网络抖动，导致消息压根就没有发送到 Broker 端；也可能是消息本身不合格导致 Broker 拒绝接收（比如消息太大了，超过了 Broker 的承受能力）等等。</p><p>上面所说比如网络原因导致消息没有成功发送到 broker 端，常见，也并不可怕。可怕的不是没发送成功，而是发送失败了你不做任何处理。</p><p>很简单的一个<strong>重试配置</strong>，基本就可以解决这种网络瞬时抖动问题。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">props.put(<span class="string">&quot;retries&quot;</span>, <span class="number">10</span>);</span><br></pre></td></tr></table></figure><p>当然还有很多其他原因导致的，不能只依靠 kafka 的配置来做处理，我们看一下 kafka 发送端的源码，其实人家是提供了两个方法的，通常会出问题的方法是那个简单的 send，没有 callback（回调）。简单的 send发送后不会去管它的结果是否成功，而 callback 能准确地告诉你消息是否真的提交成功了。一旦出现消息提交失败的情况，你就可以有针对性地进行处理。</p><p><font color='red'><strong>因此，一定要使用带有回调通知的 send 方法。</strong></font></p><p>我们知道，broker 一般不会有一个，我们就是要通过多 Broker 达到高可用的效果，所以对于生产者程序来说，也不能简单的认为发送到一台就算成功，如果只满足于一台，那台机器如果损坏了，那消息必然会丢失。设置 <strong>acks = all</strong>，表明所有副本 Broker 都要接收到消息，该消息才算是“已提交”，这样可以达到高可用的效果。</p><h3 id="Broker-端"><a href="#Broker-端" class="headerlink" title="Broker 端"></a><strong>Broker 端</strong></h3><p>数据已经保存在 broker 端，但是数据却丢失了。出现这个的原因可能是，Broker 机器 down 了，当然broker 是高可用的，假如你的消息保存在 N 个 Kafka Broker 上，那么至少有 1 个存活就不会丢。</p><h4 id="解决"><a href="#解决" class="headerlink" title="解决"></a>解决</h4><ol><li><h5 id="消息冗余"><a href="#消息冗余" class="headerlink" title="消息冗余"></a><font color='blue'>消息冗余</font></h5><p>前面我们说到，kafka 是有限度的保证消息不丢失，这里的限度，指至少要有一台 broker 可以正常提供服务。至少一台，这种说法可并不准确，应该说至少一台存储了你消息的的 broker。我们知道分区可以设置副本数，假如你只设置副本为1，只要挂的刚好是你副本的那台，即使你有1000台broker，也无济于事。</p><p>因此，副本的设置尤为重要，一般设置 <strong><code>replication.factor &gt;= 3</code>**，毕竟目前</strong>防止消息丢失的主要机制就是冗余**。</p><p>但仅仅设置副本数就有用吗？并不能保证 broker 端一定存储了三个副本呀。假如共有三个broker，发送一条消息的时候，某个 broker 刚好宕机了，即使你配置了<code>replication.factor = 3</code>，也最多只会有2台副本。因此，我们还要确认，至少要被写入到多少个副本才算是“已提交”。</p><p><strong><code>min.insync.replicas &gt; 1</code></strong> <strong>,</strong> 控制的是消息至少要被写入到多少个副本才算是“已提交”。设置成大于 1 可以提升消息持久性。</p><p>说到这，可能会有疑问，上面生产端不是已经配置 <code>acks=all</code>了，和这个参数不是冲突了吗？？注意 <code>acks = all</code> 是针对所有副本 Broker 都要接收到消息，假如 ISR中只有1个副本了，<code>acks=all</code> 也就相当于 <code>acks=1</code> 了，引入 <code>min.insync.replicas</code> 的目的就是为了做一个下限的限制，不能只满足于 <code>ISR</code> 全部写入，还要保证ISR 中的写入个数不少于 <code>min.insync.replicas</code>。</p><p>对了，请确保 <strong>replication.factor &gt; min.insync.replicas</strong>。一般设置为<strong>replication.factor = min.insync.replicas + 1</strong>。如果两者相等，有一个副本挂机，整个分区就无法正常工作了。我们不仅要考虑消息的可靠性，防止消息丢失，更应该考虑可用性问题。</p></li><li><h5 id="leader-选举"><a href="#leader-选举" class="headerlink" title="leader 选举"></a><strong><font color='blue'>leader 选举</font></strong></h5><p>我们知道kafka中有领导者副本（Leader Replica）和追随者副本（Follower Replica），而follower replica存在的唯一目的就是防止消息丢失，并不参与具体的业务逻辑的交互。只有leader 才参与服务，follower的作用就是充当leader的候补，平时的操作也只有信息同步。ISR也就是这组与leader保持同步的replica集合，我们要保证不丢消息，首先要保证ISR的存活（至少有一个备份存活），那存活的概念是什么呢，不仅需要机器正常，还需要跟上leader的消息进度，当达到一定程度的时候就会认为“非存活”状态。</p><p>假设这么一种场景，有Leader,Follow1,Follow2；其中Follow2落后于Leader太多，因此不在leader副本和follower1副本所在的ISR集合之中。此时Leader,Follow1都宕机了，只剩下Follow2了，Follow2还在，就会进行新的选举，不过在选举之前首先要判断<strong>unclean.leader.election.enable</strong>参数的值。如果<strong>unclean.leader.election.enable参数的值为false，那么就意味着非ISR中的副本不能够参与选举</strong>，此时无法进行新的选举，此时整个分区处于不可用状态。如果unclean.leader.election.enable参数的值为true，那么可以从非ISR集合中选举follower副本称为新的leader。如果让非ISR中的Follow2成为Leader会有什么后果呢？</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/W801CaTfHvWaFrR74ZUdpjXa1bibaWGJVvpGEwX6kTcN6iciaB3UxYQKpMIpjBN3zyt68JGdxvapgicBSUevcbkvQg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="img"></p><p>我们说Follow2已经落后之前的Leader很多，他成为新的Leader后从客户端继续收取消息，此时，原来的leader副本恢复，成为了新的follower副本，准备向新的leader副本同步消息，但是它发现自身的LEO（LEO是Log End Offset的缩写，它表示了当前日志文件中下一条待写入消息的offset）比leader副本的LEO还要大。Kafka中有一个准则，follower副本的LEO是不能够大于leader副本的，所以新的follower副本就需要截断日志至leader副本的LEO处，截断日志，不就丢失了之前的消息吗？即图中所示，丢失了3和4两条数据，并且新的Follow和新Leader之间的消息也不一致。</p></li></ol><p>   因此，如果要保证消息不丢失，需设置：</p><p>   <strong>unclean.leader.election.enable=false</strong>，但是Kafka的可用性就会降低，具体怎么选择需要读者根据实际的业务逻辑进行权衡，可靠性优先还是可用性优先。从Kafka 0.11.0.0版本开始将此参数从true设置为false，可以看出Kafka的设计者偏向于可靠性。</p><h3 id="消费端丢数据"><a href="#消费端丢数据" class="headerlink" title="消费端丢数据"></a><strong>消费端丢数据</strong></h3><p>Consumer 程序有个“位移”的概念，表示的是这个 Consumer 当前消费到的 Topic 分区的位置。Kafka默认是自动提交位移的，这样可能会有个问题，假如你在pull(拉取)30条数据，处理到第20条时自动提交了offset，但是在处理21条的时候出现了异常，当你再次pull数据时，由于之前是自动提交的offset，所以是从30条之后开始拉取数据，这也就意味着21-30条的数据发生了丢失。</p><p>消费端保证不丢数据，最重要就是保证offset的准确性。我们能做的，就是确保消息消费完成再提交。Consumer 端有个参数 ，设置 <strong>enable.auto.commit=</strong> <strong>false</strong>， 并且采用手动提交位移的方式。如果在处理数据时发生了异常，那就把当前处理失败的offset进行提交(放在finally代码块中)注意一定要确保offset的正确性，当下次再次消费的时候就可以从提交的offset处进行再次消费。consumer在处理数据的时候失败了，其实可以把这条数据给缓存起来，可以是redis、DB、file等，也可以把这条消息存入专门用于存储失败消息的topic中，让其它的consumer专门处理失败的消息。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="2020/04/04/Flink-%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C/Kafka%20%E6%B6%88%E6%81%AF%E6%9C%89%E5%BA%8F/"/>
      <url>2020/04/04/Flink-%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C/Kafka%20%E6%B6%88%E6%81%AF%E6%9C%89%E5%BA%8F/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>使用 Flink ProcessFunction 处理宕机告警</title>
      <link href="2020/01/28/%E4%BD%BF%E7%94%A8-Flink-ProcessFunction-%E5%A4%84%E7%90%86%E5%AE%95%E6%9C%BA%E5%91%8A%E8%AD%A6/"/>
      <url>2020/01/28/%E4%BD%BF%E7%94%A8-Flink-ProcessFunction-%E5%A4%84%E7%90%86%E5%AE%95%E6%9C%BA%E5%91%8A%E8%AD%A6/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p>Flink 的底层 API 就是 ProcessFunction，它是一个低阶的流处理操作，它可以访问流处理程序的基础构建模块：Event、State、Timer。ProcessFunction 可以被认为是一种提供了对 KeyedState 和定时器访问的 FlatMapFunction。每当数据源中接收到一个事件，就会调用来此函数来处理。对于容错的状态，ProcessFunction 可以通过 RuntimeContext 访问 KeyedState。</p><a id="more"></a><h3 id="ProcessFunction-介绍"><a href="#ProcessFunction-介绍" class="headerlink" title="ProcessFunction 介绍"></a>ProcessFunction 介绍</h3><p>Flink 的底层 API 就是 ProcessFunction，它是一个低阶的流处理操作，它可以访问流处理程序的基础构建模块：Event、State、Timer。ProcessFunction 可以被认为是一种提供了对 KeyedState 和定时器访问的 FlatMapFunction。每当数据源中接收到一个事件，就会调用来此函数来处理。对于容错的状态，ProcessFunction 可以通过 RuntimeContext 访问 KeyedState。</p><p>定时器可以对处理时间和事件时间的变化做一些处理。每次调用 processElement() 都可以获得一个 Context 对象，通过该对象可以访问元素的事件时间戳以及 TimerService。TimerService 可以为尚未发生的事件时间/处理时间实例注册回调。当定时器到达某个时刻时，会调用 onTimer() 方法。在调用期间，所有状态再次限定为定时器创建的 key，允许定时器操作 KeyedState。如果要访问 KeyedState 和定时器，那必须在 KeyedStream 上使用 KeyedProcessFunction，比如在 keyBy 算子之后使用：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">dataStream.keyBy(...).process(<span class="keyword">new</span> KeyedProcessFunction&lt;&gt;()&#123;</span><br><span class="line"></span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure><p>KeyedProcessFunction 是 ProcessFunction 函数的一个扩展，它可以在 onTimer 和 processElement 方法中获取到分区的 Key 值，这对于数据传递是很有帮助的，因为经常有这样的需求，经过 keyBy 算子之后可能还需要这个 key 字段，那么在这里直接构建成一个新的对象（新增一个 key 字段），然后下游的算子直接使用这个新对象中的 key 就好了，而不在需要重复的拼一个唯一的 key。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">processElement</span><span class="params">(String value, Context ctx, Collector&lt;String&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    System.out.println(ctx.getCurrentKey());</span><br><span class="line">    out.collect(value);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onTimer</span><span class="params">(<span class="keyword">long</span> timestamp, OnTimerContext ctx, Collector&lt;String&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    System.out.println(ctx.getCurrentKey());</span><br><span class="line">    <span class="keyword">super</span>.onTimer(timestamp, ctx, out);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="CoProcessFunction-介绍"><a href="#CoProcessFunction-介绍" class="headerlink" title="CoProcessFunction 介绍"></a>CoProcessFunction 介绍</h3><p>如果要在两个输入流上进行操作，可以使用 CoProcessFunction，这个函数可以传入两个不同的数据流输入，并为来自两个不同数据源的事件分别调用 processElement1() 和 processElement2() 方法。可以按照下面的步骤来实现一个典型的 Join 操作：</p><ul><li>为一个数据源的数据建立一个状态对象</li><li>从数据源处有新数据流过来的时候更新这个状态对象</li><li>在另一个数据源接收到元素时，关联状态对象并对其产生出连接的结果</li></ul><p>比如，将监控的 metric 数据和告警规则数据进行一个连接，在流数据的状态中存储了告警规则数据，当有监控数据过来时，根据监控数据的 metric 名称和一些 tag 去找对应告警规则计算表达式，然后通过规则的表达式对数据进行加工处理，判断是否要告警，如果是要告警则会关联构造成一个新的对象，新对象中不仅有初始的监控 metric 数据，还有含有对应的告警规则数据以及通知策略数据，组装成这样一条数据后，下游就可以根据这个数据进行通知，通知还会在状态中存储这个告警状态，表示它在什么时间告过警了，下次有新数据过来的时候，判断新数据是否是恢复的，如果属于恢复则把该状态清除。</p><h3 id="Timer-介绍"><a href="#Timer-介绍" class="headerlink" title="Timer 介绍"></a>Timer 介绍</h3><p>Timer 提供了一种定时触发器的功能，通过 TimerService 接口注册 timer。TimerService 在内部维护两种类型的定时器（处理时间和事件时间定时器）并排队执行。处理时间定时器的触发依赖于 ProcessingTimeService，它负责管理所有基于处理时间的触发器，内部使用 ScheduledThreadPoolExecutor 调度定时任务；事件时间定时器的触发依赖于系统当前的 Watermark。需要注意的一点就是：<strong>Timer 只能在 KeyedStream 中使用</strong>。</p><p>TimerService 会删除每个 Key 和时间戳重复的定时器，即每个 Key 在同一个时间戳上最多有一个定时器。如果为同一时间戳注册了多个定时器，则只会调用一次 onTimer（） 方法。Flink 会同步调用 onTimer() 和 processElement() 方法，因此不必担心状态的并发修改问题。TimerService 不仅提供了注册和删除 Timer 的功能，还可以通过它来获取当前的系统时间和 Watermark 的值。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-14-160008.png" alt="img"></p><h4 id="容错"><a href="#容错" class="headerlink" title="容错"></a>容错</h4><p>定时器具有容错能力，并且会与应用程序的状态一起进行 Checkpoint，如果发生故障重启会从 Checkpoint／Savepoint 中恢复定时器的状态。如果有处理时间定时器原本是要在恢复起来的那个时间之前触发的，那么在恢复的那一刻会立即触发该定时器。定时器始终是异步的进行 Checkpoint（除 RocksDB 状态后端存储、增量的 Checkpoint、基于堆的定时器外）。因为定时器实际上也是一种特殊状态的状态，在 Checkpoint 时会写入快照中，所以如果有大量的定时器，则无非会增加一次 Checkpoint 所需要的时间，必要的话得根据实际情况合并定时器。</p><h4 id="合并定时器"><a href="#合并定时器" class="headerlink" title="合并定时器"></a>合并定时器</h4><p>由于 Flink 仅为每个 Key 和时间戳维护一个定时器，因此可以通过降低定时器的频率来进行合并以减少定时器的数量。对于频率为 1 秒的定时器（基于事件时间或处理时间），可以将目标时间向下舍入为整秒数，则定时器最多提前 1 秒触发，但不会迟于我们的要求，精确到毫秒。因此，每个键每秒最多有一个定时器。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">long</span> coalescedTime = ((ctx.timestamp() + timeout) / <span class="number">1000</span>) * <span class="number">1000</span>;</span><br><span class="line">ctx.timerService().registerProcessingTimeTimer(coalescedTime);</span><br></pre></td></tr></table></figure><p>由于事件时间计时器仅在 Watermark 到达时才触发，因此可以将当前 Watermark 与下一个 Watermark 的定时器一起调度和合并：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">long</span> coalescedTime = ctx.timerService().currentWatermark() + <span class="number">1</span>;</span><br><span class="line">ctx.timerService().registerEventTimeTimer(coalescedTime);</span><br></pre></td></tr></table></figure><p>定时器也可以类似下面这样移除：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//删除处理时间定时器</span></span><br><span class="line"><span class="keyword">long</span> timestampOfTimerToStop = ...</span><br><span class="line">ctx.timerService().deleteProcessingTimeTimer(timestampOfTimerToStop);</span><br><span class="line"></span><br><span class="line"><span class="comment">//删除事件时间定时器</span></span><br><span class="line"><span class="keyword">long</span> timestampOfTimerToStop = ...</span><br><span class="line">ctx.timerService().deleteEventTimeTimer(timestampOfTimerToStop);</span><br></pre></td></tr></table></figure><p>如果没有该时间戳的定时器，则删除定时器无效。</p><h3 id="如果利用-ProcessFunction-处理宕机告警？"><a href="#如果利用-ProcessFunction-处理宕机告警？" class="headerlink" title="如果利用 ProcessFunction 处理宕机告警？"></a>如果利用 ProcessFunction 处理宕机告警？</h3><p>前面介绍了 ProcessFunction 和 Timer，那么这里讲下笔者公司生产环境的一个案例 —— 利用 ProcessFunction 处理宕机告警？</p><h4 id="宕机告警需求分析"><a href="#宕机告警需求分析" class="headerlink" title="宕机告警需求分析"></a>宕机告警需求分析</h4><p>首先大家应该知道生产环境的服务器一般都是有部署各种各种的服务或者中间件的，那么如果一台机器突然发生了一些突发情况，比如断电、自然灾害、人为因素、服务把机器跑宕机等，那么机器一宕机，原先跑在该机器的服务都会掉线，导致服务出现短暂不可用（可能应用会调度到其他机器）或者直接不可用（没有调度策略并且是运行的单实例），这对于生产环境来说，就麻烦比较大，可能会出现很大的损失，所以这种紧急情况就特别需要实时性非常高的告警。</p><p>在面对这个需求时首先得想一想怎么去判定一台机器是否处于宕机，因为会在机器上部署采集机器信息的 Agent，如果机器是正常的，每隔一定时间（假设时间间隔为 10 秒） Agent 会将数据进行上传，所有的监控数据上传至消息队列后，接下来就需要对这些监控数据处理。那么当机器处于宕机的状态，则运行在机器的 Agent 就已经停止工作了，则它就不会继续上传监控信息来了，所以这里就可以根据判定是否有这台机器的监控数据上来，如果持续有，那么说明机器在线，如果持续一段时间没有收到该机器的数据，则意味着该机器宕机了，那么可能就有人想问了，这个持续时间设置多少合适呢？这个得根据实际情况去做大量的测试和调优了，如果设置的过短，假设数据在消息队列中堆积了一会，那么也会出现误判的宕机告警；如果设置的过长，那么可能机器中途宕机过然后重启了，但是时间还是在设置的预定时间之内，这种情况就出现了宕机告警漏报，也是不允许的（告警延迟性增大并且可能告警漏报），所以就得根据实际情况两者之间做一个权衡。</p><p>在分析完需求后，接下来就得看如何去实现这种需求，怎么去判断机器是否一直有数据上来？那么这里就利用了 Timer 机制。</p><h4 id="宕机告警实现"><a href="#宕机告警实现" class="headerlink" title="宕机告警实现"></a>宕机告警实现</h4><p>机器监控数据有很多的指标，这里列几种比较常见的比如 Mem、CPU、Load、Swap 等，那么这几种数据采集上来的结构都是 MetricEvent 类型。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MetricEvent</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//指标名</span></span><br><span class="line">    <span class="keyword">private</span> String name;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//数据时间</span></span><br><span class="line">    <span class="keyword">private</span> Long timestamp;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//指标具体字段</span></span><br><span class="line">    <span class="keyword">private</span> Map&lt;String, Object&gt; fields;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//指标的标识</span></span><br><span class="line">    <span class="keyword">private</span> Map&lt;String, String&gt; tags;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>就拿 CPU 来举个例子，它发上来的数据是下面这种的：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="attr">&quot;name&quot;</span>: <span class="string">&quot;cpu&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;timestamp&quot;</span>: <span class="number">1571108814142</span>,</span><br><span class="line">    <span class="attr">&quot;fields&quot;</span>: &#123;</span><br><span class="line">        <span class="attr">&quot;usedPercent&quot;</span>: <span class="number">93.896484375</span>,</span><br><span class="line">        <span class="attr">&quot;max&quot;</span>: <span class="number">2048</span>,</span><br><span class="line">        <span class="attr">&quot;used&quot;</span>: <span class="number">1923</span></span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="attr">&quot;tags&quot;</span>: &#123;</span><br><span class="line">        <span class="attr">&quot;cluster_name&quot;</span>: <span class="string">&quot;zhisheng&quot;</span>,</span><br><span class="line">        <span class="attr">&quot;host_ip&quot;</span>: <span class="string">&quot;121.12.17.11&quot;</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这里笔者写了个模拟 Mem、CPU、Load、Swap 监控数据的工具类：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">BuildMachineMetricDataUtil</span> </span>&#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> String BROKER_LIST = <span class="string">&quot;localhost:9092&quot;</span>;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> String METRICS_TOPIC = <span class="string">&quot;zhisheng_metrics&quot;</span>;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> Random random = <span class="keyword">new</span> Random();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> List&lt;String&gt; hostIps = Arrays.asList(<span class="string">&quot;121.12.17.10&quot;</span>, <span class="string">&quot;121.12.17.11&quot;</span>, <span class="string">&quot;121.12.17.12&quot;</span>, <span class="string">&quot;121.12.17.13&quot;</span>);</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">writeDataToKafka</span><span class="params">()</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">        Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">        props.put(<span class="string">&quot;bootstrap.servers&quot;</span>, BROKER_LIST);</span><br><span class="line">        props.put(<span class="string">&quot;key.serializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span>);</span><br><span class="line">        props.put(<span class="string">&quot;value.serializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span>);</span><br><span class="line">        KafkaProducer producer = <span class="keyword">new</span> KafkaProducer&lt;String, String&gt;(props);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">            <span class="keyword">long</span> timestamp = System.currentTimeMillis();</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; hostIps.size(); i++) &#123;</span><br><span class="line">                MetricEvent cpuData = buildCpuData(hostIps.get(i), timestamp);</span><br><span class="line">                MetricEvent loadData = buildLoadData(hostIps.get(i), timestamp);</span><br><span class="line">                MetricEvent memData = buildMemData(hostIps.get(i), timestamp);</span><br><span class="line">                MetricEvent swapData = buildSwapData(hostIps.get(i), timestamp);</span><br><span class="line">                ProducerRecord cpuRecord = <span class="keyword">new</span> ProducerRecord&lt;String, String&gt;(METRICS_TOPIC, <span class="keyword">null</span>, <span class="keyword">null</span>, GsonUtil.toJson(cpuData));</span><br><span class="line">                ProducerRecord loadRecord = <span class="keyword">new</span> ProducerRecord&lt;String, String&gt;(METRICS_TOPIC, <span class="keyword">null</span>, <span class="keyword">null</span>, GsonUtil.toJson(loadData));</span><br><span class="line">                ProducerRecord memRecord = <span class="keyword">new</span> ProducerRecord&lt;String, String&gt;(METRICS_TOPIC, <span class="keyword">null</span>, <span class="keyword">null</span>, GsonUtil.toJson(memData));</span><br><span class="line">                ProducerRecord swapRecord = <span class="keyword">new</span> ProducerRecord&lt;String, String&gt;(METRICS_TOPIC, <span class="keyword">null</span>, <span class="keyword">null</span>, GsonUtil.toJson(swapData));</span><br><span class="line">                producer.send(cpuRecord);</span><br><span class="line">                producer.send(loadRecord);</span><br><span class="line">                producer.send(memRecord);</span><br><span class="line">                producer.send(swapRecord);</span><br><span class="line">            &#125;</span><br><span class="line">            producer.flush();</span><br><span class="line">            Thread.sleep(<span class="number">10000</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">        writeDataToKafka();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> MetricEvent <span class="title">buildCpuData</span><span class="params">(String hostIp, Long timestamp)</span> </span>&#123;</span><br><span class="line">        MetricEvent metricEvent = <span class="keyword">new</span> MetricEvent();</span><br><span class="line">        Map&lt;String, String&gt; tags = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line">        Map&lt;String, Object&gt; fields = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line">        <span class="keyword">int</span> used = random.nextInt(<span class="number">2048</span>);</span><br><span class="line">        <span class="keyword">int</span> max = <span class="number">2048</span>;</span><br><span class="line">        metricEvent.setName(<span class="string">&quot;cpu&quot;</span>);</span><br><span class="line">        metricEvent.setTimestamp(timestamp);</span><br><span class="line">        tags.put(<span class="string">&quot;cluster_name&quot;</span>, <span class="string">&quot;zhisheng&quot;</span>);</span><br><span class="line">        tags.put(<span class="string">&quot;host_ip&quot;</span>, hostIp);</span><br><span class="line">        fields.put(<span class="string">&quot;usedPercent&quot;</span>, (<span class="keyword">double</span>) used / max * <span class="number">100</span>);</span><br><span class="line">        fields.put(<span class="string">&quot;used&quot;</span>, used);</span><br><span class="line">        fields.put(<span class="string">&quot;max&quot;</span>, max);</span><br><span class="line">        metricEvent.setFields(fields);</span><br><span class="line">        metricEvent.setTags(tags);</span><br><span class="line">        <span class="keyword">return</span> metricEvent;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> MetricEvent <span class="title">buildLoadData</span><span class="params">(String hostIp, Long timestamp)</span> </span>&#123;</span><br><span class="line">        <span class="comment">//构建 load 数据，和构建 CPU 数据类似</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> MetricEvent <span class="title">buildSwapData</span><span class="params">(String hostIp, Long timestamp)</span> </span>&#123;</span><br><span class="line">        <span class="comment">//构建swap数据，和构建 CPU 数据类似</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> MetricEvent <span class="title">buildMemData</span><span class="params">(String hostIp, Long timestamp)</span> </span>&#123;</span><br><span class="line">        <span class="comment">//构建内存的数据，和构建 CPU 数据类似</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>然后 Flink 应用程序实时的去消费 Kafka 中的机器监控数据，先判断数据能够正常消费到。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">final</span> ParameterTool parameterTool = ExecutionEnvUtil.createParameterTool(args);</span><br><span class="line">StreamExecutionEnvironment env = ExecutionEnvUtil.prepare(parameterTool);</span><br><span class="line"></span><br><span class="line">Properties properties = KafkaConfigUtil.buildKafkaProps(parameterTool);</span><br><span class="line">FlinkKafkaConsumer011&lt;MetricEvent&gt; consumer = <span class="keyword">new</span> FlinkKafkaConsumer011&lt;&gt;(</span><br><span class="line">        parameterTool.get(<span class="string">&quot;metrics.topic&quot;</span>),</span><br><span class="line">        <span class="keyword">new</span> MetricSchema(),</span><br><span class="line">        properties);</span><br><span class="line">env.addSource(consumer)</span><br><span class="line">        .assignTimestampsAndWatermarks(<span class="keyword">new</span> MetricWatermark())</span><br><span class="line">        .print();</span><br></pre></td></tr></table></figure><p>再确定能够消费到机器监控数据之后，接下来需要对数据进行构造成 OutageMetricEvent 对象：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">OutageMetricEvent</span> </span>&#123;</span><br><span class="line">    <span class="comment">//机器集群名</span></span><br><span class="line">    <span class="keyword">private</span> String clusterName;</span><br><span class="line">    <span class="comment">//机器 host ip</span></span><br><span class="line">    <span class="keyword">private</span> String hostIp;</span><br><span class="line">    <span class="comment">//事件时间</span></span><br><span class="line">    <span class="keyword">private</span> Long timestamp;</span><br><span class="line">    <span class="comment">//机器告警是否恢复</span></span><br><span class="line">    <span class="keyword">private</span> Boolean recover;</span><br><span class="line">    <span class="comment">//机器告警恢复时间</span></span><br><span class="line">    <span class="keyword">private</span> Long recoverTime;</span><br><span class="line">    <span class="comment">//系统时间</span></span><br><span class="line">    <span class="keyword">private</span> Long systemTimestamp;</span><br><span class="line">    <span class="comment">//机器 CPU 使用率</span></span><br><span class="line">    <span class="keyword">private</span> Double cpuUsePercent;</span><br><span class="line">    <span class="comment">//机器内存使用率</span></span><br><span class="line">    <span class="keyword">private</span> Double memUsedPercent;</span><br><span class="line">    <span class="comment">//机器 SWAP 使用率</span></span><br><span class="line">    <span class="keyword">private</span> Double swapUsedPercent;</span><br><span class="line">    <span class="comment">//机器 load5</span></span><br><span class="line">    <span class="keyword">private</span> Double load5;</span><br><span class="line">    <span class="comment">//告警数量</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> counter = <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>通过 FlatMap 算子转换：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">new</span> FlatMapFunction&lt;MetricEvent, OutageMetricEvent&gt;() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">flatMap</span><span class="params">(MetricEvent metricEvent, Collector&lt;OutageMetricEvent&gt; collector)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        Map&lt;String, String&gt; tags = metricEvent.getTags();</span><br><span class="line">        <span class="keyword">if</span> (tags.containsKey(CLUSTER_NAME) &amp;&amp; tags.containsKey(HOST_IP)) &#123;</span><br><span class="line">            OutageMetricEvent outageMetricEvent = OutageMetricEvent.buildFromEvent(metricEvent);</span><br><span class="line">            <span class="keyword">if</span> (outageMetricEvent != <span class="keyword">null</span>) &#123;</span><br><span class="line">                collector.collect(outageMetricEvent);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>将数据转换后，需要将监控数据按照机器的 IP 进行 KeyBy，因为每台机器可能都会出现错误，所以都要将不同机器的状态都保存着，然后使用 process 算子，在该算子中，使用 ValueState 保存 OutageMetricEvent 和机器告警状态信息，另外还有一个 delay 字段定义的是持续多久没有收到监控数据的时间，alertCountLimit 表示的是告警的次数，如果超多一定的告警次数则会静默。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">OutageProcessFunction</span> <span class="keyword">extends</span> <span class="title">KeyedProcessFunction</span>&lt;<span class="title">String</span>, <span class="title">OutageMetricEvent</span>, <span class="title">OutageMetricEvent</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> ValueState&lt;OutageMetricEvent&gt; outageMetricState;</span><br><span class="line">    <span class="keyword">private</span> ValueState&lt;Boolean&gt; recover;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> delay;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> alertCountLimit;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">OutageProcessFunction</span><span class="params">(<span class="keyword">int</span> delay, <span class="keyword">int</span> alertCountLimit)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.delay = delay;</span><br><span class="line">        <span class="keyword">this</span>.alertCountLimit = alertCountLimit;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(Configuration parameters)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        TypeInformation&lt;OutageMetricEvent&gt; outageInfo = TypeInformation.of(<span class="keyword">new</span> TypeHint&lt;OutageMetricEvent&gt;() &#123;</span><br><span class="line">        &#125;);</span><br><span class="line">        TypeInformation&lt;Boolean&gt; recoverInfo = TypeInformation.of(<span class="keyword">new</span> TypeHint&lt;Boolean&gt;() &#123;</span><br><span class="line">        &#125;);</span><br><span class="line">        outageMetricState = getRuntimeContext().getState(<span class="keyword">new</span> ValueStateDescriptor&lt;&gt;(<span class="string">&quot;outage_zhisheng&quot;</span>, outageInfo));</span><br><span class="line">        recover = getRuntimeContext().getState(<span class="keyword">new</span> ValueStateDescriptor&lt;&gt;(<span class="string">&quot;recover_zhisheng&quot;</span>, recoverInfo));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">processElement</span><span class="params">(OutageMetricEvent outageMetricEvent, Context ctx, Collector&lt;OutageMetricEvent&gt; collector)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        OutageMetricEvent current = outageMetricState.value();</span><br><span class="line">        <span class="keyword">if</span> (current == <span class="keyword">null</span>) &#123;</span><br><span class="line">            current = <span class="keyword">new</span> OutageMetricEvent(outageMetricEvent.getClusterName(), outageMetricEvent.getHostIp(),</span><br><span class="line">                    outageMetricEvent.getTimestamp(), outageMetricEvent.getRecover(), System.currentTimeMillis());</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">if</span> (outageMetricEvent.getLoad5() != <span class="keyword">null</span>) &#123;</span><br><span class="line">                current.setLoad5(outageMetricEvent.getLoad5());</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">if</span> (outageMetricEvent.getCpuUsePercent() != <span class="keyword">null</span>) &#123;</span><br><span class="line">                current.setCpuUsePercent(outageMetricEvent.getCpuUsePercent());</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">if</span> (outageMetricEvent.getMemUsedPercent() != <span class="keyword">null</span>) &#123;</span><br><span class="line">                current.setMemUsedPercent(outageMetricEvent.getMemUsedPercent());</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">if</span> (outageMetricEvent.getSwapUsedPercent() != <span class="keyword">null</span>) &#123;</span><br><span class="line">                current.setSwapUsedPercent(outageMetricEvent.getSwapUsedPercent());</span><br><span class="line">            &#125;</span><br><span class="line">            current.setSystemTimestamp(System.currentTimeMillis());</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (recover.value() != <span class="keyword">null</span> &amp;&amp; !recover.value() &amp;&amp; outageMetricEvent.getTimestamp() &gt; current.getTimestamp()) &#123;</span><br><span class="line">            OutageMetricEvent recoverEvent = <span class="keyword">new</span> OutageMetricEvent(outageMetricEvent.getClusterName(), outageMetricEvent.getHostIp(),</span><br><span class="line">                    current.getTimestamp(), <span class="keyword">true</span>, System.currentTimeMillis());</span><br><span class="line">            recoverEvent.setRecoverTime(ctx.timestamp());</span><br><span class="line">            log.info(<span class="string">&quot;触发宕机恢复事件:&#123;&#125;&quot;</span>, recoverEvent);</span><br><span class="line">            collector.collect(recoverEvent);</span><br><span class="line">            current.setCounter(<span class="number">0</span>);</span><br><span class="line">            outageMetricState.update(current);</span><br><span class="line">            recover.update(<span class="keyword">true</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        current.setTimestamp(outageMetricEvent.getTimestamp());</span><br><span class="line">        outageMetricState.update(current);</span><br><span class="line">        ctx.timerService().registerEventTimeTimer(current.getSystemTimestamp() + delay);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onTimer</span><span class="params">(<span class="keyword">long</span> timestamp, OnTimerContext ctx, Collector&lt;OutageMetricEvent&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        OutageMetricEvent result = outageMetricState.value();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (result != <span class="keyword">null</span> &amp;&amp; timestamp &gt;= result.getSystemTimestamp() + delay &amp;&amp; System.currentTimeMillis() - result.getTimestamp() &gt;= delay) &#123;</span><br><span class="line">            <span class="keyword">if</span> (result.getCounter() &gt; alertCountLimit) &#123;</span><br><span class="line">                log.info(<span class="string">&quot;宕机告警次数大于:&#123;&#125; :&#123;&#125;&quot;</span>, alertCountLimit, result);</span><br><span class="line">                <span class="keyword">return</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            log.info(<span class="string">&quot;触发宕机告警事件:timestamp = &#123;&#125;, result = &#123;&#125;&quot;</span>, System.currentTimeMillis(), result);</span><br><span class="line">            result.setRecover(<span class="keyword">false</span>);</span><br><span class="line">            out.collect(result);</span><br><span class="line">            ctx.timerService().registerEventTimeTimer(timestamp + delay);</span><br><span class="line">            result.setCounter(result.getCounter() + <span class="number">1</span>);</span><br><span class="line">            result.setSystemTimestamp(timestamp);</span><br><span class="line">            outageMetricState.update(result);</span><br><span class="line">            recover.update(<span class="keyword">false</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在 processElement 方法中不断的处理数据，在处理的时候会从状态中获取看之前状态是否存在数据，在该方法内部最后通过 <code>ctx.timerService().registerEventTimeTimer(current.getSystemTimestamp() + delay);</code> 去注册一个事件时间的定时器，时间戳是当前的系统时间加上 delay 的时间。</p><p>在 onTimer 方法中就是具体的定时器，在定时器中获取到状态值，然后将状态值中的时间与 delay 的时间差是否满足，如果满足则表示一直没有数据过来，接着对比目前告警的数量与定义的限制数量，如果大于则不告警了，如果小于则表示触发了宕机告警并且打印相关的日志，然后更新状态中的值。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onTimer</span><span class="params">(<span class="keyword">long</span> timestamp, OnTimerContext ctx, Collector&lt;OutageMetricEvent&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    OutageMetricEvent result = outageMetricState.value();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (result != <span class="keyword">null</span> &amp;&amp; timestamp &gt;= result.getSystemTimestamp() + delay &amp;&amp; System.currentTimeMillis() - result.getTimestamp() &gt;= delay) &#123;</span><br><span class="line">        <span class="keyword">if</span> (result.getCounter() &gt; alertCountLimit) &#123;</span><br><span class="line">            log.info(<span class="string">&quot;宕机告警次数大于:&#123;&#125; :&#123;&#125;&quot;</span>, alertCountLimit, result);</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        log.info(<span class="string">&quot;触发宕机告警事件:timestamp = &#123;&#125;, result = &#123;&#125;&quot;</span>, System.currentTimeMillis(), result);</span><br><span class="line">        result.setRecover(<span class="keyword">false</span>);</span><br><span class="line">        out.collect(result);</span><br><span class="line">        ctx.timerService().registerEventTimeTimer(timestamp + delay);</span><br><span class="line">        result.setCounter(result.getCounter() + <span class="number">1</span>);</span><br><span class="line">        result.setSystemTimestamp(timestamp);</span><br><span class="line">        outageMetricState.update(result);</span><br><span class="line">        recover.update(<span class="keyword">false</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这样就完成了告警事件的判断了，接下来的算子就可以将告警事件转换成告警消息，然后将告警消息发送到下游去通知。那么就这样可以完成一个机器宕机告警的需求。</p>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
            <tag> Flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink Job 并行度设置</title>
      <link href="2019/12/30/Flink-Job-%E5%B9%B6%E8%A1%8C%E5%BA%A6%E8%AE%BE%E7%BD%AE/"/>
      <url>2019/12/30/Flink-Job-%E5%B9%B6%E8%A1%8C%E5%BA%A6%E8%AE%BE%E7%BD%AE/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p>生产环境如果 Job 突然消费不及时了，或者 Job 就根本不在消费数据了，那么该怎么办？首先得看下相关的监控查看 Job 是否在正常运行，是否出现反压的情况，是否这会生产数据量过大然而并行度却是根据之前数据量设置的，种种原因都需要一个个排查一下，然后找到根因才能够对应的去解决。这节来讲解下遇到这种问题后如何合理配置并行度呢？</p><a id="more"></a><h3 id="Source-端并行度的配置"><a href="#Source-端并行度的配置" class="headerlink" title="Source 端并行度的配置"></a>Source 端并行度的配置</h3><p>假设数据源端是 Kafka，在出现作业消费不及时的时候，首先看下 Kafka 的监控是不是现在生产者生产的数据上涨速度较快，从而导致作业目前的消费速度就是跟不上 Kafka 生产者的生产速度，如果是这样的话，那么就得查看作业的并行度和 Kafka 的分区数是否一致，如果小于 Kafka 的分区数，那么可以增大并行度至 Kafka 的分区数，然后再观察作业消费速度是否可以跟上数据生产速度；如果已经等于 Kafka 的分区数了，那得考虑下是否 Kafka 要扩大分区，但是这样可能会带来 Kafka 其他的问题，这个操作需要谨慎。</p><p>Kafka 中数据出现堆积的话，还可以分析下数据的类型，如果数据不重要，但是又要保证数据的及时性，可以修改作业让作业始终从最新的数据开始消费，丢弃之前堆积的数据，这样就可以保证数据的及时性。举个例子，假如一个实时告警作业它突然消费不及时，Kafka 中堆积了几亿条数据（数据延迟几个小时），那么如果作业调高并行度重启后，它还是从上一次提交的 offset 处开始消费的话，这样告警作业即使现在消费速度跟的上了，但是它要处理掉之前堆积的几亿条数据也是要一段时间的，那么就意味着这个作业仍将有段时间处于 ‘不可用’。因为即使判断出来要告警，可能这个告警信息的原数据已经是几个小时前的了，没准这个告警此时已经恢复了，但是还发出来告警这就意味着延迟性比较大，还会对告警消息接收者造成一定的干扰，所以这种场景下建议重启作业就直接开始从最新的数据开始消费。当然不同的场景可能不一样，如果金融行业的交易数据，那么是肯定不能允许这样丢数据的，即使堆积了，也要慢慢的去消费堆积的数据，直到后面追平至最新的数据。</p><p>在 Source 端设置并行度的话，如果数据源是 Kafka 的话，建议并行度不要超过 Kafka 的分区数，因为一个并行度会去处理一至多个分区的数据，如果设置过多的话，会出现部分并行度空闲。如果是其他的数据源，可以根据实际情况合理增大并行度来提高作业的处理数据能力。</p><h3 id="中间-Operator-并行度的配置"><a href="#中间-Operator-并行度的配置" class="headerlink" title="中间 Operator 并行度的配置"></a>中间 Operator 并行度的配置</h3><p>数据从 Source 端流入后，通常会进行一定的数据转换、聚合才能够满足需求，在数据转换中可能会和第三方系统进行交互，在交互的过程中可能会因为网络原因或者第三方服务原因导致有一定的延迟，从而导致这个数据交互的算子处理数据的吞吐量会降低，可能会造成反压，从而会影响上游的算子的消费。那么在这种情况下这些与第三方系统有交互的算子得稍微提高并行度，防止出现这种反压问题（当然反压问题不一定就这样可以解决，具体如何处理参见 9.1 节）。</p><p>除了这种与第三方服务做交互的外，另外可能的性能瓶颈也会出现在这类算子中，比如你 Kafka 过来的数据是 JSON 串的 String，然后需要转换成对象，在大数据量的情况下这个转换也是比较耗费性能的。</p><p>所以数据转换中间过程的算子也是非常重要的，如果哪一步算子的并行度设置的不合理，可能就会造成各种各样的问题出现。</p><h3 id="Sink-端并行度的配置"><a href="#Sink-端并行度的配置" class="headerlink" title="Sink 端并行度的配置"></a>Sink 端并行度的配置</h3><p>Sink 端是数据流向下游的地方，可以根据 Sink 端的数据量进行评估，可能有的作业是 Source 端的数据量最大，然后数据量不断的变少，最后到 Sink 端的数据就一点点了，比较常见的就是监控告警的场景。Source 端的数据是海量的，但是通过逐层的过滤和转换，到最后判断要告警的数据其实已经减少很多很多了，那么在最后的这个地方就可以将并行度设置的小一些。</p><p>当然也可能会有这样的情况，在 Source 端的数据量是最小的，拿到 Source 端流过来的数据后做了细粒度的拆分，那么数据量就不断的增加了，到 Sink 端的数据量就非常非常的大了。那么在 Sink 到下游的存储中间件的时候就需要提高并行度。</p><p>另外 Sink 端也是要与下游的服务进行交互，并行度还得根据下游的服务抗压能力来设置，如果在 Flink Sink 这端的数据量过大的话，然后在 Sink 处并行度也设置的很大，但是下游的服务完全撑不住这么大的并发写入，也是可能会造成下游服务直接被写挂的，下游服务可能还要对外提供一些其他的服务，如果稳定性不能保证的话，会造成很大的影响，所以最终还是要在 Sink 处的并行度做一定的权衡。</p><h3 id="Operator-Chain"><a href="#Operator-Chain" class="headerlink" title="Operator Chain"></a>Operator Chain</h3><p>对于一般的作业（无特殊耗性能处），可以尽量让算子的并行度从 Source 端到 Sink 端都保持一致，这样可以尽可能的让 Job 中的算子进行 chain 在一起，形成链，数据在链中可以直接传输，而不需要再次进行序列化与反序列化，这样带来的性能消耗就会得到降低。在 9.2 节中具体讲解了算子 chain 在一起的条件，忘记的话可以去回顾一下。</p>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
            <tag> Flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink Side Output 分流</title>
      <link href="2019/12/28/Flink-Side-Output-%E5%88%86%E6%B5%81/"/>
      <url>2019/12/28/Flink-Side-Output-%E5%88%86%E6%B5%81/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p>通常，在 Kafka 的 topic 中会有很多数据，这些数据虽然结构是一致的，但是类型可能不一致，举个例子：Kafka 中的监控数据有很多种：机器、容器、应用、中间件等，如果要对这些数据分别处理，就需要对这些数据流进行一个拆分。</p><a id="more"></a><h3 id="使用-Filter-分流"><a href="#使用-Filter-分流" class="headerlink" title="使用 Filter 分流"></a>使用 Filter 分流</h3><p>使用 filter 算子根据数据的字段进行过滤分成机器、容器、应用、中间件等。伪代码如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">DataStreamSource&lt;MetricEvent&gt; data = KafkaConfigUtil.buildSource(env);  <span class="comment">//从 Kafka 获取到所有的数据流</span></span><br><span class="line">SingleOutputStreamOperator&lt;MetricEvent&gt; machineData = data.filter(m -&gt; <span class="string">&quot;machine&quot;</span>.equals(m.getTags().get(<span class="string">&quot;type&quot;</span>)));  <span class="comment">//过滤出机器的数据</span></span><br><span class="line">SingleOutputStreamOperator&lt;MetricEvent&gt; dockerData = data.filter(m -&gt; <span class="string">&quot;docker&quot;</span>.equals(m.getTags().get(<span class="string">&quot;type&quot;</span>)));    <span class="comment">//过滤出容器的数据</span></span><br><span class="line">SingleOutputStreamOperator&lt;MetricEvent&gt; applicationData = data.filter(m -&gt; <span class="string">&quot;application&quot;</span>.equals(m.getTags().get(<span class="string">&quot;type&quot;</span>)));  <span class="comment">//过滤出应用的数据</span></span><br><span class="line">SingleOutputStreamOperator&lt;MetricEvent&gt; middlewareData = data.filter(m -&gt; <span class="string">&quot;middleware&quot;</span>.equals(m.getTags().get(<span class="string">&quot;type&quot;</span>)));    <span class="comment">//过滤出中间件的数据</span></span><br></pre></td></tr></table></figure><h3 id="使用-Split-分流"><a href="#使用-Split-分流" class="headerlink" title="使用 Split 分流"></a>使用 Split 分流</h3><p>先在 split 算子里面定义 OutputSelector 的匿名内部构造类，然后重写 select 方法，根据数据的类型将不同的数据放到不同的 tag 里面，这样返回后的数据格式是 SplitStream，然后要使用这些数据的时候，可以通过 select 去选择对应的数据类型，伪代码如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">DataStreamSource&lt;MetricEvent&gt; data = KafkaConfigUtil.buildSource(env);  <span class="comment">//从 Kafka 获取到所有的数据流</span></span><br><span class="line">SplitStream&lt;MetricEvent&gt; splitData = data.split(<span class="keyword">new</span> OutputSelector&lt;MetricEvent&gt;() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Iterable&lt;String&gt; <span class="title">select</span><span class="params">(MetricEvent metricEvent)</span> </span>&#123;</span><br><span class="line">        List&lt;String&gt; tags = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">        String type = metricEvent.getTags().get(<span class="string">&quot;type&quot;</span>);</span><br><span class="line">        <span class="keyword">switch</span> (type) &#123;</span><br><span class="line">            <span class="keyword">case</span> <span class="string">&quot;machine&quot;</span>:</span><br><span class="line">                tags.add(<span class="string">&quot;machine&quot;</span>);</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            <span class="keyword">case</span> <span class="string">&quot;docker&quot;</span>:</span><br><span class="line">                tags.add(<span class="string">&quot;docker&quot;</span>);</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            <span class="keyword">case</span> <span class="string">&quot;application&quot;</span>:</span><br><span class="line">                tags.add(<span class="string">&quot;application&quot;</span>);</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            <span class="keyword">case</span> <span class="string">&quot;middleware&quot;</span>:</span><br><span class="line">                tags.add(<span class="string">&quot;middleware&quot;</span>);</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            <span class="keyword">default</span>:</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> tags;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">DataStream&lt;MetricEvent&gt; machine = splitData.select(<span class="string">&quot;machine&quot;</span>);</span><br><span class="line">DataStream&lt;MetricEvent&gt; docker = splitData.select(<span class="string">&quot;docker&quot;</span>);</span><br><span class="line">DataStream&lt;MetricEvent&gt; application = splitData.select(<span class="string">&quot;application&quot;</span>);</span><br><span class="line">DataStream&lt;MetricEvent&gt; middleware = splitData.select(<span class="string">&quot;middleware&quot;</span>);</span><br></pre></td></tr></table></figure><p>上面这种只分流一次是没有问题的，注意如果要使用它来做连续的分流，那是有问题的，笔者曾经就遇到过这个问题，当时记录了博客 —— <a href="http://www.54tianzhisheng.cn/2019/06/12/flink-split/">Flink 从0到1学习—— Flink 不可以连续 Split(分流)？</a> ，当时排查这个问题还查到两个相关的 Flink Issue。</p><ul><li><a href="https://issues.apache.org/jira/browse/FLINK-5031">https://issues.apache.org/jira/browse/FLINK-5031</a></li><li><a href="https://issues.apache.org/jira/browse/FLINK-11084">https://issues.apache.org/jira/browse/FLINK-11084</a></li></ul><p>这两个 Issue 反映的就是连续 split 不起作用，在第二个 Issue 下面的评论就有回复说 Side Output 的功能比 split 更强大， split 会在后面的版本移除（其实在 1.7.x 版本就已经设置为过期），那么下面就来学习一下 Side Output。</p><h3 id="使用-Side-Output-分流"><a href="#使用-Side-Output-分流" class="headerlink" title="使用 Side Output 分流"></a>使用 Side Output 分流</h3><p>要使用 Side Output 的话，你首先需要做的是定义一个 OutputTag 来标识 Side Output，代表这个 Tag 是要收集哪种类型的数据，如果是要收集多种不一样类型的数据，那么你就需要定义多种 OutputTag。要完成本节前面的需求，需要定义 4 个 OutputTag，如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//创建 output tag</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> OutputTag&lt;MetricEvent&gt; machineTag = <span class="keyword">new</span> OutputTag&lt;MetricEvent&gt;(<span class="string">&quot;machine&quot;</span>) &#123;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> OutputTag&lt;MetricEvent&gt; dockerTag = <span class="keyword">new</span> OutputTag&lt;MetricEvent&gt;(<span class="string">&quot;docker&quot;</span>) &#123;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> OutputTag&lt;MetricEvent&gt; applicationTag = <span class="keyword">new</span> OutputTag&lt;MetricEvent&gt;(<span class="string">&quot;application&quot;</span>) &#123;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> OutputTag&lt;MetricEvent&gt; middlewareTag = <span class="keyword">new</span> OutputTag&lt;MetricEvent&gt;(<span class="string">&quot;middleware&quot;</span>) &#123;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>定义好 OutputTag 后，可以使用下面几种函数来处理数据：</p><ul><li>ProcessFunction</li><li>KeyedProcessFunction</li><li>CoProcessFunction</li><li>ProcessWindowFunction</li><li>ProcessAllWindowFunction</li></ul><p>在利用上面的函数处理数据的过程中，需要对数据进行判断，将不同种类型的数据存到不同的 OutputTag 中去，如下代码所示：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">DataStreamSource&lt;MetricEvent&gt; data = KafkaConfigUtil.buildSource(env);  <span class="comment">//从 Kafka 获取到所有的数据流</span></span><br><span class="line">SingleOutputStreamOperator&lt;MetricEvent&gt; sideOutputData = data.process(<span class="keyword">new</span> ProcessFunction&lt;MetricEvent, MetricEvent&gt;() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">processElement</span><span class="params">(MetricEvent metricEvent, Context context, Collector&lt;MetricEvent&gt; collector)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        String type = metricEvent.getTags().get(<span class="string">&quot;type&quot;</span>);</span><br><span class="line">        <span class="keyword">switch</span> (type) &#123;</span><br><span class="line">            <span class="keyword">case</span> <span class="string">&quot;machine&quot;</span>:</span><br><span class="line">                context.output(machineTag, metricEvent);</span><br><span class="line">            <span class="keyword">case</span> <span class="string">&quot;docker&quot;</span>:</span><br><span class="line">                context.output(dockerTag, metricEvent);</span><br><span class="line">            <span class="keyword">case</span> <span class="string">&quot;application&quot;</span>:</span><br><span class="line">                context.output(applicationTag, metricEvent);</span><br><span class="line">            <span class="keyword">case</span> <span class="string">&quot;middleware&quot;</span>:</span><br><span class="line">                context.output(middlewareTag, metricEvent);</span><br><span class="line">            <span class="keyword">default</span>:</span><br><span class="line">                collector.collect(metricEvent);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure><p>好了，既然上面已经将不同类型的数据放到不同的 OutputTag 里面了，那么该如何去获取呢？可以使用 getSideOutput 方法来获取不同 OutputTag 的数据，比如：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">DataStream&lt;MetricEvent&gt; machine = sideOutputData.getSideOutput(machineTag);</span><br><span class="line">DataStream&lt;MetricEvent&gt; docker = sideOutputData.getSideOutput(dockerTag);</span><br><span class="line">DataStream&lt;MetricEvent&gt; application = sideOutputData.getSideOutput(applicationTag);</span><br><span class="line">DataStream&lt;MetricEvent&gt; middleware = sideOutputData.getSideOutput(middlewareTag);</span><br></pre></td></tr></table></figure><p>这样你就可以获取到 Side Output 数据了，其实在 3.4 和 3.5 节就讲了 Side Output 在 Flink 中的应用（处理窗口的延迟数据），大家如果没有印象了可以再返回去复习一下。</p>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
            <tag> Flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink Checkpoint 和 Savepoint 区别</title>
      <link href="2019/12/28/Flink-Checkpoint-%E5%92%8C-Savepoint-%E5%8C%BA%E5%88%AB/"/>
      <url>2019/12/28/Flink-Checkpoint-%E5%92%8C-Savepoint-%E5%8C%BA%E5%88%AB/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p>Checkpoint 在 Flink 中是一个非常重要的 Feature，Checkpoint 使 Flink 的状态具有良好的容错性，通过 Checkpoint 机制，Flink 可以对作业的状态和计算位置进行恢复。</p><a id="more"></a><h3 id="Checkpoint-介绍及使用"><a href="#Checkpoint-介绍及使用" class="headerlink" title="Checkpoint 介绍及使用"></a>Checkpoint 介绍及使用</h3><p>为了保障的容错，Flink 需要对状态进行快照。Flink 可以从 Checkpoint 中恢复流的状态和位置，从而使得应用程序发生故障后能够得到与无故障执行相同的语义。</p><p>Flink 的 Checkpoint 有以下先决条件：</p><ul><li><p><strong>需要具有持久性且支持重放一定时间范围内数据的数据源。</strong></p><blockquote><p>例如：Kafka、RabbitMQ 等。这里为什么要求支持重放一定时间范围内的数据呢？因为 Flink 的容错机制决定了，当 Flink 任务失败后会自动从最近一次成功的 Checkpoint 处恢复任务，此时可能需要把任务失败前消费的部分数据再消费一遍，所以必须要求数据源支持重放。假如一个Flink 任务消费 Kafka 并将数据写入到 MySQL 中，任务从 Kafka 读取到数据，还未将数据输出到 MySQL 时任务突然失败了，此时如果 Kafka 不支持重放，就会造成这部分数据永远丢失了。支持重放数据的数据源可以保障任务消费失败后，能够重新消费来保障任务不丢数据。</p></blockquote></li><li><p><strong>需要一个能保存状态的持久化存储介质</strong></p><blockquote><p>例如：HDFS、S3 等。当 Flink 任务失败后，自动从 Checkpoint 处恢复，但是如果 Checkpoint 时保存的状态信息快照全丢了，那就会影响 Flink 任务的正常恢复。就好比我们看书时经常使用书签来记录当前看到的页码，当下次看书时找到书签的位置继续阅读即可，但是如果书签三天两头经常丢，那我们就无法通过书签来恢复阅读。</p></blockquote></li></ul><p>Flink 中 Checkpoint 是默认关闭的，对于需要保障 At Least Once 和 Exactly Once 语义的任务，强烈建议开启 Checkpoint，对于丢一小部分数据不敏感的任务，可以不开启 Checkpoint，例如：一些推荐相关的任务丢一小部分数据并不会影响推荐效果。</p><hr><p><strong>下面来介绍 Checkpoint 具体如何使用。</strong></p><p>首先调用 StreamExecutionEnvironment 的方法 enableCheckpointing(n) 来开启 Checkpoint，参数 n 以毫秒为单位表示 Checkpoint 的时间间隔。Checkpoint 配置相关的 Java 代码如下所示：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"><span class="comment">// 开启 Checkpoint，每 1000毫秒进行一次 Checkpoint</span></span><br><span class="line">env.enableCheckpointing(<span class="number">1000</span>);</span><br><span class="line"><span class="comment">// Checkpoint 语义设置为 EXACTLY_ONCE</span></span><br><span class="line">env.getCheckpointConfig().setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE);</span><br><span class="line"><span class="comment">// CheckPoint 的超时时间</span></span><br><span class="line">env.getCheckpointConfig().setCheckpointTimeout(<span class="number">60000</span>);</span><br><span class="line"><span class="comment">// 同一时间，只允许 有 1 个 Checkpoint 在发生</span></span><br><span class="line">env.getCheckpointConfig().setMaxConcurrentCheckpoints(<span class="number">1</span>);</span><br><span class="line"><span class="comment">// 两次 Checkpoint 之间的最小时间间隔为 500 毫秒</span></span><br><span class="line">env.getCheckpointConfig().setMinPauseBetweenCheckpoints(<span class="number">500</span>);</span><br><span class="line"><span class="comment">// 当 Flink 任务取消时，保留外部保存的 CheckPoint 信息</span></span><br><span class="line">env.getCheckpointConfig().enableExternalizedCheckpoints(ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION);</span><br><span class="line"><span class="comment">// 当有较新的 Savepoint 时，作业也会从 Checkpoint 处恢复</span></span><br><span class="line">env.getCheckpointConfig().setPreferCheckpointForRecovery(<span class="keyword">true</span>);</span><br><span class="line"><span class="comment">// 作业最多允许 Checkpoint 失败 1 次（flink 1.9 开始支持）</span></span><br><span class="line">env.getCheckpointConfig().setTolerableCheckpointFailureNumber(<span class="number">1</span>);</span><br><span class="line"><span class="comment">// Checkpoint 失败后，整个 Flink 任务也会失败（flink 1.9 之前）</span></span><br><span class="line">env.getCheckpointConfig.setFailTasksOnCheckpointingErrors(<span class="keyword">true</span>)</span><br></pre></td></tr></table></figure><p>以上 Checkpoint 相关的参数描述如下所示：</p><ul><li>Checkpoint 语义：EXACTLY<em>ONCE 或 AT</em>LEAST<em>ONCE，EXACTLY</em>ONCE 表示所有要消费的数据被恰好处理一次，即所有数据既不丢数据也不重复消费；AT<em>LEAST</em>ONCE 表示要消费的数据至少处理一次，可能会重复消费。</li><li>Checkpoint 超时时间：如果 Checkpoint 时间超过了设定的超时时间，则 Checkpoint 将会被终止。</li><li>同时进行的 Checkpoint 数量：默认情况下，当一个 Checkpoint 在进行时，JobManager 将不会触发下一个 Checkpoint，但 Flink 允许多个 Checkpoint 同时在发生。</li><li>两次 Checkpoint 之间的最小时间间隔：从上一次 Checkpoint 结束到下一次 Checkpoint 开始，中间的间隔时间。例如，env.enableCheckpointing(60000) 表示 1 分钟触发一次 Checkpoint，同时再设置两次 Checkpoint 之间的最小时间间隔为 30 秒，假如任务运行过程中一次 Checkpoint 就用了50s，那么等 Checkpoint 结束后，理论来讲再过 10s 就要开始下一次 Checkpoint 了，但是由于设置了最小时间间隔为30s，所以需要再过 30s 后，下次 Checkpoint 才开始。注：如果配置了该参数就决定了同时进行的 Checkpoint 数量只能为 1。</li><li>当任务被取消时，外部 Checkpoint 信息是否被清理：Checkpoint 在默认的情况下仅用于恢复运行失败的 Flink 任务，当任务手动取消时 Checkpoint 产生的状态信息并不保留。当然可以通过该配置来保留外部的 Checkpoint 状态信息，这些被保留的状态信息在作业手动取消时不会被清除，这样就可以使用该状态信息来恢复 Flink 任务，对于需要从状态恢复的任务强烈建议配置为外部 Checkpoint 状态信息不清理。可选择的配置项为：</li><li>ExternalizedCheckpointCleanup.RETAIN<em>ON</em>CANCELLATION：当作业手动取消时，保留作业的 Checkpoint 状态信息。注意，这种情况下，需要手动清除该作业保留的 Checkpoint 状态信息，否则这些状态信息将永远保留在外部的持久化存储中。</li><li>ExternalizedCheckpointCleanup.DELETE<em>ON</em>CANCELLATION：当作业取消时，Checkpoint 状态信息会被删除。仅当作业失败时，作业的 Checkpoint 才会被保留用于任务恢复。</li><li>任务失败，当有较新的 Savepoint 时，作业是否回退到 Checkpoint 进行恢复：默认情况下，当 Savepoint 比 Checkpoint 较新时，任务会从 Savepoint 处恢复。</li><li>作业可以容忍 Checkpoint 失败的次数：默认值为 0，表示不能接受 Checkpoint 失败。</li></ul><p>关于 Checkpoint 时，状态后端相关的配置请参阅本课 4.2 节。</p><h3 id="Savepoint-介绍、Savepoint-与-Checkpoint-的区别及使用"><a href="#Savepoint-介绍、Savepoint-与-Checkpoint-的区别及使用" class="headerlink" title="Savepoint 介绍、Savepoint 与 Checkpoint 的区别及使用"></a>Savepoint 介绍、Savepoint 与 Checkpoint 的区别及使用</h3><p>Savepoint 与 Checkpoint 类似，同样需要把状态信息存储到外部介质，当作业失败时，可以从外部存储中恢复。Savepoint 与 Checkpoint 的区别很多：</p><table><thead><tr><th align="center">Checkpoint</th><th align="center">Savepoint</th></tr></thead><tbody><tr><td align="center">由 Flink 的 JobManager 定时自动触发并管理</td><td align="center">由用户手动触发并管理</td></tr><tr><td align="center">主要用于任务发生故障时，为任务提供给自动恢复机制</td><td align="center">主要用于升级 Flink 版本、修改任务的逻辑代码、调整算子的并行度，且必须手动恢复</td></tr><tr><td align="center">当使用 RocksDBStateBackend 时，支持增量方式对状态信息进行快照</td><td align="center">仅支持全量快照</td></tr><tr><td align="center">Flink 任务停止后，Checkpoint 的状态快照信息默认被清除</td><td align="center">一旦触发 Savepoint，状态信息就被持久化到外部存储，除非用户手动删除</td></tr><tr><td align="center">Checkpoint 设计目标：轻量级且尽可能快地恢复任务</td><td align="center">Savepoint 的生成和恢复成本会更高一些，Savepoint 更多地关注代码的可移植性和兼容任务的更改操作</td></tr></tbody></table><p>除了上述描述外，Checkpoint 和 Savepoint 在当前的实现上基本相同。</p><p>强烈建议在程序中给算子分配 Operator ID，以便来升级程序。主要通过 <code>uid(String)</code> 方法手动指定算子的 ID ，这些 ID 将用于恢复每个算子的状态。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">DataStream&lt;String&gt; stream = env.</span><br><span class="line">  <span class="comment">// Stateful source (e.g. Kafka) with ID</span></span><br><span class="line">  .addSource(<span class="keyword">new</span> StatefulSource())</span><br><span class="line">  .uid(<span class="string">&quot;source-id&quot;</span>) <span class="comment">// ID for the source operator</span></span><br><span class="line">  .shuffle()</span><br><span class="line">  <span class="comment">// Stateful mapper with ID</span></span><br><span class="line">  .map(<span class="keyword">new</span> StatefulMapper())</span><br><span class="line">  .uid(<span class="string">&quot;mapper-id&quot;</span>) <span class="comment">// ID for the mapper</span></span><br><span class="line">  <span class="comment">// Stateless printing sink</span></span><br><span class="line">  .print(); <span class="comment">// Auto-generated ID</span></span><br></pre></td></tr></table></figure><p>如果不为算子手动指定 ID，Flink 会为算子自动生成 ID。当 Flink 任务从 Savepoint 中恢复时，是按照 Operator ID 将快照信息与算子进行匹配的，只要这些 ID 不变，Flink 任务就可以从 Savepoint 中恢复。自动生成的 ID 取决于代码的结构，并且对代码更改比较敏感，因此强烈建议给程序中所有有状态的算子手动分配 Operator ID。如下左图所示，一个 Flink 任务包含了 算子 A 和 算子 B，代码中都未指定 Operator ID，所以 Flink 为 Task A 自动生成了 Operator ID 为 aaa，为 Task B 自动生成了 Operator ID 为 bbb，且 Savepoint 成功完成。但是在代码改动后，任务并不能从 Savepoint 中正常恢复，因为 Flink 为算子生成的 Operator ID 取决于代码结构，代码改动后可能会把算子 B 的 Operator ID 改变成 ccc，导致任务从 Savepoint 恢复时，SavePoint 中只有 Operator ID 为 aaa 和 bbb 的状态信息，算子 B 找不到 Operator ID 为 ccc 的状态信息，所以算子 B 不能正常恢复。这里如果在写代码时通过 <code>uid(String)</code> 手动指定了 Operator ID，就不会存在 上述问题了。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-19-020528.jpg" alt="img"></p><p>Savepoint 需要用户手动去触发，触发 Savepoint 的方式如下所示：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/flink savepoint :jobId [:targetDirectory]</span><br></pre></td></tr></table></figure><p>这将触发 ID 为 <code>:jobId</code> 的作业进行 Savepoint，并返回创建的 Savepoint 路径，用户需要此路径来还原和删除 Savepoint 。</p><p>使用 YARN 触发 Savepoint 的方式如下所示：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin&#x2F;flink savepoint :jobId [:targetDirectory] -yid :yarnAppId</span><br></pre></td></tr></table></figure><p>这将触发 ID 为 <code>:jobId</code> 和 YARN 应用程序 ID <code>:yarnAppId</code> 的作业进行 Savepoint，并返回创建的 Savepoint 路径。</p><p>使用 Savepoint 取消 Flink 任务：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin&#x2F;flink cancel -s [:targetDirectory] :jobId</span><br></pre></td></tr></table></figure><p>这将自动触发 ID 为 <code>:jobid</code> 的作业进行 Savepoint，并在 Checkpoint 结束后取消该任务。此外，可以指定一个目标文件系统目录来存储 Savepoint 的状态信息，也可以在 flink 的 conf 目录下 flink-conf.yaml 中配置 state.savepoints.dir 参数来指定 Savepoint 的默认目录，触发 Savepoint 时，如果不指定目录则使用该默认目录。无论使用哪种方式配置，都需要保障配置的目录能被所有的 JobManager 和 TaskManager 访问。</p><h3 id="Checkpoint-流程"><a href="#Checkpoint-流程" class="headerlink" title="Checkpoint 流程"></a>Checkpoint 流程</h3><p>Flink 任务 Checkpoint 的详细流程如下所示：</p><ol><li>JobManager 端的 CheckPointCoordinator 会定期向所有 SourceTask 发送 CheckPointTrigger，Source Task 会在数据流中安插 Checkpoint barrier</li></ol><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-11-06-021819.png" alt="img"></p><ol start="2"><li>当 task 收到上游所有实例的 barrier 后，向自己的下游继续传递 barrier，然后自身同步进行快照，并将自己的状态异步写入到持久化存储中</li></ol><ul><li>如果是增量 Checkpoint，则只是把最新的一部分更新写入到外部持久化存储中</li><li>为了下游尽快进行 Checkpoint，所以 task 会先发送 barrier 到下游，自身再同步进行快照</li></ul><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-11-06-021846.png" alt="img"></p><blockquote><p>注：Task B 必须接收到上游 Task A 所有实例发送的 barrier 时，Task B 才能开始进行快照，这里有一个 barrier 对齐的概念，关于 barrier 对齐的详细介绍请参阅 9.5.1 节 Flink 内部如何保证 Exactly Once 中的 barrier 对齐部分</p></blockquote><ol start="3"><li><p>当 task 将状态信息完成备份后，会将备份数据的地址（state handle）通知给 JobManager 的CheckPointCoordinator，如果 Checkpoint 的持续时长超过了 Checkpoint 设定的超时时间CheckPointCoordinator 还没有收集完所有的 State Handle，CheckPointCoordinator 就会认为本次 Checkpoint 失败，会把这次 Checkpoint 产生的所有状态数据全部删除</p></li><li><p>如果 CheckPointCoordinator 收集完所有算子的 State Handle，CheckPointCoordinator 会把整个 StateHandle 封装成 completed Checkpoint Meta，写入到外部存储中，Checkpoint 结束</p></li></ol><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-11-06-021900.png" alt="img"></p><h3 id="基于-RocksDB-的增量-Checkpoint-实现原理"><a href="#基于-RocksDB-的增量-Checkpoint-实现原理" class="headerlink" title="基于 RocksDB 的增量 Checkpoint 实现原理"></a>基于 RocksDB 的增量 Checkpoint 实现原理</h3><p>当使用 RocksDBStateBackend 时，增量 Checkpoint 是如何实现的呢？</p><p>RocksDB 是一个基于 LSM 实现的 KV 数据库。LSM 全称 Log Structured Merge Trees，LSM 树本质是将大量的磁盘随机写操作转换成磁盘的批量写操作来极大地提升磁盘数据写入效率。一般 LSM Tree 实现上都会有一个基于内存的 MemTable 介质，所有的增删改操作都是写入到 MemTable 中，当 MemTable 足够大以后，将 MemTable 中的数据 flush 到磁盘中生成不可变且内部有序的 ssTable（Sorted String Table）文件，全量数据保存在磁盘的多个 ssTable 文件中。HBase 也是基于 LSM Tree 实现的，HBase 磁盘上的 HFile 就相当于这里的 ssTable 文件，每次生成的 HFile 都是不可变的而且内部有序的文件。基于 ssTable 不可变的特性，才实现了增量 Checkpoint，具体流程如下所示：</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-11-06-021910.png" alt="img"></p><p>第一次 Checkpoint 时生成的状态快照信息包含了两个 sstable 文件：sstable1 和 sstable2 及 Checkpoint1 的元数据文件 MANIFEST-chk1，所以第一次 Checkpoint 时需要将 sstable1、sstable2 和 MANIFEST-chk1 上传到外部持久化存储中。第二次 Checkpoint 时生成的快照信息为 sstable1、sstable2、sstable3 及元数据文件 MANIFEST-chk2，由于 sstable 文件的不可变特性，所以状态快照信息的 sstable1、sstable2 这两个文件并没有发生变化，sstable1、sstable2 这两个文件不需要重复上传到外部持久化存储中，因此第二次 Checkpoint 时，只需要将 sstable3 和 MANIFEST-chk2 文件上传到外部持久化存储中即可。这里只将新增的文件上传到外部持久化存储，也就是所谓的增量 Checkpoint。</p><p>基于 LSM Tree 实现的数据库为了提高查询效率，都需要定期对磁盘上多个 sstable 文件进行合并操作，合并时会将删除的、过期的以及旧版本的数据进行清理，从而降低 sstable 文件的总大小。图中可以看到第三次 Checkpoint 时生成的快照信息为sstable3、sstable4、sstable5 及元数据文件 MANIFEST-chk3， 其中新增了 sstable4 文件且 sstable1 和 sstable2 文件合并成 sstable5 文件，因此第三次 Checkpoint 时只需要向外部持久化存储上传 sstable4、sstable5 及元数据文件 MANIFEST-chk3。</p><p>基于 RocksDB 的增量 Checkpoint 从本质上来讲每次 Checkpoint 时只将本次 Checkpoint 新增的快照信息上传到外部的持久化存储中，依靠的是 LSM Tree 中 sstable 文件不可变的特性。对 LSM Tree 感兴趣的同学可以深入研究 RocksDB 或 HBase 相关原理及实现。</p><h3 id="状态如何从-Checkpoint-恢复"><a href="#状态如何从-Checkpoint-恢复" class="headerlink" title="状态如何从 Checkpoint 恢复"></a>状态如何从 Checkpoint 恢复</h3><p>在 Checkpoint 和 Savepoint 的比较过程中，知道了相比 Savepoint 而言，Checkpoint 的成本更低一些，但有些场景 Checkpoint 并不能完全满足我们的需求。所以在使用过程中，如果我们的需求能使用 Checkpoint 来解决优先使用 Checkpoint。当 Flink 任务中的一些依赖组件需要升级重启时，例如 hdfs、Kafka、yarn 升级或者 Flink 任务的 Sink 端对应的 MySQL、Redis 由于某些原因需要重启时，Flink 任务在这段时间也需要重启。但是由于 Flink 任务的代码并没有修改，所以 Flink 任务启动时可以从 Checkpoint 处恢复任务，此时必须配置取消 Flink 任务时保留外部存储的 Checkpoint 状态信息。从 Checkpoint 处恢复任务的命令如下所示，checkpointMetaDataPath 表示 Checkpoint 的目录。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/flink run -s :checkpointMetaDataPath xxx.jar [:runArgs]</span><br></pre></td></tr></table></figure><p>如果 flink on yarn 模式，启动命令如下所示：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/flink run -s :checkpointMetaDataPath -yid :yarnAppId xxx.jar [:runArgs]</span><br></pre></td></tr></table></figure><p>问题来了，Flink 自动维护 Checkpoint，所以用户在这里并拿不到任务取消之前最后一次 Checkpoint 的目录。那怎么办呢？如下图所示，在任务取消之前，Flink 任务的 WebUI 中可以看到 Checkpoint 的目录，可以在取消任务之前将此目录保存起来，恢复时就可以从该目录恢复任务。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-19-020530.jpg" alt="img"></p><p>上述方法最大缺陷就是用户的人力成本太高了，假如需要重启 100 个任务，难道需要用户手动维护 100 个任务的 Checkpoint 目录吗？可以做一个简单后台项目，用于管理和发布 Flink 任务，这里讲述一种通过 rest api 来获取 Checkpoint 目录的方式。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-19-20531.jpg" alt="img"></p><p>如上图所示是 Flink JobManager 的 overview 页面，只需要将端口号后面的路径和参数按照以下替换即可：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http:&#x2F;&#x2F;node107.bigdata.dmp.local.com:35524&#x2F;jobs&#x2F;a1c70b36d19b3a9fc2713ba98cfc4a4f&#x2F;metrics?get&#x3D;lastCheckpointExternalPath</span><br></pre></td></tr></table></figure><p>调用以上接口，即可返回 a1c70b36d19b3a9fc2713ba98cfc4a4f 对应的 job 最后一次 Checkpoint 的目录，返回格式如下所示。</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[</span><br><span class="line">  &#123;</span><br><span class="line">    <span class="attr">&quot;id&quot;</span>: <span class="string">&quot;lastCheckpointExternalPath&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;value&quot;</span>: <span class="string">&quot;hdfs:/user/flink/checkpoints/a1c70b36d19b3a9fc2713ba98cfc4a4f/chk-18&quot;</span></span><br><span class="line">  &#125;</span><br><span class="line">]</span><br></pre></td></tr></table></figure><p>通过这种方式可以方便地维护所有 Flink 任务的 Checkpoint 目录，当然也可以通过 Metrics 的 Reporter 将 Checkpoint 目录保存到外部存储介质中，当任务需要从 Checkpoint 处恢复时，则从外部存储中读取到相应的 Checkpoint 目录。</p><p>当设置取消 Flink 任务保留外部的 Checkpoint 状态信息时，可能会带来的负面影响是：长期运行下去，hdfs 上将会保留很多废弃的且不再会使用的 Checkpoint 目录，所以如果开启了此配置，需要制定策略，定期清理那些不再会使用到的 Checkpoint 目录。</p><h3 id="状态如何从-Savepoint-恢复"><a href="#状态如何从-Savepoint-恢复" class="headerlink" title="状态如何从 Savepoint 恢复"></a>状态如何从 Savepoint 恢复</h3><p>如下所示，从 Savepoint 恢复任务的命令与 Checkpoint 恢复命令类似，savepointPath 表示 Savepoint 保存的目录，Savepoint 的各种触发方式都会返回 Savepoint 目录。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin&#x2F;flink run -s :savepointPath xxx.jar [:runArgs]</span><br></pre></td></tr></table></figure><p>如果 flink on yarn 模式，启动命令如下所示：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/flink run -s :savepointPath -yid :yarnAppId xxx.jar [:runArgs]</span><br></pre></td></tr></table></figure><p>默认情况下，恢复操作将尝试将 Savepoint 的所有状态映射到要还原的程序。如果删除了算子，则可以通过 <code>--allowNonRestoredState</code>（short：<code>-n</code>）选项跳过那些无法映射到新程序的状态：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin&#x2F;flink run -s :savepointPath -n xxx.jar [:runArgs]</span><br></pre></td></tr></table></figure><p>如果从 Savepoint 恢复时，在任务中添加一个需要状态的新算子，会发生什么？向任务添加新算子时，它将在没有任何状态的情况下进行初始化，Savepoint 中包含每个有状态算子的状态，无状态算子根本不是 Savepoint 的一部分，新算子的行为类似于无状态算子。</p><p>如果在任务中对算子进行重新排序，会发生什么？如果给这些算子分配了 ID，它们将像往常一样恢复。如果没有分配 ID ，则有状态算子自动生成的 ID 很可能在重新排序后发生更改，这将导致无法从之前的 Savepoint 中恢复。</p><p>Savepoint 目录里的状态快照信息，目前不支持移动位置，由于技术原因元数据文件中使用绝对路径来保存数据。如果因为某种原因必须要移动 Savepoint 文件，那么有两种方案来实现：</p><ul><li>使用编辑器修改 Savepoint 的元数据文件信息，将旧路径改为新路径</li><li>可以使用 <code>SavepointV2Serializer</code> 类以编程方式读取、操作和重写元数据文件的新路径</li></ul><p>长期使用 Savepoint 同样要注意清理那些废弃 Savepoint 目录的问题。</p>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
            <tag> Flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink Parallelism 和 Slot 深度理解</title>
      <link href="2019/12/28/Flink-Parallelism-%E5%92%8C-Slot-%E6%B7%B1%E5%BA%A6%E7%90%86%E8%A7%A3/"/>
      <url>2019/12/28/Flink-Parallelism-%E5%92%8C-Slot-%E6%B7%B1%E5%BA%A6%E7%90%86%E8%A7%A3/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p>parallelism 是并行的意思，在 Flink 里面代表每个算子的并行度，适当的提高并行度可以大大提高 Job 的执行效率，比如你的 Job 消费 Kafka 数据过慢，适当调大可能就消费正常了。</p><a id="more"></a><p>相信使用过 Flink 的你或多或少遇到过下面这个问题（笔者自己的项目曾经也出现过这样的问题），错误信息如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Caused by: akka.pattern.AskTimeoutException: </span><br><span class="line">Ask timed out on [Actor[akka:&#x2F;&#x2F;flink&#x2F;user&#x2F;taskmanager_0#15608456]] after [10000 ms]. </span><br><span class="line">Sender[null] sent message of type &quot;org.apache.flink.runtime.rpc.messages.LocalRpcInvocation&quot;.</span><br></pre></td></tr></table></figure><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/FkaM6A.jpg" alt="img"></p><p>跟着这问题在 Flink 的 Issue 列表里看到了一个类似的问题：<a href="https://gitbook.cn/gitchat/column/undefined/topic/5db6bf5cf6a6211cb961664b">https://issues.apache.org/jira/browse/FLINK-9056</a><a href="https://issues.apache.org/jira/browse/FLINK-9056">https://issues.apache.org/jira/browse/FLINK-9056</a> ，看下面的评论意思大概就是 TaskManager 的 Slot 数量不足导致的 Job 提交失败，在 Flink 1.63 中已经修复了，变成抛出异常了。</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/p4Tr9Z.jpg" alt="img"></p><p>竟然知道了是因为 Slot 不足的原因了，那么我们就要先了解下 Slot 是什么呢？不过再了解 Slot 之前这里先介绍下 parallelism。</p><h3 id="什么是-Parallelism？"><a href="#什么是-Parallelism？" class="headerlink" title="什么是 Parallelism？"></a>什么是 Parallelism？</h3><p>parallelism 是并行的意思，在 Flink 里面代表每个算子的并行度，适当的提高并行度可以大大提高 Job 的执行效率，比如你的 Job 消费 Kafka 数据过慢，适当调大可能就消费正常了。</p><p>那么在 Flink 中怎么设置并行度呢？</p><h3 id="如何设置-Parallelism？"><a href="#如何设置-Parallelism？" class="headerlink" title="如何设置 Parallelism？"></a>如何设置 Parallelism？</h3><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-06-055925.png" alt="img"></p><p>如上图，在 Flink 配置文件中可以看到默认并行度是 1。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cat flink-conf.yaml | grep parallelism</span><br><span class="line"></span><br><span class="line"># The parallelism used for programs that did not specify and other parallelism.</span><br><span class="line">parallelism.default: 1</span><br></pre></td></tr></table></figure><p>所以如果在你的 Flink Job 里面不设置任何 parallelism 的话，那么它也会有一个默认的 parallelism（默认为 1），那也意味着可以修改这个配置文件的默认并行度来提高 Job 的执行效率。如果是使用命令行启动你的 Flink Job，那么你也可以这样设置并行度(使用 -p n 参数)：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">.&#x2F;bin&#x2F;flink run -p 10 &#x2F;Users&#x2F;zhisheng&#x2F;word-count.jar</span><br></pre></td></tr></table></figure><p>你也可以通过 <code>env.setParallelism(n)</code> 来设置整个程序的并行度：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">StreamExecutionEnvironment env &#x3D; StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">env.setParallelism(10);</span><br></pre></td></tr></table></figure><p>注意：这样设置的并行度是整个程序的并行度，那么后面如果每个算子不单独设置并行度覆盖的话，那么后面每个算子的并行度就都是以这里设置的并行度为准了。如何给每个算子单独设置并行度呢？</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">data.keyBy(<span class="keyword">new</span> xxxKey())</span><br><span class="line">    .flatMap(<span class="keyword">new</span> XxxFlatMapFunction()).setParallelism(<span class="number">5</span>)</span><br><span class="line">    .map(<span class="keyword">new</span> XxxMapFunction).setParallelism(<span class="number">5</span>)</span><br><span class="line">    .addSink(<span class="keyword">new</span> XxxSink()).setParallelism(<span class="number">1</span>)</span><br></pre></td></tr></table></figure><p>如上就是给每个算子单独设置并行度，这样的话，就算程序设置了 <code>env.setParallelism(10)</code> 也是会被覆盖的。这也说明优先级是：算子设置并行度 &gt; env 设置并行度 &gt; 配置文件默认并行度。</p><p>并行度讲到这里应该都懂了，下面就继续讲什么是 Slot？</p><h3 id="什么是-Slot？"><a href="#什么是-Slot？" class="headerlink" title="什么是 Slot？"></a>什么是 Slot？</h3><p>其实 Slot 的概念在 1.2 节中已经提及到，这里再细讲一点。</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/r19yJh.jpg" alt="img"></p><p>图中 TaskManager 是从 JobManager 处接收需要部署的 Task，任务能配置的最大并行度由 TaskManager 上可用的 Slot 决定。每个任务代表分配给任务槽的一组资源，Slot 在 Flink 里面可以认为是资源组，Flink 将每个任务分成子任务并且将这些子任务分配到 Slot 中，这样就可以并行的执行程序。</p><p>例如，如果 TaskManager 有四个 Slot，那么它将为每个 Slot 分配 25％ 的内存。 可以在一个 Slot 中运行一个或多个线程。 同一 Slot 中的线程共享相同的 JVM。 同一 JVM 中的任务共享 TCP 连接和心跳消息。TaskManager 的一个 Slot 代表一个可用线程，该线程具有固定的内存，注意 Slot 只对内存隔离，没有对 CPU 隔离。默认情况下，Flink 允许子任务共享 Slot，即使它们是不同 Task 的 subtask，只要它们来自相同的 Job，这种共享模式可以大大的提高资源利用率。拿下面的图片来讲解会更好些。</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/ECv5y2.jpg" alt="img"></p><p>上面图片中有两个 TaskManager，每个 TaskManager 有三个 Slot，这样我们的算子最大并行度那么就可以达到 6 个，在同一个 Slot 里面可以执行 1 至多个子任务。那么再看上面的图片，source/map/keyby/window/apply 算子最大可以设置 6 个并行度，sink 只设置了 1 个并行度。</p><p>每个 Flink TaskManager 在集群中提供 Slot，Slot 的数量通常与每个 TaskManager 的可用 CPU 内核数成比例（一般情况下 Slot 个数是每个 TaskManager 的 CPU 核数）。Flink 配置文件中设置的一个 TaskManager 默认的 Slot 是 1。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-06-062913.png" alt="img"></p><p><code>taskmanager.numberOfTaskSlots: 1</code> 该参数可以根据实际情况做一定的修改。</p><h3 id="Slot-和-Parallelism-的关系"><a href="#Slot-和-Parallelism-的关系" class="headerlink" title="Slot 和 Parallelism 的关系"></a>Slot 和 Parallelism 的关系</h3><p>下面用几张图片来更加深刻的理解下 Slot 和 Parallelism，并清楚它们之间的关系。</p><p>1、Slot 是指 TaskManager 最大能并发执行的能力</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/zpX2sh.jpg" alt="img"></p><p>如上图，如果设置的单个 TaskManager 的 Slot 个数为 3，启动 3 个 TaskManager 后，那么就一共有 9 个 Slot。</p><p>2、parallelism 是指 TaskManager 实际使用的并发能力</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/npq4kW.jpg" alt="img"></p><p>运行程序默认的并行度为 1，9 个 Slot 只用了 1 个，有 8 个处于空闲，设置合适的并行度才能提高 Job 计算效率。</p><p>3、parallelism 是可配置、可指定的</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/xAuHJn.jpg" alt="img"></p><p>上图中 example2 每个算子设置的并行度是 2， example3 每个算子设置的并行度是 9。</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/syrCLs.jpg" alt="img"></p><p>example4 除了 sink 是设置的并行度为 1，其他算子设置的并行度都是 9。</p><h3 id="可能会遇到-Slot-和-Parallelism-的问题"><a href="#可能会遇到-Slot-和-Parallelism-的问题" class="headerlink" title="可能会遇到 Slot 和 Parallelism 的问题"></a>可能会遇到 Slot 和 Parallelism 的问题</h3><p>好了，既然 Slot 和 Parallelism 大家都了解了，现在再来看前面提到的问题（Slot 资源不够），这时问题的答案就已经很明显了，就是程序设置的并行度超过了 TaskManager 可用的 Slot 数量，所以程序一直在等待资源调度并超过了一定的时间（该时间可配置），所以才会抛出该异常错误。</p><p>还原代码查找根因，当时笔者的程序设置的并行度是 30（设置 30 是因为 Kafka 分区数有 30 个，想着一个并行度去消费一个分区的数据），没曾想到 Flink 的 Slot 不够，后面了解到该情况后就降低并行度到 10，这样就意味着一个并行度要去消费 3 个 Kafka 分区的数据，调整并行度后速度还是跟的上并且再也没有抛出该异常了。注意如果调小并行度后消费速度过慢，那可以再试试调大些试试，如果还是这样，那么只能增加 TaskManager 的个数从而间接性的增加 Slot 个数来解决该问题了。</p><p>该问题对于刚接触 Flink 的来说是比较容易遇见的，如果你对 Slot 和 Parallelism 不了解的话，那么就会感觉很苦恼，相信你看完这篇文章后就能够豁然开朗了。另外可能还会有各种各样的并行度设置的问题，比如：</p><ul><li>程序某个算子执行了比较复杂的操作，延迟很久，导致该算子处理数据特别慢，那么可以考虑给该算子处增加并行度</li><li>Flink Source 处的并行度超过 Kafka 分区数，因为 Flink 的一个并行度可以处理一至多个分区的数据，如果并行度多于 Kafka 的分区数，那么就会造成有的并行度空闲，浪费资源，建议最多 Flink Source 端的并行度不要超过 Kafka 分区数</li></ul><p>总之，要做到既让 Job 能够及时消费数据，又能够节省资源，需要理解并合理设置并行度和 Slot。</p>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
            <tag> Flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>使用 Flink ParameterTool 读取配置</title>
      <link href="2019/12/28/%E4%BD%BF%E7%94%A8-Flink-ParameterTool-%E8%AF%BB%E5%8F%96%E9%85%8D%E7%BD%AE/"/>
      <url>2019/12/28/%E4%BD%BF%E7%94%A8-Flink-ParameterTool-%E8%AF%BB%E5%8F%96%E9%85%8D%E7%BD%AE/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p>Flink 配置的管理很不方便，比如像算子的并行度配置、Kafka 数据源的配置（broker 地址、topic 名、group.id）、Checkpoint 是否开启、状态后端存储路径、数据库地址、用户名和密码等，Flink 作为流计算引擎，处理源源不断的数据是其本意，但是在处理数据的过程中，往往可能需要一些参数的传递</p><a id="more"></a><h3 id="Flink-Job-配置"><a href="#Flink-Job-配置" class="headerlink" title="Flink Job 配置"></a>Flink Job 配置</h3><p>在 Flink 中其实是有几种方法来管理配置。</p><h4 id="使用-Configuration"><a href="#使用-Configuration" class="headerlink" title="使用 Configuration"></a>使用 Configuration</h4><p>Flink 提供了 withParameters 方法，它可以传递 Configuration 中的参数给，要使用它，需要实现那些 Rich 函数，比如实现 RichMapFunction，而不是 MapFunction，因为 Rich 函数中有 open 方法，然后可以重写 open 方法通过 Configuration 获取到传入的参数值。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"><span class="comment">// Configuration 类来存储参数</span></span><br><span class="line">Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">configuration.setString(<span class="string">&quot;name&quot;</span>, <span class="string">&quot;zhisheng&quot;</span>);</span><br><span class="line"></span><br><span class="line">env.fromElements(WORDS)</span><br><span class="line">        .flatMap(<span class="keyword">new</span> RichFlatMapFunction&lt;String, Tuple2&lt;String, Integer&gt;&gt;() &#123;</span><br><span class="line"></span><br><span class="line">            String name;</span><br><span class="line"></span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(Configuration parameters)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="comment">//读取配置</span></span><br><span class="line">                name = parameters.getString(<span class="string">&quot;name&quot;</span>, <span class="string">&quot;&quot;</span>);</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">flatMap</span><span class="params">(String value, Collector&lt;Tuple2&lt;String, Integer&gt;&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                String[] splits = value.toLowerCase().split(<span class="string">&quot;\\W+&quot;</span>);</span><br><span class="line"></span><br><span class="line">                <span class="keyword">for</span> (String split : splits) &#123;</span><br><span class="line">                    <span class="keyword">if</span> (split.length() &gt; <span class="number">0</span>) &#123;</span><br><span class="line">                        out.collect(<span class="keyword">new</span> Tuple2&lt;&gt;(split + name, <span class="number">1</span>));</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;).withParameters(configuration)    <span class="comment">//将参数传递给函数</span></span><br><span class="line">        .print();</span><br></pre></td></tr></table></figure><p>但是要注意这个 withParameters 只在批程序中支持，流程序中是没有该方法的，并且这个 withParameters 要在每个算子后面使用才行，并不是一次使用就所有都可以获取到，如果所有算子都要该配置，那么就重复设置多次就会比较繁琐。</p><h3 id="ParameterTool-管理配置"><a href="#ParameterTool-管理配置" class="headerlink" title="ParameterTool 管理配置"></a>ParameterTool 管理配置</h3><p>上面通过 Configuration 的局限性很大，其实在 Flink 中还可以通过使用 ParameterTool 类读取配置，它可以读取环境变量、运行参数、配置文件，下面分别讲下每种如何使用。</p><h4 id="读取运行参数"><a href="#读取运行参数" class="headerlink" title="读取运行参数"></a>读取运行参数</h4><p>我们知道 Flink UI 上是支持为每个 Job 单独传入 arguments（参数）的，它的格式要求是如下这种。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">--brokers 127.0.0.1:9200</span><br><span class="line">--username admin</span><br><span class="line">--password 123456</span><br></pre></td></tr></table></figure><p>或者这种</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">-brokers 127.0.0.1:9200</span><br><span class="line">-username admin</span><br><span class="line">-password 123456</span><br></pre></td></tr></table></figure><p>然后在 Flink 程序中你可以直接使用 <code>ParameterTool.fromArgs(args)</code> 获取到所有的参数，然后如果你要获取某个参数对应的值的话，可以通过 <code>parameterTool.get(&quot;username&quot;)</code> 方法。那么在这个地方其实你就可以将配置放在一个第三方的接口，然后这个参数值中传入一个接口，拿到该接口后就能够通过请求去获取更多你想要的配置。</p><h4 id="读取系统属性"><a href="#读取系统属性" class="headerlink" title="读取系统属性"></a>读取系统属性</h4><p>ParameterTool 还支持通过 <code>ParameterTool.fromSystemProperties()</code> 方法读取系统属性。</p><h4 id="读取配置文件"><a href="#读取配置文件" class="headerlink" title="读取配置文件"></a>读取配置文件</h4><p>除了上面两种外，ParameterTool 还支持 <code>ParameterTool.fromPropertiesFile(&quot;/application.properties&quot;)</code> 读取 properties 配置文件。你可以将所有要配置的地方（比如并行度和一些 Kafka、MySQL 等配置）都写成可配置的，然后其对应的 key 和 value 值都写在配置文件中，最后通过 ParameterTool 去读取配置文件获取对应的值。</p><h4 id="ParameterTool-获取值"><a href="#ParameterTool-获取值" class="headerlink" title="ParameterTool 获取值"></a>ParameterTool 获取值</h4><p>ParameterTool 类提供了很多便捷方法去获取值。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-09-134119.png" alt="img"></p><p>你可以在应用程序的 main() 方法中直接使用这些方法返回的值，例如：你可以按如下方法来设置一个算子的并行度：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ParameterTool parameters = ParameterTool.fromArgs(args);</span><br><span class="line"><span class="keyword">int</span> parallelism = parameters.get(<span class="string">&quot;mapParallelism&quot;</span>, <span class="number">2</span>);</span><br><span class="line">DataStream&lt;Tuple2&lt;String, Integer&gt;&gt; counts = data.flatMap(<span class="keyword">new</span> Tokenizer()).setParallelism(parallelism);</span><br></pre></td></tr></table></figure><p>因为 ParameterTool 是可序列化的，所以你可以将它当作参数进行传递给自定义的函数。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ParameterTool parameters = ParameterTool.fromArgs(args);</span><br><span class="line">DataStream&lt;Tuple2&lt;String, Integer&gt;&gt; counts = dara.flatMap(<span class="keyword">new</span> Tokenizer(parameters));</span><br></pre></td></tr></table></figure><p>然后在函数内部使用 ParameterTool 来获取命令行参数，这样就意味着你在作业任何地方都可以获取到参数，而不是像 withParameters 一样需要每次都设置。</p><h4 id="注册全局参数"><a href="#注册全局参数" class="headerlink" title="注册全局参数"></a>注册全局参数</h4><p>在 ExecutionConfig 中可以将 ParameterTool 注册为全作业参数的参数，这样就可以被 JobManager 的 web 端以及用户自定义函数中以配置值的形式访问。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">env.getConfig().setGlobalJobParameters(ParameterTool.fromArgs(args));</span><br></pre></td></tr></table></figure><p>然后就可以在用户自定义的 Rich 函数中像如下这样获取到参数值了。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">env.addSource(<span class="keyword">new</span> RichSourceFunction&lt;String&gt;() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">(SourceContext&lt;String&gt; sourceContext)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">            ParameterTool parameterTool = (ParameterTool) getRuntimeContext().getExecutionConfig().getGlobalJobParameters();</span><br><span class="line">            sourceContext.collect(System.currentTimeMillis() + parameterTool.get(<span class="string">&quot;os.name&quot;</span>) + parameterTool.get(<span class="string">&quot;user.home&quot;</span>));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">cancel</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure><p>在笔者公司内通常是以 Job 运行的环境变量为准，比如我们是运行在 K8s 上面，那么我们会为我们的这个 Flink Job 设置很多环境变量，设置的环境变量的值就得通过 ParameterTool 类去获取，我们是会优先根据环境变量的值为准，如果环境变量的值没有就会去读取应用运行参数，如果应用运行参数也没有才会去读取之前已经写好在配置文件中的配置。大概代码如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> ParameterTool <span class="title">createParameterTool</span><span class="params">(<span class="keyword">final</span> String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> ParameterTool</span><br><span class="line">            .fromPropertiesFile(ExecutionEnv.class.getResourceAsStream(<span class="string">&quot;/application.properties&quot;</span>))</span><br><span class="line">            .mergeWith(ParameterTool.fromArgs(args))</span><br><span class="line">            .mergeWith(ParameterTool.fromSystemProperties())</span><br><span class="line">            .mergeWith(ParameterTool.fromMap(getenv()));<span class="comment">// mergeWith 会使用最新的配置</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//获取 Job 设置的环境变量</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">static</span> Map&lt;String, String&gt; <span class="title">getenv</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    Map&lt;String, String&gt; map = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line">    <span class="keyword">for</span> (Map.Entry&lt;String, String&gt; entry : System.getenv().entrySet()) &#123;</span><br><span class="line">        map.put(entry.getKey().toLowerCase().replace(<span class="string">&#x27;_&#x27;</span>, <span class="string">&#x27;.&#x27;</span>), entry.getValue());</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> map;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这样如果 Job 要更改一些配置，直接在 Job 在 K8s 上面的环境变量进行配置就好了，修改配置后然后重启 Job 就可以运行起来了，整个过程都不需要再次将作业重新编译打包的。但是这样其实也有一定的坏处，重启一个作业的代价很大，因为在重启后你又要去保证状态要恢复到之前未重启时的状态，尽管 Flink 中的 Checkpoint 和 Savepoint 已经很强大了，但是对于复杂的它来说我们多一事不如少一事，所以其实更希望能够直接动态的获取配置，如果配置做了更改，作业能够感知到。在 Flink 中有的配置是不能够动态设置的，但是比如应用业务配置却是可以做到动态的配置，这时就需要使用比较强大的广播变量，广播变量在之前 3.4 节已经介绍过了，如果忘记可以再回去查看，另外在 11.4 节中会通过一个实际案例来教你如何使用广播变量去动态的更新配置数据。</p><h3 id="ParameterTool-源码剖析"><a href="#ParameterTool-源码剖析" class="headerlink" title="ParameterTool 源码剖析"></a>ParameterTool 源码剖析</h3><p>ParameterTool 这个类还是比较简单的，它继承自 ExecutionConfig.GlobalJobParameters 类，然后提供了上面讲的哪几种方法去获取配置数据：</p><ul><li>fromArgs(String[] args)</li><li>fromPropertiesFile(String path)</li><li>fromPropertiesFile(File file)</li><li>fromPropertiesFile(InputStream inputStream)</li><li>fromSystemProperties()</li></ul><p>还可以传入的一个 <code>Map</code> 配置进去，这样最后也是返回一个 ParameterTool 对象。另外就是提供了好些个 get() 方法去获取不同类型的参数值，也支持通过 mergeWith 方法来将两个不同的 ParameterTool 类进行合并，优先以新传入的参数为准，因为内部是使用的 Map 来存储的，mergeWith 操作会将新的 ParameterTool 数据全部 putAll 进一个 Map 集合中，所以会覆盖前一个相同 key 的值。</p>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
            <tag> Flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink CEP API 学习</title>
      <link href="2019/12/26/Flink-CEP-API-%E5%AD%A6%E4%B9%A0/"/>
      <url>2019/12/26/Flink-CEP-API-%E5%AD%A6%E4%B9%A0/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><h3 id="准备依赖"><a href="#准备依赖" class="headerlink" title="准备依赖"></a>准备依赖</h3><p>要开发 Flink CEP 应用程序，首先你得在项目的 <code>pom.xml</code> 中添加依赖。</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-cep_$&#123;scala.binary.version&#125;<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><p>这个依赖有两种，一个是 Java 版本的，一个是 Scala 版本，根据项目的开发语言自行选择。</p><a id="more"></a><h3 id="Flink-CEP-应用入门"><a href="#Flink-CEP-应用入门" class="headerlink" title="Flink CEP 应用入门"></a>Flink CEP 应用入门</h3><p>准备好依赖后，我们开始第一个 Flink CEP 应用程序，这里我们只做一个简单的数据流匹配，当匹配成功后将匹配的两条数据打印出来。首先定义实体类 Event 如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Event</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> Integer id;</span><br><span class="line">    <span class="keyword">private</span> String name;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>然后构造读取 Socket 数据流将数据进行转换成 Event，代码如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">SingleOutputStreamOperator&lt;Event&gt; eventDataStream = env.socketTextStream(<span class="string">&quot;127.0.0.1&quot;</span>, <span class="number">9200</span>)</span><br><span class="line">    .flatMap(<span class="keyword">new</span> FlatMapFunction&lt;String, Event&gt;() &#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">flatMap</span><span class="params">(String s, Collector&lt;Event&gt; collector)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">            <span class="keyword">if</span> (StringUtil.isNotEmpty(s)) &#123;</span><br><span class="line">                String[] split = s.split(<span class="string">&quot;,&quot;</span>);</span><br><span class="line">                <span class="keyword">if</span> (split.length == <span class="number">2</span>) &#123;</span><br><span class="line">                    collector.collect(<span class="keyword">new</span> Event(Integer.valueOf(split[<span class="number">0</span>]), split[<span class="number">1</span>]));</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;);</span><br></pre></td></tr></table></figure><p>接着就是定义 CEP 中的匹配规则了，下面的规则表示第一个事件的 id 为 42，紧接着的第二个事件 id 要大于 10，满足这样的连续两个事件才会将这两条数据进行打印出来。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">Pattern&lt;Event, ?&gt; pattern = Pattern.&lt;Event&gt;begin(<span class="string">&quot;start&quot;</span>).where(</span><br><span class="line">        <span class="keyword">new</span> SimpleCondition&lt;Event&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">filter</span><span class="params">(Event event)</span> </span>&#123;</span><br><span class="line">                log.info(<span class="string">&quot;start &#123;&#125;&quot;</span>, event.getId());</span><br><span class="line">                <span class="keyword">return</span> event.getId() == <span class="number">42</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">).next(<span class="string">&quot;middle&quot;</span>).where(</span><br><span class="line">        <span class="keyword">new</span> SimpleCondition&lt;Event&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">filter</span><span class="params">(Event event)</span> </span>&#123;</span><br><span class="line">                log.info(<span class="string">&quot;middle &#123;&#125;&quot;</span>, event.getId());</span><br><span class="line">                <span class="keyword">return</span> event.getId() &gt;= <span class="number">10</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">);</span><br><span class="line"></span><br><span class="line">CEP.pattern(eventDataStream, pattern).select(<span class="keyword">new</span> PatternSelectFunction&lt;Event, String&gt;() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">select</span><span class="params">(Map&lt;String, List&lt;Event&gt;&gt; p)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        StringBuilder builder = <span class="keyword">new</span> StringBuilder();</span><br><span class="line">        log.info(<span class="string">&quot;p = &#123;&#125;&quot;</span>, p);</span><br><span class="line">        builder.append(p.get(<span class="string">&quot;start&quot;</span>).get(<span class="number">0</span>).getId()).append(<span class="string">&quot;,&quot;</span>).append(p.get(<span class="string">&quot;start&quot;</span>).get(<span class="number">0</span>).getName()).append(<span class="string">&quot;\n&quot;</span>)</span><br><span class="line">                .append(p.get(<span class="string">&quot;middle&quot;</span>).get(<span class="number">0</span>).getId()).append(<span class="string">&quot;,&quot;</span>).append(p.get(<span class="string">&quot;middle&quot;</span>).get(<span class="number">0</span>).getName());</span><br><span class="line">        <span class="keyword">return</span> builder.toString();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;).print();<span class="comment">//打印结果</span></span><br></pre></td></tr></table></figure><p>然后笔者在终端开启 Socket，输入的两条数据如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">42,zhisheng</span><br><span class="line">20,zhisheng</span><br></pre></td></tr></table></figure><p>作业打印出来的日志如下图：</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-29-072247.png" alt="img"></p><p>整个作业 print 出来的结果如下图：</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-29-072320.png" alt="img"></p><p>好了，一个完整的 Flink CEP 应用程序如上，相信你也能大概理解上面的代码，接着来详细的讲解一下 Flink CEP 中的 Pattern API。</p><h3 id="Pattern-API"><a href="#Pattern-API" class="headerlink" title="Pattern API"></a>Pattern API</h3><p>你可以通过 Pattern API 去定义从流数据中匹配事件的 Pattern，每个复杂 Pattern 都是由多个简单的 Pattern 组成的，拿前面入门的应用来讲，它就是由 <code>start</code> 和 <code>middle</code> 两个简单的 Pattern 组成的，在其每个 Pattern 中都只是简单的处理了流数据。在处理的过程中需要标示该 Pattern 的名称，以便后续可以使用该名称来获取匹配到的数据，如 <code>p.get(&quot;start&quot;).get(0)</code> 它就可以获取到 Pattern 中匹配的第一个事件。接下来我们先来看下简单的 Pattern 。</p><h4 id="单个-Pattern"><a href="#单个-Pattern" class="headerlink" title="单个 Pattern"></a>单个 Pattern</h4><h5 id="数量"><a href="#数量" class="headerlink" title="数量"></a>数量</h5><p>单个 Pattern 后追加的 Pattern 如果都是相同的，那如果要都重新再写一遍，换做任何人都会比较痛苦，所以就提供了 times(n) 来表示期望出现的次数，该 times() 方法还有很多写法，如下所示：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"> <span class="comment">//期望符合的事件出现 4 次</span></span><br><span class="line"> start.times(<span class="number">4</span>);</span><br><span class="line"></span><br><span class="line"> <span class="comment">//期望符合的事件不出现或者出现 4 次</span></span><br><span class="line"> start.times(<span class="number">4</span>).optional();</span><br><span class="line"></span><br><span class="line">  <span class="comment">//期望符合的事件出现 2 次或者 3 次或者 4 次</span></span><br><span class="line"> start.times(<span class="number">2</span>, <span class="number">4</span>);</span><br><span class="line"></span><br><span class="line"> <span class="comment">//期望出现 2 次、3 次或 4 次，并尽可能多地重复</span></span><br><span class="line"> start.times(<span class="number">2</span>, <span class="number">4</span>).greedy();</span><br><span class="line"></span><br><span class="line"><span class="comment">//期望出现 2 次、3 次、4 次或者不出现</span></span><br><span class="line"> start.times(<span class="number">2</span>, <span class="number">4</span>).optional();</span><br><span class="line"></span><br><span class="line"> <span class="comment">//期望出现 0、2、3 或 4 次并尽可能多地重复</span></span><br><span class="line"> start.times(<span class="number">2</span>, <span class="number">4</span>).optional().greedy();</span><br><span class="line"></span><br><span class="line"> <span class="comment">//期望出现一个或多个事件</span></span><br><span class="line"> start.oneOrMore();</span><br><span class="line"></span><br><span class="line"> <span class="comment">//期望出现一个或多个事件，并尽可能多地重复这些事件</span></span><br><span class="line"> start.oneOrMore().greedy();</span><br><span class="line"></span><br><span class="line"> <span class="comment">//期望出现一个或多个事件或者不出现</span></span><br><span class="line"> start.oneOrMore().optional();</span><br><span class="line"></span><br><span class="line"> <span class="comment">//期望出现更多次，并尽可能多地重复或者不出现</span></span><br><span class="line"> start.oneOrMore().optional().greedy();</span><br><span class="line"></span><br><span class="line"> <span class="comment">//期望出现两个或多个事件</span></span><br><span class="line"> start.timesOrMore(<span class="number">2</span>);</span><br><span class="line"></span><br><span class="line"> <span class="comment">//期望出现 2 次或 2 次以上，并尽可能多地重复</span></span><br><span class="line"> start.timesOrMore(<span class="number">2</span>).greedy();</span><br><span class="line"></span><br><span class="line"> <span class="comment">//期望出现 2 次或更多的事件，并尽可能多地重复或者不出现</span></span><br><span class="line"> start.timesOrMore(<span class="number">2</span>).optional().greedy();</span><br></pre></td></tr></table></figure><h5 id="条件"><a href="#条件" class="headerlink" title="条件"></a>条件</h5><p>可以通过 <code>pattern.where()</code>、<code>pattern.or()</code> 或 <code>pattern.until()</code> 方法指定事件属性的条件。条件可以是 <code>IterativeConditions</code> 或<code>SimpleConditions</code>。比如 SimpleCondition 可以像下面这样使用：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">start.where(<span class="keyword">new</span> SimpleCondition&lt;Event&gt;() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">filter</span><span class="params">(Event value)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&quot;zhisheng&quot;</span>.equals(value.getName());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure><h4 id="组合-Pattern"><a href="#组合-Pattern" class="headerlink" title="组合 Pattern"></a>组合 Pattern</h4><p>前面已经对单个 Pattern 做了详细对讲解，接下来讲解如何将多个 Pattern 进行组合来完成一些需求。在完成组合 Pattern 之前需要定义第一个 Pattern，然后在第一个的基础上继续添加新的 Pattern。比如定义了第一个 Pattern 如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Pattern&lt;Event, ?&gt; start = Pattern.&lt;Event&gt;begin(<span class="string">&quot;start&quot;</span>);</span><br></pre></td></tr></table></figure><p>接下来，可以为此指定更多的 Pattern，通过指定的不同的连接条件。比如：</p><ul><li>next()：要求比较严格，该事件一定要紧跟着前一个事件。</li><li>followedBy()：该事件在前一个事件后面就行，两个事件之间可能会有其他的事件。</li><li>followedByAny()：该事件在前一个事件后面的就满足条件，两个事件之间可能会有其他的事件，返回值比上一个多。</li><li>notNext()：不希望前一个事件后面紧跟着该事件出现。</li><li>notFollowedBy()：不希望后面出现该事件。</li></ul><p>具体怎么写呢，可以看下样例：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Pattern&lt;Event, ?&gt; strict = start.next(<span class="string">&quot;middle&quot;</span>).where(...);</span><br><span class="line"></span><br><span class="line">Pattern&lt;Event, ?&gt; relaxed = start.followedBy(<span class="string">&quot;middle&quot;</span>).where(...);</span><br><span class="line"></span><br><span class="line">Pattern&lt;Event, ?&gt; nonDetermin = start.followedByAny(<span class="string">&quot;middle&quot;</span>).where(...);</span><br><span class="line"></span><br><span class="line">Pattern&lt;Event, ?&gt; strictNot = start.notNext(<span class="string">&quot;not&quot;</span>).where(...);</span><br><span class="line"></span><br><span class="line">Pattern&lt;Event, ?&gt; relaxedNot = start.notFollowedBy(<span class="string">&quot;not&quot;</span>).where(...);</span><br></pre></td></tr></table></figure><p>可能概念讲了很多，但是还是不太清楚，这里举个例子说明一下，假设有个 Pattern 是 <code>a b</code>，给定的数据输入顺序是 <code>a c b b</code>，对于上面那种不同的连接条件可能最后返回的值不一样。</p><ol><li>a 和 b 之间使用 next() 连接，那么则返回 {}，即没有匹配到数据</li><li>a 和 b 之间使用 followedBy() 连接，那么则返回 {a, b}</li><li>a 和 b 之间使用 followedByAny() 连接，那么则返回 {a, b}, {a, b}</li></ol><p>相信通过上面的这个例子讲解你就知道了它们的区别，尤其是 followedBy() 和 followedByAny()，笔者一开始也是毕竟懵，后面也是通过代码测试才搞明白它们之间的区别的。除此之外，还可以为 Pattern 定义时间约束。例如，可以通过 <code>pattern.within(Time.seconds(10))</code> 方法定义此 Pattern 应该 10 秒内完成匹配。 该时间不仅支持处理时间还支持事件时间。另外还可以与 consecutive()、allowCombinations() 等组合，更多的请看下图中 Pattern 类的方法。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-29-164118.png" alt="img"></p><h4 id="Group-Pattern"><a href="#Group-Pattern" class="headerlink" title="Group Pattern"></a>Group Pattern</h4><p>业务需求比较复杂的场景，如果要使用 Pattern 来定义的话，可能这个 Pattern 会很长并且还会嵌套，比如由 begin、followedBy、followedByAny、next 组成和嵌套，另外还可以再和 oneOrMore()、times(#ofTimes)、times(#fromTimes, #toTimes)、optional()、consecutive()、allowCombinations() 等结合使用。效果如下面这种：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">Pattern&lt;Event, ?&gt; start = Pattern.begin(</span><br><span class="line">    Pattern.&lt;Event&gt;begin(<span class="string">&quot;start&quot;</span>).where(...).followedBy(<span class="string">&quot;start_middle&quot;</span>).where(...)</span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="comment">//next 表示连续</span></span><br><span class="line">Pattern&lt;Event, ?&gt; strict = start.next(</span><br><span class="line">    Pattern.&lt;Event&gt;begin(<span class="string">&quot;next_start&quot;</span>).where(...).followedBy(<span class="string">&quot;next_middle&quot;</span>).where(...)</span><br><span class="line">).times(<span class="number">3</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">//followedBy 代表在后面就行</span></span><br><span class="line">Pattern&lt;Event, ?&gt; relaxed = start.followedBy(</span><br><span class="line">    Pattern.&lt;Event&gt;begin(<span class="string">&quot;followedby_start&quot;</span>).where(...).followedBy(<span class="string">&quot;followedby_middle&quot;</span>).where(...)</span><br><span class="line">).oneOrMore();</span><br><span class="line"></span><br><span class="line"><span class="comment">//followedByAny</span></span><br><span class="line">Pattern&lt;Event, ?&gt; nonDetermin = start.followedByAny(</span><br><span class="line">    Pattern.&lt;Event&gt;begin(<span class="string">&quot;followedbyany_start&quot;</span>).where(...).followedBy(<span class="string">&quot;followedbyany_middle&quot;</span>).where(...)</span><br><span class="line">).optional();</span><br></pre></td></tr></table></figure><p>关于上面这些 Pattern 操作的更详细的解释可以查看<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.9/dev/libs/cep.html#groups-of-patterns">官网</a>。</p><h4 id="事件匹配跳过策略"><a href="#事件匹配跳过策略" class="headerlink" title="事件匹配跳过策略"></a>事件匹配跳过策略</h4><p>对于给定组合的复杂 Pattern，有的事件可能会匹配到多个 Pattern，如果要控制将事件的匹配数，需要指定跳过策略。在 Flink CEP 中跳过策略有四种类型，如下所示：</p><ul><li>NO_SKIP：不跳过，将发出所有可能的匹配事件。</li><li>SKIP_TO_FIRST：丢弃包含 PatternName 第一个之前匹配事件的每个部分匹配。</li><li>SKIP_TO_LAST：丢弃包含 PatternName 最后一个匹配事件之前的每个部分匹配。</li><li>SKIP_PAST_LAST_EVENT：丢弃包含匹配事件的每个部分匹配。</li><li>SKIP_TO_NEXT：丢弃以同一事件开始的所有部分匹配。</li></ul><p>这几种策略都是根据 AfterMatchSkipStrategy 来实现的，可以看下它们的类结构图，如下所示：</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-29-133737.png" alt="img"></p><p>关于这几种跳过策略的具体区别可以查看<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.9/dev/libs/cep.html#after-match-skip-strategy">官网</a>，至于如何使用跳过策略，其实 AfterMatchSkipStrategy 抽象类中已经提供了 5 种静态方法可以直接使用，方法如下：</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-29-135526.png" alt="img"></p><p>使用方法如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">AfterMatchSkipStrategy skipStrategy = ...; <span class="comment">// 使用 AfterMatchSkipStrategy 调用不同的静态方法</span></span><br><span class="line">Pattern.begin(<span class="string">&quot;start&quot;</span>, skipStrategy);</span><br></pre></td></tr></table></figure><h3 id="检测-Pattern"><a href="#检测-Pattern" class="headerlink" title="检测 Pattern"></a>检测 Pattern</h3><p>编写好了 Pattern 之后，你需要的是将其应用在流数据中去做匹配。这时要做的就是构造一个 PatternStream，它可以通过 <code>CEP.pattern(eventDataStream, pattern)</code> 来获取一个 PatternStream 对象，在 <code>CEP.pattern()</code> 方法中，你可以选择传入两个参数（DataStream 和 Pattern），也可以选择传入三个参数 （DataStream、Pattern 和 EventComparator），因为 CEP 类中它有两个不同参数数量的 pattern 方法。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CEP</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> &lt;T&gt; <span class="function">PatternStream&lt;T&gt; <span class="title">pattern</span><span class="params">(DataStream&lt;T&gt; input, Pattern&lt;T, ?&gt; pattern)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> PatternStream(input, pattern);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> &lt;T&gt; <span class="function">PatternStream&lt;T&gt; <span class="title">pattern</span><span class="params">(DataStream&lt;T&gt; input, Pattern&lt;T, ?&gt; pattern, EventComparator&lt;T&gt; comparator)</span> </span>&#123;</span><br><span class="line">        PatternStream&lt;T&gt; stream = <span class="keyword">new</span> PatternStream(input, pattern);</span><br><span class="line">        <span class="keyword">return</span> stream.withComparator(comparator);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="选择-Pattern"><a href="#选择-Pattern" class="headerlink" title="选择 Pattern"></a>选择 Pattern</h4><p>在获取到 PatternStream 后，你可以通过 select 或 flatSelect 方法从匹配到的事件流中查询。如果使用的是 select 方法，则需要实现传入一个 PatternSelectFunction 的实现作为参数，PatternSelectFunction 具有为每个匹配事件调用的 select 方法，该方法的参数是 <code>Map&gt;</code>，这个 Map 的 key 是 Pattern 的名字，在前面入门案例中设置的 <code>start</code> 和 <code>middle</code> 在这时就起作用了，你可以通过类似 <code>get(&quot;start&quot;)</code> 方法的形式来获取匹配到 <code>start</code>的所有事件。如果使用的是 flatSelect 方法，则需要实现传入一个 PatternFlatSelectFunction 的实现作为参数，这个和 PatternSelectFunction 不一致地方在于它可以返回多个结果，因为这个接口中的 flatSelect 方法含有一个 Collector，它可以返回多个数据到下游去。两者的样例如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">CEP.pattern(eventDataStream, pattern).select(<span class="keyword">new</span> PatternSelectFunction&lt;Event, String&gt;() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">select</span><span class="params">(Map&lt;String, List&lt;Event&gt;&gt; p)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        StringBuilder builder = <span class="keyword">new</span> StringBuilder();</span><br><span class="line">        builder.append(p.get(<span class="string">&quot;start&quot;</span>).get(<span class="number">0</span>).getId()).append(<span class="string">&quot;,&quot;</span>).append(p.get(<span class="string">&quot;start&quot;</span>).get(<span class="number">0</span>).getName()).append(<span class="string">&quot;\n&quot;</span>)</span><br><span class="line">                .append(p.get(<span class="string">&quot;middle&quot;</span>).get(<span class="number">0</span>).getId()).append(<span class="string">&quot;,&quot;</span>).append(p.get(<span class="string">&quot;middle&quot;</span>).get(<span class="number">0</span>).getName());</span><br><span class="line">        <span class="keyword">return</span> builder.toString();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;).print();</span><br><span class="line"></span><br><span class="line">CEP.pattern(eventDataStream, pattern).flatSelect(<span class="keyword">new</span> PatternFlatSelectFunction&lt;Event, String&gt;() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">flatSelect</span><span class="params">(Map&lt;String, List&lt;Event&gt;&gt; map, Collector&lt;String&gt; collector)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">for</span> (Map.Entry&lt;String, List&lt;Event&gt;&gt; entry : map.entrySet()) &#123;</span><br><span class="line">            collector.collect(entry.getKey() + <span class="string">&quot; &quot;</span> + entry.getValue().get(<span class="number">0</span>).getId() + <span class="string">&quot;,&quot;</span> + entry.getValue().get(<span class="number">0</span>).getName());</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;).print();</span><br></pre></td></tr></table></figure><p>关于 PatternStream 中的 select 或 flatSelect 方法其实可以传入不同的参数，比如传入 OutputTag 和 PatternTimeoutFunction 去处理延迟的数据，具体查看下图。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-29-125416.png" alt="img"></p><p>如果使用的 Flink CEP 版本是大于等于 1.8 的话，还可以使用 process 方法，在上图中也可以看到在 PatternStream 类中包含了该方法。要使用 process 的话，得传入一个 PatternProcessFunction 的实现作为参数，在该实现中需要重写 processMatch 方法。使用 PatternProcessFunction 比使用 PatternSelectFunction 和 PatternFlatSelectFunction 更好的是，它支持获取应用的的上下文，那么也就意味着它可以访问时间（因为 Context 接口继承自 TimeContext 接口）。另外如果要处理延迟的数据可以与 TimedOutPartialMatchHandler 接口的实现类一起使用。</p><h3 id="CEP-时间属性"><a href="#CEP-时间属性" class="headerlink" title="CEP 时间属性"></a>CEP 时间属性</h3><h4 id="根据事件时间处理延迟数据"><a href="#根据事件时间处理延迟数据" class="headerlink" title="根据事件时间处理延迟数据"></a>根据事件时间处理延迟数据</h4><p>在 CEP 中，元素处理的顺序很重要，当时间策略设置为事件时间时，为了确保能够按照事件时间的顺序来处理元素，先来的事件会暂存在缓冲区域中，然后对缓冲区域中的这些事件按照事件时间进行排序，当水印到达时，比水印时间小的事件会按照顺序依次处理的。这意味着水印之间的元素是按照事件时间顺序处理的。</p><p>注意：当作业设置的时间属性是事件时间是，CEP 中会认为收到的水印时间是正确的，会严格按照水印的时间来处理元素，从而保证能顺序的处理元素。另外对于这种延迟的数据（和 3.5 节中的延迟数据类似），CEP 中也是支持通过 side output 设置 OutputTag 标签来将其收集。使用方式如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">PatternStream&lt;Event&gt; patternStream = CEP.pattern(inputDataStream, pattern);</span><br><span class="line"></span><br><span class="line">OutputTag&lt;String&gt; lateDataOutputTag = <span class="keyword">new</span> OutputTag&lt;String&gt;(<span class="string">&quot;late-data&quot;</span>)&#123;&#125;;</span><br><span class="line"></span><br><span class="line">SingleOutputStreamOperator&lt;ComplexEvent&gt; result = patternStream</span><br><span class="line">    .sideOutputLateData(lateDataOutputTag)</span><br><span class="line">    .select(</span><br><span class="line">        <span class="keyword">new</span> PatternSelectFunction&lt;Event, ComplexEvent&gt;() &#123;...&#125;</span><br><span class="line">    );</span><br><span class="line"></span><br><span class="line">DataStream&lt;String&gt; lateData = result.getSideOutput(lateDataOutputTag);</span><br></pre></td></tr></table></figure><h4 id="时间上下文"><a href="#时间上下文" class="headerlink" title="时间上下文"></a>时间上下文</h4><p>在 PatternProcessFunction 和 IterativeCondition 中可以通过 TimeContext 访问当前正在处理的事件的时间（Event Time）和此时机器上的时间（Processing Time）。你可以查看到这两个类中都包含了 Context，而这个 Context 继承自 TimeContext，在 TimeContext 接口中定义了获取事件时间和处理时间的方法。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">TimeContext</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">long</span> <span class="title">timestamp</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">long</span> <span class="title">currentProcessingTime</span><span class="params">()</span></span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
            <tag> Flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink CEP深入理解</title>
      <link href="2019/12/26/Flink-CEP%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3/"/>
      <url>2019/12/26/Flink-CEP%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p>CEP 的英文全称是 Complex Event Processing，翻译成中文为复杂事件处理。它可以用于处理实时数据并在事件流到达时从事件流中提取信息，并根据定义的规则来判断事件是否匹配，如果匹配则会触发新的事件做出响应。除了支持单个事件的简单无状态的模式匹配（例如基于事件中的某个字段进行筛选过滤），也可以支持基于关联／聚合／时间窗口等多个事件的复杂有状态模式的匹配（例如判断用户下单事件后 30 分钟内是否有支付事件）。</p><a id="more"></a><h3 id="CEP-是什么？"><a href="#CEP-是什么？" class="headerlink" title="CEP 是什么？"></a>CEP 是什么？</h3><p>CEP 的英文全称是 Complex Event Processing，翻译成中文为复杂事件处理。</p><p><font color='blue'>CEP 可以用于处理实时数据并在事件流到达时从事件流中提取信息，根据定义的规则来判断事件是否匹配，如果匹配则会触发新的事件做出响应。除了支持单个事件的简单无状态的模式匹配（例如基于事件中的某个字段进行筛选过滤），也可以支持基于关联／聚合／时间窗口等多个事件的复杂有状态模式的匹配（例如判断用户下单事件后 30 分钟内是否有支付事件）。</font></p><p>因为这种事件匹配通常是根据提前制定好的规则去匹配的，而这些规则一般来说不仅多，而且复杂，所以就会引入一些规则引擎来处理这种复杂事件匹配。市面上常用的规则引擎有如下这些。</p><h3 id="规则引擎对比"><a href="#规则引擎对比" class="headerlink" title="规则引擎对比"></a>规则引擎对比</h3><h4 id="Drools"><a href="#Drools" class="headerlink" title="Drools"></a>Drools</h4><p>Drools 是一款使用 Java 编写的开源规则引擎，通常用来解决业务代码与业务规则的分离，它内置的 Drools Fusion 模块也提供 CEP 的功能。</p><p>优势：</p><ul><li>功能较为完善，具有如系统监控、操作平台等功能。</li><li>规则支持动态更新。</li></ul><p>劣势：</p><ul><li>以内存实现时间窗功能，无法支持较长跨度的时间窗。</li><li>无法有效支持定时触达（如用户在浏览发生一段时间后触达条件判断）。</li></ul><h4 id="Aviator"><a href="#Aviator" class="headerlink" title="Aviator"></a>Aviator</h4><p>Aviator 是一个高性能、轻量级的 Java 语言实现的表达式求值引擎，主要用于各种表达式的动态求值。</p><p>优势：</p><ul><li>支持大部分运算操作符。</li><li>支持函数调用和自定义函数。</li><li>支持正则表达式匹配。</li><li>支持传入变量并且性能优秀。</li></ul><p>劣势：</p><ul><li>没有 if else、do while 等语句，没有赋值语句，没有位运算符。</li></ul><h4 id="EasyRules"><a href="#EasyRules" class="headerlink" title="EasyRules"></a>EasyRules</h4><p>EasyRules 集成了 MVEL 和 SpEL 表达式的一款轻量级规则引擎。</p><p>优势：</p><ul><li>轻量级框架，学习成本低。</li><li>基于 POJO。</li><li>为定义业务引擎提供有用的抽象和简便的应用</li><li>支持从简单的规则组建成复杂规则</li></ul><h4 id="Esper"><a href="#Esper" class="headerlink" title="Esper"></a>Esper</h4><p>Esper 设计目标为 CEP 的轻量级解决方案，可以方便的嵌入服务中，提供 CEP 功能。</p><p>优势：</p><ul><li>轻量级可嵌入开发，常用的 CEP 功能简单好用。</li><li>EPL 语法与 SQL 类似，学习成本较低。</li></ul><p>劣势：</p><ul><li>单机全内存方案，需要整合其他分布式和存储。</li><li>以内存实现时间窗功能，无法支持较长跨度的时间窗。</li><li>无法有效支持定时触达（如用户在浏览发生一段时间后触达条件判断）。</li></ul><h4 id="Flink-CEP"><a href="#Flink-CEP" class="headerlink" title="Flink CEP"></a>Flink CEP</h4><p>Flink 是一个流式系统，具有高吞吐低延迟的特点，Flink CEP 是一套极具通用性、易于使用的实时流式事件处理方案。</p><p>优势：</p><ul><li>继承了 Flink 高吞吐的特点</li><li>事件支持存储到外部，可以支持较长跨度的时间窗。</li><li>可以支持定时触达（用 followedBy ＋ PartternTimeoutFunction 实现）</li></ul><p>劣势：</p><ul><li>无法动态更新规则（痛点）</li></ul><h3 id="Flink-CEP-介绍"><a href="#Flink-CEP-介绍" class="headerlink" title="Flink CEP 介绍"></a>Flink CEP 介绍</h3><p>Flink CEP是在 Flink 之上实现的复杂事件处理(CEP)库，它允许我们在事件流中检测事件的模式。</p><p>因为搭配了 Flink 实时处理的能力，所以 Flink CEP 能够在流处理的场景去做一些实时的复杂事件匹配，它与传统的数据库查询是不一致的，比如，传统的数据库的数据是静态的，但是查询却是动态的，所以传统的数据库查询做不到实时的反馈查询结果，而 Flink CEP 则是查询规则是静态的，数据是动态实时的，如果它作用于一个无限的数据流上，这就意味着它可以将某种规则的数据匹配一直保持下去（除非作业停止）；另外 Flink CEP 不需要去存储那些与匹配不相关联的数据，遇到这种数据它会立即丢弃。</p><p>虽然 Flink CEP 拥有 Flink 的本身优点和支持复杂场景的规则处理，但是它本身其实也有非常严重的缺点，那就是不能够动态的更新规则。通常引入规则引擎比较友好的一点是可以将一些业务规则抽象出来成为配置，然后更改这些配置后其实是能够自动生效的，但是在 Flink 中却无法做到这点，甚至规则通常还是要写死在代码里面。</p><p><font color='grey'>举个例子，你在一个 Flink CEP 的作业中定义了一条规则：机器的 CPU 使用率连续 30 秒超过 90% 则发出告警，然后将这个作业上线，上线后发现告警很频繁，你可能会觉得可能规则之前定义的不合适，那么接下来你要做的就是将作业取消，然后重新修改代码并进行编译打包成一个 fat jar，接着上传该 jar 并运行。整个流程下来，你有没有想过会消耗多长的时间？五分钟？或者更长？但是你的目的就是要修改一个配置，如果你在作业中将上面的 30 秒和 90% 做成了配置，可能这样所需要的时间会减少，你只需要重启作业，然后通过传入新的参数将作业重新启动，但是重启作业这步是不是不能少，然而对于流作业来说，重启作业带来的代价很大。</font></p><h3 id="Flink-CEP-如何动态更新规则"><a href="#Flink-CEP-如何动态更新规则" class="headerlink" title="Flink CEP 如何动态更新规则"></a>Flink CEP 如何动态更新规则</h3><p>国内的 Flink 技术分享会却看到有几家公司对这块做了优化，让 Flink CEP 支持动态的更新规则，下面分享一下他们几家公司的思路。</p><ul><li>A 公司：用户更新规则后，新规则会被翻译成 Java 代码，并编译打包成可执行 jar，停止作业并使用 Savepoint 将状态保存下来，启动新的作业并读取之前保存的状态，会根据规则文件中的数量和复杂度对作业的数量做一个规划，防止单作业负载过高，架构如下图所示。</li></ul><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-28-142601.png" alt="img"></p><ul><li>B 公司：规则中心存储规则，规则里面直接存储了 Java 代码，加载这些规则后然后再用 Groovy 做动态编译解析，其架构如下图所示。</li></ul><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-28-143822.png" alt="img"></p><ul><li>C 公司：增加函数，在函数方法中监听规则的变化，如果需要更新则通过 Groovy 加载 Pattern 类进行动态注入，采用 Zookeeper 和 MySQL 管理规则，如果规则发生变化，则从数据库中获取到新的规则，然后更新 Flink CEP 中的 NFA 逻辑，注意状态要根据业务需要选择是否重置，其架构设计如下图所示。</li></ul><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-28-150827.png" alt="img"></p><p>第一种方法，笔者不推荐，因为它这样的做法还是要将作业重启，无非就是做了一个自动化的操作，不是人为的手动重启，从 B 公司和 C 公司两种方法可以发现要实现 Flink CEP 动态的更新规则无非要做的就是：</p><ul><li>监听规则的变化</li><li>将规则变成 Java 代码</li><li>通过 Groovy 动态编译解析</li><li>更改 NFA 的内部逻辑</li><li>状态是否保留</li></ul><h3 id="Flink-CEP-使用场景分析"><a href="#Flink-CEP-使用场景分析" class="headerlink" title="Flink CEP 使用场景分析"></a>Flink CEP 使用场景分析</h3><p>上面虽然提到了一个 Flink CEP 的痛点，但是并不能就此把它的优势给抹去，它可以运用的场景其实还有很多，这里笔者拿某些场景来做个分析。</p><h4 id="实时反作弊和风控"><a href="#实时反作弊和风控" class="headerlink" title="实时反作弊和风控"></a>实时反作弊和风控</h4><p>对于电商来说，羊毛党是必不可少的，国内拼多多曾爆出 100 元的无门槛券随便领，当晚被人褥几百亿，对于这种情况肯定是没有做好及时的风控。另外还有就是商家上架商品时通过频繁修改商品的名称和滥用标题来提高搜索关键字的排名、批量注册一批机器账号快速刷单来提高商品的销售量等作弊行为，各种各样的作弊手法也是需要不断的去制定规则去匹配这种行为。</p><h4 id="实时营销"><a href="#实时营销" class="headerlink" title="实时营销"></a>实时营销</h4><p>分析用户在手机 APP 的实时行为，统计用户的活动周期，通过为用户画像来给用户进行推荐。比如用户在登录 APP 后 1 分钟内只浏览了商品没有下单；用户在浏览一个商品后，3 分钟内又去查看其他同类的商品，进行比价行为；用户商品下单后 1 分钟内是否支付了该订单。如果这些数据都可以很好的利用起来，那么就可以给用户推荐浏览过的类似商品，这样可以大大提高购买率。</p><h4 id="实时网络攻击检测"><a href="#实时网络攻击检测" class="headerlink" title="实时网络攻击检测"></a>实时网络攻击检测</h4><p>当下互联网安全形势仍然严峻，网络攻击屡见不鲜且花样众多，这里我们以 DDOS（分布式拒绝服务攻击）产生的流入流量来作为遭受攻击的判断依据。对网络遭受的潜在攻击进行实时检测并给出预警，云服务厂商的多个数据中心会定时向监控中心上报其瞬时流量，如果流量在预设的正常范围内则认为是正常现象，不做任何操作；如果某数据中心在 10 秒内连续 5 次上报的流量超过正常范围的阈值，则触发一条警告的事件；如果某数据中心 30 秒内连续出现 30 次上报的流量超过正常范围的阈值，则触发严重的告警。</p>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
            <tag> Flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink 状态后端存储</title>
      <link href="2019/12/19/Flink-%E7%8A%B6%E6%80%81%E5%90%8E%E7%AB%AF%E5%AD%98%E5%82%A8/"/>
      <url>2019/12/19/Flink-%E7%8A%B6%E6%80%81%E5%90%8E%E7%AB%AF%E5%AD%98%E5%82%A8/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p>Flink 提供了以下三种开箱即用的<strong>状态后端</strong>(用于存储状态数据)，可以为所有 Flink 作业配置相同的状态后端，也可以为每个 Flink 作业配置指定的状态后端。当需要对具体的某一种 State 做 Checkpoint 时，此时就需要具体的状态后端存储，刚好 Flink 内置提供了不同的状态后端存储，用于指定状态的存储方式和位置。状态可以存储在 Java 堆内存中或者堆外</p><a id="more"></a><h3 id="State-Backends"><a href="#State-Backends" class="headerlink" title="State Backends"></a>State Backends</h3><p>当需要对具体的某一种 State 做 Checkpoint 时，此时就需要具体的状态后端存储，Flink 内置提供了不同的状态后端存储，用于指定状态的存储方式和位置。状态可以存储在 Java 堆内存中或者堆外，在 Flink 安装路径下 conf 目录中的 flink-conf.yaml 配置文件中也有状态后端存储相关的配置，Flink 还特有一个 CheckpointingOptions 类来控制 state 存储的相关配置，该类中有如下配置：</p><ol><li>state.backend: 用于存储和进行状态 checkpoint 的状态后端存储方式，无默认值</li><li>state.checkpoints.num-retained: 要保留的已完成 checkpoint 的最大数量，默认值为 1</li><li>state.backend.async: 状态后端是否使用异步快照方法，默认值为 true</li><li>state.backend.incremental: 状态后端是否创建增量检查点，默认值为 false</li><li>state.backend.local-recovery: 状态后端配置本地恢复，默认情况下，本地恢复被禁用</li><li>taskmanager.state.local.root-dirs: 定义存储本地恢复的基于文件的状态的目录</li><li>state.savepoints.dir: 存储 savepoints 的目录</li><li>state.checkpoints.dir: 存储 checkpoint 的数据文件和元数据</li><li>state.backend.fs.memory-threshold: 状态数据文件的最小大小，默认值是 1024</li></ol><p>虽然配置这么多，但是，Flink 还支持基于每个 Job 单独设置状态后端存储，方法如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">env.setStateBackend(<span class="keyword">new</span> MemoryStateBackend());  <span class="comment">//设置堆内存存储</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//env.setStateBackend(new FsStateBackend(checkpointDir, asyncCheckpoints));   //设置文件存储</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//env.setStateBackend(new RocksDBStateBackend(checkpointDir, incrementalCheckpoints));  //设置 RocksDB 存储</span></span><br></pre></td></tr></table></figure><p>Flink提供了以下三种开箱即用的状态后端(用于存储状态数据)，可以为所有 Flink 作业配置相同的状态后端(flink-conf.yaml )，也可以为每个 Flink 作业配置指定的状态后端。</p><ol><li>MemoryStateBackend</li><li>FsStateBackend</li><li>RocksDBStateBackend</li></ol><p><img src="../images/flink/2.png"></p><h3 id="MemoryStateBackend-使用及剖析"><a href="#MemoryStateBackend-使用及剖析" class="headerlink" title="MemoryStateBackend 使用及剖析"></a>MemoryStateBackend 使用及剖析</h3><p><font color='blue'>如果 Job 没有配置指定状态后端存储的话，就会默认采取 MemoryStateBackend 策略。</font></p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">2019-04-28 00:16:41.892 [Sink: zhisheng (1/4)] INFO  org.apache.flink.streaming.runtime.tasks.StreamTask  - No state backend has been configured, using default (Memory / Job Manager) MemoryStateBackend (data in heap memory / checkpoints to Job Manager) (checkpoints: &#x27;null&#x27;, savepoints: &#x27;null&#x27;, asynchronous: TRUE, maxStateSize: 5242880)</span><br></pre></td></tr></table></figure><p><font color='blue'>如果没有配置任何状态存储，使用默认的 MemoryStateBackend 策略，这种状态后端存储把数据以内部对象的形式保存在 Task Managers 的内存（JVM 堆）中，当应用程序触发 checkpoint 时，会将此时的状态进行快照然后存储在 Job Manager 的内存中。因为状态是存储在内存中的，所以这种情况会有点限制</font></font></p><ol><li>不适合在生产环境中使用，仅用于本地测试的情况较多，主要适用于状态很小的 Job，因为它会将状态最终存储在 Job Manager 中，如果状态较大的话，那么会使得 Job Manager 的内存比较紧张，从而导致 Job Manager 会出现 OOM 等问题，然后造成连锁反应使所有的 Job 都挂掉，所以 Job 的状态与之前的 Checkpoint 的数据所占的内存要小于 JobManager 的内存。</li><li>每个单独的状态大小不能超过最大的 DEFAULT<em>MAX</em>STATE_SIZE(5MB)，可以通过构造 MemoryStateBackend 参数传入不同大小的 maxStateSize。</li><li>Job 的操作符状态和 keyed 状态加起来都不要超过 RPC 系统的默认配置 10 MB，虽然可以修改该配置，但是不建议去修改。</li></ol><p>另外就是 MemoryStateBackend 支持配置是否是异步快照还是同步快照，它有一个字段 asynchronousSnapshots 来表示，可选值有：</p><ul><li>TRUE（true 代表使用异步的快照，这样可以避免因快照而导致数据流处理出现阻塞等问题）</li><li>FALSE（同步）</li><li>UNDEFINED（默认值）</li></ul><h3 id="FsStateBackend-使用及剖析"><a href="#FsStateBackend-使用及剖析" class="headerlink" title="FsStateBackend 使用及剖析"></a>FsStateBackend 使用及剖析</h3><p><font color='blue'>这种状态后端存储也是将工作状态存储在 Task Manager 中的内存（JVM 堆）中，但是 checkpoint 的时候，它和 MemoryStateBackend 不一样，它是将状态存储在文件（可以是本地文件，也可以是 HDFS）中，这个文件具体是哪种需要配置</font></p><p>比如：”hdfs://namenode:40010/flink/checkpoints” 或 “file://flink/checkpoints” (通常使用 HDFS 比较多，如果是使用本地文件，可能会造成 Job 恢复的时候找不到之前的 checkkpoint，因为 Job 重启后如果由调度器重新分配在不同的机器的 Task Manager 执行时就会导致这个问题，所以还是建议使用 HDFS 或者其他的分布式文件系统)。</p><p>同样 FsStateBackend 也是支持通过 asynchronousSnapshots 字段来控制是使用异步还是同步来进行 checkpoint 的，异步可以避免在状态 checkpoint 时阻塞数据流的处理，然后还有一点的就是在 FsStateBackend 有个参数 fileStateThreshold，如果状态大小比 MAX<em>FILE</em>STATE_THRESHOLD（1MB） 小的话，那么会将状态数据直接存储在 meta data 文件中，而不是存储在配置的文件中（避免出现很小的状态文件），如果该值为 “-1” 表示尚未配置，在这种情况下会使用默认值（1024，该默认值可以通过 <code>state.backend.fs.memory-threshold</code>来配置）。</p><p>那么我们该什么时候使用 FsStateBackend 呢？</p><ul><li>如果你要处理大状态，长窗口等有状态的任务，那么 FsStateBackend 就比较适合</li><li>使用分布式文件系统，如 HDFS 等，这样 failover 时 Job 的状态可以恢复</li></ul><p>使用 FsStateBackend 需要注意的地方有什么呢？</p><ul><li>工作状态仍然是存储在 Task Manager 中的内存中，虽然在 Checkpoint 的时候会存在文件中，所以还是得注意这个状态要保证不超过 Task Manager 的内存</li></ul><h3 id="如何使用-RocksDBStateBackend-及剖析"><a href="#如何使用-RocksDBStateBackend-及剖析" class="headerlink" title="如何使用 RocksDBStateBackend 及剖析"></a>如何使用 RocksDBStateBackend 及剖析</h3><p><font color='blue'>RocksDBStateBackend 和上面两种都有点不一样，RocksDB 是一种嵌入式的本地数据库，它会在本地文件系统中维护状态，KeyedStateBackend 等会直接写入本地 RocksDB 中，它还需要配置一个文件系统（一般是 HDFS），比如 <code>hdfs://namenode:40010/flink/checkpoints</code>，当触发 checkpoint 的时候，会把整个 RocksDB 数据库复制到配置的文件系统中去，当 failover 时从文件系统中将数据恢复到本地。</font></p><p>官方推荐使用 RocksDB 来作为状态的后端存储</p><ol><li>state 直接存放在 RocksDB 中，不需要存在内存中，这样就可以减少 Task Manager 的内存压力，如果是存内存的话大状态的情况下会导致 GC 次数比较多，同时还能在 checkpoint 时将状态持久化到远端的文件系统，那么就比较适合在生产环境中使用</li><li>RocksDB 本身支持 checkpoint 功能</li><li>RocksDBStateBackend 支持增量的 checkpoint，在 RocksDBStateBackend 中有一个字段 enableIncrementalCheckpointing 来确认是否开启增量的 checkpoint，默认是不开启的，在 CheckpointingOptions 类中有个 state.backend.incremental 参数来表示，增量 checkpoint 非常使用于超大状态的场景。</li></ol><p><strong>RocksDBStateBackend 这个类的相关属性以及构造函数。</strong></p><p><strong>属性</strong>：</p><ul><li>checkpointStreamBackend：用于创建 checkpoint 流的状态后端</li><li>localRocksDbDirectories：RocksDB 目录的基本路径，默认是 Task Manager 的临时目录</li><li>enableIncrementalCheckpointing：是否增量 checkpoint</li><li>numberOfTransferingThreads：用于传输(下载和上传)状态的线程数量，默认为 1</li><li>enableTtlCompactionFilter：是否启用压缩过滤器来清除带有 TTL 的状态</li></ul><p><strong>构造函数</strong>：</p><ul><li>RocksDBStateBackend(String checkpointDataUri)：单参数，只传入一个路径</li><li>RocksDBStateBackend(String checkpointDataUri, boolean enableIncrementalCheckpointing)：两个参数，传入 checkpoint 数据目录路径和是否开启增量 checkpoint</li><li>RocksDBStateBackend(StateBackend checkpointStreamBackend)：传入一种 StateBackend</li><li>RocksDBStateBackend(StateBackend checkpointStreamBackend, TernaryBoolean enableIncrementalCheckpointing)：传入一种 StateBackend 和是否开启增量 checkpoint</li><li>RocksDBStateBackend(RocksDBStateBackend original, Configuration config, ClassLoader classLoader)：私有的构造方法，用于重新配置状态后端</li></ul><p>既然知道这么多构造函数了，那么使用就很简单了，根据你的场景考虑使用哪种构造函数创建 RocksDBStateBackend 对象就行了，然后通过 <code>env.setStateBackend()</code> 传入对象实例就行，如下所示：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//env.setStateBackend(new RocksDBStateBackend(checkpointDir, incrementalCheckpoints));  //设置 RocksDB 存储</span></span><br></pre></td></tr></table></figure><p>那么在使用 RocksDBStateBackend 时该注意什么呢：</p><ul><li>当使用 RocksDB 时，状态大小将受限于磁盘可用空间的大小</li><li>状态存储在 RocksDB 中，整个更新和获取状态的操作都是要通过序列化和反序列化才能完成的，跟状态直接存储在内存中，性能可能会略低些</li><li>如果你应用程序的状态很大，那么使用 RocksDB 无非是最佳的选择</li></ul><p>另外在 Flink 源码中有一个专门的 RocksDBOptions 来表示 RocksDB 相关的配置：</p><ul><li>state.backend.rocksdb.localdir：本地目录(在 Task Manager 上)，RocksDB 将其文件放在其中</li><li>state.backend.rocksdb.timer-service.factory：定时器服务实现，默认值是 HEAP</li><li>state.backend.rocksdb.checkpoint.transfer.thread.num：用于在后端传输(下载和上载)文件的线程数，默认是 1</li><li>state.backend.rocksdb.ttl.compaction.filter.enabled：是否启用压缩过滤器来清除带有 TTL 的状态，默认值是 false</li></ul><h3 id="如何选择状态后端存储？"><a href="#如何选择状态后端存储？" class="headerlink" title="如何选择状态后端存储？"></a>如何选择状态后端存储？</h3><p>通过上面三种 State Backends 的介绍，让大家了解了状态存储有哪些种类，然后对每种状态存储是该如何使用的、它们内部的实现、使用场景、需要注意什么都细讲了一遍，三种存储方式各有特点，可以满足不同场景的需求，通常来说，在开发程序之前，我们要先分析自己 Job 的场景和状态大小的预测，然后根据预测来进行选择何种状态存储，如果拿捏不定的话，建议先在测试环境进行测试，只有选择了正确的状态存储后端，这样才能够保证后面自己的 Job 在生产环境能够稳定的运行。</p>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
            <tag> Flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink 常用的 Source 和 Sink Connectors</title>
      <link href="2019/12/18/Flink-%E5%B8%B8%E7%94%A8%E7%9A%84-Source-%E5%92%8C-Sink-Connectors/"/>
      <url>2019/12/18/Flink-%E5%B8%B8%E7%94%A8%E7%9A%84-Source-%E5%92%8C-Sink-Connectors/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p>Flink Job 的大致结构就是 <code>Source ——&gt; Transformation ——&gt; Sink</code>。</p><a id="more"></a><h3 id="Data-Source-介绍"><a href="#Data-Source-介绍" class="headerlink" title="Data Source 介绍"></a>Data Source 介绍</h3><p>Data Source 是什么呢？就字面意思其实就可以知道：数据来源。</p><p>Flink 做为一款流式计算框架，它可用来做批处理，即处理静态的数据集、历史的数据集；也可以用来做流处理，即处理实时的数据流（做计算操作），然后将处理后的数据实时下发，只要数据源源不断过来，Flink 就能够一直计算下去。</p><p>Flink 中你可以使用 <code>StreamExecutionEnvironment.addSource(sourceFunction)</code> 来为你的程序添加数据来源。</p><p>Flink 已经提供了若干实现好了的 source function，当然你也可以通过实现 SourceFunction 来自定义非并行的 source 或者实现 ParallelSourceFunction 接口或者扩展 RichParallelSourceFunction 来自定义并行的 source。</p><p>那么常用的 Data Source 有哪些呢？</p><h3 id="常用的-Data-Source"><a href="#常用的-Data-Source" class="headerlink" title="常用的 Data Source"></a>常用的 Data Source</h3><p>StreamExecutionEnvironment 中可以使用以下这些已实现的 stream source。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-23-083744.png" alt="img"></p><p>总的来说可以分为下面几大类：</p><h4 id="基于集合"><a href="#基于集合" class="headerlink" title="基于集合"></a>基于集合</h4><ol><li>fromCollection(Collection) - 从 Java 的 Java.util.Collection 创建数据流。集合中的所有元素类型必须相同。</li><li>fromCollection(Iterator, Class) - 从一个迭代器中创建数据流。Class 指定了该迭代器返回元素的类型。</li><li>fromElements(T …) - 从给定的对象序列中创建数据流。所有对象类型必须相同。</li></ol><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">DataStream&lt;Event&gt; input = env.fromElements(</span><br><span class="line">    <span class="keyword">new</span> Event(<span class="number">1</span>, <span class="string">&quot;barfoo&quot;</span>, <span class="number">1.0</span>),</span><br><span class="line">    <span class="keyword">new</span> Event(<span class="number">2</span>, <span class="string">&quot;start&quot;</span>, <span class="number">2.0</span>),</span><br><span class="line">    <span class="keyword">new</span> Event(<span class="number">3</span>, <span class="string">&quot;foobar&quot;</span>, <span class="number">3.0</span>),</span><br><span class="line">    ...</span><br><span class="line">);</span><br></pre></td></tr></table></figure><ol><li>fromParallelCollection(SplittableIterator, Class) - 从一个迭代器中创建并行数据流。Class 指定了该迭代器返回元素的类型。</li><li>generateSequence(from, to) - 创建一个生成指定区间范围内的数字序列的并行数据流。</li></ol><h4 id="基于文件"><a href="#基于文件" class="headerlink" title="基于文件"></a>基于文件</h4><p>1、readTextFile(path) - 读取文本文件，即符合 TextInputFormat 规范的文件，并将其作为字符串返回。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">final</span> StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">DataStream&lt;String&gt; text = env.readTextFile(<span class="string">&quot;file:///path/to/file&quot;</span>);</span><br></pre></td></tr></table></figure><p>2、readFile(fileInputFormat, path) - 根据指定的文件输入格式读取文件（一次）。</p><p>3、readFile(fileInputFormat, path, watchType, interval, pathFilter, typeInfo) - 这是上面两个方法内部调用的方法。它根据给定的 fileInputFormat 和读取路径读取文件。根据提供的 watchType，这个 source 可以定期（每隔 interval 毫秒）监测给定路径的新数据（FileProcessingMode.PROCESS<em>CONTINUOUSLY），或者处理一次路径对应文件的数据并退出（FileProcessingMode.PROCESS</em>ONCE）。你可以通过 pathFilter 进一步排除掉需要处理的文件。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">final</span> StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">DataStream&lt;MyEvent&gt; stream = env.readFile(</span><br><span class="line">        myFormat, myFilePath, FileProcessingMode.PROCESS_CONTINUOUSLY, <span class="number">100</span>,</span><br><span class="line">        FilePathFilter.createDefaultFilter(), typeInfo);</span><br></pre></td></tr></table></figure><p><strong>实现:</strong></p><p>在具体实现上，Flink 把文件读取过程分为两个子任务，即目录监控和数据读取。每个子任务都由单独的实体实现。目录监控由单个非并行（并行度为1）的任务执行，而数据读取由并行运行的多个任务执行。后者的并行性等于作业的并行性。单个目录监控任务的作用是扫描目录（根据 watchType 定期扫描或仅扫描一次），查找要处理的文件并把文件分割成切分片（splits），然后将这些切分片分配给下游 reader。reader 负责读取数据。每个切分片只能由一个 reader 读取，但一个 reader 可以逐个读取多个切分片。</p><p><strong>重要注意：</strong></p><p>如果 watchType 设置为 FileProcessingMode.PROCESS_CONTINUOUSLY，则当文件被修改时，其内容将被重新处理。这会打破“exactly-once”语义，因为在文件末尾附加数据将导致其所有内容被重新处理。</p><p>如果 watchType 设置为 FileProcessingMode.PROCESS_ONCE，则 source 仅扫描路径一次然后退出，而不等待 reader 完成文件内容的读取。当然 reader 会继续阅读，直到读取所有的文件内容。关闭 source 后就不会再有检查点。这可能导致节点故障后的恢复速度较慢，因为该作业将从最后一个检查点恢复读取。</p><h4 id="基于-Socket"><a href="#基于-Socket" class="headerlink" title="基于 Socket"></a>基于 Socket</h4><p>socketTextStream(String hostname, int port) - 从 socket 读取。元素可以用分隔符切分。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">DataStream&lt;Tuple2&lt;String, Integer&gt;&gt; dataStream = env</span><br><span class="line">        .socketTextStream(<span class="string">&quot;localhost&quot;</span>, <span class="number">9999</span>) <span class="comment">// 监听 localhost 的 9999 端口过来的数据</span></span><br><span class="line">        .flatMap(<span class="keyword">new</span> Splitter())</span><br><span class="line">        .keyBy(<span class="number">0</span>)</span><br><span class="line">        .timeWindow(Time.seconds(<span class="number">5</span>))</span><br><span class="line">        .sum(<span class="number">1</span>);</span><br></pre></td></tr></table></figure><h4 id="自定义"><a href="#自定义" class="headerlink" title="自定义"></a>自定义</h4><p>addSource - 添加一个新的 source function。例如，你可以用 addSource(new FlinkKafkaConsumer011&lt;&gt;(…)) 从 Apache Kafka 读取数据。</p><p><strong>说说上面几种的特点</strong></p><ol><li>基于集合：有界数据集，更偏向于本地测试用</li><li>基于文件：适合监听文件修改并读取其内容</li><li>基于 Socket：监听主机的 host port，从 Socket 中获取数据</li><li>自定义 addSource：大多数的场景数据都是无界的，会源源不断过来。比如去消费 Kafka 某个 topic 上的数据，这时候就需要用到这个 addSource，可能因为用的比较多的原因吧，Flink 直接提供了 FlinkKafkaConsumer011 等类可供你直接使用。你可以去看看 FlinkKafkaConsumerBase 这个基础类，它是 Flink Kafka 消费的最根本的类。</li></ol><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">DataStream&lt;KafkaEvent&gt; input = env</span><br><span class="line">        .addSource(</span><br><span class="line">            <span class="keyword">new</span> FlinkKafkaConsumer011&lt;&gt;(</span><br><span class="line">                parameterTool.getRequired(<span class="string">&quot;input-topic&quot;</span>), <span class="comment">//从参数中获取传进来的 topic </span></span><br><span class="line">                <span class="keyword">new</span> KafkaEventSchema(),</span><br><span class="line">                parameterTool.getProperties())</span><br><span class="line">            .assignTimestampsAndWatermarks(<span class="keyword">new</span> CustomWatermarkExtractor()));</span><br></pre></td></tr></table></figure><p>Flink 目前支持如下面常见的 Source：</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/UTfWCZ.jpg" alt="img"></p><p>如果你想自定义自己的 Source 呢？在后面 3.8 节会讲解。</p><h3 id="Data-Sink-介绍"><a href="#Data-Sink-介绍" class="headerlink" title="Data Sink 介绍"></a>Data Sink 介绍</h3><p>Data sink 有点把数据存储下来（落库）的意思。Flink 在拿到数据后做一系列的计算后，最后要将计算的结果往下游发送。比如将数据存储到 MySQL、ElasticSearch、Cassandra，或者继续发往 Kafka、 RabbitMQ 等消息队列，更或者直接调用其他的第三方应用服务（比如告警）。</p><h3 id="常用的-Data-Sink"><a href="#常用的-Data-Sink" class="headerlink" title="常用的 Data Sink"></a>常用的 Data Sink</h3><p>上面介绍了 Flink Data Source 有哪些，这里也看看 Flink Data Sink 支持的有哪些。</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/siWsAK.jpg" alt="img"></p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-23-084839.png" alt="img"></p><p>可以看到有 Kafka、ElasticSearch、Socket、RabbitMQ、JDBC、Cassandra POJO、File、Print 等 Sink 的方式。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
            <tag> Flink </tag>
            
            <tag> HBase </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink 多种时间语义对比</title>
      <link href="2019/12/18/Flink-%E5%A4%9A%E7%A7%8D%E6%97%B6%E9%97%B4%E8%AF%AD%E4%B9%89%E5%AF%B9%E6%AF%94/"/>
      <url>2019/12/18/Flink-%E5%A4%9A%E7%A7%8D%E6%97%B6%E9%97%B4%E8%AF%AD%E4%B9%89%E5%AF%B9%E6%AF%94/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p>Flink 在流应用程序中支持不同的 <strong>Time</strong> 概念，有 Processing Time、Event Time 和 Ingestion Time。下面我们一起来看看这三个 Time。</p><a id="more"></a><h3 id="Processing-Time"><a href="#Processing-Time" class="headerlink" title="Processing Time"></a>Processing Time</h3><p><strong>Processing Time 是指事件被处理时机器的系统时间。</strong></p><p><font color='grey'>如果 Flink Job 设置的时间策略是 Processing Time 的话，那么后面所有基于时间的操作（如时间窗口）都将会使用当时机器的系统时间。每小时 Processing Time 窗口将包括在系统时钟指示整个小时之间到达特定操作的所有事件。</font></p><p><font color='grey'>例如，如果应用程序在上午 9:15 开始运行，则第一个每小时 Processing Time 窗口将包括在上午 9:15 到上午 10:00 之间处理的事件，下一个窗口将包括在上午 10:00 到 11:00 之间处理的事件。</font></p><p>Processing Time 是最简单的 “Time” 概念，不需要流和机器之间的协调，它提供了最好的性能和最低的延迟。但是，在分布式和异步的环境下，Processing Time 不能提供确定性，因为它容易受到事件到达系统的速度（例如从消息队列）、事件在系统内操作流动的速度以及中断的影响。</p><h3 id="Event-Time"><a href="#Event-Time" class="headerlink" title="Event Time"></a>Event Time</h3><p><strong>Event Time 是指事件发生的时间，一般就是数据本身携带的时间。这个时间通常是在事件到达 Flink 之前就确定的，并且可以从每个事件中获取到事件时间戳。</strong></p><p><strong>在 Event Time 中，时间取决于数据，而跟其他没什么关系。Event Time 程序必须指定如何生成 Event Time 水印，这是表示 Event Time 进度的机制。</strong></p><p><font color='grey'>完美的说，无论事件什么时候到达或者其怎么排序，最后处理 Event Time 将产生完全一致和确定的结果。但是，除非事件按照已知顺序（事件产生的时间顺序）到达，否则处理 Event Time 时将会因为要等待一些无序事件而产生一些延迟。由于只能等待一段有限的时间，因此就难以保证处理 Event Time 将产生完全一致和确定的结果。</font></p><p><font color='grey'>假设所有数据都已到达，Event Time 操作将按照预期运行，即使在处理无序事件、延迟事件、重新处理历史数据时也会产生正确且一致的结果。 例如，每小时事件时间窗口将包含带有落入该小时的事件时间戳的所有记录，不管它们到达的顺序如何（是否按照事件产生的时间）。</font></p><h3 id="Ingestion-Time"><a href="#Ingestion-Time" class="headerlink" title="Ingestion Time"></a>Ingestion Time</h3><p><strong>Ingestion Time 是事件进入 Flink 的时间。 在数据源操作处（进入 Flink source 时），每个事件将进入 Flink 时当时的时间作为时间戳，并且基于时间的操作（如时间窗口）会利用这个时间戳。</strong></p><p>Ingestion Time 在概念上位于 Event Time 和 Processing Time 之间。 与 Processing Time 相比，成本可能会高一点，但结果更可预测。因为 Ingestion Time 使用稳定的时间戳（只在进入 Flink 的时候分配一次），所以对事件的不同窗口操作将使用相同的时间戳（第一次分配的时间戳），而在 Processing Time 中，每个窗口操作符可以将事件分配给不同的窗口（基于机器系统时间和到达延迟）。</p><p>与 Event Time 相比，Ingestion Time 程序无法处理任何无序事件或延迟数据，但程序中不必指定如何生成水印。</p><p>在 Flink 中，Ingestion Time 与 Event Time 非常相似，唯一区别就是 Ingestion Time 具有自动分配时间戳和自动生成水印功能。</p><h3 id="三种-Time-对比结果"><a href="#三种-Time-对比结果" class="headerlink" title="三种 Time 对比结果"></a>三种 Time 对比结果</h3><p>一张图概括上面说的三种 Time：</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-04-28-Flink-time.png" alt="img"></p><h3 id="使用场景分析"><a href="#使用场景分析" class="headerlink" title="使用场景分析"></a>使用场景分析</h3><p>通过上面两个图相信大家已经对 Flink 中的这三个 Time 有所了解了，那么我们实际生产环境中通常该如何选择哪种 Time 呢？</p><p>一般来说在生产环境中将 Event Time 与 Processing Time 对比的比较多，这两个也是我们常用的策略，Ingestion Time 一般用的较少。</p><p>用 Processing Time 的场景大多是用户不关心事件时间，它只需要关心这个时间窗口要有数据进来，只要有数据进来了，我就可以对进来窗口中的数据进行一系列的计算操作，然后再将计算后的数据发往下游。</p><p>而用 Event Time 的场景一般是业务需求需要时间这个字段（比如购物时是要先有下单事件、再有支付事件；借贷事件的风控是需要依赖时间来做判断的；机器异常检测触发的告警也是要具体的异常事件的时间展示出来；商品广告及时精准推荐给用户依赖的就是用户在浏览商品的时间段/频率/时长等信息），只能根据事件时间来处理数据，而且还要从事件中获取到事件的时间。</p><p>但是使用事件时间的话，就可能有这样的情况：数据源采集的数据往消息队列中发送时可能因为网络抖动、服务可用性、消息队列的分区数据堆积的影响而导致数据到达的不一定及时，可能会出现数据出现一定的乱序、延迟几分钟等，庆幸的是 Flink 支持通过 WaterMark 机制来处理这种延迟的数据。关于 WaterMark 的机制我会在后面的文章讲解。</p><h3 id="设置-Time-策略"><a href="#设置-Time-策略" class="headerlink" title="设置 Time 策略"></a>设置 Time 策略</h3><p>在创建完流运行环境的时候，然后就可以通过 <code>env.setStreamTimeCharacteristic</code> 设置时间策略：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">final</span> StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 其他两种:</span></span><br><span class="line"><span class="comment">// env.setStreamTimeCharacteristic(TimeCharacteristic.IngestionTime);</span></span><br><span class="line"><span class="comment">// env.setStreamTimeCharacteristic(TimeCharacteristic.ProcessingT</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
            <tag> Flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink Window 机制深入理解</title>
      <link href="2019/12/17/Flink-Window-%E6%9C%BA%E5%88%B6%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3/"/>
      <url>2019/12/17/Flink-Window-%E6%9C%BA%E5%88%B6%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p>目前有许多数据分析的场景从批处理到流处理的演变， 虽然可以将批处理作为流处理的特殊情况来处理，但是分析无穷集的流数据通常需要思维方式的转变并且具有其自己的术语，例如，“windowing（窗口化）”、“at-least-once（至少一次）”、“exactly-once（只有一次）” 。</p><a id="more"></a><h2 id="Flink-Window-基础概念与实现原理"><a href="#Flink-Window-基础概念与实现原理" class="headerlink" title="Flink Window 基础概念与实现原理"></a>Flink Window 基础概念与实现原理</h2><p>Apache Flink 是一个为生产环境而生的流处理器，具有易于使用的 API，可以用于定义高级流分析程序。Flink 的 API 在数据流上具有非常灵活的窗口定义，使其在其他开源流处理框架中脱颖而出。</p><h3 id="什么是-Window？"><a href="#什么是-Window？" class="headerlink" title="什么是 Window？"></a>什么是 Window？</h3><p>下面我们结合一个现实的例子来说明。</p><p>就拿交通传感器的示例：统计经过某红绿灯的汽车数量之和？</p><p>假设在一个红绿灯处，我们每隔 15 秒统计一次通过此红绿灯的汽车数量，如下图：</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-23-064257.png" alt="img"></p><p>可以把汽车的经过看成一个流，无穷的流，不断有汽车经过此红绿灯，因此无法统计总共的汽车数量。但是，我们可以换一种思路，每隔 15 秒，我们都将与上一次的结果进行 sum 操作（滑动聚合），如下：</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-23-064320.png" alt="img"></p><p>这个结果似乎还是无法回答我们的问题，根本原因在于流是无界的，我们不能限制流，但可以在有一个有界的范围内处理无界的流数据。因此，我们需要换一个问题的提法：每分钟经过某红绿灯的汽车数量之和？</p><p>这个问题，就相当于一个定义了一个 Window（窗口），Window 的界限是 1 分钟，且每分钟内的数据互不干扰，因此也可以称为翻滚（不重合）窗口，如下图：</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-23-065851.png" alt="img"></p><p>第一分钟的数量为 18，第二分钟是 28，第三分钟是 24……这样，1 个小时内会有 60 个 Window。</p><p>再考虑一种情况，每 30 秒统计一次过去 1 分钟的汽车数量之和：</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-23-071008.png" alt="img"></p><p>此时，Window 出现了重合。这样，1 个小时内会有 120 个 Window。</p><h3 id="Window-有什么作用？"><a href="#Window-有什么作用？" class="headerlink" title="Window 有什么作用？"></a>Window 有什么作用？</h3><p>通常来讲，Window 就是用来对一个无限的流设置一个有限的集合，在有界的数据集上进行操作的一种机制。Window 又可以分为基于时间（Time-based）的 Window 以及基于数量（Count-based）的 window。</p><h3 id="Flink-自带的-Window"><a href="#Flink-自带的-Window" class="headerlink" title="Flink 自带的 Window"></a>Flink 自带的 Window</h3><p>Flink 认为 Batch 是 Streaming 的一个特例，所以 Flink 底层引擎是一个流式引擎，在上面实现了流处理和批处理。而窗口 [window] 就是从 Streaming 到 Batch 的一个桥梁。</p><p>在流处理应用中，数据是连续不断的，因此我们不可能等到所有数据都到了才开始处理。当然我们可以每来一个消息就处理一次，但是有时我们需要做一些聚合类的处理，例如：在过去的1分钟内有多少用户点击了我们的网页。在这种情况下，我们必须定义一个窗口，用来收集最近一分钟内的数据，并对这个窗口内的数据进行计算。</p><p>Flink 在 KeyedStream 中提供了下面几种 Window：</p><ul><li>以时间驱动的 Time Window</li><li>以事件数量驱动的 Count Window</li><li>以会话间隔驱动的 Session Window</li></ul><p>提供上面三种 Window 机制后，由于某些特殊的需要，DataStream API 也提供了定制化的 Window 操作，供用户自定义 Window。</p><p>下面将先围绕上面说的三种 Window 来进行分析并教大家如何使用，然后对其原理分析，最后在解析其源码实现。</p><h3 id="Time-Window-使用"><a href="#Time-Window-使用" class="headerlink" title="Time Window 使用"></a>Time Window 使用</h3><p>正如命名那样，Time Window 根据时间来聚合流数据。例如：一分钟的时间窗口就只会收集一分钟的元素，并在一分钟过后对窗口中的所有元素应用于下一个算子。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">dataStream.keyBy(<span class="number">1</span>)</span><br><span class="line">    .timeWindow(Time.minutes(<span class="number">1</span>)) <span class="comment">//time Window 每分钟统计一次数量和</span></span><br><span class="line">    .sum(<span class="number">1</span>);</span><br></pre></td></tr></table></figure><p>在第一个窗口中（1 ～ 2 分钟）和为 7、第二个窗口中（2 ～ 3 分钟）和为 12、第三个窗口中（3 ～ 4 分钟）和为 7、第四个窗口中（4 ～ 5 分钟）和为 19。</p><p>该 timeWindow 方法在 KeyedStream 中对应的源码如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//时间窗口</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> WindowedStream&lt;T, KEY, TimeWindow&gt; <span class="title">timeWindow</span><span class="params">(Time size)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (environment.getStreamTimeCharacteristic() == TimeCharacteristic.ProcessingTime) &#123;</span><br><span class="line">        <span class="keyword">return</span> window(TumblingProcessingTimeWindows.of(size));</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> window(TumblingEventTimeWindows.of(size));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>另外在 Time Window 中还支持滑动的时间窗口，比如定义了一个每 30s 滑动一次的 1 分钟时间窗口，它会每隔 30s 去统计过去一分钟窗口内的数据，同样使用也很简单，输入两个时间参数，如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">dataStream.keyBy(<span class="number">1</span>)</span><br><span class="line">    .timeWindow(Time.minutes(<span class="number">1</span>), Time.seconds(<span class="number">30</span>)) <span class="comment">//sliding time Window 每隔 30s 统计过去一分钟的数量和</span></span><br><span class="line">    .sum(<span class="number">1</span>);</span><br></pre></td></tr></table></figure><p>滑动时间窗口的数据聚合流程如下图所示：</p><h3 id="Count-Window-使用及源码分析"><a href="#Count-Window-使用及源码分析" class="headerlink" title="Count Window 使用及源码分析"></a>Count Window 使用及源码分析</h3><p>Apache Flink 还提供计数窗口功能，如果计数窗口的值设置的为 3 ，那么将会在窗口中收集 3 个事件，并在添加第 3 个元素时才会计算窗口中所有事件的值。</p><ul><li><p><strong><code>Tumbling Count Window</code></strong></p><blockquote><p>当我们想要每100个用户购买行为事件统计购买总数，那么每当窗口中填满100个元素了，就会对窗口进行计算，这种窗口我们称之为滚动计数窗口</p></blockquote></li><li><p><strong><code>Sliding Count Window</code></strong></p><blockquote><p>但是对于某些应用，它们需要的窗口是不间断的，需要平滑地进行窗口聚合。比如，我们可以每30秒计算一次最近一分钟用户购买的商品总数。这种窗口我们称为滑动时间窗口（Sliding Time Window）。在滑窗中，一个元素可以对应多个窗口。通过使用 DataStream API，我们可以这样实现：</p></blockquote></li></ul><p>在 Flink 中使用 Count Window 非常简单，输入一个 long 类型的参数，这个参数代表窗口中事件的数量，使用如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">dataStream.keyBy(<span class="number">1</span>)</span><br><span class="line">    .countWindow(<span class="number">3</span>) <span class="comment">//统计每 3 个元素的数量之和</span></span><br><span class="line">    .sum(<span class="number">1</span>);</span><br></pre></td></tr></table></figure><p>计数窗口的数据窗口聚合流程如下图所示：</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-05-16-045758.jpg" alt="img"></p><p>该 countWindow 方法在 KeyedStream 中对应的源码如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//计数窗口</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> WindowedStream&lt;T, KEY, GlobalWindow&gt; <span class="title">countWindow</span><span class="params">(<span class="keyword">long</span> size)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> window(GlobalWindows.create()).trigger(PurgingTrigger.of(CountTrigger.of(size)));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>另外在 Count Window 中还支持滑动的计数窗口，比如定义了一个每 3 个事件滑动一次的 4 个事件的计数窗口，它会每隔 3 个事件去统计过去 4 个事件计数窗口内的数据，使用也很简单，输入两个 long 类型的参数，如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">dataStream.keyBy(<span class="number">1</span>) </span><br><span class="line">    .countWindow(<span class="number">4</span>, <span class="number">3</span>) <span class="comment">//每隔 3 个元素统计过去 4 个元素的数量之和</span></span><br><span class="line">    .sum(<span class="number">1</span>);</span><br></pre></td></tr></table></figure><p>滑动计数窗口的数据窗口聚合流程如下图所示：</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-05-16-065833.jpg" alt="img"></p><p>该 countWindow 方法在 KeyedStream 中对应的源码如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//滑动计数窗口</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> WindowedStream&lt;T, KEY, GlobalWindow&gt; <span class="title">countWindow</span><span class="params">(<span class="keyword">long</span> size, <span class="keyword">long</span> slide)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> window(GlobalWindows.create()).evictor(CountEvictor.of(size)).trigger(CountTrigger.of(slide));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="Session-Window-使用及源码分析"><a href="#Session-Window-使用及源码分析" class="headerlink" title="Session Window 使用及源码分析"></a>Session Window 使用及源码分析</h3><p>Apache Flink 还提供了会话窗口，是什么意思呢？使用该窗口的时候你可以传入一个时间参数（表示某种数据维持的会话持续时长），如果超过这个时间，就代表着超出会话时长。</p><p>在 Flink 中使用 Session Window 非常简单，你该使用 Flink KeyedStream 中的 window 方法，然后使用 ProcessingTimeSessionWindows.withGap()（不一定就是只使用这个），在该方法里面你需要做的是传入一个时间参数，如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">dataStream.keyBy(<span class="number">1</span>)</span><br><span class="line">    .window(ProcessingTimeSessionWindows.withGap(Time.seconds(<span class="number">5</span>)))<span class="comment">//表示如果 5s 内没出现数据则认为超出会话时长，然后计算这个窗口的和</span></span><br><span class="line">    .sum(<span class="number">1</span>);</span><br></pre></td></tr></table></figure><p>会话窗口的数据窗口聚合流程如下图所示：</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-05-16-150258.jpg" alt="img"></p><p>该 Window 方法在 KeyedStream 中对应的源码如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//提供自定义 Window</span></span><br><span class="line"><span class="keyword">public</span> &lt;W extends Window&gt; <span class="function">WindowedStream&lt;T, KEY, W&gt; <span class="title">window</span><span class="params">(WindowAssigner&lt;? <span class="keyword">super</span> T, W&gt; assigner)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> WindowedStream&lt;&gt;(<span class="keyword">this</span>, assigner);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="如何自定义-Window？"><a href="#如何自定义-Window？" class="headerlink" title="如何自定义 Window？"></a>如何自定义 Window？</h3><p>当然除了上面几种自带的 Window 外，Apache Flink 还提供了用户可自定义的 Window，那么该如何操作呢？其实细心的同学可能已经发现了上面我写的每种 Window 的实现方式了，它们有 assigner、 evictor、trigger。如果你没发现的话，也不要紧，这里我们就来了解一下实现 Window 的机制，这样我们才能够更好的学会如何自定义 Window。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-23-073301.png" alt="img"></p><h3 id="3-2-8-Window-源码定义"><a href="#3-2-8-Window-源码定义" class="headerlink" title="3.2.8 Window 源码定义"></a>3.2.8 Window 源码定义</h3><p>上面说了 Flink 中自带的 Window，主要利用了 KeyedStream 的 API 来实现，我们这里来看下 Window 的源码定义如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">Window</span> </span>&#123;</span><br><span class="line">    <span class="comment">//获取属于此窗口的最大时间戳</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="keyword">long</span> <span class="title">maxTimestamp</span><span class="params">()</span></span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>查看源码可以看见 Window 这个抽象类有如下实现类：</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-17-163050.png" alt="img"></p><p><strong>TimeWindow</strong> 源码定义如下:</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TimeWindow</span> <span class="keyword">extends</span> <span class="title">Window</span> </span>&#123;</span><br><span class="line">    <span class="comment">//窗口开始时间</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">long</span> start;</span><br><span class="line">    <span class="comment">//窗口结束时间</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">long</span> end;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>GlobalWindow</strong> 源码定义如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">GlobalWindow</span> <span class="keyword">extends</span> <span class="title">Window</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> GlobalWindow INSTANCE = <span class="keyword">new</span> GlobalWindow();</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="title">GlobalWindow</span><span class="params">()</span> </span>&#123; &#125;</span><br><span class="line">    <span class="comment">//对外提供 get() 方法返回 GlobalWindow 实例，并且是个全局单例</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> GlobalWindow <span class="title">get</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> INSTANCE;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="Window-组件之-WindowAssigner-使用及源码分析"><a href="#Window-组件之-WindowAssigner-使用及源码分析" class="headerlink" title="Window 组件之 WindowAssigner 使用及源码分析"></a>Window 组件之 WindowAssigner 使用及源码分析</h3><p>到达窗口操作符的元素被传递给 WindowAssigner。WindowAssigner 将元素分配给一个或多个窗口，可能会创建新的窗口。</p><p>窗口本身只是元素列表的标识符，它可能提供一些可选的元信息，例如 TimeWindow 中的开始和结束时间。注意，元素可以被添加到多个窗口，这也意味着一个元素可以同时在多个窗口存在。我们来看下 WindowAssigner 的代码的定义吧：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">WindowAssigner</span>&lt;<span class="title">T</span>, <span class="title">W</span> <span class="keyword">extends</span> <span class="title">Window</span>&gt; <span class="keyword">implements</span> <span class="title">Serializable</span> </span>&#123;</span><br><span class="line">    <span class="comment">//分配数据到窗口并返回窗口集合</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> Collection&lt;W&gt; <span class="title">assignWindows</span><span class="params">(T element, <span class="keyword">long</span> timestamp, WindowAssignerContext context)</span></span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>查看源码可以看见 WindowAssigner 这个抽象类有如下实现类：</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-17-163413.png" alt="img"></p><p>这些 WindowAssigner 实现类的作用介绍：</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-05-16-155715.jpg" alt="img"></p><p>如果你细看了上面图中某个类的具体实现的话，你会发现一个规律，比如我拿 TumblingEventTimeWindows 的源码来分析，如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TumblingEventTimeWindows</span> <span class="keyword">extends</span> <span class="title">WindowAssigner</span>&lt;<span class="title">Object</span>, <span class="title">TimeWindow</span>&gt; </span>&#123;</span><br><span class="line">    <span class="comment">//定义属性</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">long</span> size;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">long</span> offset;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//构造方法</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="title">TumblingEventTimeWindows</span><span class="params">(<span class="keyword">long</span> size, <span class="keyword">long</span> offset)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (Math.abs(offset) &gt;= size) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> IllegalArgumentException(<span class="string">&quot;TumblingEventTimeWindows parameters must satisfy abs(offset) &lt; size&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">this</span>.size = size;</span><br><span class="line">        <span class="keyword">this</span>.offset = offset;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//重写 WindowAssigner 抽象类中的抽象方法 assignWindows</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Collection&lt;TimeWindow&gt; <span class="title">assignWindows</span><span class="params">(Object element, <span class="keyword">long</span> timestamp, WindowAssignerContext context)</span> </span>&#123;</span><br><span class="line">        <span class="comment">//实现该 TumblingEventTimeWindows 中的具体逻辑</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//其他方法，对外提供静态方法，供其他类调用</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>从上面你就会发现<strong>套路</strong>：</p><p>1、定义好实现类的属性</p><p>2、根据定义的属性添加构造方法</p><p>3、重写 WindowAssigner 中的 assignWindows 等方法</p><p>4、定义其他的方法供外部调用</p><h3 id="Window-组件之-Trigger-使用及源码分析"><a href="#Window-组件之-Trigger-使用及源码分析" class="headerlink" title="Window 组件之 Trigger 使用及源码分析"></a>Window 组件之 Trigger 使用及源码分析</h3><p>Trigger 表示触发器，每个窗口都拥有一个 Trigger（触发器），该 Trigger 决定何时计算和清除窗口。当先前注册的计时器超时时，将为插入窗口的每个元素调用触发器。在每个事件上，触发器都可以决定触发，即清除（删除窗口并丢弃其内容），或者启动并清除窗口。一个窗口可以被求值多次，并且在被清除之前一直存在。注意，在清除窗口之前，窗口将一直消耗内存。</p><p>说了这么一大段，我们还是来看看 Trigger 的源码，定义如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">Trigger</span>&lt;<span class="title">T</span>, <span class="title">W</span> <span class="keyword">extends</span> <span class="title">Window</span>&gt; <span class="keyword">implements</span> <span class="title">Serializable</span> </span>&#123;</span><br><span class="line">    <span class="comment">//当有数据进入到 Window 运算符就会触发该方法</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> TriggerResult <span class="title">onElement</span><span class="params">(T element, <span class="keyword">long</span> timestamp, W window, TriggerContext ctx)</span> <span class="keyword">throws</span> Exception</span>;</span><br><span class="line">    <span class="comment">//当使用触发器上下文设置的处理时间计时器触发时调用</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> TriggerResult <span class="title">onProcessingTime</span><span class="params">(<span class="keyword">long</span> time, W window, TriggerContext ctx)</span> <span class="keyword">throws</span> Exception</span>;</span><br><span class="line">    <span class="comment">//当使用触发器上下文设置的事件时间计时器触发时调用该方法</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> TriggerResult <span class="title">onEventTime</span><span class="params">(<span class="keyword">long</span> time, W window, TriggerContext ctx)</span> <span class="keyword">throws</span> Exception</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>当有数据流入 Window 运算符时就会触发 onElement 方法、当处理时间和事件时间生效时会触发 onProcessingTime 和 onEventTime 方法。每个触发动作的返回结果用 TriggerResult 定义。继续来看下 TriggerResult 的源码定义：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">enum</span> <span class="title">TriggerResult</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//不做任何操作</span></span><br><span class="line">    CONTINUE(<span class="keyword">false</span>, <span class="keyword">false</span>),</span><br><span class="line"></span><br><span class="line">    <span class="comment">//处理并移除窗口中的数据</span></span><br><span class="line">    FIRE_AND_PURGE(<span class="keyword">true</span>, <span class="keyword">true</span>),</span><br><span class="line"></span><br><span class="line">    <span class="comment">//处理窗口数据，窗口计算后不做清理</span></span><br><span class="line">    FIRE(<span class="keyword">true</span>, <span class="keyword">false</span>),</span><br><span class="line"></span><br><span class="line">    <span class="comment">//清除窗口中的所有元素，并且在不计算窗口函数或不发出任何元素的情况下丢弃窗口</span></span><br><span class="line">    PURGE(<span class="keyword">false</span>, <span class="keyword">true</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>查看源码可以看见 Trigger 这个抽象类有如下实现类：</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-17-163751.png" alt="img"></p><p>这些 Trigger 实现类的作用介绍：</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-05-17-145735.jpg" alt="img"></p><p>如果你细看了上面图中某个类的具体实现的话，你会发现一个规律，拿 CountTrigger 的源码来分析，如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CountTrigger</span>&lt;<span class="title">W</span> <span class="keyword">extends</span> <span class="title">Window</span>&gt; <span class="keyword">extends</span> <span class="title">Trigger</span>&lt;<span class="title">Object</span>, <span class="title">W</span>&gt; </span>&#123;</span><br><span class="line">    <span class="comment">//定义属性</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">long</span> maxCount;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> ReducingStateDescriptor&lt;Long&gt; stateDesc = <span class="keyword">new</span> ReducingStateDescriptor&lt;&gt;(<span class="string">&quot;count&quot;</span>, <span class="keyword">new</span> Sum(), LongSerializer.INSTANCE);</span><br><span class="line">    <span class="comment">//构造方法</span></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="title">CountTrigger</span><span class="params">(<span class="keyword">long</span> maxCount)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.maxCount = maxCount;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//重写抽象类 Trigger 中的抽象方法 </span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> TriggerResult <span class="title">onElement</span><span class="params">(Object element, <span class="keyword">long</span> timestamp, W window, TriggerContext ctx)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="comment">//实现 CountTrigger 中的具体逻辑</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> TriggerResult <span class="title">onEventTime</span><span class="params">(<span class="keyword">long</span> time, W window, TriggerContext ctx)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> TriggerResult.CONTINUE;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> TriggerResult <span class="title">onProcessingTime</span><span class="params">(<span class="keyword">long</span> time, W window, TriggerContext ctx)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> TriggerResult.CONTINUE;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>套路</strong>：</p><ol><li>定义好实现类的属性</li><li>根据定义的属性添加构造方法</li><li>重写 Trigger 中的 onElement、onEventTime、onProcessingTime 等方法</li><li>定义其他的方法供外部调用</li></ol><h3 id="Window-组件之-Evictor-使用及源码分析"><a href="#Window-组件之-Evictor-使用及源码分析" class="headerlink" title="Window 组件之 Evictor 使用及源码分析"></a>Window 组件之 Evictor 使用及源码分析</h3><p>Evictor 表示驱逐者，它可以遍历窗口元素列表，并可以决定从列表的开头删除首先进入窗口的一些元素，然后其余的元素被赋给一个计算函数，如果没有定义 Evictor，触发器直接将所有窗口元素交给计算函数。</p><p>我们来看看 Evictor 的源码定义如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">Evictor</span>&lt;<span class="title">T</span>, <span class="title">W</span> <span class="keyword">extends</span> <span class="title">Window</span>&gt; <span class="keyword">extends</span> <span class="title">Serializable</span> </span>&#123;</span><br><span class="line">    <span class="comment">//在窗口函数之前调用该方法选择性地清除元素</span></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">evictBefore</span><span class="params">(Iterable&lt;TimestampedValue&lt;T&gt;&gt; elements, <span class="keyword">int</span> size, W window, EvictorContext evictorContext)</span></span>;</span><br><span class="line">    <span class="comment">//在窗口函数之后调用该方法选择性地清除元素</span></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">evictAfter</span><span class="params">(Iterable&lt;TimestampedValue&lt;T&gt;&gt; elements, <span class="keyword">int</span> size, W window, EvictorContext evictorContext)</span></span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>查看源码可以看见 Evictor 这个接口有如下实现类：</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-17-163942.png" alt="img"></p><p>这些 Evictor 实现类的作用介绍：</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-05-17-153505.jpg" alt="img"></p><p>如果你细看了上面三种中某个类的实现的话，你会发现一个规律，比如我就拿 CountEvictor 的源码来分析，如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CountEvictor</span>&lt;<span class="title">W</span> <span class="keyword">extends</span> <span class="title">Window</span>&gt; <span class="keyword">implements</span> <span class="title">Evictor</span>&lt;<span class="title">Object</span>, <span class="title">W</span>&gt; </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">1L</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//定义属性</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">long</span> maxCount;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">boolean</span> doEvictAfter;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//构造方法</span></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="title">CountEvictor</span><span class="params">(<span class="keyword">long</span> count, <span class="keyword">boolean</span> doEvictAfter)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.maxCount = count;</span><br><span class="line">        <span class="keyword">this</span>.doEvictAfter = doEvictAfter;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//构造方法</span></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="title">CountEvictor</span><span class="params">(<span class="keyword">long</span> count)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.maxCount = count;</span><br><span class="line">        <span class="keyword">this</span>.doEvictAfter = <span class="keyword">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//重写 Evictor 中的 evictBefore 方法</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">evictBefore</span><span class="params">(Iterable&lt;TimestampedValue&lt;Object&gt;&gt; elements, <span class="keyword">int</span> size, W window, EvictorContext ctx)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (!doEvictAfter) &#123;</span><br><span class="line">            <span class="comment">//调用内部的关键实现方法 evict</span></span><br><span class="line">            evict(elements, size, ctx);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//重写 Evictor 中的 evictAfter 方法</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">evictAfter</span><span class="params">(Iterable&lt;TimestampedValue&lt;Object&gt;&gt; elements, <span class="keyword">int</span> size, W window, EvictorContext ctx)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (doEvictAfter) &#123;</span><br><span class="line">            <span class="comment">//调用内部的关键实现方法 evict</span></span><br><span class="line">            evict(elements, size, ctx);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">evict</span><span class="params">(Iterable&lt;TimestampedValue&lt;Object&gt;&gt; elements, <span class="keyword">int</span> size, EvictorContext ctx)</span> </span>&#123;</span><br><span class="line">        <span class="comment">//内部的关键实现方法</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//其他的方法</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>发现<strong>套路</strong>：</p><ol><li>定义好实现类的属性</li><li>根据定义的属性添加构造方法</li><li>重写 Evictor 中的 evictBefore 和 evictAfter 方法</li><li>定义关键的内部实现方法 evict，处理具体的逻辑</li><li>定义其他的方法供外部调用</li></ol><p>上面我们详细讲解了 Window 中的组件 WindowAssigner、Trigger、Evictor，然后继续回到问题：如何自定义 Window？</p><p>上文讲解了 Flink 自带的 Window（Time Window、Count Window、Session Window），然后还分析了他们的源码实现，通过这几个源码，我们可以发现，它最后调用的都有一个方法，那就是 Window 方法，如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//提供自定义 Window</span></span><br><span class="line"><span class="keyword">public</span> &lt;W extends Window&gt; <span class="function">WindowedStream&lt;T, KEY, W&gt; <span class="title">window</span><span class="params">(WindowAssigner&lt;? <span class="keyword">super</span> T, W&gt; assigner)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> WindowedStream&lt;&gt;(<span class="keyword">this</span>, assigner);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//构造一个 WindowedStream 实例</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">WindowedStream</span><span class="params">(KeyedStream&lt;T, K&gt; input,</span></span></span><br><span class="line"><span class="function"><span class="params">        WindowAssigner&lt;? <span class="keyword">super</span> T, W&gt; windowAssigner)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">this</span>.input = input;</span><br><span class="line">    <span class="keyword">this</span>.windowAssigner = windowAssigner;</span><br><span class="line">    <span class="comment">//获取一个默认的 Trigger</span></span><br><span class="line">    <span class="keyword">this</span>.trigger = windowAssigner.getDefaultTrigger(input.getExecutionEnvironment());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>可以看到这个 Window 方法传入的参数是一个 WindowAssigner 对象（你可以利用 Flink 现有的 WindowAssigner，也可以根据上面的方法来自定义自己的 WindowAssigner），然后再通过构造一个 WindowedStream 实例（在构造实例的会传入 WindowAssigner 和获取默认的 Trigger）来创建一个 Window。</p><p>另外你可以看到滑动计数窗口，在调用 window 方法之后，还调用了 WindowedStream 的 evictor 和 trigger 方法，trigger 方法会覆盖掉你之前调用 Window 方法中默认的 trigger，如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//滑动计数窗口</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> WindowedStream&lt;T, KEY, GlobalWindow&gt; <span class="title">countWindow</span><span class="params">(<span class="keyword">long</span> size, <span class="keyword">long</span> slide)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> window(GlobalWindows.create()).evictor(CountEvictor.of(size)).trigger(CountTrigger.of(slide));</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//trigger 方法</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> WindowedStream&lt;T, K, W&gt; <span class="title">trigger</span><span class="params">(Trigger&lt;? <span class="keyword">super</span> T, ? <span class="keyword">super</span> W&gt; trigger)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (windowAssigner <span class="keyword">instanceof</span> MergingWindowAssigner &amp;&amp; !trigger.canMerge()) &#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> UnsupportedOperationException(<span class="string">&quot;A merging window assigner cannot be used with a trigger that does not support merging.&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (windowAssigner <span class="keyword">instanceof</span> BaseAlignedWindowAssigner) &#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> UnsupportedOperationException(<span class="string">&quot;Cannot use a &quot;</span> + windowAssigner.getClass().getSimpleName() + <span class="string">&quot; with a custom trigger.&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//覆盖之前的 trigger</span></span><br><span class="line">    <span class="keyword">this</span>.trigger = trigger;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">this</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>从上面的各种窗口实现，你就会发现了：Evictor 是可选的，但是 WindowAssigner 和 Trigger 是必须会有的，这种创建 Window 的方法充分利用了 KeyedStream 和 WindowedStream 的 API，再加上现有的 WindowAssigner、Trigger、Evictor，你就可以创建 Window 了，另外你还可以自定义这三个窗口组件的实现类来满足你公司项目的需求。</p>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
            <tag> Flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>状态一致性</title>
      <link href="2019/12/16/Flink%20%E7%8A%B6%E6%80%81%E4%B8%80%E8%87%B4%E6%80%A7/"/>
      <url>2019/12/16/Flink%20%E7%8A%B6%E6%80%81%E4%B8%80%E8%87%B4%E6%80%A7/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><h1 id="1-概述"><a href="#1-概述" class="headerlink" title="1. 概述"></a>1. 概述</h1><h1 id="2-状态一致性分类"><a href="#2-状态一致性分类" class="headerlink" title="2. 状态一致性分类"></a>2. 状态一致性分类</h1><h2 id="2-1-AT-MOST-ONCE"><a href="#2-1-AT-MOST-ONCE" class="headerlink" title="2.1. AT-MOST-ONCE"></a>2.1. <code>AT-MOST-ONCE</code></h2><h2 id="2-2-AT-LEAST-ONCE"><a href="#2-2-AT-LEAST-ONCE" class="headerlink" title="2.2. AT-LEAST-ONCE"></a>2.2. <code>AT-LEAST-ONCE</code></h2><h2 id="2-3-EXACTLY-ONCE"><a href="#2-3-EXACTLY-ONCE" class="headerlink" title="2.3. EXACTLY-ONCE"></a>2.3. <code>EXACTLY-ONCE</code></h2><blockquote><p> <code>Flink</code> 使用 <code>checkpoint</code>保证 <code>EXACTLY-ONCE</code></p></blockquote><h1 id="3-端到端状态一致性"><a href="#3-端到端状态一致性" class="headerlink" title="3. 端到端状态一致性"></a>3. 端到端状态一致性</h1><blockquote><p><strong><code>Flink</code> 通过快照机制和 <code>Barrier</code> 来实现一致性的保证，当任务中途 <code>crash</code> 或者<code>cancel</code> 之后，可以通过<code>checkpoing</code> 或者 <code>savepoint</code> 来进行恢复，实现数据流的重放。从而让任务达到一致性的效果，这种一致性需要开启 <code>exactly_once</code>模式之后才行。</strong></p><p>需要记住的是这边的 <code>Flink</code>  <code>exactly_once</code> 只是说在 <code>Flink</code> 内部是 <code>exactly_once</code> 的，并不能保证与外部存储交互时的 <code>exactly_once</code>，如果要实现外部存储连接后的 <code>exactly_once</code>，需要进行做一些特殊的处理。</p></blockquote><h2 id="3-1-预写日志"><a href="#3-1-预写日志" class="headerlink" title="3.1. 预写日志"></a>3.1. 预写日志</h2><h2 id="3-2-两阶段提交"><a href="#3-2-两阶段提交" class="headerlink" title="3.2. 两阶段提交"></a>3.2. 两阶段提交</h2><h1 id="4-Flink-Kafka-端到端状态一致性"><a href="#4-Flink-Kafka-端到端状态一致性" class="headerlink" title="4. Flink + Kafka 端到端状态一致性"></a>4. <code>Flink</code> + <code>Kafka</code> 端到端状态一致性</h1>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
            <tag> Flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark Streaming 和 Kafka 整合开发</title>
      <link href="2019/12/15/Spark%20Streaming%20%E5%92%8C%20Kafka%20%E6%95%B4%E5%90%88%E5%BC%80%E5%8F%91/"/>
      <url>2019/12/15/Spark%20Streaming%20%E5%92%8C%20Kafka%20%E6%95%B4%E5%90%88%E5%BC%80%E5%8F%91/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p>Apache Kafka 是一个分布式的消息发布-订阅系统。可以说，任何实时大数据处理工具缺少与 Kafka 整合都是不完整的。本文将介绍如何使用 Spark Streaming 从 Kafka 中接收数据，</p><p>这里介绍两种方法</p><ol><li>使用 Receivers 和 Kafka 高层次的 API</li><li>使用 Direct API，这是使用低层次的 KafkaAPI，并没有使用到 Receivers，是 Spark 1.3.0中开始引入的。这两种方法有不同的编程模型，性能特点和语义担保</li></ol><a id="more"></a><h2 id="基于-Receivers-的方法"><a href="#基于-Receivers-的方法" class="headerlink" title="基于 Receivers 的方法"></a>基于 Receivers 的方法</h2><p>使用了 Receivers 来接收数据。Receivers 的实现使用到 Kafka 高层次的消费者API。对于所有的Receivers，接收到的数据将会保存在 Spark executors 中，然后由 Spark Streaming 启动的 Job 来处理这些数据。</p><p>然而，在默认的配置下，这种方法在失败的情况下会丢失数据，为了保证零数据丢失，你可以在 Spark Streaming 中使用 WAL 日志，这是在 Spark 1.2.0 才引入的功能，这使得我们可以将接收到的数据保存到WAL 中，所以在失败的时候，我们可以从 WAL 中恢复，而不至于丢失数据。</p><h3 id="Direct-API"><a href="#Direct-API" class="headerlink" title="Direct API"></a>Direct API</h3><p>和基于 Receiver 接收数据不一样，这种方式定期地从 Kafka 的 <code>topic+partition</code> 中查询最新的偏移量，再根据定义的偏移量范围在每个 batch 里面处理数据。当作业需要处理的数据来临时，Spark 通过调用 Kafka 的简单消费者 API 读取一定范围的数据。</p><p>和基于 Receiver 方式相比，这种方式主要有一些几个优点：</p><ol><li><p><strong>简化并行</strong></p><p>我们不需要创建多个Kafka 输入流，然后union他们。而使用directStream，Spark Streaming将会创建和 Kafka 分区一样的 RDD 分区个数，而且会从 Kafka 并行地读取数据，也就是说Spark分区将会和Kafka分区有一一对应的关系，这对我们来说很容易理解和使用；</p></li><li><p><strong>高效</strong></p><p>第一种实现零数据丢失是通过将数据预先保存在 WAL 中，这将会复制一遍数据，这种方式实际上很不高效，因为这导致了数据被拷贝两次：一次是被 Kafka 复制；另一次是写到WAL中。但是本文介绍的方法因为没有Receiver，从而消除了这个问题，所以不需要WAL日志；</p></li><li><p><strong>恰好一次语义（Exactly-once semantics）</strong></p><p>文章中通过使用Kafka高层次的API把偏移量写入Zookeeper中，这是读取Kafka中数据的传统方法。虽然这种方法可以保证零数据丢失，但是还是存在一些情况导致数据会丢失，因为在失败情况下通过Spark Streaming读取偏移量和Zookeeper中存储的偏移量可能不一致。而本文提到的方法是通过Kafka低层次的API，并没有使用到Zookeeper，偏移量仅仅被Spark Streaming保存在Checkpoint中。这就消除了Spark Streaming和Zookeeper中偏移量的不一致，而且可以保证每个记录仅仅被Spark Streaming读取一次，即使是出现故障。</p></li></ol><p>但是本方法唯一的坏处就是没有更新 Zookeeper 中的偏移量，所以基于 Zookeeper 的 Kafka 监控工具将会无法显示消费的状况。然而你可以通过 Spark 提供的 API 手动地将偏移量写入到 Zookeeper 中。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><h4 id="Spark-Streaming-和-Kafka-整合是如何保证数据零丢失"><a href="#Spark-Streaming-和-Kafka-整合是如何保证数据零丢失" class="headerlink" title="Spark Streaming 和 Kafka 整合是如何保证数据零丢失 ? "></a><font color='blue'>Spark Streaming 和 Kafka 整合是如何保证数据零丢失 ? </font></h4><p>当我们正确地部署好 Spark Streaming，我们就可以使用 Spark Streaming 提供的零数据丢失机制。为了体验这个关键的特性，你需要满足以下几个先决条件：</p><ol><li>输入的数据来自可靠的数据源和可靠的接收器</li><li>应用程序的 metadata 被 application 的 driver 持久化了(checkpointed)</li><li>启用了 WAL 特性(Write ahead log)</li></ol>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
            <tag> spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Watermark 与 Window 结合来处理延迟数据</title>
      <link href="2019/12/15/Watermark-%E4%B8%8E-Window-%E7%BB%93%E5%90%88%E6%9D%A5%E5%A4%84%E7%90%86%E5%BB%B6%E8%BF%9F%E6%95%B0%E6%8D%AE/"/>
      <url>2019/12/15/Watermark-%E4%B8%8E-Window-%E7%BB%93%E5%90%88%E6%9D%A5%E5%A4%84%E7%90%86%E5%BB%B6%E8%BF%9F%E6%95%B0%E6%8D%AE/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p>在设置 Periodic Watermark 时，是允许提供一个参数，表示数据最大的延迟时间。其实这个值要结合自己的业务以及数据的情况来设置，如果该值设置的太小会导致数据因为网络或者其他的原因从而导致乱序或者延迟的数据太多，那么最后窗口触发的时候，可能窗口里面的数据量很少，那么这样计算的结果很可能误差会很大，对于有的场景（要求正确性比较高）是不太符合需求的。但是如果该值设置的太大，那么就会导致很多窗口一直在等待延迟的数据，从而一直不触发，这样首先就会导致数据的实时性降低，另外将这么多窗口的数据存在内存中，也会增加作业的内存消耗，从而可能会导致作业发生 OOM 的问题。</p><a id="more"></a><p>综上建议：</p><ul><li>合理设置允许数据最大延迟时间</li><li>不太依赖事件时间的场景就不要设置时间策略为 EventTime</li></ul><h3 id="延迟数据该如何处理-三种方法"><a href="#延迟数据该如何处理-三种方法" class="headerlink" title="延迟数据该如何处理(三种方法)"></a>延迟数据该如何处理(三种方法)</h3><h4 id="丢弃（默认）"><a href="#丢弃（默认）" class="headerlink" title="丢弃（默认）"></a>丢弃（默认）</h4><p>在 Flink 中，对这么延迟数据的默认处理方式是丢弃。</p><h4 id="allowedLateness-再次指定允许数据延迟的时间"><a href="#allowedLateness-再次指定允许数据延迟的时间" class="headerlink" title="allowedLateness 再次指定允许数据延迟的时间"></a>allowedLateness 再次指定允许数据延迟的时间</h4><p>allowedLateness 表示允许数据延迟的时间，这个方法是在 WindowedStream 中的，用来设置允许窗口数据延迟的时间，超过这个时间的元素就会被丢弃，这个的默认值是 0，该设置仅针对于以事件时间开的窗口，它的源码如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> WindowedStream&lt;T, K, W&gt; <span class="title">allowedLateness</span><span class="params">(Time lateness)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">final</span> <span class="keyword">long</span> millis = lateness.toMilliseconds();</span><br><span class="line">    checkArgument(millis &gt;= <span class="number">0</span>, <span class="string">&quot;The allowed lateness cannot be negative.&quot;</span>);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">this</span>.allowedLateness = millis;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">this</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>之前有多个小伙伴问过我 Watermark 中允许的数据延迟和这个数据延迟的区别是啥？我的回复是该允许延迟的时间是在 Watermark 允许延迟的基础上增加的时间。那么具体该如何使用 allowedLateness 呢。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">dataStream.assignTimestampsAndWatermarks(<span class="keyword">new</span> TestWatermarkAssigner())</span><br><span class="line">    .keyBy(<span class="keyword">new</span> TestKeySelector())</span><br><span class="line">    .timeWindow(Time.milliseconds(<span class="number">1</span>), Time.milliseconds(<span class="number">1</span>))</span><br><span class="line">    .allowedLateness(Time.milliseconds(<span class="number">2</span>))  <span class="comment">//表示允许再次延迟 2 毫秒</span></span><br><span class="line">    .apply(<span class="keyword">new</span> WindowFunction&lt;Integer, String, Integer, TimeWindow&gt;() &#123;</span><br><span class="line">        <span class="comment">//计算逻辑</span></span><br><span class="line">    &#125;);</span><br></pre></td></tr></table></figure><h4 id="sideOutputLateData-收集迟到的数据"><a href="#sideOutputLateData-收集迟到的数据" class="headerlink" title="sideOutputLateData 收集迟到的数据"></a>sideOutputLateData 收集迟到的数据</h4><p>sideOutputLateData 这个方法同样是 WindowedStream 中的方法，该方法会将延迟的数据发送到给定 OutputTag 的 side output 中去，然后你可以通过 <code>SingleOutputStreamOperator.getSideOutput(OutputTag)</code> 来获取这些延迟的数据。具体的操作方法如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//定义 OutputTag</span></span><br><span class="line">OutputTag&lt;Integer&gt; lateDataTag = <span class="keyword">new</span> OutputTag&lt;Integer&gt;(<span class="string">&quot;late&quot;</span>)&#123;&#125;;</span><br><span class="line"></span><br><span class="line">SingleOutputStreamOperator&lt;String&gt; windowOperator = dataStream</span><br><span class="line">        .assignTimestampsAndWatermarks(<span class="keyword">new</span> TestWatermarkAssigner())</span><br><span class="line">        .keyBy(<span class="keyword">new</span> TestKeySelector())</span><br><span class="line">        .timeWindow(Time.milliseconds(<span class="number">1</span>), Time.milliseconds(<span class="number">1</span>))</span><br><span class="line">        .allowedLateness(Time.milliseconds(<span class="number">2</span>))</span><br><span class="line">        .sideOutputLateData(lateDataTag)    <span class="comment">//指定 OutputTag</span></span><br><span class="line">        .apply(<span class="keyword">new</span> WindowFunction&lt;Integer, String, Integer, TimeWindow&gt;() &#123;</span><br><span class="line">            <span class="comment">//计算逻辑</span></span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">windowOperator.addSink(resultSink);</span><br><span class="line"></span><br><span class="line"><span class="comment">//通过指定的 OutputTag 从 Side Output 中获取到延迟的数据之后，你可以通过 addSink() 方法存储下来，这样可以方便你后面去排查哪些数据是延迟的。</span></span><br><span class="line">windowOperator.getSideOutput(lateDataTag)</span><br><span class="line">        .addSink(lateResultSink);</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
            <tag> Flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink 数据倾斜</title>
      <link href="2019/12/15/Flink-%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C/"/>
      <url>2019/12/15/Flink-%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p>在大数据计算场景，无论使用 MapReduce、Spark 还是 Flink 计算框架，无论是批处理还是流处理都存在数据倾斜的问题，通过本节学习产生数据倾斜的原因及如何在生产环境解决数据倾斜。</p><a id="more"></a><h3 id="数据倾斜简介"><a href="#数据倾斜简介" class="headerlink" title="数据倾斜简介"></a>数据倾斜简介</h3><p>分析一个计算各 app PV 的案例，如下图所示，圆球表示 app1 的日志，方块表示 app2 的日志，Source 端从外部系统读取用户上报的各 app 行为日志，要计算各 app 的 PV，所以按照 app 进行 keyBy，相同 app 的数据发送到同一个 Operator 实例中处理，keyBy 后对 app 的 PV 值进行累加来，最后将计算的 PV 结果输出到外部 Sink 端。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-11-12-004442.jpg" alt="img"></p><p>可以看到在任务运行过程中，计算 Count 的算子有两个并行度，其中一个并行度处理 app1 的数据，另一个并行度处理 app2 的数据。由于 app1 比较热门，所以 app1 的日志量远大于 app2 的日志量，造成计算 app1 PV 的并行度压力过大成为整个系统的瓶颈，而计算 app2 PV 的并行度数据量较少所以 CPU、内存以及网络资源的使用率整体都比较低，这就是产生数据倾斜的案例。</p><h3 id="判断是否存在数据倾斜"><a href="#判断是否存在数据倾斜" class="headerlink" title="判断是否存在数据倾斜"></a>判断是否存在数据倾斜</h3><p>这里再通过一个案例来讲述 Flink 任务如何来判断是否存在数据倾斜，如下图所示，是 Flink Web UI Job 页面展示的任务执行计划，可以看到任务经过 Operator Chain 后，总共有两个 Task，上游 Task 将数据 keyBy 后发送到下游 Task，如何判断第二个 Task 计算的数据是否存在数据呢？</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-11-12-004443.jpg" alt="img"></p><p>如下图所示，通过 Flink Web UI 中 Job 页面的第一个 Subtasks 选项卡，可以看到任务的两个 Task，点击 Task，可以看到 Task 相应的 Subtask 详情。例如 Subtask 的启动时间、结束时间、持续时长、接收数据量的字节数以及接收数据的个数。图中可以看到，相同 Task 的多个 Subtask 中，有的 Subtask 接收到 1.69 TB 的数据量，有的 Subtask 接收到 17.6 TB 的数据量，通过 Flink Web UI 可以精确地看到每个 Subtask 处理了多少数据，即可判断出 Flink 任务是否存在数据倾斜，接下来学习 Flink 中如何来解决数据倾斜。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-11-12-004431.jpg" alt="img"></p><h3 id="分析和解决数据倾斜问题"><a href="#分析和解决数据倾斜问题" class="headerlink" title="分析和解决数据倾斜问题"></a>分析和解决数据倾斜问题</h3><p>在 Flink 中，很多因素都会导致数据倾斜，例如 9.6.1 节描述的 keyBy 后的聚合操作存在数据倾斜。keyBy 之前的数据直接来自于数据源，一般不会出现数据倾斜，除非数据源中的数据发生了数据倾斜。本小节将从多个角度来解决数据倾斜。</p><h5 id="keyBy-后的聚合操作存在数据倾斜"><a href="#keyBy-后的聚合操作存在数据倾斜" class="headerlink" title="keyBy 后的聚合操作存在数据倾斜"></a>keyBy 后的聚合操作存在数据倾斜</h5><p>Flink 社区关于数据倾斜的解决方案炒得最热的也莫过于 LocalKeyBy 了。Flink 中数据倾斜一般发生于 keyBy 之后的聚合操作，LocalKeyBy 的思想是：在 keyBy 上游算子数据发送之前，首先在上游算子的本地对数据进行聚合后再发送到下游，使下游接收到的数据量大大减少，从而使得 keyBy 之后的聚合操作不再是任务的瓶颈。</p><p>如下图所示，Source 算子向下游发送数据之前，首先对数据进行预聚合，Source Subtask 0 预聚合后，圆圈 PV 值为 5、方块 PV 值为 2，Source Subtask 1 预聚合后，圆圈 PV 值为 6、方块 PV 值为 1。keyBy 后，Count 算子进行 PV 值的累加，计算圆圈 PV 的 Subtask 接收到 5 和 6，只需要将 5+6 即可计算出圆圈总 PV 值为 11，计算方块 PV 的 Subtask 接收到 2 和 1，只需要将 2 +1 即可计算出方块总 PV 值为 3，最后将圆圈和方块的 PV 结果输出到 Sink 端即可。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-11-12-004439.jpg" alt="img"></p><p>使用该方案计算 PV，带来了两个非常大的好处。</p><ul><li>在上游算子中对数据进行了预聚合，因此大大减少了上游往下游发送的数据量，从而减少了网络间的数据传输，节省了集群的带宽资源。上图案例中如果不聚合，上游需要往下游发送 14 条数据，聚合后仅仅需要发送 4 条数据即可。如果上游算子接收 1 万条数据后聚合一次，那么数据的压缩比会更大，优化效果会更加明显。</li><li>下游拿到的直接是上游聚合好的中间结果，因此下游 Count 算子计算的数据量大大减少，而且 Count 算子不再会有数据倾斜的问题。</li></ul><p>上游算子相比之前多了一个聚合的工作，所以压力必然会增加，但是只要数据源不发生数据倾斜，那么上游 Source 算子的各并行度之间的负载就会比较均衡。</p><p>这里就是 MapReduce 中 Combiner 的思想嘛，在 Map 端对数据进行预聚合之后，再将预聚合后的数据发送到 Reduce 端去处理，从而大大减少了 shuffle 的数据量。</p><p>虽然思想一样，但 Flink 流处理的预聚合相比 MapReduce 的批处理而言，带来了一个新的挑战：Flink 是天然的流式处理，即来一条数据处理一条（这里不考虑 Flink 网络传输层的 Buffer 机制），但是聚合操作要求必须是多条数据或者一批数据才能聚合，单条数据没有办法通过聚合来减少数据量。</p><p>所以从 Flink LocalKeyBy 实现原理来讲，必然会存在一个积攒批次的过程，在上游算子中必须攒够一定的数据量，对这些数据聚合后再发送到下游。既然是积攒批次，那肯定有一个积攒批次的策略，上图案例可以理解为每个批次 7 条数据，当读取到 7 条数据后，将这 7 条数据聚合后发送到下游。</p><p><strong>具体实现逻辑是：内存里维护一个计数器，每来一条数据计数器加一，并将数据聚合放到内存 Buffer 中，当计数器到达 7 时，将内存 Buffer 中的数据发送到下游、计数器清零、Buffer 清空。</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LocalKeyByFlatMap</span> <span class="keyword">extends</span> <span class="title">FlatMapFunction</span>&lt;<span class="title">String</span>, <span class="title">Tuple2</span>&lt;<span class="title">String</span>, <span class="title">Long</span>&gt;&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//本地 buffer，存放 local 端缓存的 app 的 pv 信息</span></span><br><span class="line">    <span class="keyword">private</span> HashMap&lt;String, Long&gt; localPvStat;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//缓存的数据量大小，即：缓存多少数据再向下游发送</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> batchSize;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//计数器，获取当前批次接收的数据量</span></span><br><span class="line">    <span class="keyword">private</span> AtomicInteger currentSize = <span class="keyword">new</span> AtomicInteger(<span class="number">0</span>);;</span><br><span class="line"></span><br><span class="line">    LocalKeyByFlatMap(<span class="keyword">int</span> batchSize)&#123;</span><br><span class="line">        <span class="keyword">this</span>.batchSize = batchSize;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">flatMap</span><span class="params">(String in, Collector collector)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="comment">//  将新来的数据添加到 buffer 中</span></span><br><span class="line">        Long pv = localPvStat.getOrDefault(in, <span class="number">0L</span>);</span><br><span class="line">        localPvStat.put(in, pv + <span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 如果到达设定的批次，则将 buffer 中的数据发送到下游</span></span><br><span class="line">        <span class="keyword">if</span>(currentSize.incrementAndGet() &gt;= batchSize)&#123;</span><br><span class="line">            <span class="comment">// 遍历 Buffer 中数据，发送到下游</span></span><br><span class="line">            <span class="keyword">for</span>(Map.Entry&lt;String, Long&gt; appIdPv: localPvStat.entrySet()) &#123;</span><br><span class="line">                collector.collect(Tuple2.of(appIdPv.getKey(), appIdPv.getValue()));</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">// buffer 清空，计数器清零</span></span><br><span class="line">            localPvStat.clear();</span><br><span class="line">            currentSize.set(<span class="number">0</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>代码逻辑比较简单，使用了 FlatMap 算子来做缓冲，每来一条数据都需要检索，为了提高检索效率，所以这里使用 HashMap 类型的 localPvStat 用来做 Buffer 来缓存数据，currentSize 记录当前批次已经往 localPvStat 中写入的数据量。在 LocalKeyByFlatMap 构造器中需要初始化 batchSize，即批次大小。flatMap 方法将新数据添加到 localPvStat 中，currentSize 进行加一操作，且 currentSize 加一后如果大于 batchSize 则表示当前批次的数据已经够了，需要将数据发送到下游，则遍历 localPvStat，将 Buffer 中的数据发送到下游，并将 localPvStat 清空且 currentSize 清零。</p><p>代码逻辑简单易懂，但是问题又来了，在积攒批次的过程中，如果发生故障，Flink 任务能保障 Exactly Once 吗？</p><p>直接给出答案：不能保证 Exactly Once，可能会丢数据，为什么呢？</p><p>如下图所示，batchSize 设置的 7，但是当 JobManager 触发 Checkpoint 的时候，Source Subtask 0 消费到 offset 为 13 的位置、Source Subtask 1 消费到 offset 为 12 的位置，所以 Source 0 会将 offset=13 保存到状态后端，Source 1 会将 offset=12 保存到状态后端。接着 Checkpoint barrier 跟随着数据往下游发送到 LocalKeyBy，此时 LocalKeyBy 0 的 Buffer 中只有 6 条数据、LocalKeyBy 1 的 Buffer 中只有 5 条数据，所以 LocalKeyBy 0 和 1 都不会将数据发送到下游。但是 barrier 会接着往下游传递到 Count 算子，Count 算子会对自身状态信息进行快照，Count 0 会将圆圈 PV=11 保存到状态后端、Count 1 会将圆圈 PV=3 保存到状态后端，各 task 向 JobManager 反馈，最后 Checkpoint 成功了，紧接着数据正常开始处理。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-11-12-004433.jpg" alt="img"></p><p>数据正常处理一段时间后，由于机器故障 Flink 任务突然挂了，如下图所示，Flink 任务会从状态恢复，Source Subtask 0 从 offset 为 13 的位置开始消费 Kafka，Source Subtask 1 从 offset 为 12 的位置开始消费 Kafka。Count 0 恢复后保存圆圈的 PV 为 11，Count 1 恢复后保存方块的 PV 为 3。此时任务从状态中恢复完成，正常开始处理数据，请问 Flink 任务从状态恢复后丢数据了吗？</p><p>丢了，因为 Source 0 对应的 offset 13 表示 Source 0 消费了 13 条数据，但是其中有 6 条数据缓存在 LocalKeyBy 0 的 Buffer 中没及时发送到下游，所以这 6 条数据丢了，同理 Source 1 对应的 offset 12 表示 Source 1 消费了 12 条数据，其中还有 5 条数据缓存在 LocalKeyBy 1 的 Buffer 中没及时发送到下游，所以这 5 条数据也丢了。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-11-13-%E7%A7%AF%E6%94%92%E6%89%B9%E6%AC%A1%E7%9A%84%E8%BF%87%E7%A8%8B%E4%B8%AD%20Restore.png" alt="img"></p><p>通过上述详细案例分析，知道了我们设计的 LocalKeyBy 虽然能够提高性能，但存在丢数据的风险。，<strong>Flink 虽然支持 Exactly Once，但不是说你的代码随便瞎写 Flink 也能保证 Exactly Once，做为使用 Flink 的一员，我们应该根据原理书写出能保证 Flink Exactly Once 的代码。</strong></p><p>上述方案该如何完善才能保证 Exactly Once 呢？在 Checkpoint 时上述方案会把 LocalKeyBy 算子 Buffer 中的数据丢弃，所以重点应该是如何来保证 LocalKeyBy 算子 Buffer 中的数据不丢。在 Checkpoint 时可以将 Buffer 中还未发送到下游的数据保存到 Flink 的状态中，这样当 Flink 任务从 Checkpoint 处恢复时，可以将那些在 Buffer 中的数据从状态后端恢复。如下图所示，相比上述方案，Checkpoint 时会将 LocalKeyBy 算子 Buffer 中的数据也保存到状态后端。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-11-12-4434.jpg" alt="img"></p><p>如下图所示，当 Flink 任务从 Checkpoint 处恢复时，不仅恢复 offset 信息和 PV 信息，还需要把 LocalKeyBy 算子 Buffer 中的数据恢复，这样就可以保证不丢数据了。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-11-12-004436.jpg" alt="img"></p><p>具体代码如何实现呢？Checkpoint 时 LocalKeyBy 算子可能还有缓冲的数据没发送到下游，为了保证 Exactly Once，这里需要将 Buffer 中的数据保存在状态中。</p><p>Flink 有两种 State 分别是 OperatorState 和 KeyedState，OperatorState 是一个 Operator 实例对应一个State，KeyedState 是每个 key 对应一个 State，KeyedState 只能作用于 keyby 算子之后的 KeyedStream。</p><p>上图中我们可以看出，LocalKeyBy 算子位于 keyBy 算子之前，因此 LocalKeyBy 算子内部不能使用 KeyedState，只能使用 OperatorState，且 OperatorState 只支持一种数据结构，即 ListState，所以这里 buffer 中的数据只能保存在 OperatorState 类型的 ListState 中。当 Checkpoint 时，需要将内存 buffer 中的数据添加到 ListState，状态中需要保存 KV 类型的数据，key 是 appId、value 是 app 对应的 PV 值。</p><p>这里为了在 ListState 中保存 KV 格式的数据，需要将 buffer 中 KV 类型的数据转化为 Tuple2 类型后再添加到 ListState 中。代码具体实现如下所示：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LocalKeyByFlatMap</span> <span class="keyword">extends</span> <span class="title">RichFlatMapFunction</span>&lt;<span class="title">String</span>, <span class="title">Tuple2</span>&lt;<span class="title">String</span>, <span class="title">Long</span>&gt;&gt; <span class="keyword">implements</span> <span class="title">CheckpointedFunction</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//Checkpoint 时为了保证 Exactly Once，将 buffer 中的数据保存到该 ListState 中</span></span><br><span class="line">    <span class="keyword">private</span> ListState&lt;Tuple2&lt;String, Long&gt;&gt; localPvStatListState;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//本地 buffer，存放 local 端缓存的 app 的 pv 信息</span></span><br><span class="line">    <span class="keyword">private</span> HashMap&lt;String, Long&gt; localPvStat;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//缓存的数据量大小，即：缓存多少数据再向下游发送</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> batchSize;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//计数器，获取当前批次接收的数据量</span></span><br><span class="line">    <span class="keyword">private</span> AtomicInteger currentSize;</span><br><span class="line"></span><br><span class="line">    LocalKeyByFlatMap(<span class="keyword">int</span> batchSize)&#123;</span><br><span class="line">        <span class="keyword">this</span>.batchSize = batchSize;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">flatMap</span><span class="params">(String in, Collector collector)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="comment">//  将新来的数据添加到 buffer 中</span></span><br><span class="line">        Long pv = localPvStat.getOrDefault(in, <span class="number">0L</span>);</span><br><span class="line">        localPvStat.put(in, pv + <span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 如果到达设定的批次，则将 buffer 中的数据发送到下游</span></span><br><span class="line">        <span class="keyword">if</span>(currentSize.incrementAndGet() &gt;= batchSize)&#123;</span><br><span class="line">            <span class="comment">// 遍历 Buffer 中数据，发送到下游</span></span><br><span class="line">            <span class="keyword">for</span>(Map.Entry&lt;String, Long&gt; appIdPv: localPvStat.entrySet()) &#123;</span><br><span class="line">                collector.collect(Tuple2.of(appIdPv.getKey(), appIdPv.getValue()));</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">// Buffer 清空，计数器清零</span></span><br><span class="line">            localPvStat.clear();</span><br><span class="line">            currentSize.set(<span class="number">0</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">snapshotState</span><span class="params">(FunctionSnapshotContext functionSnapshotContext)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 将 buffer 中的数据保存到状态中，来保证 Exactly Once</span></span><br><span class="line">        localPvStatListState.clear();</span><br><span class="line">        <span class="keyword">for</span>(Map.Entry&lt;String, Long&gt; appIdPv: localPvStat.entrySet()) &#123;</span><br><span class="line">            localPvStatListState.add(Tuple2.of(appIdPv.getKey(), appIdPv.getValue()));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">initializeState</span><span class="params">(FunctionInitializationContext context)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 从状态中恢复 buffer 中的数据</span></span><br><span class="line">        localPvStatListState = context.getOperatorStateStore().getListState(</span><br><span class="line">                <span class="keyword">new</span> ListStateDescriptor&lt;&gt;(<span class="string">&quot;localPvStat&quot;</span>,</span><br><span class="line">                        TypeInformation.of(<span class="keyword">new</span> TypeHint&lt;Tuple2&lt;String, Long&gt;&gt;() &#123;</span><br><span class="line">                        &#125;)));</span><br><span class="line">        localPvStat = <span class="keyword">new</span> HashMap();</span><br><span class="line">        <span class="keyword">if</span>(context.isRestored()) &#123;</span><br><span class="line">            <span class="comment">// 从状态中恢复数据到 localPvStat 中</span></span><br><span class="line">            <span class="keyword">for</span>(Tuple2&lt;String, Long&gt; appIdPv: localPvStatListState.get())&#123;</span><br><span class="line">                localPvStat.put(appIdPv.f0, appIdPv.f1);</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">//  从状态恢复时，默认认为 buffer 中数据量达到了 batchSize，需要向下游发送数据了</span></span><br><span class="line">            currentSize = <span class="keyword">new</span> AtomicInteger(batchSize);</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            currentSize = <span class="keyword">new</span> AtomicInteger(<span class="number">0</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>上述改进方案后的 LocalKeyByFlatMap 相比之前方案仅仅增加了一个属性，即：<code>ListState&gt;</code> 类型的 localPvStatListState 用来存放 Checkpoint 时 buffer 中那些可能丢失的数据。在 snapshotState 方法中将 buffer 中的数据保存到状态中，在 initializeState 方法中将状态中恢复的数据 put 到 buffer 中并初始化计数器 currentSize。代码相对比较简答，容易看懂。</p><p>请问上述代码能保障 buffer 中的数据不丢吗？如果不修改 Source Task 和 LocalKeyByFlatMap 算子的并行度，理论来讲可以保证 Exactly Once，但是一旦修改并行度，还能保证 Exactly Once 吗？当并行度降低后，getOperatorStateStore().getListState() 恢复 ListState 时，会把 ListState 中的状态信息均匀分布到各个 Operator 实例中。当上述案例中 LocalKeyBy 的并行度从 2 调节为 1 时，数据恢复如下图所示：</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-11-12-004441.jpg" alt="img"></p><p>首先 Source 端 partition 0 和 partition 1 的 offset 信息恢复没有问题，Count 算子圆圈和方块的 PV 信息恢复也没有问题。关键在于 LocalKeyBy 算子中 PV 信息恢复时会丢数据吗？状态恢复时，从状态中将 PV 信息恢复到 buffer 中的核心代码如下所示：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 从状态中恢复数据到 localPvStat 中</span></span><br><span class="line"><span class="keyword">for</span>(Tuple2&lt;String, Long&gt; appIdPv: localPvStatListState.get())&#123;</span><br><span class="line">    localPvStat.put(appIdPv.f0, appIdPv.f1);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>从状态中会恢复 4 个 Tuple2，分别是 &lt;圆圈,4&gt;、&lt;方块,2&gt;、&lt;圆圈,4&gt;、&lt;方块,1&gt;，这里有两个圆圈、两个方块，恢复到 HashMap 类型的 localPvStat，HashMap 中相同的 key 不能重复，所以 HashMap 中不可能保存两个圆圈和两个方块。恢复时 app 相同的数据，应该将其 PV 值累加，所以恢复的结果应该是 &lt;圆圈,8&gt;、&lt;方块,3&gt;。但是上述代码，仅仅是覆盖操作，假如遍历状态时返回的顺序为 &lt;圆圈,4&gt;、&lt;方块,2&gt;、&lt;圆圈,4&gt;、&lt;方块,1&gt;，那么上述恢复流程为：将上述元素依次 put 到 HashMap 中，所以 HashMap 类型的 buffer 恢复完数据后，buffer 中保存的 PV 信息为 &lt;圆圈,4&gt;、&lt;方块,1&gt;。显然恢复过程中的覆盖操作将状态数据 &lt;圆圈,4&gt;、&lt;方块,2&gt; 丢了，所以上述方案如果不修改并行度时，不会丢数据，如果修改并行度时，可能会丢数据。</p><p>在使用状态来保证 Exactly Once 时，必须考虑修改并行度后，状态如何正常恢复的情况。优化后的代码如下所示，仅仅修改 initializeState 方法中恢复状态的逻辑：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 从状态中恢复 buffer 中的数据</span></span><br><span class="line"><span class="keyword">for</span>(Tuple2&lt;String, Long&gt; appIdPv: localPvStatListState.get())&#123;</span><br><span class="line">    <span class="keyword">long</span> pv = localPvStat.getOrDefault(appIdPv.f0, <span class="number">0L</span>);</span><br><span class="line">    <span class="comment">// 如果出现 pv != 0，说明改变了并行度，</span></span><br><span class="line">    <span class="comment">// ListState 中的数据会被均匀分发到新的 subtask 中</span></span><br><span class="line">    <span class="comment">// 所以单个 subtask 恢复的状态中可能包含两个相同的 app 的数据</span></span><br><span class="line">    localPvStat.put(appIdPv.f0, pv + appIdPv.f1);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>代码中，首先从 buffer 中获取当前 app 的 PV 数据，如果 buffer 中不包含当前 app 则 PV 值返回 0，如果 buffer 中包含了当前 app 则返回相应的 PV 值，将 buffer 中的 pv 加当前的 pv，put 到 buffer 中即可保证恢复时不丢数据。</p><p>到这里 LocalKeyBy 的思路及具体代码实现都讲完了，也带着大家分析了多种可能丢数据的情况，并一一解决。上述完整的代码实现请参阅。上述代码实现有个局限性，就是需要了解业务，按照下游的聚合逻辑，在上游 keyBy 之前同样也需要实现一遍。关于通用的 LocalKeyBy 实现，Flink 源码中目前还没有此功能，对具体实现原理感兴趣的可以参阅腾讯杨华老师贡献的 <a href="https://cwiki.apache.org/confluence/display/FLINK/FLIP-44%3A+Support+Local+Aggregation+in+Flink">FLIP-44</a>。</p><h5 id="keyBy-之前发生数据倾斜"><a href="#keyBy-之前发生数据倾斜" class="headerlink" title="keyBy 之前发生数据倾斜"></a>keyBy 之前发生数据倾斜</h5><p>上一部分分析了 keyBy 后由于数据本身的特征可能会发生数据倾斜，可以在 keyBy 之前进行一次预聚合，从而使得 keyBy 后的数据量大大降低。但是如果 keyBy 之前就存在数据倾斜呢？这样上游算子的某些实例可能处理的数据较多，某些实例可能处理的数据较少，产生该情况可能是因为数据源的数据本身就不均匀，例如由于某些原因 Kafka 的 topic 中某些 partition 的数据量较大，某些 partition 的数据量较少。对于不存在 keyBy 的 Flink 任务也会出现该情况，解决思路都一样，主要在于没有 shuffle 的 Flink 任务如何来解决数据倾斜。对于这种情况，需要让 Flink 任务强制进行 shuffle。如何强制 shuffle 呢？了解一下 DataStream 的物理分区策略。</p><table><thead><tr><th align="left">分区策略</th><th align="left">描述</th></tr></thead><tbody><tr><td align="left">dataStream.partitionCustom(partitioner, “someKey”); dataStream.partitionCustom(partitioner, 0);</td><td align="left">根据指定的字段进行分区，指定字段值相同的数据发送到同一个 Operator 实例处理</td></tr><tr><td align="left">dataStream.shuffle();</td><td align="left">将数据随机地分配到下游 Operator 实例</td></tr><tr><td align="left">dataStream.rebalance();</td><td align="left">使用轮循的策略将数据发送到下游 Operator 实例</td></tr><tr><td align="left">dataStream.rescale();</td><td align="left">基于 rebalance 优化的策略，依然使用轮循策略，但仅仅是 TaskManager 内的轮循，只会在 TaskManager 本地进行 shuffle 操作，减少了网络传输</td></tr><tr><td align="left">dataStream.broadcast();</td><td align="left">将数据广播到下游所有的 Operator 实例</td></tr></tbody></table><p>在这里需要解决数据倾斜，只需要使用 shuffle、rebalance 或 rescale 即可将数据均匀分配，从而解决数据倾斜的问题。</p>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
            <tag> Flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink WaterMark 深入理解</title>
      <link href="2019/12/13/Flink-WaterMark-%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3/"/>
      <url>2019/12/13/Flink-WaterMark-%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p>如果在进行 Window 计算操作的时候，如果使用的时间是 Processing Time，那么在 Flink 消费数据的时候，它完全不需要关心的数据本身的时间，意思也就是说不需要关心数据到底是延迟数据还是乱序数据。因为 Processing Time 只是代表数据在 Flink 被处理时的时间，这个时间是顺序的。但是如果你使用的是 Event Time 的话，那么你就不得不面临着这么个问题：事件乱序 &amp; 事件延迟。</p><a id="more"></a><p>在理想的情况下，Event Time 和 Process Time 是相等的，数据发生的时间与数据处理的时间没有延迟，但是现实流处理从事件产生，到流经 source，再到 operator，中间是有一个过程和时间的。虽然大部分情况下，流到 operator 的数据都是按照事件产生的时间顺序来的，但是也不排除由于网络延迟, 分布式等原因，导致乱序的产生，特别是使用 kafka 的话，多个分区的数据无法保证有序。</p><img src="/images/flink/屏幕快照 2020-02-28 下午9.04.30.png" alt="" style="zoom:80%;" /><p><strong>然而在有些场景下，其实是特别依赖于事件时间而不是处理时间，比如：</strong></p><ul><li>错误日志的时间戳，代表着发生的错误的具体时间，开发们只有知道了这个时间戳，才能去还原那个时间点系统到底发生了什么问题，或者根据那个时间戳去关联其他的事件，找出导致问题触发的罪魁祸首</li><li>设备传感器或者监控系统实时上传对应时间点的设备周围的监控情况，通过监控大屏可以实时查看，不错漏重要或者可疑的事件</li></ul><p>这种情况下，最有意义的事件发生的顺序，而不是事件到达 Flink 后被处理的顺序。Flink 支持用户以事件时间来定义窗口（也支持以处理时间来定义窗口），那么这样就要去解决上面所说的两个问题。针对上面的问题（事件乱序 &amp; 事件延迟），Flink 引入了 Watermark 机制来解决。</p><h3 id="Watermark-是什么？"><a href="#Watermark-是什么？" class="headerlink" title="Watermark 是什么？"></a>Watermark 是什么？</h3><p>举个例子：</p><p><strong><font color='grey'>统计 8:00 ~ 9:00 这个时间段打开淘宝 App 的用户数量，Flink 这边可以开个窗口做聚合操作，但是由于网络的抖动或者应用采集数据发送延迟等问题，于是无法保证在窗口时间结束的那一刻窗口中是否已经收集好了在 8:00 ~ 9:00 中用户打开 App 的事件数据，但又不能无限期的等下去？当基于事件时间的数据流进行窗口计算时，最为困难的一点也就是如何确定对应当前窗口的事件已经全部到达。然而实际上并不能百分百的准确判断，因此业界常用的方法就是基于已经收集的消息来估算是否还有消息未到达，这就是 Watermark 的思想。</font></strong></p><p>Watermark 是一种衡量 Event Time 进展的机制，它是数据本身的一个隐藏属性，数据本身携带着对应的 Watermark。Watermark 本质来说就是一个时间戳，代表着比这时间戳早的事件已经全部到达窗口，即假设不会再有比这时间戳还小的事件到达，这个假设是触发窗口计算的基础，只有 Watermark 大于窗口对应的结束时间，窗口才会关闭和进行计算。按照这个标准去处理数据，那么如果后面还有比这时间戳更小的数据，那么就视为迟到的数据，对于这部分迟到的数据，Flink 也有相应的机制去处理。</p><p>下面通过几个图来了解一下 Watermark 是如何工作的！</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-07-08-154340.jpg" alt="img"></p><p>上图中的数据是 Flink 从消息队列中消费的，然后在 Flink 中有个 4s 的时间窗口（根据事件时间定义的窗口），消息队列中的数据是乱序过来的，数据上的数字代表着数据本身的 timestamp，<code>W(4)</code> 和 <code>W(9)</code> 是水印。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-07-08-154747.jpg" alt="img"></p><p>经过 Flink 的消费，数据 <code>1</code>、<code>3</code>、<code>2</code> 进入了第一个窗口，然后 <code>7</code> 会进入第二个窗口，接着 <code>3</code> 依旧会进入第一个窗口，然后就有水印了，此时水印过来了，就会发现水印的 timestamp 和第一个窗口结束时间是一致的，那么它就表示在后面不会有比 <code>4</code> 还小的数据过来了，接着就会触发第一个窗口的计算操作，如下图所示：</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-07-08-155309.jpg" alt="img"></p><p>那么接着后面的数据 <code>5</code> 和 <code>6</code> 会进入到第二个窗口里面，数据 <code>9</code> 会进入在第三个窗口里面。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-07-08-155558.jpg" alt="img"></p><p>那么当遇到水印 <code>9</code> 时，发现水印比第二个窗口的结束时间 <code>8</code> 还大，所以第二个窗口也会触发进行计算，然后以此继续类推下去。</p><p>相信看完上面几个图的讲解，你已经知道了 Watermark 的工作原理是啥了，那么在 Flink 中该如何去配置水印呢，下面一起来看看。</p><h3 id="Flink-中-Watermark-的设置"><a href="#Flink-中-Watermark-的设置" class="headerlink" title="Flink 中 Watermark 的设置"></a>Flink 中 Watermark 的设置</h3><p>在 Flink 中，数据处理中需要通过调用 DataStream 中的 assignTimestampsAndWatermarks 方法来分配时间和水印，该方法可以传入两种参数，一个是 AssignerWithPeriodicWatermarks，另一个是 AssignerWithPunctuatedWatermarks。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> SingleOutputStreamOperator&lt;T&gt; <span class="title">assignTimestampsAndWatermarks</span><span class="params">(AssignerWithPeriodicWatermarks&lt;T&gt; timestampAndWatermarkAssigner)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">final</span> <span class="keyword">int</span> inputParallelism = getTransformation().getParallelism();</span><br><span class="line">    <span class="keyword">final</span> AssignerWithPeriodicWatermarks&lt;T&gt; cleanedAssigner = clean(timestampAndWatermarkAssigner);</span><br><span class="line"></span><br><span class="line">    TimestampsAndPeriodicWatermarksOperator&lt;T&gt; operator = <span class="keyword">new</span> TimestampsAndPeriodicWatermarksOperator&lt;&gt;(cleanedAssigner);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> transform(<span class="string">&quot;Timestamps/Watermarks&quot;</span>, getTransformation().getOutputType(), operator).setParallelism(inputParallelism);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> SingleOutputStreamOperator&lt;T&gt; <span class="title">assignTimestampsAndWatermarks</span><span class="params">(AssignerWithPunctuatedWatermarks&lt;T&gt; timestampAndWatermarkAssigner)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">final</span> <span class="keyword">int</span> inputParallelism = getTransformation().getParallelism();</span><br><span class="line">    <span class="keyword">final</span> AssignerWithPunctuatedWatermarks&lt;T&gt; cleanedAssigner = clean(timestampAndWatermarkAssigner);</span><br><span class="line"></span><br><span class="line">    TimestampsAndPunctuatedWatermarksOperator&lt;T&gt; operator = <span class="keyword">new</span> TimestampsAndPunctuatedWatermarksOperator&lt;&gt;(cleanedAssigner);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> transform(<span class="string">&quot;Timestamps/Watermarks&quot;</span>, getTransformation().getOutputType(), operator).setParallelism(inputParallelism);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>所以设置 Watermark 是有如下两种方式：</p><ul><li><p>AssignerWithPunctuatedWatermarks：数据流中每一个递增的 EventTime 都会产生一个 Watermark。</p><blockquote><p>在实际的生产环境中，在 TPS 很高的情况下会产生大量的 Watermark，可能在一定程度上会对下游算子造成一定的压力，所以只有在实时性要求非常高的场景才会选择这种方式来进行水印的生成。</p></blockquote></li><li><p>AssignerWithPeriodicWatermarks：周期性的（一定时间间隔或者达到一定的记录条数）产生一个 Watermark。</p><blockquote><p>在实际的生产环境中，通常这种使用较多，它会周期性产生 Watermark 的方式，但是必须结合时间或者积累条数两个维度，否则在极端情况下会有很大的延时，所以 Watermark 的生成方式需要根据业务场景的不同进行不同的选择。</p></blockquote></li></ul><p>下面再分别详细讲下这两种的实现方式。</p><h3 id="Punctuated-Watermark"><a href="#Punctuated-Watermark" class="headerlink" title="Punctuated Watermark"></a>Punctuated Watermark</h3><p>AssignerWithPunctuatedWatermarks 接口中包含了 checkAndGetNextWatermark 方法，这个方法会在每次 extractTimestamp() 方法被调用后调用，它可以决定是否要生成一个新的水印，返回的水印只有在不为 null 并且时间戳要大于先前返回的水印时间戳的时候才会发送出去，如果返回的水印是 null 或者返回的水印时间戳比之前的小则不会生成新的水印。</p><p>那么该怎么利用这个来定义水印生成器呢？</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordPunctuatedWatermark</span> <span class="keyword">implements</span> <span class="title">AssignerWithPunctuatedWatermarks</span>&lt;<span class="title">Word</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Nullable</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Watermark <span class="title">checkAndGetNextWatermark</span><span class="params">(Word lastElement, <span class="keyword">long</span> extractedTimestamp)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> extractedTimestamp % <span class="number">3</span> == <span class="number">0</span> ? <span class="keyword">new</span> Watermark(extractedTimestamp) : <span class="keyword">null</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">extractTimestamp</span><span class="params">(Word element, <span class="keyword">long</span> previousElementTimestamp)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> element.getTimestamp();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>需要注意的是这种情况下可以为每个事件都生成一个水印，但是因为水印是要在下游参与计算的，所以过多的话会导致整体计算性能下降。</p><h3 id="3-5-4-Periodic-Watermark"><a href="#3-5-4-Periodic-Watermark" class="headerlink" title="3.5.4 Periodic Watermark"></a>3.5.4 Periodic Watermark</h3><p>通常在生产环境中使用 AssignerWithPeriodicWatermarks 来定期分配时间戳并生成水印比较多，那么先来讲下这个该如何使用。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordWatermark</span> <span class="keyword">implements</span> <span class="title">AssignerWithPeriodicWatermarks</span>&lt;<span class="title">Word</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">long</span> currentTimestamp = Long.MIN_VALUE;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">extractTimestamp</span><span class="params">(Word word, <span class="keyword">long</span> previousElementTimestamp)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (word.getTimestamp() &gt; currentTimestamp) &#123;</span><br><span class="line">            <span class="keyword">this</span>.currentTimestamp = word.getTimestamp();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> currentTimestamp;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Nullable</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Watermark <span class="title">getCurrentWatermark</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">long</span> maxTimeLag = <span class="number">5000</span>;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> Watermark(currentTimestamp == Long.MIN_VALUE ? Long.MIN_VALUE : currentTimestamp - maxTimeLag);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>上面的是我根据 Word 数据自定义的水印周期性生成器，在这个类中，有两个方法 extractTimestamp() 和 getCurrentWatermark()。extractTimestamp() 方法是从数据本身中提取 Event Time，该方法会返回当前时间戳与事件时间进行比较，如果事件的时间戳比 currentTimestamp 大的话，那么就将当前事件的时间戳赋值给 currentTimestamp。getCurrentWatermark() 方法是获取当前的水位线，这里有个 maxTimeLag 参数代表数据能够延迟的时间，上面代码中定义的 <code>long maxTimeLag = 5000;</code> 表示最大允许数据延迟时间为 5s，超过 5s 的话如果还来了之前早的数据，那么 Flink 就会丢弃了，因为 Flink 的窗口中的数据是要触发的，不可能一直在等着这些迟到的数据（由于网络的问题数据可能一直没发上来）而不让窗口触发结束进行计算操作。</p><p>通过定义这个时间，可以避免部分数据因为网络或者其他的问题导致不能够及时上传从而不把这些事件数据作为计算的，那么如果在这延迟之后还有更早的数据到来的话，那么 Flink 就会丢弃了，所以合理的设置这个允许延迟的时间也是一门细活，得观察生产环境数据的采集到消息队列再到 Flink 整个流程是否会出现延迟，统计平均延迟大概会在什么范围内波动。这也就是说明了一个事实那就是 Flink 中设计这个水印的根本目的是来解决部分数据乱序或者数据延迟的问题，而不能真正做到彻底解决这个问题，不过这一特性在相比于其他的流处理框架已经算是非常给力了。</p><p>AssignerWithPeriodicWatermarks 这个接口有四个实现类，分别如下图：</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-23-082804.png" alt="img"></p><ul><li>BoundedOutOfOrdernessTimestampExtractor：该类用来发出滞后于数据时间的水印，它的目的其实就是和我们上面定义的那个类作用是类似的，你可以传入一个时间代表着可以允许数据延迟到来的时间是多长。该类内部实现如下：</li></ul><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-23-083043.png" alt="img"></p><p>你可以像下面一样使用该类来分配时间和生成水印：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//Time.seconds(10) 代表允许延迟的时间大小</span></span><br><span class="line">dataStream.assignTimestampsAndWatermarks(<span class="keyword">new</span> BoundedOutOfOrdernessTimestampExtractor&lt;Event&gt;(Time.seconds(<span class="number">10</span>)) &#123;</span><br><span class="line">    <span class="comment">//重写 BoundedOutOfOrdernessTimestampExtractor 中的 extractTimestamp()抽象方法</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">extractTimestamp</span><span class="params">(Event event)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> event.getTimestamp();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure><ul><li>CustomWatermarkExtractor：这是一个自定义的周期性生成水印的类，在这个类里面的数据是 KafkaEvent。</li><li>AscendingTimestampExtractor：时间戳分配器和水印生成器，用于时间戳单调递增的数据流，如果数据流的时间戳不是单调递增，那么会有专门的处理方法，代码如下：</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">long</span> <span class="title">extractTimestamp</span><span class="params">(T element, <span class="keyword">long</span> elementPrevTimestamp)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">final</span> <span class="keyword">long</span> newTimestamp = extractAscendingTimestamp(element);</span><br><span class="line">    <span class="keyword">if</span> (newTimestamp &gt;= <span class="keyword">this</span>.currentTimestamp) &#123;</span><br><span class="line">        <span class="keyword">this</span>.currentTimestamp = ne∏wTimestamp;</span><br><span class="line">        <span class="keyword">return</span> newTimestamp;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        violationHandler.handleViolation(newTimestamp, <span class="keyword">this</span>.currentTimestamp);</span><br><span class="line">        <span class="keyword">return</span> newTimestamp;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>IngestionTimeExtractor：依赖于机器系统时间，它在 extractTimestamp 和 getCurrentWatermark 方法中是根据 <code>System.currentTimeMillis()</code> 来获取时间的，而不是根据事件的时间，如果这个时间分配器是在数据源进 Flink 后分配的，那么这个时间就和 Ingestion Time 一致了，所以命名也取的就是叫 IngestionTimeExtractor。</li></ul><p><strong>注意</strong>：</p><p>1、使用这种方式周期性生成水印的话，你可以通过 <code>env.getConfig().setAutoWatermarkInterval(...);</code> 来设置生成水印的间隔（每隔 n 毫秒）。</p><p>2、通常建议在数据源（source）之后就进行生成水印，或者做些简单操作比如 filter/map/flatMap 之后再生成水印，越早生成水印的效果会更好，也可以直接在数据源头就做生成水印。比如你可以在 source 源头类中的 run() 方法里面这样定义</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">(SourceContext&lt;MyType&gt; ctx)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    <span class="keyword">while</span> (<span class="comment">/* condition */</span>) &#123;</span><br><span class="line">        MyType next = getNext();</span><br><span class="line">        ctx.collectWithTimestamp(next, next.getEventTimestamp());</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (next.hasWatermarkTime()) &#123;</span><br><span class="line">            ctx.emitWatermark(<span class="keyword">new</span> Watermark(next.getWatermarkTime()));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="每个-Kafka-分区的时间戳"><a href="#每个-Kafka-分区的时间戳" class="headerlink" title="每个 Kafka 分区的时间戳"></a>每个 Kafka 分区的时间戳</h3><p>当以 Kafka 来作为数据源的时候，通常每个 Kafka 分区的数据时间戳是递增的（事件是有序的），但是当你作业设置多个并行度的时候，Flink 去消费 Kafka 数据流是并行的，那么并行的去消费 Kafka 分区的数据就会导致打乱原每个分区的数据时间戳的顺序。在这种情况下，你可以使用 Flink 中的 <code>Kafka-partition-aware</code> 特性来生成水印，使用该特性后，水印会在 Kafka 消费端生成，然后每个 Kafka 分区和每个分区上的水印最后的合并方式和水印在数据流 shuffle 过程中的合并方式一致。</p><p>如果事件时间戳严格按照每个 Kafka 分区升序，则可以使用前面提到的 AscendingTimestampExtractor 水印生成器来为每个分区生成水印。下面代码教大家如何使用 <code>per-Kafka-partition</code> 来生成水印。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">FlinkKafkaConsumer011&lt;Event&gt; kafkaSource = <span class="keyword">new</span> FlinkKafkaConsumer011&lt;&gt;(<span class="string">&quot;zhisheng&quot;</span>, schema, props);</span><br><span class="line">kafkaSource.assignTimestampsAndWatermarks(<span class="keyword">new</span> AscendingTimestampExtractor&lt;Event&gt;() &#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">extractAscendingTimestamp</span><span class="params">(Event event)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> event.eventTimestamp();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">DataStream&lt;Event&gt; stream = env.addSource(kafkaSource);</span><br></pre></td></tr></table></figure><p>下图表示水印在 Kafka 分区后如何通过流数据流传播：</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-07-09-014107.jpg" alt="img"></p>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
            <tag> Flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>任务提交流程</title>
      <link href="2019/12/11/Flink%20%E4%BB%BB%E5%8A%A1%E6%8F%90%E4%BA%A4%E6%B5%81%E7%A8%8B/"/>
      <url>2019/12/11/Flink%20%E4%BB%BB%E5%8A%A1%E6%8F%90%E4%BA%A4%E6%B5%81%E7%A8%8B/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><h1 id="1-概述"><a href="#1-概述" class="headerlink" title="1. 概述"></a>1. 概述</h1><blockquote><p>用户提交的 <code>Flink Job</code> 会被转化成一个 <code>DAG</code> 任务运行，分别是：<code>StreamGraph</code>、<code>JobGraph</code>、<code>ExecutionGraph</code>，<code>Flink</code> 中 <code>JobManager</code> 与 <code>TaskManager</code>，<code>JobManager</code> 与 <code>Client</code> 的交互是基于Akka工具包的，是通过消息驱动。整个Flink Job的提交还包含着ActorSystem的创建，JobManager的启动，TaskManager的启动和注册。</p></blockquote><h1 id="2-任务提交流程"><a href="#2-任务提交流程" class="headerlink" title="2.任务提交流程"></a>2.任务提交流程</h1><h2 id="2-1-Standalone"><a href="#2-1-Standalone" class="headerlink" title="2.1.Standalone"></a>2.1.<code>Standalone</code></h2><blockquote><ol><li><code>App</code> 程序通过 <code>rest</code> 接口提交给 <code>Dispatcher</code>[ <code>rest</code>接口是跨平台，并且可以直接穿过防火墙，不需考虑拦截]。</li><li><code>Dispatcher</code> 把 <code>JobManager</code> 进程启动，把应用交给<code>JobManager</code>。</li><li>JobManager拿到应用后，向ResourceManager申请资源（slots），ResouceManager会启动对应的TaskManager进程，TaskManager空闲的slots会向ResourceManager注册。</li><li>ResourceManager会根据JobManager申请的资源数量，向TaskManager发出指令（这些slots由你提供给JobManager）。</li><li>接着，TaskManager可以直接和JobManager通信了（它们之间会有心跳包的连接），TaskManager向JobManager提供slots，JobManager向TaskManager分配在slots中执行的任务。</li><li>最后，在执行任务过程中，不同的TaskManager会有数据之间的交换。</li></ol></blockquote><h2 id="2-2-Yarn"><a href="#2-2-Yarn" class="headerlink" title="2.2. Yarn"></a>2.2. <code>Yarn</code></h2><blockquote><ol><li>提交 <code>App</code> 之前，先上传 <code>Flink</code> 的 <code>Jar</code> 包和配置到 <code>HDFS</code> ，以便 <code>JobManager</code> 和 <code>TaskManager</code> 共享<code>HDFS</code> 的数据。</li><li>客户端向ResourceManager提交Job，ResouceManager接到请求后，先分配container资源，然后通知NodeManager启动ApplicationMaster。</li><li>ApplicationMaster会加载HDFS的配置，启动对应的JobManager，然后JobManager会分析当前的作业图，将它转化成执行图（包含了所有可以并发执行的任务），从而知道当前需要的具体资源。</li><li>接着，JobManager会向ResourceManager申请资源，ResouceManager接到请求后，继续分配container资源，然后通知ApplictaionMaster启动更多的TaskManager（先分配好container资源，再启动TaskManager）。container在启动TaskManager时也会从HDFS加载数据。</li><li>最后，TaskManager启动后，会向JobManager发送心跳包。JobManager向TaskManager分配任务。</li></ol></blockquote><h1 id="3-Graph"><a href="#3-Graph" class="headerlink" title="3. Graph"></a>3. <code>Graph</code></h1><h2 id="3-1-StreamGraph"><a href="#3-1-StreamGraph" class="headerlink" title="3.1. StreamGraph"></a>3.1. <code>StreamGraph</code></h2><h2 id="3-2-JobGraph"><a href="#3-2-JobGraph" class="headerlink" title="3.2. JobGraph"></a>3.2. <code>JobGraph</code></h2><h2 id="3-3-ExecutionGraph"><a href="#3-3-ExecutionGraph" class="headerlink" title="3.3. ExecutionGraph"></a>3.3. <code>ExecutionGraph</code></h2><h2 id="3-4-物理执行图"><a href="#3-4-物理执行图" class="headerlink" title="3.4. 物理执行图"></a>3.4. <code>物理执行图</code></h2>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
            <tag> Flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello Flink</title>
      <link href="2019/12/10/Flink%E6%A6%82%E8%BF%B0/"/>
      <url>2019/12/10/Flink%E6%A6%82%E8%BF%B0/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><h1 id="1-简介"><a href="#1-简介" class="headerlink" title="1. 简介"></a>1. 简介</h1><p>Flink 核心是一个流式的数据流执行引擎，其针对数据流的分布式计算提供了数据分布、数据通信以及容错机制等功能。基于流执行引擎，Flink 提供了诸多更高抽象层的 API 以便用户编写分布式任务</p><h2 id="1-1-无界数据流与有界数据流"><a href="#1-1-无界数据流与有界数据流" class="headerlink" title="1.1.无界数据流与有界数据流"></a>1.1.无界数据流与有界数据流</h2><h3 id="1-1-1-无界数据流"><a href="#1-1-1-无界数据流" class="headerlink" title="1.1.1.无界数据流"></a>1.1.1.无界数据流</h3><p><strong>无界数据流有一个开始但是没有结束 ，</strong> 它们不会在生成时终止并 提供数据，必须连续处理无界流，也就是说必须在获取后立即 处理 event 。对于无界 数据流我们无法等待所有数据都到达， 因为输入是无界的， 并且在任何时间点都不 会完成。处理无界数据通常要求以特定顺序（例如事件发生的顺序）获取 event ，以 便能够推断结果完整性 ，无界流的处理称为流处理 。</p><h3 id="1-1-2-有界数据流"><a href="#1-1-2-有界数据流" class="headerlink" title="1.1.2.有界数据流"></a>1.1.2.有界数据流</h3><p><strong>有界数据流有明确定义的开始和结束，</strong> 可以在执行任何计算之前 通过获取所有数据来处理有界流， 处理有界流不需要有序获取， 因为可以始终对有 界数据集进行排序， 有界流的处理也称为批处理。</p><h2 id="1-2-批处理和流处理"><a href="#1-2-批处理和流处理" class="headerlink" title="1.2.批处理和流处理"></a>1.2.批处理和流处理</h2><p>批处理的特点是有界、持久、大量， 批处理非常适合需要访问全套记录才能完成的计算工作， 一般用于离线统计。 </p><p>流处理的特点是无界、实时， 流处理方式无需针对整个数据集执行操作， 而是对通过系统传输的每个数据项执行操作 ， 一般用于实时统计 。</p><h2 id="1-3-Flink-批流一体处理"><a href="#1-3-Flink-批流一体处理" class="headerlink" title="1.3.Flink 批流一体处理"></a>1.3.Flink 批流一体处理</h2><p>在 Spark 生态体系中， 对于批处理和流处理采用了不同的技术框架，批处理由 Spark SQL 实现， 流处理由 Spark Streaming 实现， 这也是大部分框架采用的策略， 使用独立的处理器实现批处理和流处理， 而 Flink 可以同时实现批处理和流处理。</p><p>Apache Flink  是一个面向分布式数据流处理和批量数据处理的开源计算平台，能够基于同一个 Flink 运行时 (Flink Runtime) ， 提供支持流处理和批处理两种类 型应用的功能 。 现有的开源计算 方案， 会把流处理和批处理作为两种不同的应用类 型，因为它们要实现的目标是完全不相同的：流处理一般需要支持低延迟、 Exactly-once 保证 ， 而 批处理需要支持高吞吐、高效处理 ， 所以在实现的时候通常 是分别给出两套实现方法， 或者通过一个独立的开源框架来实现其中每一种处理方 案。</p><p>Flink 是完全支持流处理，作为流处理时将输入数据流视为无界数据流 ； 批处理被作为一种特殊的流处理， 只是它的输入数据流被定义为有界的 。 </p><h2 id="1-4-Flink-amp-Spark-Streaming"><a href="#1-4-Flink-amp-Spark-Streaming" class="headerlink" title="1.4.Flink &amp; Spark Streaming"></a>1.4.<strong>Flink &amp; Spark Streaming</strong></h2><p><strong>Flink 是标准的实时流处理引擎，基于事件驱动。而 Spark Streaming 是微批[Micro-Batch的模型。</strong></p><h3 id="1-4-1-任务调度"><a href="#1-4-1-任务调度" class="headerlink" title="1.4.1.任务调度"></a><strong>1.4.1.任务调度</strong></h3><p>Spark Streaming 连续不断的生成微小的数据批次，构建有向无环图 DAG，Spark Streaming 会依次创建 DStreamGraph、JobGenerator、JobScheduler。</p><p>Flink 根据用户提交的代码生成 StreamGraph，经过优化生成 JobGraph，然后提交给 JobManager进行处理，JobManager 会根据 JobGraph 生成 ExecutionGraph，ExecutionGraph 是 Flink 调度最核心的数据结构，JobManager 根据 ExecutionGraph 对 Job 进行调度。</p><h3 id="1-4-2-时间机制"><a href="#1-4-2-时间机制" class="headerlink" title="1.4.2. 时间机制"></a><strong>1.4.2. 时间机制</strong></h3><p>Spark Streaming 支持的时间机制有限，只支持<strong>处理时间</strong>。<br>Flink 支持了流处理程序在时间上的三个定义：<strong>处理时间、事件时间、注入时间</strong>。同时也支持 <strong>watermark</strong> 机制来处理滞后数据。</p><h3 id="1-4-3-容错机制"><a href="#1-4-3-容错机制" class="headerlink" title="1.4.3. 容错机制"></a><strong>1.4.3. 容错机制</strong></h3><p>对于 Spark Streaming 任务，我们可以设置 checkpoint，然后假如发生故障并重启，我们可以从上次 checkpoint 之处恢复，但是这个行为只能使得数据不丢失，可能会重复处理，不能做到恰一次处理语义。</p><h1 id="2-系统架构"><a href="#2-系统架构" class="headerlink" title="2.系统架构"></a>2.系统架构</h1><p>当 Flink 集群启动后，首先会启动一个JobManger 和一个或多个的 TaskManager。由  Client 提交任务给 JobManager，JobManager 再调度任务到各个 TaskManager 去执行， TaskManager 将心跳和统计信息汇报给 JobManager。</p><h2 id="Client"><a href="#Client" class="headerlink" title="Client"></a>Client</h2><p> Client 为提交 Job 的客户端，可以是运行在任何机器上[与 JobManager 环境连通即可]。提交 Job 后，Client 可以结束进程，也可以不结束并等待结果返回。</p><h2 id="Dispatcher"><a href="#Dispatcher" class="headerlink" title="Dispatcher"></a>Dispatcher</h2><p> 提供 REST 接口来接收 client 的 application 提交，它负责启动 TaskManager 和提交 application，同时运行 Web UI。</p><h2 id="JobManager"><a href="#JobManager" class="headerlink" title="JobManager"></a>JobManager</h2><p><strong>JobManager  负责整个 Flink  集群任务的调度以及资源的管理，从客户端中获取提交的 Flink Job，然后根据集群中 TaskManager 上 TaskSlot 的使用情况，为提交的Job 分配相应的 TaskSlot 资源并命令 TaskManager 启动从客户端中获取的应用。</strong> </p><p><strong>JobManager 相当于整个集群的 Master 节点，且整个集群有且只有一个活跃的 JobManager ，负责整个集群的任务调度和资源管理。</strong></p><p>JobManager包含了3个重要的组件</p><h3 id="Actor"><a href="#Actor" class="headerlink" title="Actor"></a>Actor</h3><p>JobManager 和 TaskManager 之间通过 Actor System 进行通信，获取任务执行的情况并通过 Actor System 将应用的任务执行情况发送给客户端。</p><h3 id="调度"><a href="#调度" class="headerlink" title="调度"></a>调度</h3><h3 id="检查点"><a href="#检查点" class="headerlink" title="检查点"></a>检查点</h3><p>Flink的检查点机制是保证其一致性容错功能的骨架。它持续的为分布式的数据流和有状态的operator生成一致性的快照。Flink的容错机制持续的构建轻量级的分布式快照，因此负载非常低。通常这些有状态的快照都被放在HDFS中存储（state backend）。程序一旦失败，Flink将停止executor并从最近的完成了的检查点开始恢复（依赖可重发的数据源+快照）</p><h2 id="TaskManager"><a href="#TaskManager" class="headerlink" title="TaskManager"></a>TaskManager</h2><p> <strong>TaskManager  相当于整个集群的 Slave 节点，负责具体的任务执行和对应任务在每个节点上的资源申请和管理。</strong></p><p> 客户端通过将编写好的 Flink 应用编译打包，提交到 JobManager，然后 JobManager 会根据已注册在 JobManager 中 TaskManager 的资源情况，将任务分配给有资源的 TaskManager节点，然后启动并运行任务。</p><p> TaskManager 从 JobManager 接收 Job然后使用 Slot 资源启动 Task，建立数据接入的网络连接，接收数据并开始数据处理。同时 TaskManager 之间的数据交互都是通过数据流的方式进行的。</p><p> 可以看出，<strong>Flink 的任务运行其实是采用多线程的方式，这和 MapReduce 多 进程的方式有很大的区别</strong>，Flink 能够极大提高 CPU 使用效率，在多个任务和 Task之间通过 TaskSlot 方式共享系统资源，每个 TaskManager 中通过管理多个 TaskSlot 资源池进行对资源进行有效管理。</p><h3 id="Slots"><a href="#Slots" class="headerlink" title="Slots"></a>Slots</h3><p><strong>Flink 中每一个 TaskManager 都是一个 JVM 进程，他可能会在独立的线程上执行一个或多个 subtask</strong></p><p><strong>为了控制一个 TaskManager 能接收多少个 task，TaskManager通过 task slot 来进行控制， 一个TaskManager至少有一个 slot</strong></p><ul><li><p><strong>Slot 共享</strong></p><p> 默认情况下，Flink 允许 subtasks共享 slot</p><p> 条件是它们都来自<strong>同一个 Job 的不同 task 的 subtask</strong>。结果可能是一个 slot 持有该 job的整个pipeline。</p><p> <strong>优点</strong></p><ul><li><p>Flink 集群需要的任务槽与作业中使用的最高并行度正好相同(前提，保持默认SlotSharingGroup)。也就是说我们不需要再去计算一个程序总共会起多少个task了。</p></li><li><p>更容易获得更充分的资源利用。如果没有slot共享，那么非密集型操作source/flatmap就会占用同密集型操作 keyAggregation/sink 一样多的资源。如果有slot共享，将task的2个并行度增加到6个，能充分利用slot资源，同时保证每个TaskManager能平均分配到重的subtasks。</p></li></ul></li><li><p><strong>SlotSharingGroup[soft]</strong></p><p> SlotSharingGroup 是Flink中用来实现 slot  共享的类，它尽可能地让 subtasks 共享一个slot。</p><p> 保证同一个 group 的并行度相同的 sub-tasks 共享同一个slots。</p><p> 算子的默认group为default [即默认一个 Job下 的 subtask都可以共享一个 slot]</p><p> 为了防止不合理的共享，用户也能通过API来强制指定 operator 的共享组</p><p> 比如：someStream.filter(…).slotSharingGroup(“group1”);就强制指定了filter的slot共享组为group1。</p><p> 怎么确定一个未做SlotSharingGroup设置算子的SlotSharingGroup什么呢(根据上游算子的group 和自身是否设置group共同确定)。适当设置可以减少每个slot运行的线程数，从而整体上减少机器的负载。</p></li></ul><h2 id=""><a href="#" class="headerlink" title=""></a></h2>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
            <tag> Flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink 保证Exactly Once[2]</title>
      <link href="2019/12/10/Flink-%E4%BF%9D%E8%AF%81Exactly-Once-2/"/>
      <url>2019/12/10/Flink-%E4%BF%9D%E8%AF%81Exactly-Once-2/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p>FlinkKafkaConsumer 做为 Source，从 Kafka 读取数据到 Flink 中，首先想一下设计 FlinkKafkaConsumer，需要考虑哪些？</p><a id="more"></a><h3 id="分析-FlinkKafkaConsumer-的设计思想"><a href="#分析-FlinkKafkaConsumer-的设计思想" class="headerlink" title="分析 FlinkKafkaConsumer 的设计思想"></a>分析 FlinkKafkaConsumer 的设计思想</h3><p>FlinkKafkaConsumer 做为 Source，从 Kafka 读取数据到 Flink 中，首先想一下设计 FlinkKafkaConsumer，需要考虑哪些？</p><ul><li>Flink 中 kafka 的 offset 保存在哪里，具体如何保存呢？任务重启恢复时，如何读取之前消费的 offset？</li><li>如果 Source 端并行度改变了，如何来恢复 offset？</li><li>如何保证每个 FlinkKafkaConsumer 实例消费的 partition 负载均衡？如何保证不出现有的实例消费 5 个 kafka partition，有的实例仅消费 1 个 kafka partition？</li><li>当前消费的 topic 如果动态增加了 partition，Flink 如何实现自动发现并消费？</li></ul><p>带着这些问题来看一看 FlinkKafkaConsumer 是怎么解决上述问题的。</p><h4 id="Kafka-offset-存储及如何实现-Consumer-实例消费-partition-的负载均衡"><a href="#Kafka-offset-存储及如何实现-Consumer-实例消费-partition-的负载均衡" class="headerlink" title="Kafka offset 存储及如何实现 Consumer 实例消费 partition 的负载均衡"></a>Kafka offset 存储及如何实现 Consumer 实例消费 partition 的负载均衡</h4><p>Flink 将任务恢复需要的信息都保存在状态中，当然 Kafka 的 offset 信息也保存在 Flink 的状态中，当任务从状态中恢复时会从状态中读取相应的 offset，并从 offset 位置开始消费。</p><p>在 Flink 中有两个基本的 State：Keyed State 和 Operator State。</p><ul><li>Keyed State 只能用于 KeyedStream 的 function 和 Operator 中，一个 Key 对应一个 State；</li><li>而 Operator State 可以用于所有类型的 function 和 Operator 中，一个 Operator 实例对应一个 State，假如一个算子并行度是 5 且使用 Operator State，那么这个算子的每个并行度都对应一个 State，总共 5 个 State。</li></ul><p>FlinkKafkaConsumer 做为 Source 只能使用 Operator State，Operator State 只支持一种数据结构 ListState，可以当做 List 类型的 State。所以 FlinkKafkaConsumer 中，将状态保存在 Operator State 对应的 ListState 中。具体如何保存呢？需要先了解每个 FlinkKafkaConsumer 具体怎么消费 Kafka。</p><p>对于同一个消费者组，Kafka 要求 topic 的每个 partition 只能被一个 Consumer 实例消费，相反一个 Consumer 实例可以去消费多个 partition。当 Flink 消费 Kafka 时，出现了以下三种情况：</p><table><thead><tr><th align="left">情况</th><th align="left">现象</th></tr></thead><tbody><tr><td align="left">FlinkKafkaConsumer 并行度大于 topic 的 partition 数</td><td align="left">有些 FlinkKafkaConsumer 不会消费 Kafka</td></tr><tr><td align="left">FlinkKafkaConsumer 并行度等于 topic 的 partition 数</td><td align="left">每个 FlinkKafkaConsumer 消费 1 个 partition</td></tr><tr><td align="left">FlinkKafkaConsumer 并行度小于 topic 的 partition 数</td><td align="left">每个 FlinkKafkaConsumer 至少消费 1 个 partition，可能会消费多个 partition</td></tr></tbody></table><p>Flink 是如何为每个 Consumer 实例合理地分配去消费哪些 partition 呢？源码中 KafkaTopicPartitionAssigner 类的 assign 方法，负责分配 partition 给 Consumer 实例。assign 方法的输入参数为 KafkaTopicPartition 和 Consumer 的并行度，KafkaTopicPartition 主要包含两个字段：String 类型的 topic 和 int 类型的 partition。assign 方法返回该 KafkaTopicPartition 应该分配给哪个 Consumer 实例去消费。假如 Consumer 的并行度为 5，表示包含了 5 个 subtask，assign 方法的返回值范围为 0~4，分别表示该 partition 分配给 subtask0-subtask4。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> partition Kafka 中 topic 和 partition 信息</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> numParallelSubtasks subtask 的数量</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span> 该 KafkaTopicPartition 分配给哪个 subtask 去消费</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">int</span> <span class="title">assign</span><span class="params">(KafkaTopicPartition partition, <span class="keyword">int</span> numParallelSubtasks)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> startIndex = ((partition.getTopic().hashCode() * <span class="number">31</span>) &amp; <span class="number">0x7FFFFFFF</span>) % numParallelSubtasks;</span><br><span class="line">    <span class="keyword">return</span> (startIndex + partition.getPartition()) % numParallelSubtasks;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>assign 方法是如何给 KafkaTopicPartition 分配 Consumer 实例的呢？</p><p>第一行代码根据 topic name 的 hashCode 运算后对 subtask 的数量求余生成一个 startIndex，第二行代码用 startIndex + partition 编号对 subtask 的数量求余，可以保证该 topic 的 0 号 partition 分配给 startIndex 对应的 subtask，后续的 partition 依次分配给后续的 subtask。</p><p>例如，名为 “test-topic” 的 topic 有 11 个 partition 分别为 partition0-partition10，Consumer 有 5 个并行度分别为 subtask0-subtask4。计算后的 startIndex 为 1，表示 partition0 分配给 subtask1，partition1 分配给 subtask2 以此类推，subtask 与 partition 的对应关系如下图所示。</p><p>assign 方法给 partition 分配 subtask 实际上是轮循的策略，首先计算一个起点 startIndex 分配给 partition0，后续的 partition 轮循地分配给 subtask，从而使得每个 subtask 消费的 partition 得以均衡。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-19-122937.jpg" alt="img"></p><p>每个 subtask 只负责一部分 partition，所以在维护 partition 的 offset 信息时，每个 subtask 只需要将自己消费的 partition 的 offset 信息保存到状态中即可。</p><p>保存的格式理论来讲应该是 kv 键值对，key 为 KafkaTopicPartition，value 为 Long 类型的 offset 值。但 Flink 的 Operator State 只支持 ListState 一种数据结构，不支持 kv 格式，可以将 KafkaTopicPartition 和 Long 封装为 Tuple2&lt;KafkaTopicPartition, Long&gt; 存储到 ListState 中。如下所示，Flink 源码中确实如此，使用 ListState&lt;Tuple2&lt;KafkaTopicPartition, Long&gt;&gt; 类型的 unionOffsetStates 来保存 Kafka 的 offset 信息。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/** Accessor for state in the operator state backend. */</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">transient</span> ListState&lt;Tuple2&lt;KafkaTopicPartition, Long&gt;&gt; unionOffsetStates;</span><br></pre></td></tr></table></figure><p>当 Flink 应用从 Checkpoint 恢复任务时，会从 unionOffsetStates 中读取上一次 Checkpoint 保存的 offset 信息，并从 offset 的位置开始继续消费，从而实现 Flink 任务的故障容错。例如，任务重启后，Operator State 是一个 Operator 实例对应一个 State，subtask0 依然消费 partition4 和 partition9，subtask0 从自己的 State 中可以读取到 partition4 和 partition9 消费的 offset，从 offset 位置接着往后消费即可。问题来了，若 FlinkKafkaConsumer 的并行度改变后，offset 信息如何恢复呢？</p><h4 id="Source-端并行度改变了，如何来恢复-offset"><a href="#Source-端并行度改变了，如何来恢复-offset" class="headerlink" title="Source 端并行度改变了，如何来恢复 offset"></a>Source 端并行度改变了，如何来恢复 offset</h4><p>subtask1 当前消费了 3 个 partition，而其他 subtask 仅消费 2 个 partition，当发现 subtask1 读取 Kafka 成为瓶颈后，需要调大 Consumer 的并行度，使得每个 subtask 最多仅消费 2 个 partition。将 Consumer 实例的并行度增大到 6 以后，分配器对 partition 重新分配给 6 个 subtask，计算后的 startIndex 为 0，表示 partition0 分配给 subtask0，后续的 partition 采用轮循策略，partition 与 subtask 的对应关系如下。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-19-122939.jpg" alt="img"></p><p>之前 subtask0 消费 partition 4 和 9，并行度调大以后，subtask0 被分配消费 partition 0 和 6。但是 Flink 任务从 Checkpoint 恢复后，能保证 subtask0 读取到 partition 0 和 6 的 offset 吗？这个就需要深入了解当 Flink 算子并行度改变后，Operator State 的 ListState 两种恢复策略。两种策略如下所示，在 initializeState 方法中执行相应 API 来恢复。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">OperatorStateStore stateStore = context.getOperatorStateStore();</span><br><span class="line"></span><br><span class="line"><span class="comment">// 通过 getListState 获取 ListState</span></span><br><span class="line">stateStore.getListState(ListStateDescriptor&lt;S&gt; var1);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 通过 getUnionListState 获取 ListState</span></span><br><span class="line">stateStore.getUnionListState(ListStateDescriptor&lt;S&gt; var1);</span><br></pre></td></tr></table></figure><p>当并行度改变后，getListState 恢复策略是均匀分配，将 ListState 中保存的所有元素均匀地分配到所有并行度中，每个 subtask 获取到其中一部分状态信息。</p><p>getUnionListState 策略是将所有的状态信息合并后，每个 subtask 都获取到全量的状态信息。在 FlinkKafkaConsumer 中，假如使用 getListState 来获取 ListState，采用均匀分配状态信息的策略，Flink 可能给 subtask0 分配了 partition0 和 partition1 的 offset 信息，但实际上分配器让 subtask0 去消费 partition0 和 partition6，此时 subtask0 并拿不到 partition 6 的 offset 信息，不知道该从 partition 6 哪个位置消费，所以均匀分配状态信息的策略并不能满足需求。</p><p>这里应该使用 getUnionListState 来获取 ListState，也就是说每个 subtask 都可以获取到所有 partition 的 offset 信息，然后根据分配器让 subtask 0 去消费 partition0 和 partition6 时，subtask0 只需要从全量的 offset 中拿到 partition0 和 partition6 的状态信息即可。</p><p>这么做会使得每个 subtask 获取到一些无用的 offset 的信息，但实际上这些 offset 信息占用的空间会比较小，所以该方案成本比较低。关于 OperatorState 的 ListState 两种获取方式请参考代码：</p><blockquote><p><a href="https://github.com/zhisheng17/flink-learning/blob/master/flink-learning-state/src/main/java/com/zhisheng/state/operator/state/UnionListStateExample.java">https://github.com/zhisheng17/flink-learning/blob/master/flink-learning-state/src/main/java/com/zhisheng/state/operator/state/UnionListStateExample.java</a></p></blockquote><p>FlinkKafkaConsumer 初始化时，恢复 offset 相关的源码如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// initializeState  方法中用于恢复 offset 状态信息</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">void</span> <span class="title">initializeState</span><span class="params">(FunctionInitializationContext context)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    OperatorStateStore stateStore = context.getOperatorStateStore();</span><br><span class="line">    <span class="comment">// 此处省略了兼容 Flink 1.2 之前状态 API 的场景</span></span><br><span class="line">    ...</span><br><span class="line">    <span class="comment">// 此处使用的 getUnionListState，而不是 getListState。因为重启后，可能并行度被改变了</span></span><br><span class="line">    <span class="keyword">this</span>.unionOffsetStates = stateStore.getUnionListState(<span class="keyword">new</span> ListStateDescriptor&lt;&gt;(</span><br><span class="line">            OFFSETS_STATE_NAME,</span><br><span class="line">            TypeInformation.of(<span class="keyword">new</span> TypeHint&lt;Tuple2&lt;KafkaTopicPartition, Long&gt;&gt;() &#123;&#125;)));</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (context.isRestored() &amp;&amp; !restoredFromOldState) &#123;</span><br><span class="line">        restoredState = <span class="keyword">new</span> TreeMap&lt;&gt;(<span class="keyword">new</span> KafkaTopicPartition.Comparator());</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 将状态中恢复的 offset 信息 put 到 TreeMap 类型的 restoredState 中，方便查询</span></span><br><span class="line">        <span class="keyword">for</span> (Tuple2&lt;KafkaTopicPartition, Long&gt; kafkaOffset : unionOffsetStates.get()) &#123;</span><br><span class="line">            restoredState.put(kafkaOffset.f0, kafkaOffset.f1);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// open 方法对 FlinkKafkaConsumer 做初始化</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(Configuration configuration)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    <span class="comment">// 创建 Kafka partition 的发现器，用于检测该 subtask 应该去消费哪些 partition</span></span><br><span class="line">    <span class="keyword">this</span>.partitionDiscoverer = createPartitionDiscoverer(</span><br><span class="line">            topicsDescriptor,</span><br><span class="line">            getRuntimeContext().getIndexOfThisSubtask(),</span><br><span class="line">            getRuntimeContext().getNumberOfParallelSubtasks());</span><br><span class="line">    <span class="keyword">this</span>.partitionDiscoverer.open();</span><br><span class="line">    <span class="comment">// subscribedPartitionsToStartOffsets 存储当前 subtask 需要消费的 partition 及对应的 offset 初始信息</span></span><br><span class="line">    subscribedPartitionsToStartOffsets = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line">    <span class="comment">//用 partition 发现器获取该 subtask 应该消费且新发现的 partition</span></span><br><span class="line">    <span class="keyword">final</span> List&lt;KafkaTopicPartition&gt; allPartitions = partitionDiscoverer.discoverPartitions();</span><br><span class="line">    <span class="comment">// restoredState 在 initializeState 时初始化，所以 != null 表示任务从 Checkpoint 处恢复</span></span><br><span class="line">    <span class="keyword">if</span> (restoredState != <span class="keyword">null</span>) &#123;</span><br><span class="line">        <span class="keyword">for</span> (KafkaTopicPartition partition : allPartitions) &#123;</span><br><span class="line">            <span class="comment">// 若分配给该 subtask 的 partition 在 restoredState 中不包含</span></span><br><span class="line">            <span class="comment">// 说明该 partition 是新创建的 partition，默认从 earliest 开始消费</span></span><br><span class="line">              <span class="comment">// 并添加到 restoredState 中</span></span><br><span class="line">            <span class="keyword">if</span> (!restoredState.containsKey(partition)) &#123;</span><br><span class="line">                restoredState.put(partition, KafkaTopicPartitionStateSentinel.EARLIEST_OFFSET);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (Map.Entry&lt;KafkaTopicPartition, Long&gt; restoredStateEntry : restoredState.entrySet()) &#123;</span><br><span class="line">            <span class="comment">// 遍历 restoredState，使用分配器检测当前的 partition 是否分配给当前的 subtask</span></span><br><span class="line">            <span class="comment">// assign 方法返回当前 partition 应该分配的 subtask index 编号</span></span><br><span class="line">            <span class="comment">// getRuntimeContext().getIndexOfThisSubtask()  返回当前 subtask 的 index 编号</span></span><br><span class="line">            <span class="keyword">if</span> (KafkaTopicPartitionAssigner.assign(</span><br><span class="line">                restoredStateEntry.getKey(), getRuntimeContext().getNumberOfParallelSubtasks())</span><br><span class="line">                    == getRuntimeContext().getIndexOfThisSubtask())&#123;</span><br><span class="line">                <span class="comment">// 如果当前遍历的 partition 分配给当前 subtask 来消费，则将 partition 信息加到  subscribedPartitionsToStartOffsets 中</span></span><br><span class="line">                subscribedPartitionsToStartOffsets.put(restoredStateEntry.getKey(), restoredStateEntry.getValue());</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="comment">// else 表示任务不是从 Checkpoint 处恢复，本次源码主要分析状态恢复，不考虑该情况</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>对 offset 信息快照相关的源码如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">void</span> <span class="title">snapshotState</span><span class="params">(FunctionSnapshotContext context)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    <span class="comment">// 把旧的 offset 信息从 unionOffsetStates 清除掉</span></span><br><span class="line">    unionOffsetStates.clear();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">final</span> AbstractFetcher&lt;?, ?&gt; fetcher = <span class="keyword">this</span>.kafkaFetcher;</span><br><span class="line">    <span class="comment">// 通过提取器从 Kafka 读取数据，若 fetcher == null 表示提取器还未初始化</span></span><br><span class="line">    <span class="keyword">if</span> (fetcher == <span class="keyword">null</span>) &#123;</span><br><span class="line">        <span class="comment">// Kafka 提取器还未初始化，说明还未从 Kafka 中读取数据</span></span><br><span class="line">                <span class="comment">// 所以遍历 subscribedPartitionsToStartOffsets，将 offset 的初始信息写入到状态中</span></span><br><span class="line">        <span class="keyword">for</span> (Map.Entry&lt;KafkaTopicPartition, Long&gt; subscribedPartition : subscribedPartitionsToStartOffsets.entrySet()) &#123;</span><br><span class="line">            unionOffsetStates.add(Tuple2.of(subscribedPartition.getKey(), subscribedPartition.getValue()));</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (offsetCommitMode == OffsetCommitMode.ON_CHECKPOINTS) &#123;</span><br><span class="line">            <span class="comment">// 将 offset put 到 pendingOffsetsToCommit，后续 Commit 到 Kafka </span></span><br><span class="line">            pendingOffsetsToCommit.put(context.getCheckpointId(), restoredState);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="comment">// 从 Kafka 提取器中获取该 subtask 订阅的 partition 当前消费的 offset 信息</span></span><br><span class="line">        HashMap&lt;KafkaTopicPartition, Long&gt; currentOffsets = fetcher.snapshotCurrentState();</span><br><span class="line">        <span class="keyword">if</span> (offsetCommitMode == OffsetCommitMode.ON_CHECKPOINTS) &#123;</span><br><span class="line">            <span class="comment">// 将 offset put 到 pendingOffsetsToCommit，后续 Commit 到 Kafka </span></span><br><span class="line">            pendingOffsetsToCommit.put(context.getCheckpointId(), currentOffsets);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (Map.Entry&lt;KafkaTopicPartition, Long&gt; kafkaTopicPartitionLongEntry : currentOffsets.entrySet()) &#123;</span><br><span class="line">            <span class="comment">// 将该 subtask 订阅的 partition 以及当前 partition 消费到的 offset 写入到状态中</span></span><br><span class="line">            unionOffsetStates.add(</span><br><span class="line">                    Tuple2.of(kafkaTopicPartitionLongEntry.getKey(), kafkaTopicPartitionLongEntry.getValue()));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>上述源码分析描述了，当 Checkpoint 时 FlinkKafkaConsumer 如何将 offset 信息保存到状态中，当任务从 Checkpoint 处恢复时 FlinkKafkaConsumer 如何从状态中获取相应的 offset 信息，并解答了当 Source 并行度改变时 FlinkKafkaConsumer 如何来恢复 offset 信息。</p><h4 id="如何实现自动发现当前消费-topic-下新增的-partition"><a href="#如何实现自动发现当前消费-topic-下新增的-partition" class="headerlink" title="如何实现自动发现当前消费 topic 下新增的 partition"></a>如何实现自动发现当前消费 topic 下新增的 partition</h4><p>当 FlinkKafkaConsumer 初始化时，每个 subtask 会订阅一批 partition，但是当 Flink 任务运行过程中，如果被订阅的 topic 创建了新的 partition，FlinkKafkaConsumer 如何实现动态发现新创建的 partition 并消费呢？</p><p>在使用 FlinkKafkaConsumer 时，可以通过 Properties 传递一些配置参数，当配置了参数FlinkKafkaConsumerBase.KEY_PARTITION<em>DISCOVERY_INTERVAL</em>MILLIS 时，就会开启 partition 的动态发现，该参数表示间隔多久检测一次是否有新创建的 partition。那具体实现原理呢？相关源码的 UML 图如下所示：</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-11-15-132311.png" alt="img"></p><p>笔者生产环境使用的 FlinkKafkaConsumer011，FlinkKafkaConsumer011 继承 FlinkKafkaConsumer09，FlinkKafkaConsumer09 继承 FlinkKafkaConsumerBase。将参数 KEY_PARTITION<em>DISCOVERY_INTERVAL_MILLIS 传递给 FlinkKafkaConsumer011 时，在 FlinkKafkaConsumer09 的构造器中会调用 getLong(checkNotNull(props, “props”), KEY_PARTITION_DISCOVERY_INTERVAL</em>MILLIS, PARTITION_DISCOVERY_DISABLED) 解析该参数，并最终赋值给 FlinkKafkaConsumerBase 的 discoveryIntervalMillis 属性。后续相关源码如下所示：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// FlinkKafkaConsumerBase 的 run 方法</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">(SourceContext&lt;T&gt; sourceContext)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">      ...</span><br><span class="line">    <span class="keyword">if</span> (discoveryIntervalMillis == PARTITION_DISCOVERY_DISABLED) &#123;</span><br><span class="line">            kafkaFetcher.runFetchLoop();</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="comment">// discoveryIntervalMillis 被设置了，则开启 PartitionDiscovery</span></span><br><span class="line">            runWithPartitionDiscovery();</span><br><span class="line">        &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// runWithPartitionDiscovery 方法会调用 createAndStartDiscoveryLoop 方法</span></span><br><span class="line"><span class="comment">// createAndStartDiscoveryLoop 方法内创建了一个线程去循环检测发现新分区</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">createAndStartDiscoveryLoop</span><span class="params">(AtomicReference&lt;Exception&gt; discoveryLoopErrorRef)</span> </span>&#123;</span><br><span class="line">    <span class="comment">//  创建一个线程去循环检测发现新分区</span></span><br><span class="line">    discoveryLoopThread = <span class="keyword">new</span> Thread(() -&gt; &#123;</span><br><span class="line">        <span class="keyword">while</span> (running) &#123;</span><br><span class="line">            <span class="keyword">final</span> List&lt;KafkaTopicPartition&gt; discoveredPartitions;</span><br><span class="line">            <span class="comment">//  用 partition 发现器获取该 subtask 应该消费且新发现的 partition</span></span><br><span class="line">            discoveredPartitions = partitionDiscoverer.discoverPartitions();</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 发现了新的 partition，则添加到 Kafka 提取器</span></span><br><span class="line">            <span class="keyword">if</span> (running &amp;&amp; !discoveredPartitions.isEmpty()) &#123;</span><br><span class="line">                <span class="comment">//  kafkaFetcher 添加 新发现的 partition</span></span><br><span class="line">                kafkaFetcher.addDiscoveredPartitions(discoveredPartitions);</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (running &amp;&amp; discoveryIntervalMillis != <span class="number">0</span>) &#123;</span><br><span class="line">                <span class="comment">//  sleep 设置的间隔时间</span></span><br><span class="line">                Thread.sleep(discoveryIntervalMillis);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;, <span class="string">&quot;Kafka Partition Discovery for &quot;</span> + getRuntimeContext().getTaskNameWithSubtasks());</span><br><span class="line"></span><br><span class="line">    discoveryLoopThread.start();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>discoveryLoopThread 线程中每间隔 discoveryIntervalMillis 时间会调用 partition 发现器获取该 subtask 应该消费且新发现的 partition，在 open 方法初始化时，同样也调用 partitionDiscoverer.discoverPartitions() 方法来获取新发现的 partition，partition 发现器的 discoverPartitions 方法第一次调用时，会返回该 subtask 所有的 partition，后续调用只会返回新发现的且应该被当前 subtask 消费的 partition。discoverPartitions 方法源码如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> List&lt;KafkaTopicPartition&gt; <span class="title">discoverPartitions</span><span class="params">()</span> <span class="keyword">throws</span> WakeupException, ClosedException </span>&#123;</span><br><span class="line">    List&lt;KafkaTopicPartition&gt; newDiscoveredPartitions;</span><br><span class="line">    <span class="comment">// 获取订阅的 Topic 的所有 partition </span></span><br><span class="line">    newDiscoveredPartitions = getAllPartitionsForTopics(topicsDescriptor.getFixedTopics());</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 剔除 旧的 partition 和 不应该被该 subtask 去消费的 partition</span></span><br><span class="line">    Iterator&lt;KafkaTopicPartition&gt; iter = newDiscoveredPartitions.iterator();</span><br><span class="line">    KafkaTopicPartition nextPartition;</span><br><span class="line">    <span class="keyword">while</span> (iter.hasNext()) &#123;</span><br><span class="line">        nextPartition = iter.next();</span><br><span class="line">        <span class="comment">// setAndCheckDiscoveredPartition 方法设计比较巧妙，</span></span><br><span class="line">          <span class="comment">// 将旧的 partition 和 不应该被该 subtask 消费的 partition，返回 false</span></span><br><span class="line">        <span class="comment">// 将这些partition 剔除，就是新发现的 partition</span></span><br><span class="line">        <span class="keyword">if</span> (!setAndCheckDiscoveredPartition(nextPartition)) &#123;</span><br><span class="line">            iter.remove();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> newDiscoveredPartitions;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// discoveredPartitions 中存放着所有发现的 partition</span></span><br><span class="line"><span class="keyword">private</span> Set&lt;KafkaTopicPartition&gt; discoveredPartitions = <span class="keyword">new</span> HashSet&lt;&gt;();</span><br><span class="line"></span><br><span class="line"><span class="comment">// setAndCheckDiscoveredPartition 方法实现</span></span><br><span class="line"><span class="comment">// 当参数的 partition 是新发现的 partition 且应该被当前 subtask 消费时，返回 true</span></span><br><span class="line"><span class="comment">// 旧的 partition 和 不应该被该 subtask 消费的 partition，返回 false</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">setAndCheckDiscoveredPartition</span><span class="params">(KafkaTopicPartition partition)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// discoveredPartitions 中不存在，表示发现了新的 partition，将其加入到 discoveredPartitions  </span></span><br><span class="line">    <span class="keyword">if</span> (!discoveredPartitions.contains(partition)) &#123;</span><br><span class="line">        discoveredPartitions.add(partition);</span><br><span class="line">        <span class="comment">// 再通过分配器来判断该 partition 是否应该被当前 subtask 去消费</span></span><br><span class="line">        <span class="keyword">return</span> KafkaTopicPartitionAssigner.assign(partition, numParallelSubtasks) == indexOfThisSubtask;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>上述代码中依赖 Set 类型的 discoveredPartitions 来判断 partition 是否是新的 partition，刚开始 discoveredPartitions 是一个空的 Set，所以任务初始化第一次调用发现器的 discoverPartitions 方法时，会把所有属于当前 subtask 的 partition 都返回，来保证所有属于当前 subtask 的 partition 都能被消费到。之后任务运行过程中，若创建了新的 partition，则新 partition 对应的那一个 subtask 会自动发现并从 earliest 位置开始消费，新创建的 partition 对其他 subtask 并不会产生影响。</p>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
            <tag> Flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink 保证Exactly Once[1]</title>
      <link href="2019/12/10/Flink-%E4%BF%9D%E8%AF%81Exactly-Once-1/"/>
      <url>2019/12/10/Flink-%E4%BF%9D%E8%AF%81Exactly-Once-1/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p>在分布式场景下，应用程序随时可能出现任何形式的故障，例如：机器硬件故障、程序 OOM 等。当应用程序出现故障时，Flink 为了保证数据消费的 Exactly Once，需要有相应的故障容错能力。Flink 是通过周期性 Checkpoint 的方式来实现故障容错，这里使用的是基于 Chandy-Lamport 改进的算法。</p><a id="more"></a><h3 id="Flink-内部如何保证-Exactly-Once？"><a href="#Flink-内部如何保证-Exactly-Once？" class="headerlink" title="Flink 内部如何保证 Exactly Once？"></a>Flink 内部如何保证 Exactly Once？</h3><p>Flink 官网的定义是 Stateful Computations over Data Streams（数据流上的有状态计算），那到底什么是状态呢？举一个无状态计算的例子，比如：我们只是进行一个字符串拼接，输入 a，输出a_666；输入b，输出 b_666。无状态表示计算输出的结果跟之前的状态没关系，符合幂等性。</p><p>幂等性就是用户对于同一操作发起的一次请求或者多次请求的结果是一致的，不会因为多次点击而产生副作用。而计算 PV、UV 就属于有状态计算。实时计算 PV 时，每次都需要从某个存储介质的结果表中拿到之前的 PV 值，+1 后 set 到结果表中。有状态计算表示输出的结果跟之前的状态有关系，不符合幂等性，访问多次，PV 会增加。</p><h4 id="Flink-的-Checkpoint-功能简介"><a href="#Flink-的-Checkpoint-功能简介" class="headerlink" title="Flink 的 Checkpoint 功能简介"></a>Flink 的 Checkpoint 功能简介</h4><p><strong>Flink Checkpoint 机制的存在就是为了解决 Flink 任务在运行过程中由于各种原因导致任务失败后，能够正常恢复任务。</strong></p><p><strong>Checkpoint 是通过给程序做快照的方式使得将整个程序某些时刻的状态保存下来，当任务挂掉之后，默认从最近一次保存的完整快照处进行恢复任务。</strong></p><p>SnapShot 翻译为快照，是指将程序中某些信息存一份，后期可以用这些信息来恢复任务。对于一个 Flink 任务来讲，快照里面到底保存着什么信息呢？理论知识一般比较晦涩难懂，我们分析一个案例，用案例辅助大家理解快照里面到底存储什么信息。</p><blockquote><p>计算各个 app 的 PV，使用 Flink 该怎么统计呢？</p></blockquote><p>可以把要统计的 app_id 做为 key，对应的 PV 值做为 value，将统计的结果放到一个 Map 集合中，这个 Map 集合可以是内存里的 HashMap 或其他 kv 数据库，例如放到 Redis 的 key、value 结构中。从 Kafka 读取到一条条日志，由于要统计各 app 的 PV，所以我们需要从日志中解析出 app_id 字段，每来一条日志，只需要从 Map 集合将相应 app_id 的 PV 值拿出来，+1 后 put 到 Map 中，这样我们的 Map 中永远保存着所有 app 最新的 PV 数据。详细流程如下所示：</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-07-151541.jpg" alt="flink任务task图.png"></p><p>图中包含三部分：第一个是 Kafka 的一个名为 test 的 Topic，我们的数据来源于这个 Topic，第二个是 Flink 的 Source Task，是 Flink 应用程序读取数据的 Task，第三个是计算 PV 的 Flink Task，用于统计各个 app 的 PV 值，并将 PV 结果输出到 Map 集合。</p><p>Flink 的 Source Task 记录了当前消费到 test Topic 所有 partition 的 offset，为了方便理解 Checkpoint 的作用，这里先用一个 partition 进行讲解，假设名为 test 的 Topic 只有一个 partition0。</p><p>例：(0,60000) 表示 0 号 partition，目前消费到 offset 为 60000 的数据。Flink 的 PV task 记录了当前计算的各 app 的 PV 值，为了方便讲解，这里假设有两个 app：app1、app2。</p><p>例：(app1,50000) (app2,10000) 表示 app1 当前 PV 值为50000、app2 当前 PV 值为 10000。计算过程中，每来一条数据，只需要确定相应 app_id，将相应的 PV 值 +1 后 put 到 map 中即可。</p><p>该案例中，Checkpoint 到底记录了什么信息呢？记录的其实就是第 n 次 Checkpoint 消费的 offset 信息和各 app 的 PV 值信息，记录下发生 Checkpoint 当前的状态信息，并将该状态信息保存到相应的状态后端。（注：<strong>状态后端是保存状态的地方</strong>，决定状态如何保存，如何保证状态高可用，我们只需要知道，我们能从状态后端拿到 offset 信息和 PV 信息即可。状态后端必须是高可用的，否则我们的状态后端经常出现故障，会导致无法通过 Checkpoint 来恢复我们的应用程序）。下面列出了第 100 次 Checkpoint 的时候，状态后端保存的状态信息：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">chk-100</span><br><span class="line">    - offset：(0,60000)</span><br><span class="line">    - PV：(app1,50000) (app2,10000)</span><br></pre></td></tr></table></figure><p>该状态信息表示第 100 次 Checkpoint 的时候，partition 0 offset 消费到了 60000，PV 统计结果为 (app1,50000) (app2,10000)。如果任务挂了，如何恢复？</p><p>假如我们设置了一分钟进行一次 Checkpoint，第 100 次 Checkpoint 成功后，过了十秒钟，offset 已经消费到 (0,60100)，PV 统计结果变成了 (app1,50080) (app2,10020)，突然任务挂了，怎么办？</p><p>其实很简单，Flink 只需要从最近一次成功的 Checkpoint，也就是从第 100 次 Checkpoint 保存的 offset(0,60000) 处接着消费即可，当然 PV 值也要从第 100 次 Checkpoint 里保存的 PV 值 (app1,50000) (app2,10000) 进行累加，不能从 (app1,50080) (app2,10020) 处进行累加，因为 **partition 0 offset 消费到 60000 时，对应的 PV 统计结果为 (app1,50000) (app2,10000)**。</p><p>当然如果你想从 offset(0,60100)、PV(app1,50080)(app2,10020) 这个状态恢复，也是做不到的，因为那个时刻程序突然挂了，这个状态根本没有保存下来，只有在 Checkpoint 的时候，才会把这些完整的状态保存到状态后端，供我们恢复任务。我们能做的最高效方式就是从最近一次成功的 Checkpoint 处恢复，也就是一直所说的 chk-100。以上基本就是 Checkpoint 承担的工作，为了方便理解，描述的业务场景比较简单。</p><p>补充两个问题：计算 PV 的 task 在一直运行，它怎么知道什么时候去做 Checkpoint 呢？计算 PV 的 task 怎么保证它自己计算的 PV 值 (app1,50000) (app2,10000) 就是 offset(0,60000) 那一刻的统计结果呢？Flink 在数据中加了一个叫做 barrier 的东西，下图中红圈处就是两个 barrier。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-07-151548.jpg" alt="img"></p><p>barrier 从 Source Task 处生成，一直流到 Sink Task，期间所有的 Task 只要碰到 barrier，就会触发自身进行快照。如图所示，Checkpoint barrier n-1 处做的快照就是指 Job 从开始处理到 barrier n-1 所有的状态数据，barrier n 处做的快照就是指从 Job 开始到处理到 barrier n 所有的状态数据。</p><p>对应到 PV 案例中就是，Source Task 接收到 JobManager 的编号为 chk-100 的 Checkpoint 触发请求后，发现自己恰好接收到 Kafka offset(0,60000) 处的数据，所以会往 offset(0,60000) 数据之后 offset(0,60001) 数据之前插入一个barrier，然后自己开始做快照，也就是将 offset(0,60000) 保存到状态后端 chk-100 中。</p><p>然后，Source Task 会把 barrier 和我们要处理的数据一块往下游发送，当统计 PV 的 task 接收到 barrier 后，意味着 barrier 之前的数据已经被 PV task 处理完了，此时也会暂停处理 barrier 之后的数据，将自己内存中保存的 PV 信息 (app1,50000) (app2,10000) 保存到状态后端 chk-100 中。Flink 大概就是通过以上过程来保存快照的。</p><p>上述过程中，barrier 的作用就是为了把数据区分开，barrier 之前的数据是本次 Checkpoint 之前必须处理完的数据，barrier 之后的数据在本次 Checkpoint 之前不能被处理。</p><p>Checkpoint 过程中有一个同步做快照的环节不能处理 barrier 之后的数据，为什么呢？如果做快照的同时，也在处理数据，那么处理的数据可能会修改快照内容，所以先暂停处理数据，把内存中快照保存好后，再处理数据。</p><p>结合案例来讲就是，PV task 在对 (app1,50000)、(app2,10000) 做快照的同时，如果 barrier 之后的数据还在处理，可能会导致状态信息还没保存到磁盘，状态已经变成了 (app1,50001) (app2,10001)，导致我们最后快照里保存的 PV 值变成了 (app1,50001) (app2,10001)，这样如果从 Checkpoint 恢复任务时，我们从 offset 60000 开始消费，PV 值从 (app1,50001) (app2,10001) 开始累加，就会造成计算的 PV 结果偏高，结果不准确，就不能保证 Exactly Once。</p><p>所以，Checkpoint 同步做快照的过程中，不能处理 barrier 之后的数据。Checkpoint 将快照信息写入到磁盘后，为了保证快照信息的高可用，需要将快照上传到 HDFS，这个上传快照到 HDFS 的过程是异步进行的，这个过程也可以处理 barrier 之后的数据，处理 barrier 之后的数据不会影响到磁盘上的快照信息。</p><p>从 PV 案例再分析 Flink 是如何做 Checkpoint 并从 Checkpoint 处恢复任务的，首先 JobManager 端会向所有 SourceTask 发送 Checkpoint，Source Task 会在数据流中安插 Checkpoint barrier。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-07-151547.jpg" alt="单并行度 PV 案例 Checkpoint 过程图示1"></p><p>Source Task 安插好 barrier 后，会将 barrier 跟数据一块发送给下游，然后自身开始做快照，并将快照信息 offset(0,60000) 发送到高可用的持久化存储介质，例如 HDFS 上。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-07-151542.jpg" alt="单并行度 PV 案例 Checkpoint 过程图示2"></p><p>下游的 PV task 接收到 barrier 后，也会做快照，并将快照信息 PV：(app1,50000) (app2,10000) 发送到 HDFS。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-07-151543.jpg" alt="img"></p><p>假设第 100 次 Checkpoint 完成后，一段时间后任务挂了，Flink 任务会自动从状态后端恢复任务。Source Task 去读取自己需要的状态信息 offset(0,60000)，并从 offset 为 60000 的位置接着开始消费数据，PV task 也会去读取需要的状态信息 PV：(app1,50000) (app2,10000)，并在该状态值的基础上，往上累积计算 PV 值。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-07-151544.jpg" alt="img"></p><h4 id="多并行度、多-Operator-情况下，Checkpoint-的过程"><a href="#多并行度、多-Operator-情况下，Checkpoint-的过程" class="headerlink" title="多并行度、多 Operator 情况下，Checkpoint 的过程"></a>多并行度、多 Operator 情况下，Checkpoint 的过程</h4><p>上一节中讲述了单并行度情况下 Checkpoint 的过程，但是生产环境中，一般都是多并行度，而且算子也会比较多，这种情况下 Checkpoint 的过程就会变得复杂。分布式状态容错面临的问题与挑战：</p><ul><li>如何确保状态拥有<strong>精确一次</strong>的容错保证？</li><li>如何在分布式场景下替多个拥有本地状态的算子产生<strong>一个全域一致的快照</strong>？</li><li>如何在<strong>不中断运算</strong>的前提下产生快照？</li></ul><p>多并行度、多 Operator 实例的情况下，如何做全域一致的快照？所有的 Operator 运行过程中接收到所有上游算子发送 barrier 后，对自身的状态进行一次快照，保存到相应状态后端。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-07-151553.jpg" alt="img"></p><p>当任务从状态恢复时，每个 Operator 从状态后端读取自己相应的状态信息，数据源会从状态中保存的位置开始重新消费，后续的其他算子也会基于 Checkpoint 中保存的状态进行计算。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-07-151556.jpg" alt="多并行度下，任务从 Checkpoint 恢复图示"></p><p>整个 Checkpoint 的过程跟之前单并行度类似，图中有 4 个带状态的 Operator 实例，相应的状态后端就可以想象成 4 个格子。整个 Checkpoint 的过程可以当做 Operator 实例填自己格子的过程，Operator 实例将自身的状态写到状态后端中相应的格子，当所有的格子填满可以简单地认为一次完整的 Checkpoint 做完了。</p><p>上面只是快照的过程，Checkpoint 执行过程如下：</p><p>\1. JobManager 端的 CheckPointCoordinator 向所有 Source Task 发送 CheckPointTrigger，Source Task 会在数据流中安插 Checkpoint barrier。</p><p>\2. 当 task 收到所有的 barrier 后，向自己的下游继续传递 barrier，然后自身执行快照，并将自己的状态<strong>异步写入到持久化存储</strong>中。</p><ul><li>增量 CheckPoint 只是把最新的一部分数据更新写入到外部存储；</li><li>为了下游尽快开始做 CheckPoint，所以会先发送 barrier 到下游，自身再同步进行快照。</li></ul><p>\3. 当 task 对状态的快照信息完成备份后，会将备份数据的地址（state handle）通知给 JobManager 的 CheckPointCoordinator。</p><ul><li>如果 Checkpoint 的持续时长超过了 Checkpoint 设定的超时时间，CheckPointCoordinator 还没有收集完所有的 State Handle，CheckPointCoordinator就会认为本次 Checkpoint 失败，会把这次 Checkpoint 产生的所有状态数据全部删除。</li></ul><p>\4. CheckPointCoordinator 把整个 StateHandle 封装成 completed Checkpoint Meta，写入到 HDFS，整个 Checkpoint 结束。</p><h4 id="barrier-对齐"><a href="#barrier-对齐" class="headerlink" title="barrier 对齐"></a>barrier 对齐</h4><p>什么是 barrier 对齐？如图所示，当前的 Operator 实例接收上游两个流的数据，一个是字母流，一个是数字流。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-07-151558.jpg" alt="img"></p><p>当 Checkpoint 时，上游字母流和数字流都会往 Operator 实例发送 Checkpoint barrier，但是由于每个算子的执行速率不同，所以不可能保证上游两个流的 barrier 同时到达 Operator 实例，那图中的 Operator 实例到底什么时候进行快照呢？接收到任意一个 barrier 就可以开始进行快照了吗，还是接收到所有的 barrier 才能开始进行快照呢？</p><p>答案是：当一个 Operator 实例有多个输入流时，Operator 实例需要在做快照之前进行 barrier 对齐，等待所有输入流的 barrier 都到达。barrier 对齐的详细过程如下所示：</p><ol><li>对于一个有多个输入流的 Operator 实例，当 Operator 实例从其中一个输入流接收到 Checkpoint barrier n 时，就不能处理来自该流的任何数据记录了，直到它从其他所有输入流接收到 barrier n 为止，否则 <strong>Operator 实例 Checkpoint n 的快照会混入快照 n 的记录和快照 n+1 的记录</strong>。如上图中第 1 个小图所示，数字流的 barrier 先到达了。</li><li>接收到 barrier n 的流暂时被搁置，从这些流接收的记录不会被处理，而是放入输入缓冲区。图 2 中，我们可以看到虽然数字流对应的 barrier 已经到达了，但是 barrier 之后的 1、2、3 这些数据只能放到缓冲区中，等待字母流的 barrier 到达。</li><li>一旦最后所有输入流都接收到 barrier n，Operator 实例就会把 barrier 之前所有已经处理完成的数据和 barrier n 一块发送给下游。然后 Operator 实例就可以对状态信息进行快照。如图 3 所示，Operator 实例接收到上游所有流的 barrier n，此时 Operator 实例就可以将 barrier 和 barrier 之前的数据发送到下游，然后自身状态进行快照。</li><li>快照做完后，Operator 实例将继续处理缓冲区的记录，然后就可以处理输入流的数据。如图 4 所示，先处理完缓冲区数据，就可以正常处理输入流的数据了。</li></ol><p>上面的过程就是 Flink 在 Operator 实例有多个输入流的情况下，整个 barrier 对齐的过程。那什么是 barrier 不对齐呢？</p><p>barrier 不对齐是指当还有其他流的 barrier 还没到达时，为了提高 Operator 实例的处理性能，Operator 实例会直接处理 barrier 之后的数据，等到所有流的barrier 都到达后，就可以对该 Operator 做 Checkpoint 快照了。</p><p>对应到图中就是，barrier 不对齐时会直接把 barrier 之后的数据 1、2、3 直接处理掉，而<strong>不是</strong>放到缓冲区中等待其他的输入流的 barrier 到达，当所有输入流的 barrier 都到达后，才开始对 Operator 实例的状态信息进行快照，这样会导致做快照之前，Operator 实例已经处理了一些 barrier n 之后的数据。</p><p>Checkpoint 的目的是为了保存快照信息，如果 barrier 不对齐，那么 Operator 实例在做第 n 次 Checkpoint 之前，已经处理了一些 barrier n 之后的数据，当程序从第 n 次 Checkpoint 恢复任务时，程序会从第 n 次 Checkpoint 保存的 offset 位置开始消费数据，就会导致一些数据被处理了两次，就出现了重复消费。如果进行 barrier 对齐，就不会出现这种重复消费的问题，所以，<strong>barrier 对齐就可以实现 Exactly Once，barrier 不对齐就变成了 At Least Once。</strong></p><p>再结合计算 PV 的案例来证明一下，为什么 barrier 对齐就可以实现 Exactly Once，barrier 不对齐就变成了 At Least Once。之前的案例为了简单，描述的 Kafka topic 只有 1 个 partition，这里为了讲述 barrier 对齐，假设 topic 有 2 个 partittion，且计算的是我们平台的总 PV，也就是说不需要区分 app，每条一条数据，我们都需要将其 PV 值 +1 即可。如下图所示，Flink 应用程序有两个 Source Task，一个计算 PV 的 Task，这里计算 PV 的 Task 就出现了存在多个输入流的情况。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-07-151546.jpg" alt="img"></p><p>假设 barrier 不对齐，那么 Checkpoint 过程是怎么样呢？</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-07-151554.jpg" alt="img"></p><p>如左图所示，Source Subtask 0 和 Subtask 1 已经完成了快照操作，它们的状态信息为 offset(0,10000)(1,10005)，表示 partition0 消费到 offset 为 10000 的位置，partition 1 消费到 offset 为 10005 的位置。当 Source Subtask 1 的 barrier 到达 PV task 时，计算的 PV 结果为 20002，但 PV task 还没有接收到 Source Subtask 0 发送的 barrier，所以 PV task 还不能对自身状态信息进行快照。由于设置的 barrier 不对齐，所以此时 PV task 会继续处理 Source Subtask 0 和 Source Subtask 1 传来的数据。</p><p>很快，如右图所示，PV task 接收到 Source Subtask 0 发来的 barrier，但是 PV task 已经处理了 Source Subtask 1 barrier 之后的三条数据，所以 PV 值目前已经为 20008 了，这里的 PV=20008 实际上已经处理到 partition 1 offset 为 10008 的位置，此时 PV task 会对自身的状态信息（PV = 20008）做快照，整体的快照信息为 offset(0,10000)(1,10005) PV=20008。</p><p>接着程序在继续运行，过了 10 秒，由于某个服务器故障，导致我们的 Operator 实例有一个挂了，所以 Flink 会从最近一次 Checkpoint 保存的状态恢复。那具体是怎么恢复的呢？</p><p>Flink 同样会起三个 Operator 实例，我还称它们是 Source Subtask 0、Source Subtask 1 和 PV task。三个 Operator 会从状态后端读取保存的状态信息。Source Subtask 0 会从 partition 0 offset 为 10000 的位置开始消费，Source Subtask 1 会从 partition 1 offset 为 10005 的位置开始消费，PV task 会基于 PV=20008 进行累加统计。然后就会发现的 PV 值 20008 实际上已经包含了 partition 1 的offset 10005<del>10008 的数据，所以 partition 1 从 offset 10005 恢复任务时，partition1 的 offset 10005</del>10008 的数据被消费了两次，出现了重复消费的问题，所以 barrier 不对齐只能保证 At Least Once。</p><p>如果设置为 barrier 对齐，这里能保证 Exactly Once 吗？如下图所示，当 PV task 接收到 Source Subtask 1 的 barrier 后，并不会处理 Source Subtask 1 barrier 之后的数据，而是把这些数据放到 PV task 的输入缓冲区中，直到等到 Source Subtask 0 的 barrier 到达后，PV task 才会对自身状态信息进行快照。</p><p>此时 PV task 会把 PV=20005 保存到快照信息中，整体的快照状态信息为 offset(0,10000)(1,10005) PV=20005，当任务从 Checkpoint 恢复时，Source Subtask 0 会从 partition 0 offset 为 10000 的位置开始消费，Source Subtask 1 会从 partition 1 offset 为 10005 的位置开始消费，PV task 会基于 PV=20005 进行累加统计，所以 barrier 对齐能保证 Flink 内部的 Exactly Once。</p><p>在 Flink 应用程序中，当 Checkpoint 语义设置 Exactly Once 或 At Least Once 时，唯一的区别就是 barrier 对不对齐。当设置为 Exactly Once 时，就会 barrier 对齐，当设置为 At Least Once 时，就会 barrier 不对齐。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-07-151555.jpg" alt="img"></p><p>通过本案例，我们应该发现了 barrier 在 Flink 的 Checkpoint 中起着非常大的作用。barrier 告诉 Flink 应用程序，Checkpoint 之前哪些数据不应该被处理，barrier 对齐的过程其实就是为了防止 Flink 应用程序处理重复的数据。</p><p>总结一下，满足哪些条件时，会出现 barrier 对齐？在代码中设置了 Flink 的 Checkpoint 语义是 Exactly Once，其次 Operator 实例必须有多个输入流才会出现 barrier 对齐。</p><p>对齐，汉语词汇，释义为使两个以上事物配合或接触得整齐。由汉语解释可得对齐肯定需要两个以上事物，所以必须有多个输入流才可能存在对齐。</p><p>barrier 对齐就是上游多个流配合使得数据对齐的过程。言外之意：如果 Operator 实例只有一个输入流，就根本不存在 barrier 对齐，自己跟自己默认永远都是对齐的，所以当我们的应用程序从 Source 到 Sink 所有算子的并行度都是 1 的话，就算设置的 At Least Once，无形中也实现了 barrier 对齐，此时 Checkpoint 设置成 Exactly Once 和 At Least Once 一点区别都没有，都可以保证 Exactly Once。</p><p>看到这里你应该已经知道了哪种情况会出现重复消费了，也应该要掌握为什么 barrier 对齐就能保证 Exactly Once，为什么 barrier 不对齐就是 At Least Once。</p><p>barrier 对齐其实是要付出代价的，从 barrier 对齐的过程可以看出，PV task 明明可以更高效的处理数据，但因为 barrier 对齐，导致 Source Subtask 1 barrier 之后的数据被放到缓冲区中，暂时性地没有被处理，假如生产环境中，Source Subtask 0 的 barrier 迟迟没有到达，比 Source Subtask 1 延迟了 30 秒，那么这 30 秒期间，Source Subtask 1 barrier 之后的数据不能被处理，所以 PV task 相当于被闲置了。</p><p>所以，当我们的一些业务场景对 Exactly Once 要求不高时，我们可以设置 Flink 的 Checkpoint 语义是 At Least Once 来小幅度的提高应用程序的执行效率。Flink Web UI 的 Checkpoint 选项卡中可以看到 barrier 对齐的耗时，如果发现耗时比较长，且对 Exactly Once 语义要求不高时，可以考虑使用该优化方案。</p><p>前面提到如何在不中断运算的前提下产生快照？在 Flink 的 Checkpoint 过程中，无论下游算子有没有做完快照，只要上游算子将 barrier 发送到下游且上游算子自身已经做完快照时，那么上游算子就可以处理 barrier 之后的数据了，从而使得整个系统 Checkpoint 的过程影响面尽量缩到最小，来提升系统整体的吞吐量。</p><p>在整个 Checkpoint 的过程中，还存在一个问题，假设我们设置的 10 分钟一次 Checkpoint。在第 n 次 Checkpoint 成功后，过了 9 分钟，任务突然挂了，我们需要从最近一次成功的 Checkpoint 处恢复任务，也就是从 9 分钟之前的状态恢复任务，就需要把这 9分钟的数据全部再消费一次，成本比较大。</p><p>有的同学可能会想，那可以不可以设置为 100ms 就做一次 Checkpoint 呢？这样的话，当任务出现故障时，就不需要从 9 分钟前的状态进行恢复了，直接从 100ms 之前的状态恢复即可，恢复就会很快，不需要处理大量重复数据了。</p><p>但是，这样做会导致应用程序频繁的访问状态后端，一般我们为了高可用，会把状态里的数据比如 offset(0,60000) PV(app1,50000)(app2,10000) 信息保存到 HDFS 中，如果频繁访问 HDFS，肯定会造成吞吐量下降，所以一般我们的 Checkpoint 时间间隔可以设置为分钟级别，例如 1 分钟、3 分钟，对于状态很大的任务每次 Checkpoint 访问 HDFS 比较耗时，我们甚至可以设置为 5 分钟一次 Checkpoint，毕竟我们的应用程序挂的概率并不高，偶尔一次从 5 分钟前的状态恢复，我们是可以接受的。</p><p>可以根据业务场景合理地调节 Checkpoint 的间隔时长，对于状态很小的 Job Checkpoint 会很快，我们可以调小时间间隔，对于状态比较大的 Job Checkpoint 会比较慢，我们可以调大 Checkpoint 时间间隔。</p><p>有的同学可能还有疑问，明明说好的 Exactly Once，但在 Checkpoint 成功后 10s 发生了故障，从最近一次成功的 Checkpoint 处恢复时，由于发生故障前的 10s Flink 也在处理数据，所以 Flink 应用程序肯定是把一些数据重复处理了呀。</p><p>在面对任意故障时，不可能保证每个算子中用户定义的逻辑在每个事件中只执行一次，因为用户代码被部分执行的可能性是永远存在的。那么，当引擎声明 Exactly Once 处理语义时，它们能保证什么呢？如果不能保证用户逻辑只执行一次，那么哪些逻辑只执行一次？当引擎声明 Exactly Once 处理语义时，它们实际上是在说，它们可以保证引擎管理的状态更新只提交一次到持久的后端存储。换言之，无论以什么维度计算 PV、无论 Flink 应用程序发生多少次故障导致重启从 Checkpoint 恢复，Flink 都可以保证 PV 结果是准确的，不会因为各种任务重启而导致 PV 值计算偏高。</p><p>为了下游尽快做 Checkpoint，所以会先发送 barrier 到下游，自身再同步进行快照。这一步，如果向下发送 barrier 后，自己同步快照慢怎么办？下游已经同步好了，自己还没？可能会出现下游比上游快照还早的情况，但是这不影响快照结果，只是下游做快照更及时了，我只要保证下游把 barrier 之前的数据都处理了，并且不处理 barrier 之后的数据，然后做快照，那么下游也同样支持 Exactly Once。</p><p>这个问题不要从全局思考，单独思考上游和下游的实例，你会发现上下游的状态都是准确的，既没有丢，也没有重复计算。这里需要注意，如果有一个 Operator 的 Checkpoint 失败了或者因为 Checkpoint 超时也会导致失败，那么 JobManager 会认为整个 Checkpoint 失败。失败的 Checkpoint 是不能用来恢复任务的，必须所有的算子的 Checkpoint 都成功，那么这次 Checkpoint 才能认为是成功的，才能用来恢复任务。</p><p>对应到 PV 案例就是，PV task 做快照速度较快，PV=20005 较早地写入到了 HDFS，但是 offset(0,10000)(1,10005) 过了几秒才写入到 HDFS，这种情况就算出现了，也不会影响计算结果，因为我们的快照信息是完全正确的。</p><p>再分享一个案例，Flink 的 Checkpoint 语义设置了 Exactly Once，程序中设置了 1 分钟 1 次 Checkpoint，5 秒向 MySQL 写一次数据，并 commit。最后发现 MySQL 中数据重复了。为什么会重复呢？Flink 要求端对端的 Exactly Once 都必须实现 TwoPhaseCommitSinkFunction。如果你的 Checkpoint 成功了，过了 30 秒突然程序挂了，由于 5 秒 Commit 一次，所以在应用程序挂之前的 30 秒实际上已经写入了 6 批数据进入 MySQL。从 Checkpoint 处恢复时，之前提交的 6 批数据就会重复写入，所以出现了重复消费。</p><p>Flink 的 Exactly Once 有两种情况，一个是我们本节所讲的 Flink 内部的 Exactly Once，一个是端对端的 Exactly Once。关于端对端如何保证 Exactly Once，我们在下一节中深入分析。</p><h3 id="端对端如何保证-Exactly-Once？"><a href="#端对端如何保证-Exactly-Once？" class="headerlink" title="端对端如何保证 Exactly Once？"></a>端对端如何保证 Exactly Once？</h3><p>Flink 与外部存储介质之间进行数据交互统称为端对端或 end to end 数据传输。上一节讲述了 Flink 内部如何保证 Exactly Once，这一节来分析端对端的 Exactly Once。</p><p>正如上述 Flink 写 MySQL 的案例所示，在第 n 次 Checkpoint 结束后，第 n+1 次 Checkpoint 之前，如果 Flink 应用程序已经向外部的存储介质中成功写入并提交了一些数据后，Flink 应用程序由于某些原因挂了，导致任务从第 n 次 Checkpoint 处恢复。这种情况下，就会导致第 n 次 Checkpoint 结束后且任务失败之前往外部存储介质中写入的那一部分数据重复写入两次，可能会导致相同的数据在存储介质中存储了两份，从而端对端的一致性语义保证从 Exactly Once 退化为 At Least Once。</p><p>这里只考虑了数据重复的情况，为什么不考虑丢数据的情况呢？在写数据时可以对异常进行捕获增加重试策略，如果重试多次还没有成功可以让 Flink 任务失败，Flink 任务就会从最近一次成功的 Checkpoint 处恢复，就不会出现丢数据的情况，所以我们本节内容主要用来解决数据重复的问题。</p><p>针对上述端对端 Exactly Once 的问题，我们可以使用以下方案来解决：</p><ol><li>假如我们使用的存储介质支持按照全局主键去重，那么比较容易实现 Exactly Once，无论相同的数据往外部存储中写入了几次，外部存储都会进行去重，只保留一条数据。例如，app1 的 PV 值为 10，现在把（key=app1，value=10）往 Redis 中写入 10 次，只是说把 value 值覆盖了 10 次，并不会导致结果错误，这种方案属于幂等性写入。</li><li>我们上述案例中为什么会导致重复写入数据到外部存储呢？是因为在下一次 Checkpoint 之前如果任务失败时，一些数据已经成功写入到了外部存储中，没办法删除那些数据。既然问题是这样，那可以想办法把“向外部存储中提交数据”与 Checkpoint 强关联，两次 Checkpoint 之间不允许向外部存储介质中提交数据，Checkpoint 的时候再向外部存储提交。如果提交成功，则 Checkpoint 成功，提交失败，则 Checkpoint 也失败。这样在下一次 Checkpoint 之前，如果任务失败，也没有重复数据被提交到外部存储。这里只是描述一下大概思想，好多细节这里并没有详细描述，会在下文中详细描述。基于上述思想，Flink 实现了 TwoPhaseCommitSinkFunction，它提取了两阶段提交协议的通用逻辑，使得通过 Flink 来构建端到端的 Exactly Once 程序成为可能。它提供了一个抽象层，用户只需要实现少数方法就能实现端到端的 Exactly Once 语义。不过这种方案必须要求我们的输出端（Sink 端）必须支持事务。</li></ol><p>下面我们通过两部分来详细介绍上述两种方案。</p><h4 id="幂等性写入如何保证端对端的-Exactly-Once"><a href="#幂等性写入如何保证端对端的-Exactly-Once" class="headerlink" title="幂等性写入如何保证端对端的 Exactly Once"></a>幂等性写入如何保证端对端的 Exactly Once</h4><p>实时 ETL 当 HBase 做为 Sink 端时，就是典型的应用场景。把日志中的主键做为 HBase 的 rowkey，就可以保证数据不重复，实现比较简单，这里不多赘述。</p><p>继续探讨实时计算各 app PV 的案例，将统计结果以普通键值对的形式保存到 Redis 中供业务方查询。到底如何实现，才能保证 Redis 中的结果是精准的呢？在之前 Strom 或 Spark Streaming 的方案中，将统计的 PV 结果保存在 Redis 中，每来一条数据，从 Redis 中获取相应 app 对应的 PV 值然后内存中进行 +1 后，再将 PV 值 put 到 Redis 中。</p><p>例如：Redis 中保存 app1 的 PV 为 10，现在来了一条 app1 的日志，首先从 Redis 中获取 app1 的 PV 值 =10，内存中 10+1=11，将 (app1,11) put 到 Redis 中，这里的 11 就是我们统计的 app1 的 PV 结果。可以将这种方案优化为 incr 或 incrby，直接对 Redis 中的 10 进行累加，不需要手动在内存中进行累加操作。</p><p>当然 Flink 也可以用上述的这种方案来统计各 app 的 PV，但是上述方案并不能保证 Exactly Once，为什么呢？当第 n 次 Checkpoint 时，app1 的 PV 结果为 10000，第 n 次 Checkpoint 结束后运行了 10 秒，Redis 中 app1 的 PV 结果已经累加到了 10200。此时如果任务挂了，从第 n 次 Checkpoint 恢复任务时，会继续按照 Redis 中保存的 PV=10200 进行累加，但是正确的结果应该是从 PV=10000 开始累加。</p><p>如果按照上面的方案统计 PV，就可能会出现统计值偏高的情况。这里也证实了一点：并不是说 Flink 程序的 Checkpoint 语义设置为 Exactly Once，就能保证我们的统计结果或者各种输出结果都能满足 Exactly Once。为了编写真正满足 Exactly Once 的代码，我们需要对 Flink 的 Checkpoint 原理做一些了解，编写对 Exactly Once 友好的代码。</p><p>那如何编写代码才能使得最后在 Redis 中保存的 PV 结果满足 Exactly Once 呢？上一节中，讲述了 Flink 内部状态可以保证 Exactly Once，这里可以将统计的 PV 结果保存在 Flink 内部的状态里，每次基于状态进行累加操作，并将累加到的结果 put 到 Redis 中，这样当任务从 Checkpoint 处恢复时，并不是基于 Redis 中实时统计的 PV 值进行累加，而是基于 Checkpoint 中保存的 PV 值进行累加，Checkpoint 中会保存每次 Checkpoint 时对应的 PV 快照信息，例如：第 n 次 Checkpoint 会把当时 pv=10000 保存到快照信息里，同时状态后端还保存着一份实时的状态信息用于实时累加。</p><p>示例代码如下所示：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">final</span> StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"><span class="comment">// 1 分钟一次 Checkpoint</span></span><br><span class="line">env.enableCheckpointing(TimeUnit.MINUTES.toMillis(<span class="number">1</span>));</span><br><span class="line"></span><br><span class="line">CheckpointConfig checkpointConf = env.getCheckpointConfig();</span><br><span class="line"><span class="comment">// Checkpoint 语义 EXACTLY ONCE</span></span><br><span class="line">checkpointConf.setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE);</span><br><span class="line">checkpointConf.enableExternalizedCheckpoints(CheckpointConfig.ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION);</span><br><span class="line"></span><br><span class="line">Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">&quot;localhost:9092&quot;</span>);</span><br><span class="line">props.put(ConsumerConfig.GROUP_ID_CONFIG, <span class="string">&quot;app-pv-stat&quot;</span>);</span><br><span class="line"></span><br><span class="line">DataStreamSource&lt;String&gt; appInfoSource = env.addSource(<span class="keyword">new</span> FlinkKafkaConsumer011&lt;&gt;(</span><br><span class="line">        <span class="comment">// kafka topic， String 序列化</span></span><br><span class="line">        <span class="string">&quot;app-topic&quot;</span>,  <span class="keyword">new</span> SimpleStringSchema(), props));</span><br><span class="line"></span><br><span class="line"><span class="comment">// 按照 appId 进行 keyBy</span></span><br><span class="line">appInfoSource.keyBy((KeySelector&lt;String, String&gt;) appId -&gt; appId)</span><br><span class="line">        .map(<span class="keyword">new</span> RichMapFunction&lt;String, Tuple2&lt;String, Long&gt;&gt;() &#123;</span><br><span class="line">            <span class="keyword">private</span> ValueState&lt;Long&gt; pvState;</span><br><span class="line">            <span class="keyword">private</span> <span class="keyword">long</span> pv = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(Configuration parameters)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">super</span>.open(parameters);</span><br><span class="line">                <span class="comment">// 初始化状态</span></span><br><span class="line">                pvState = getRuntimeContext().getState(</span><br><span class="line">                        <span class="keyword">new</span> ValueStateDescriptor&lt;&gt;(<span class="string">&quot;pvStat&quot;</span>,</span><br><span class="line">                        TypeInformation.of(<span class="keyword">new</span> TypeHint&lt;Long&gt;() &#123;&#125;)));</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> Tuple2&lt;String, Long&gt; <span class="title">map</span><span class="params">(String appId)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="comment">// 从状态中获取该 app 的 PV 值，+1 后，update 到状态中</span></span><br><span class="line">                <span class="keyword">if</span>(<span class="keyword">null</span> == pvState.value())&#123;</span><br><span class="line">                    pv = <span class="number">1</span>;</span><br><span class="line">                &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                    pv = pvState.value();</span><br><span class="line">                    pv += <span class="number">1</span>;</span><br><span class="line">                &#125;</span><br><span class="line">                pvState.update(pv);</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> Tuple2&lt;&gt;(appId, pv);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;)</span><br><span class="line">        .print();</span><br><span class="line"></span><br><span class="line">env.execute(<span class="string">&quot;Flink PV stat&quot;</span>);</span><br></pre></td></tr></table></figure><p>详细代码请参考：</p><blockquote><p><a href="https://github.com/zhisheng17/flink-learning/blob/master/flink-learning-examples/src/main/java/com/zhisheng/examples/streaming/checkpoint/PvStatExactlyOnce.java">PvStatExactlyOnce.java</a></p></blockquote><p>代码中设置 1 分钟一次 Checkpoint，Checkpoint 语义 EXACTLY ONCE，从 Kafka 中读取数据，这里为了简化代码，所以 Kafka 中读取的直接就是 String 类型的 appId，按照 appId KeyBy 后，执行 RichMapFunction，RichMapFunction 的 open 方法中会初始化 ValueState<Long> 类型的 pvState，pvState 就是上文一直强调的状态信息，每次 Checkpoint 的时候，会把 pvState 的状态信息快照一份到 HDFS 来提供恢复。</p><p>这里按照 appId 进行 keyBy，所以每一个 appId 都会对应一个 pvState，pvState 里存储着该 appId 对应的 pv 值。每来一条数据都会执行一次 map 方法，当这条数据对应的 appId 是新 app 时，pvState 里就没有存储这个 appId 当前的 pv 值，将 pv 值赋值为 1，当 pvState 里存储的 value 不为 null 时，拿出 pv 值 +1后 update 到 pvState 里。map 方法再将 appId 和 pv 值发送到下游算子，下游直接调用了 print 进行输出，这里完全可以替换成相应的 RedisSink 或 HBaseSink。</p><p>本案例中计算 pv 的工作交给了 Flink 内部的 ValueState，不依赖外部存储介质进行累加，外部介质承担的角色仅仅是提供数据给业务方查询，所以无论下游使用什么形式的 Sink，只要 Sink 端能够按照主键去重，该统计方案就可以保证 Exactly Once。本案例使用的 ValueState，关于 State 的详细使用请参阅第 3.1 节。</p><h4 id="TwoPhaseCommitSinkFunction-如何保证端对端的-Exactly-Once"><a href="#TwoPhaseCommitSinkFunction-如何保证端对端的-Exactly-Once" class="headerlink" title="TwoPhaseCommitSinkFunction 如何保证端对端的 Exactly Once"></a>TwoPhaseCommitSinkFunction 如何保证端对端的 Exactly Once</h4><p>Flink 的源码中有这么一段注释：</p><blockquote><p>This is a recommended base class for all of the {@link SinkFunction} that intend to implement exactly-once semantic.</p></blockquote><p>意思是对于打算实现 Exactly Once 语义的所有 SinkFunction 都推荐继承该抽象类。在介绍 TwoPhaseCommitSinkFunction 之前，先了解一下 2PC 分布式一致性协议。</p><p>在分布式系统中，每一个机器节点虽然都能明确地知道自己在进行事务操作过程中的结果是成功或失败，但无法直接获取到其他分布式节点的操作结果。因此，当一个事务操作需要跨越多个分布式节点的时候，为了让每个节点都能够获取到其他节点的事务执行状况，需要引入一个“协调者（Coordinator）”节点来统一调度所有分布式节点的执行逻辑，这些被调度的分布式节点被称为“参与者（Participant）”。协调者负责调度参与者的行为，并最终决定这些参与者是否要把事务真正的提交。</p><p>普通的事务可以保证单个事务内所有操作要么全部成功，要么全部失败，而分布式系统中具体如何保证多台节点上执行的事务要么所有节点事务都成功，要么所有节点事务都失败呢？先了解一下 2PC 一致性协议。</p><p>2PC 是 Two-Phase Commit 的缩写，即两阶段提交。2PC 将分布式事务分为了两个阶段，分别是提交事务请求（投票）和执行事务提交。协调者会根据参与者在第一阶段的投票结果，来决定第二阶段是否真正的执行事务，具体流程如下。</p><p><strong>提交事务请求（投票）阶段</strong></p><ol><li>协调者向所有参与者发送 prepare 请求与事务内容，询问是否可以准备事务提交，并等待参与者的响应。</li><li>各参与者执行事务操作，并记录 Undo 日志（用于回滚）和 Redo日志（用于重放），但不真正提交。</li><li>参与者向协调者返回事务操作的执行结果，执行成功返回 Yes，否则返回 No。</li></ol><p><strong>执行事务提交阶段</strong></p><p>分为成功与失败两种情况。</p><p>若第一阶段所有参与者都返回 Yes，说明事务可以提交：</p><ol><li>协调者向所有参与者发送 Commit 请求。</li><li>参与者收到 Commit 请求后，会正式执行事务提交操作，并在提交完成后释放事务资源。</li><li>完成事务提交后，向协调者发送 Ack 消息。</li><li>协调者收到所有参与者的 Ack 消息，完成事务。</li><li>参与者收到 Commit 请求后，将事务真正地提交上去，并释放占用的事务资源，并向协调者返回 Ack。</li><li>协调者收到所有参与者的 Ack 消息，事务成功完成。</li></ol><p>若第一阶段有参与者返回 No 或者超时未返回，说明事务中断，需要回滚：</p><ol><li>协调者向所有参与者发送 Rollback 请求。</li><li>参与者收到 Rollback 请求后，根据 Undo 日志回滚到事务执行前的状态，释放占用的事务资源。</li><li>参与者在完成事务回滚后，向协调者返回 Ack。</li><li>协调者收到所有参与者的 Ack 消息，事务回滚完成。</li></ol><p>简单来讲，2PC 讲一个事务的处理过程分为了投票和执行两个阶段，其核心是每个事务都采用先尝试后提交的处理方式。下面分别图示出这两种情况：</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-07-151552.jpg" alt="img"></p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-07-151557.jpg" alt="img"></p><p>2PC 的优点：原理简单，实现方便。</p><p>2PC 的缺点：</p><ul><li>协调者单点问题：协调者在整个 2PC 协议中非常重要，一旦协调者故障，则 2PC 将无法运转。</li><li>过于保守：在 2PC 的阶段一，如果参与者出现故障而导致协调者无法获取到参与者的响应信息，这时协调者只能依靠自身的超时机制来判断是否需要中断事务，这种策略比较保守。换言之，2PC 没有涉及较为完善的容错机制，任意一个节点失败都会导致整个事务的失败。</li><li>同步阻塞：执行过程是完全同步的，各个参与者在等待其他参与者投票响应的的过程中，将无法进行其他任何操作。</li><li>数据不一致：在二阶段提交协议的阶段二，当协调者向所有的参与者发送 Commit 请求后，出现了局部网络异常或局部参与者机器故障等因素导致一部分的参与者执行了 Commit 操作，而发生故障的参与者没有执行 Commit，于是整个分布式系统便出现了数据不一致现象。</li></ul><p>Flink 的 TwoPhaseCommitSinkFunction 是基于 2PC 实现的。Flink 的 JobManager 对应到 2PC 中的协调者，Operator 实例对应到 2PC 中的参与者。TwoPhaseCommitSinkFunction 实现了 CheckpointedFunction 和 CheckpointListener 接口。</p><p>CheckpointedFunction 接口中有两个方法 snapshotState 和 initializeState，snapshotState 方法会在 Checkpoint 时且做快照之前被调用，initializeState 方法会在自定义 Function 初始化恢复状态时被调用。</p><p>CheckpointListener 接口中有一个 notifyCheckpointComplete 方法，Operator 实例的 Checkpoint 成功后，会反馈给 JobManager，当 JobManager 接收到所有 Operator 实例 Checkpoint 成功的通知后，就认为本次 Checkpoint 成功了，会给所有 Operator 实例发送一个 Checkpoint 完成的通知，Operator 实例接收到通知后，就会调用 notifyCheckpointComplete 方法。</p><p>TwoPhaseCommitSinkFunction定义了如下 5 个抽象方法：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 处理每一条数据</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">abstract</span> <span class="keyword">void</span> <span class="title">invoke</span><span class="params">(TXN transaction, IN value, Context context)</span> <span class="keyword">throws</span> Exception</span>;</span><br><span class="line"><span class="comment">// 开始一个事务，返回事务信息的句柄</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">abstract</span> TXN <span class="title">beginTransaction</span><span class="params">()</span> <span class="keyword">throws</span> Exception</span>;</span><br><span class="line"><span class="comment">// 预提交（即提交请求）阶段的逻辑</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">abstract</span> <span class="keyword">void</span> <span class="title">preCommit</span><span class="params">(TXN transaction)</span> <span class="keyword">throws</span> Exception</span>;</span><br><span class="line"><span class="comment">// 正式提交阶段的逻辑</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">abstract</span> <span class="keyword">void</span> <span class="title">commit</span><span class="params">(TXN transaction)</span></span>;</span><br><span class="line"><span class="comment">// 取消事务，Rollback 相关的逻辑</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">abstract</span> <span class="keyword">void</span> <span class="title">abort</span><span class="params">(TXN transaction)</span></span>;</span><br></pre></td></tr></table></figure><p>TwoPhaseCommitSinkFunction 里这些方法什么时候会被执行呢？如下图所示，在状态初始化的 initializeState 方法内或者每次 Checkpoint 的 snapshotState 方法内都会调用 beginTransaction 方法开启新的事务。开启新的事务后，Flink 开始处理数据，每来一条数据都会调用 invoke 方法，按照业务逻辑将数据添加到本次的事务中。等到下一次 Checkpoint 执行 snapshotState 时，会调用 preCommit 方法进行预提交，预提交一般会对事务进行 flush 操作，到这里为止可以理解为 2PC 的第一阶段。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-11-15-TwoPhaseCommitSinkFunction%20%E7%AC%AC%E4%B8%80%E9%98%B6%E6%AE%B5.png" alt="img">)</p><p>第一阶段运行期间无论是机器故障还是 invoke 失败或者 preCommit 对应预提交的 flush 失败都可以理解为 2PC 的第一阶段返回了 No，即投票失败就会执行 2PC 第二阶段的 Rollback，对应到 TwoPhaseCommitSinkFunction 中就是执行 abort 方法，abort 方法内一般会对本次事务进行 abortTransaction 操作。</p><p>只有当 2PC 的第一阶段所有参与者都完全成功，也就是说 Flink TwoPhaseCommitSinkFunction 对应的所有并行度在本次事务中 invoke 全部成功且 preCommit 对应预提交的 flush 也全部成功才认为 2PC 的第一阶段返回了Yes，即投票成功就会执行 2PC 第二阶段的 Commit，对应到 TwoPhaseCommitSinkFunction 中就是执行 Commit 方法，Commit 方法内一般会对本次事务进行 commitTransaction 操作，以上就是 Flink 中 TwoPhaseCommitSinkFunction 的大概执行流程。</p><p>在第一阶段结束时，数据被写入到了外部存储，但是当事务的隔离级别为读已提交（Read Committed）时，在外部存储中并读取不到我们写入的数据，因为并没有执行 Commit 操作。如下图所示，是第二阶段的两种情况。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-11-15-TwoPhaseCommitSinkFunction%20%E7%AC%AC%E4%BA%8C%E9%98%B6%E6%AE%B5.png" alt="img"></p><p>FlinkKafkaProducer011 继承了 TwoPhaseCommitSinkFunction，如下图所示，Flink 应用使用 FlinkKafkaProducer011 时，Checkpoint 的时候不仅要将快照保存到状态后端，还要执行 preCommit 操作将缓存中的数据 flush 到 Sink 端的 Kafka 中。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-07-151549.jpg" alt="img"></p><p>当所有的实例快照完成且所有 Sink 实例执行完 preCommit 操作时，会把快照完成的消息发送给 JobManager，JobManager 收到所有实例的 Checkpoint 完成消息时，就认为这次 Checkpoint 完成了，会向所有的实例发送 Checkpoint 完成的通知（Notify Checkpoint Completed），当 FlinkKafkaProducer011 接收到 Checkpoint 完成的消息时，就会执行 Commit 方法。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-07-151550.jpg" alt="img"></p><p>上文提到过 2PC 有一些缺点存在，关于协调者和参与者故障的问题，对应到 Flink 中如果节点发生故障会申请资源并从最近一次成功的 Checkpoint 处恢复任务，所以，节点故障的问题 Flink 已经解决了。关于 2PC 同步阻塞的问题，2PC 算法在没有等到第一阶段所有参与者的投票之前肯定是不能执行第二阶段的 Commit，所以基于 2PC 实现原理同步阻塞的问题没有办法解决，除非使用其他算法。</p><p>那数据不一致的问题呢？</p><p>在整个的第一阶段不会真正地提交数据到 Kafka，所以只要设置事务隔离识别为读已提交（Read Committed），那么第一阶段就不会导致数据不一致的问题。</p><p>那 Flink 的第二阶段呢？</p><p>Flink 中，Checkpoint 成功后，会由 JobManager 给所有的实例发送 Checkpoint 完成的通知，然后 KafkaSink 在 notifyCheckpointComplete 方法内执行 commit。假如现在执行第 n 次 Checkpoint，快照完成且预提交完成，我们认为第 n 次 Checkpoint 已经成功了，这里一定要记住无论第二阶段是否 commit 成功，Flink 都会认为第 n 次 Checkpoint 已经结束了，换言之 Flink 可能会出现第 n 次 Checkpoint 成功了，但是第 n 次 Checkpoint 对应的事务 commit 并没有成功。</p><p>当 Checkpoint 成功后，JobManager 会向所有的 KafkaSink 发送 Checkpoint 完成的通知，所有的 KafkaSink 接收到通知后才会执行 Commit 操作。假如 JobManager 发送通知时出现了故障，导致 KafkaSink 的所有并行度都没有收到通知或者只有其中一部分 KafkaSink 接收到了通知，最后有一部分的 KafkaSink 执行了 Commit，另外一部分 KafkaSink 并没有执行 Commit，此时出现了 Checkpoint 成功，但是数据并没有完整地提交到 Kafka 的情况，出现了数据不一致的问题。</p><p>那 Flink 如何解决这个问题呢？</p><p>在任务执行过程中，如果因为各种原因导致有任意一个 KafkaSink 没有 Commit 成功，就会认为 Flink 任务出现故障，就会从最近一次成功的 Checkpoint 处恢复任务，也就是从第 n 次 Checkpoint 处恢复，TwoPhaseCommitSinkFunction 将每次 Checkpoint 时需要 Commit 的事务保存在状态里，当从第 n 次 Checkpoint 恢复时会从状态中拿到第 n 次 Checkpoint 可能没有提交的事务并执行 Commit，通过这种方式来保证所有的 KafkaSink 都能将事务进行 Commit，从而解决了 2PC 协议中可能出现的数据不一致的问题。</p><p>也就是说 Flink 任务重启后，会检查之前 Checkpoint 是否有未提交的事务，如果有则执行 Commit，从而保证了 Checkpoint 之前的数据被完整地提交。</p><p>简单描述一下 FlinkKafkaProducer011 的实现原理：</p><ul><li>FlinkKafkaProducer011 继承了 TwoPhaseCommitSinkFunction，所有并行度在 initializeState 初始化状态时，会开启新的事务，并把状态里保存的之前未提交事务进行 commit。</li><li>接下来开始调用 invoke 方法处理数据，会把数据通过事务 api 发送到 Kafka。一段时间后，开始 Checkpoint，checkpoint 时 snapshotState 方法会被执行，snapshotState 方法会调用 preCommit 方法并把当前还未 Commit 的事务添加到状态中来提供故障容错。</li><li>snapshotState 方法执行完成后，会对自身状态信息进行快照并上传到 HDFS 上来提供恢复。所有的实例都将状态信息备份完成后就认为本次 Checkpoint 结束了，此时 JobManager 会向所有的实例发送 Checkpoint 完成的通知，各实例收到通知后，会调用 notifyCheckpointComplete 方法把未提交的事务进行 commit。</li><li>期间如果出现其中某个并行度出现故障，JobManager 会停止此任务，向所有的实例发送通知，各实例收到通知后，调用 close 方法，关闭 Kafka 事务 Producer。</li></ul><p>以上就是 FlinkKafkaProducer011 实现原理的简单描述，具体实现细节请参考源码。</p><p>TwoPhaseCommitSinkFunction 还存在一个问题，假如我们设置的一分钟一次 Checkpoint，事务隔离级别设置为读已提交时，那么我们这一分钟内写入的数据，都必须等到 Checkpoint 结束后，下游才能读取到，导致我们的 Flink 任务数据延迟了一分钟。所以我们要结合这个特性，合理的设置我们的 Checkpoint 周期。</p>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
            <tag> Flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink 重启策略</title>
      <link href="2019/12/09/Flink-%E9%87%8D%E5%90%AF%E7%AD%96%E7%95%A5/"/>
      <url>2019/12/09/Flink-%E9%87%8D%E5%90%AF%E7%AD%96%E7%95%A5/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><h3 id="Flink-Job-常见重启错误"><a href="#Flink-Job-常见重启错误" class="headerlink" title="Flink Job 常见重启错误"></a>Flink Job 常见重启错误</h3><p>不知道大家是否有遇到过这样的问题：整个 Job 一直在重启，并且还会伴随着一些错误（可以通过 UI 查看 Exceptions 日志）</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-04-152844.png" alt="img"></p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-06-140519.png" alt="img"></p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-09-26-2019-05-14_00-59-25.png" alt="img"></p><p>其实遇到上面这种问题比较常见的，比如有时候因为数据的问题（不合规范、为 null 等），这时在处理这些脏数据的时候可能就会遇到各种各样的异常错误，比如空指针、数组越界、数据类型转换错误等。可能你会说只要过滤掉这种脏数据就行了，或者进行异常捕获就不会导致 Job 不断重启的问题了。</p><p>确实如此，如果做好了脏数据的过滤和异常的捕获，Job 的稳定性确实有保证，但是复杂的 Job 下每个算子可能都会产生出脏数据（包含源数据可能也会为空或者不合法的数据），你不可能在每个算子里面也用一个大的 try catch 做一个异常捕获，所以脏数据和异常简直就是防不胜防，不过我们还是要尽力的保证代码的健壮性，但是也要配置好 Flink Job 的 RestartStrategy（重启策略）。</p><h3 id="RestartStrategy"><a href="#RestartStrategy" class="headerlink" title="RestartStrategy"></a>RestartStrategy</h3><p>RestartStrategy，重启策略，在遇到机器或者代码等不可预知的问题时导致 Job 或者 Task 挂掉的时候，它会根据配置的重启策略将 Job 或者受影响的 Task 拉起来重新执行，以使得作业恢复到之前正常执行状态。Flink 中的重启策略决定了是否要重启 Job 或者 Task，以及重启的次数和每次重启的时间间隔。</p><h3 id="为什么需要-RestartStrategy？"><a href="#为什么需要-RestartStrategy？" class="headerlink" title="为什么需要 RestartStrategy？"></a>为什么需要 RestartStrategy？</h3><p>重启策略会让 Job 从上一次完整的 Checkpoint 处恢复状态，保证 Job 和挂之前的状态保持一致，另外还可以让 Job 继续处理数据，不会出现 Job 挂了导致消息出现大量堆积的问题，合理的设置重启策略可以减少 Job 不可用时间和避免人工介入处理故障的运维成本，因此重启策略对于 Flink Job 的稳定性来说有着举足轻重的作用。</p><h3 id="怎么配置-RestartStrategy？"><a href="#怎么配置-RestartStrategy？" class="headerlink" title="怎么配置 RestartStrategy？"></a>怎么配置 RestartStrategy？</h3><p>既然 Flink 中的重启策略作用这么大，那么该如何配置呢？其实如果 Flink Job 没有单独设置重启重启策略的话，则会使用集群启动时加载的默认重启策略，如果 Flink Job 中单独设置了重启策略则会覆盖默认的集群重启策略。默认重启策略可以在 Flink 的配置文件 <code>flink-conf.yaml</code> 中设置，由 <code>restart-strategy</code> 参数控制，有 fixed-delay（固定延时重启策略）、failure-rate（故障率重启策略）、none（不重启策略）三种可以选择，如果选择的参数不同，对应的其他参数也不同。下面分别介绍这几种重启策略和如何配置。</p><h4 id="FixedDelayRestartStrategy（固定延时重启策略）"><a href="#FixedDelayRestartStrategy（固定延时重启策略）" class="headerlink" title="FixedDelayRestartStrategy（固定延时重启策略）"></a>FixedDelayRestartStrategy（固定延时重启策略）</h4><p>FixedDelayRestartStrategy 是固定延迟重启策略，程序按照集群配置文件中或者程序中额外设置的重启次数尝试重启作业，如果尝试次数超过了给定的最大次数，程序还没有起来，则停止作业，另外还可以配置连续两次重启之间的等待时间，在 <code>flink-conf.yaml</code> 中可以像下面这样配置。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">restart-strategy:</span> <span class="string">fixed-delay</span></span><br><span class="line"><span class="attr">restart-strategy.fixed-delay.attempts:</span> <span class="number">3</span>  <span class="comment">#表示作业重启的最大次数，启用 checkpoint 的话是 Integer.MAX_VALUE，否则是 1。</span></span><br><span class="line"><span class="attr">restart-strategy.fixed-delay.delay:</span> <span class="number">10</span> <span class="string">s</span>  <span class="comment">#如果设置分钟可以类似 1 min，该参数表示两次重启之间的时间间隔，当程序与外部系统有连接交互时延迟重启可能会有帮助，启用 checkpoint 的话，延迟重启的时间是 10 秒，否则使用 akka.ask.timeout 的值。</span></span><br></pre></td></tr></table></figure><p>在程序中设置固定延迟重启策略的话如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">env.setRestartStrategy(RestartStrategies.fixedDelayRestart(</span><br><span class="line">  <span class="number">3</span>, <span class="comment">// 尝试重启的次数</span></span><br><span class="line">  Time.of(<span class="number">10</span>, TimeUnit.SECONDS) <span class="comment">// 延时</span></span><br><span class="line">));</span><br></pre></td></tr></table></figure><h4 id="FailureRateRestartStrategy（故障率重启策略）"><a href="#FailureRateRestartStrategy（故障率重启策略）" class="headerlink" title="FailureRateRestartStrategy（故障率重启策略）"></a>FailureRateRestartStrategy（故障率重启策略）</h4><p>FailureRateRestartStrategy 是故障率重启策略，在发生故障之后重启作业，如果固定时间间隔之内发生故障的次数超过设置的值后，作业就会失败停止，该重启策略也支持设置连续两次重启之间的等待时间。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">restart-strategy:</span> <span class="string">failure-rate</span></span><br><span class="line"><span class="attr">restart-strategy.failure-rate.max-failures-per-interval:</span> <span class="number">3</span>  <span class="comment">#固定时间间隔内允许的最大重启次数，默认 1</span></span><br><span class="line"><span class="attr">restart-strategy.failure-rate.failure-rate-interval:</span> <span class="number">5</span> <span class="string">min</span>  <span class="comment">#固定时间间隔，默认 1 分钟</span></span><br><span class="line"><span class="attr">restart-strategy.failure-rate.delay:</span> <span class="number">10</span> <span class="string">s</span> <span class="comment">#连续两次重启尝试之间的延迟时间，默认是 akka.ask.timeout </span></span><br></pre></td></tr></table></figure><p>可以在应用程序中这样设置来配置故障率重启策略：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">env.setRestartStrategy(RestartStrategies.failureRateRestart(</span><br><span class="line">  <span class="number">3</span>, <span class="comment">// 固定时间间隔允许 Job 重启的最大次数</span></span><br><span class="line">  Time.of(<span class="number">5</span>, TimeUnit.MINUTES), <span class="comment">// 固定时间间隔</span></span><br><span class="line">  Time.of(<span class="number">10</span>, TimeUnit.SECONDS) <span class="comment">// 两次重启的延迟时间</span></span><br><span class="line">));</span><br></pre></td></tr></table></figure><h4 id="NoRestartStrategy（不重启策略）"><a href="#NoRestartStrategy（不重启策略）" class="headerlink" title="NoRestartStrategy（不重启策略）"></a>NoRestartStrategy（不重启策略）</h4><p>NoRestartStrategy 作业不重启策略，直接失败停止，在 <code>flink-conf.yaml</code> 中配置如下：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">restart-strategy:</span> <span class="string">none</span></span><br></pre></td></tr></table></figure><p>在程序中如下设置即可配置不重启：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">env.setRestartStrategy(RestartStrategies.noRestart());</span><br></pre></td></tr></table></figure><h4 id="Fallback（备用重启策略）"><a href="#Fallback（备用重启策略）" class="headerlink" title="Fallback（备用重启策略）"></a>Fallback（备用重启策略）</h4><p>如果程序没有启用 Checkpoint，则采用不重启策略，如果开启了 Checkpoint 且没有设置重启策略，那么采用固定延时重启策略，最大重启次数为 Integer.MAX_VALUE。</p><p>在应用程序中配置好了固定延时重启策略，可以测试一下代码异常后导致 Job 失败后重启的情况，然后观察日志，可以看到 Job 重启相关的日志：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[flink-akka.actor.default-dispatcher-5] INFO org.apache.flink.runtime.executiongraph.ExecutionGraph - Try to restart or fail the job zhisheng default RestartStrategy example (a890361aed156610b354813894d02cd0) if no longer possible.</span><br><span class="line">[flink-akka.actor.default-dispatcher-5] INFO org.apache.flink.runtime.executiongraph.ExecutionGraph - Job zhisheng default RestartStrategy example (a890361aed156610b354813894d02cd0) switched from state FAILING to RESTARTING.</span><br><span class="line">[flink-akka.actor.default-dispatcher-5] INFO org.apache.flink.runtime.executiongraph.ExecutionGraph - Restarting the job zhisheng default RestartStrategy example (a890361aed156610b354813894d02cd0).</span><br></pre></td></tr></table></figure><p>最后重启次数达到配置的最大重启次数后 Job 还没有起来的话，则会停止 Job 并打印日志：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[flink-akka.actor.default-dispatcher-2] INFO org.apache.flink.runtime.executiongraph.ExecutionGraph - Could not restart the job zhisheng default RestartStrategy example (a890361aed156610b354813894d02cd0) because the restart strategy prevented it.</span><br></pre></td></tr></table></figure><p>Flink 中几种重启策略的设置如上，大家可以根据需要选择合适的重启策略，比如如果程序抛出了空指针异常，但是你配置的是一直无限重启，那么就会导致 Job 一直在重启，这样无非再浪费机器资源，这种情况下可以配置重试固定次数，每次隔多久重试的固定延时重启策略，这样在重试一定次数后 Job 就会停止，如果对 Job 的状态做了监控告警的话，那么你就会收到告警信息，这样也会提示你去查看 Job 的运行状况，能及时的去发现和修复 Job 的问题。</p><h3 id="RestartStrategy-源码剖析"><a href="#RestartStrategy-源码剖析" class="headerlink" title="RestartStrategy 源码剖析"></a>RestartStrategy 源码剖析</h3><p>再介绍重启策略应用程序代码配置的时候不知道你有没有看到设置重启策略都是使用 RestartStrategies 类，通过该类的方法就可以创建不同的重启策略，在 RestartStrategies 类中提供了五个方法用来创建四种不同的重启策略（有两个方法是创建 FixedDelay 重启策略的，只不过方法的参数不同），如下图所示：</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-08-151745.png" alt="img"></p><p>在每个方法内部其实调用的是 RestartStrategies 中的内部静态类，分别是 NoRestartStrategyConfiguration、FixedDelayRestartStrategyConfiguration、FailureRateRestartStrategyConfiguration、FallbackRestartStrategyConfiguration，这四个类都继承自 RestartStrategyConfiguration 抽象类。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-08-151617.png" alt="img"></p><p>上面是定义的四种重启策略的配置类，在 Flink 中是靠 RestartStrategyResolving 类中的 resolve 方法来解析 RestartStrategies.RestartStrategyConfiguration，然后根据配置使用 RestartStrategyFactory 创建 RestartStrategy。RestartStrategy 是一个接口，它有 canRestart 和 restart 两个方法，它有四个实现类： FixedDelayRestartStrategy、FailureRateRestartStrategy、ThrowingRestartStrategy、NoRestartStrategy。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-08-151311.png" alt="img"></p><h3 id="Failover-Strategies（故障恢复策略）"><a href="#Failover-Strategies（故障恢复策略）" class="headerlink" title="Failover Strategies（故障恢复策略）"></a>Failover Strategies（故障恢复策略）</h3><p>Flink 通过重启策略和故障恢复策略来控制 Task 重启：重启策略决定是否可以重启以及重启的间隔；故障恢复策略决定哪些 Task 需要重启。在 Flink 中支持两种不同的故障重启策略，该策略可以在 flink-conf.yaml 中的配置，默认为：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">jobmanager.execution.failover-strategy:</span> <span class="string">region</span></span><br></pre></td></tr></table></figure><p>该配置有两个可选值，full（重启所有的 Task）和 region（重启 pipelined region），在 Flink 1.9 中默认设置的恢复策略变成 region 了。</p><p>参考 Flink Issue：<a href="https://issues.apache.org/jira/browse/FLINK-13223">https://issues.apache.org/jira/browse/FLINK-13223</a></p><h4 id="重启所有的任务"><a href="#重启所有的任务" class="headerlink" title="重启所有的任务"></a>重启所有的任务</h4><p>在 full 故障恢复策略下，Task 发生故障时会重启作业中的所有 Task 来恢复，会造成一定的资源浪费，但却是恢复作业一致性的最安全策略，会在其他 Failover 策略失败时作为保底策略使用。</p><h4 id="基于-Region-的局部故障重启策略"><a href="#基于-Region-的局部故障重启策略" class="headerlink" title="基于 Region 的局部故障重启策略"></a>基于 Region 的局部故障重启策略</h4><p>基于 Region 的局部故障恢复策略会将作业中的 Task 划分为数个 Region，根据数据传输决定的，有数据传输的 Task 会被放在同一个 Region，不同 Region 之间无数据交换。如果有 Task 发生故障的时候，它会重启发生错误的 Task 所在 Region 的所有 Task，这种策略相对于重启所有的 Task 策略来说重启的 Task 数量会变少。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-08-131936.png" alt="img"></p><p>如上图如果 C2 Task 因为错误挂了，它会根据数据流往上找到 Source，然后根据 Source 可以知道数据流到下游的所有 Task，进而将这些 Task 重启（见下图）。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-08-140828.png" alt="img"></p><p>当然你会发现上面这种重启方式其实重启的 Task 数量还是不少，为了进一步减少需要重新启动的 Task 数量，可以使用某些类型的数据流交换，将 Task 运算的结果暂存在中间，然后如果有 Task 失败了，那么就往前去找中间结果，然后重启中间结果到数据流向的最后 Task 之间所有的 Task。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-08-144622.png" alt="img"></p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-08-144713.png" alt="img"></p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-08-145101.png" alt="img"></p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-08-150015.png" alt="img"></p><p>从上面四个图可以看到这样的话，故障恢复的需要重启的 Task 数量就降低了，但是适合这种的策略的场景是有限的，详情可以参考：</p><blockquote><p><a href="https://cwiki.apache.org/confluence/display/FLINK/FLIP-1+:+Fine+Grained+Recovery+from+Task+Failures">https://cwiki.apache.org/confluence/display/FLINK/FLIP-1+%3A+Fine+Grained+Recovery+from+Task+Failures</a></p></blockquote><p>在查看源码的时候还看到一种恢复策略是 RestartIndividualStrategy，这种策略只会重启挂掉的那个 Task，如果该 Task 没有包含数据源，这会导致它不能重流数据而导致一部分数据丢失，所以这种策略的使用是有局限性的，不能保证数据的一致性。</p>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
            <tag> Flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink State 深入理解</title>
      <link href="2019/12/07/Flink-State-%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3/"/>
      <url>2019/12/07/Flink-State-%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p>Flink 是一款有状态的流处理框架</p><h1 id="State-概述"><a href="#State-概述" class="headerlink" title="State 概述"></a>State 概述</h1><h2 id="为什么需要-state？"><a href="#为什么需要-state？" class="headerlink" title="为什么需要 state？"></a>为什么需要 state？</h2><p>对于流处理系统，数据是一条一条被处理的，如果没有对数据处理的进度进行记录，那么如果这个处理数据的 Job 因为机器问题或者其他问题而导致重启，那么它是不知道上一次处理数据是到哪个地方了，这样的情况下如果是批数据，倒是可以很好的解决（重新将这份固定的数据再执行一遍），但是流数据那就麻烦了，你根本不知道什么在 Job 挂的那个时刻数据消费到哪里了？那么你重启的话该从哪里开始重新消费呢？你可以有以下选择（因为你可能也不确定 Job 挂的具体时间）：</p><ol><li>Job 挂的那个时间之前：如果是从 Job 挂之前开始重新消费的话，那么会导致部分数据（从新消费的时间点到之前 Job 挂的那个时间点之前的数据）重复消费</li><li>Job 挂的那个时间之后：如果是从 Job 挂之后开始消费的话，那么会导致部分数据（从 Job 挂的那个时间点到新消费的时间点产生的数据）丢失，没有消费</li></ol><p><img src="/images/flink/1.png"></p><p>为解决上面两种情况（数据重复消费或者数据没有消费）的发生，Flink state 诞生了，state 中存储着每条数据消费后数据的消费点（生产环境需要持久化这些状态）,当 Job 因为某种错误或者其他原因导致重启时，就能够从 checkpoint 中的 state 数据进行恢复</p><h2 id="state-种类"><a href="#state-种类" class="headerlink" title="state 种类"></a>state 种类</h2><p><strong>在 Flink 中有两个基本的 state：Keyed state 和 Operator state</strong></p><h3 id="Keyed-State"><a href="#Keyed-State" class="headerlink" title="Keyed State"></a>Keyed State</h3><p><strong>Keyed State 总是和具体的 key 相关联，也只能在 KeyedStream 的 function 和 operator 上使用。可以将 Keyed State 当作是 Operator State 的一种特例，但是它是被分区或分片的。每个 Keyed State 分区对应一个 key 的 Operator State，对于某个 key 在某个分区上有唯一的状态。</strong></p><h3 id="Operator-State"><a href="#Operator-State" class="headerlink" title="Operator State"></a>Operator State</h3><p>对 Operator State 而言，每个 operator state 都对应着一个并行实例。</p><p>Kafka Connector 就是一个很好的例子。每个 Kafka consumer 的并行实例都会持有一份topic partition 和 offset 的 map，这个 map 就是它的 Operator State。</p><p>当并行度发生变化时，Operator State 可以将状态在所有的并行实例中进行重分配，并且提供了多种方式来进行重分配。</p><p>当并行度发生变化时，Operator State 可以将状态在所有的并行实例中进行重分配，并且提供了多种方式来进行重分配。</p><hr><hr><h3 id="Raw-and-Managed-State"><a href="#Raw-and-Managed-State" class="headerlink" title="Raw and Managed State"></a>Raw and Managed State</h3><p><strong>Keyed State 和 Operator State 都有两种存在形式，即 Raw State（原始状态）和 Managed State（托管状态）。</strong></p><p><strong>Raw State</strong></p><blockquote><p>原始状态是算子保存它们自己的数据结构中的 state，当 checkpoint 时，原始状态会以字节流的形式写入进 checkpoint 中。Flink 并不知道 State 的数据结构长啥样，仅能看到原生的字节数组。</p></blockquote><p><strong>Managed State</strong></p><blockquote><p>托管状态可以使用 Flink 提供的数据结构来表示，例如内部哈希表或者 RocksDB。具体有 ValueState，ListState 等。Flink 会对这些状态进行编码然后将它们写入到 checkpoint 中。</p></blockquote><p>DataStream 的所有 function 都可以使用托管状态，但是原生状态只能在自定义 operator 的时候使用。相对于原生状态，推荐使用托管状态，因为如果使用托管状态，当并行度发生改变时，Flink 可以自动的帮你重分配 state，同时还可以更好的管理内存。</p><p>注意：如果你的托管状态需要特殊的序列化，目前 Flink 还不支持。</p><hr><hr><h3 id="使用托管-Keyed-State"><a href="#使用托管-Keyed-State" class="headerlink" title="使用托管 Keyed State"></a>使用托管 Keyed State</h3><p>托管的 Keyed State 接口提供对不同类型状态（这些状态的范围都是当前输入元素的 key）的访问，这意味着这种状态只能在通过 stream.keyBy() 创建的 KeyedStream 上使用。</p><p>我们首先来看一下有哪些可以使用的状态，然后再来看看它们在程序中是如何使用的：</p><ol><li><p>ValueState</p><p>保存一个可以更新和获取的值（每个 Key 一个 value），可以用 update(T) 来更新 value，可以用 value() 来获取 value。</p></li><li><p>ListState</p><p>保存一个值的列表，用 add(T) 或者 addAll(List) 来添加，用 Iterable get() 来获取。</p></li><li><p>ReducingState</p><p>保存一个值，这个值是状态的很多值的聚合结果，接口和 ListState 类似，但是可以用相应的 ReduceFunction 来聚合。</p></li><li><p>AggregatingState</p><p>保存很多值的聚合结果的单一值，与 ReducingState 相比，不同点在于聚合类型可以和元素类型不同，提供 AggregateFunction 来实现聚合。</p><p>FoldingState</p><p>与 AggregatingState 类似，除了使用 FoldFunction 进行聚合。</p><p><font color='red'>注意：FoldingState 已经不推荐使用，可以用 AggregatingState 来代替。</font></p></li><li><p>MapState: 保存一组映射，可以将 kv 放进这个状态，使用 put(UK, UV) 或者 putAll(Map) 添加，或者使用 get(UK) 获取。</p></li></ol><p>所有类型的状态都有一个 clear() 方法来清除当前的状态。</p><p>需要注意，上面的这些状态对象仅用来和状态打交道，状态不一定保存在内存中，也可以存储在磁盘或者其他地方。另外，你获取到的状态的值是取决于输入元素的 key，因此如果 key 不同，那么在一次调用用户函数中获得的值可能与另一次调用的值不同。</p><p>要使用一个状态对象，需要先创建一个 StateDescriptor，它包含了状态的名字（你可以创建若干个 state，但是它们必须要有唯一的值以便能够引用它们），状态的值的类型，或许还有一个用户定义的函数，比如 ReduceFunction。根据你想要使用的 state 类型，你可以创建 ValueStateDescriptor、ListStateDescriptor、ReducingStateDescriptor、FoldingStateDescriptor 或者 MapStateDescriptor。</p><p>状态只能通过 RuntimeContext 来获取，所以只能在 RichFunction 里面使用。RichFunction 中你可以通过 RuntimeContext 用下述方法获取状态：</p><ul><li>ValueState getState(ValueStateDescriptor)</li><li>ReducingState getReducingState(ReducingStateDescriptor)</li><li>ListState getListState(ListStateDescriptor)</li><li>AggregatingState getAggregatingState(AggregatingState)</li><li>FoldingState getFoldingState(FoldingStateDescriptor)</li><li>MapState getMapState(MapStateDescriptor)</li></ul><p>上面讲了这么多概念，那么来一个例子来看看如何使用状态：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CountWindowAverage</span> <span class="keyword">extends</span> <span class="title">RichFlatMapFunction</span>&lt;<span class="title">Tuple2</span>&lt;<span class="title">Long</span>, <span class="title">Long</span>&gt;, <span class="title">Tuple2</span>&lt;<span class="title">Long</span>, <span class="title">Long</span>&gt;&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//ValueState 使用方式，第一个字段是 count，第二个字段是运行的和 </span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">transient</span> ValueState&lt;Tuple2&lt;Long, Long&gt;&gt; sum;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">flatMap</span><span class="params">(Tuple2&lt;Long, Long&gt; input, Collector&lt;Tuple2&lt;Long, Long&gt;&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//访问状态的 value 值</span></span><br><span class="line">        Tuple2&lt;Long, Long&gt; currentSum = sum.value();</span><br><span class="line"></span><br><span class="line">        <span class="comment">//更新 count</span></span><br><span class="line">        currentSum.f0 += <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//更新 sum</span></span><br><span class="line">        currentSum.f1 += input.f1;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//更新状态</span></span><br><span class="line">        sum.update(currentSum);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//如果 count 等于 2, 发出平均值并清除状态</span></span><br><span class="line">        <span class="keyword">if</span> (currentSum.f0 &gt;= <span class="number">2</span>) &#123;</span><br><span class="line">            out.collect(<span class="keyword">new</span> Tuple2&lt;&gt;(input.f0, currentSum.f1 / currentSum.f0));</span><br><span class="line">            sum.clear();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(Configuration config)</span> </span>&#123;</span><br><span class="line">        ValueStateDescriptor&lt;Tuple2&lt;Long, Long&gt;&gt; descriptor =</span><br><span class="line">                <span class="keyword">new</span> ValueStateDescriptor&lt;&gt;(</span><br><span class="line">                        <span class="string">&quot;average&quot;</span>, <span class="comment">//状态名称</span></span><br><span class="line">                        TypeInformation.of(<span class="keyword">new</span> TypeHint&lt;Tuple2&lt;Long, Long&gt;&gt;() &#123;&#125;), <span class="comment">//类型信息</span></span><br><span class="line">                        Tuple2.of(<span class="number">0L</span>, <span class="number">0L</span>)); <span class="comment">//状态的默认值</span></span><br><span class="line">        sum = getRuntimeContext().getState(descriptor);<span class="comment">//获取状态</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">env.fromElements(Tuple2.of(<span class="number">1L</span>, <span class="number">3L</span>), Tuple2.of(<span class="number">1L</span>, <span class="number">5L</span>), Tuple2.of(<span class="number">1L</span>, <span class="number">7L</span>), Tuple2.of(<span class="number">1L</span>, <span class="number">4L</span>), Tuple2.of(<span class="number">1L</span>, <span class="number">2L</span>))</span><br><span class="line">        .keyBy(<span class="number">0</span>)</span><br><span class="line">        .flatMap(<span class="keyword">new</span> CountWindowAverage())</span><br><span class="line">        .print();</span><br><span class="line"></span><br><span class="line"><span class="comment">//结果会打印出 (1,4) 和 (1,5)</span></span><br></pre></td></tr></table></figure><p>这个例子实现了一个简单的计数器，我们使用元组的第一个字段来进行分组(这个例子中，所有的 key 都是 1)，这个 CountWindowAverage 函数将计数和运行时总和保存在一个 ValueState 中，一旦计数等于 2，就会发出平均值并清理 state，因此又从 0 开始。请注意，如果在第一个字段中具有不同值的元组，则这将为每个不同的输入 key保存不同的 state 值。</p><h3 id="State-TTL-存活时间"><a href="#State-TTL-存活时间" class="headerlink" title="State TTL(存活时间)"></a>State TTL(存活时间)</h3><h4 id="State-TTL-介绍"><a href="#State-TTL-介绍" class="headerlink" title="State TTL 介绍"></a>State TTL 介绍</h4><p>TTL 可以分配给任何类型的 Keyed state，如果一个状态设置了 TTL，那么当状态过期时，那么之前存储的状态值会被清除。所有的状态集合类型都支持单个入口的 TTL，这意味着 List 集合元素和 Map 集合都支持独立到期。为了使用状态 TTL，首先必须要构建 StateTtlConfig 配置对象，然后可以通过传递配置在 State descriptor 中启用 TTL 功能：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.state.StateTtlConfig;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.state.ValueStateDescriptor;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.time.Time;</span><br><span class="line"></span><br><span class="line">StateTtlConfig ttlConfig = StateTtlConfig</span><br><span class="line">    .newBuilder(Time.seconds(<span class="number">1</span>))</span><br><span class="line">    .setUpdateType(StateTtlConfig.UpdateType.OnCreateAndWrite)</span><br><span class="line">    .setStateVisibility(StateTtlConfig.StateVisibility.NeverReturnExpired)</span><br><span class="line">    .build();</span><br><span class="line"></span><br><span class="line">ValueStateDescriptor&lt;String&gt; stateDescriptor = <span class="keyword">new</span> ValueStateDescriptor&lt;&gt;(<span class="string">&quot;zhisheng&quot;</span>, String.class);</span><br><span class="line">stateDescriptor.enableTimeToLive(ttlConfig);    <span class="comment">//开启 ttl</span></span><br></pre></td></tr></table></figure><p>上面配置中有几个选项需要注意：</p><p>1、newBuilder 方法的第一个参数是必需的，它代表着状态存活时间。</p><p>2、UpdateType 配置状态 TTL 更新时（默认为 OnCreateAndWrite）：</p><ul><li>StateTtlConfig.UpdateType.OnCreateAndWrite: 仅限创建和写入访问时更新</li><li>StateTtlConfig.UpdateType.OnReadAndWrite: 除了创建和写入访问，还支持在读取时更新</li></ul><p>3、StateVisibility 配置是否在读取访问时返回过期值（如果尚未清除），默认是 NeverReturnExpired：</p><ul><li>StateTtlConfig.StateVisibility.NeverReturnExpired: 永远不会返回过期值</li><li>StateTtlConfig.StateVisibility.ReturnExpiredIfNotCleanedUp: 如果仍然可用则返回</li></ul><p>在 NeverReturnExpired 的情况下，过期状态表现得好像它不再存在，即使它仍然必须被删除。该选项对于在 TTL 之后必须严格用于读取访问的数据的用例是有用的，例如，应用程序使用隐私敏感数据.</p><p>另一个选项 ReturnExpiredIfNotCleanedUp 允许在清理之前返回过期状态。</p><p>注意：</p><ul><li>状态后端会存储上次修改的时间戳以及对应的值，这意味着启用此功能会增加状态存储的消耗，堆状态后端存储一个额外的 Java 对象，其中包含对用户状态对象的引用和内存中原始的 long 值。RocksDB 状态后端存储为每个存储值、List、Map 都添加 8 个字节。</li><li>目前仅支持参考 processing time 的 TTL</li><li>使用启用 TTL 的描述符去尝试恢复先前未使用 TTL 配置的状态可能会导致兼容性失败或者 StateMigrationException 异常。</li><li>TTL 配置并不是 Checkpoint 和 Savepoint 的一部分，而是 Flink 如何在当前运行的 Job 中处理它的方式。</li><li>只有当用户值序列化器可以处理 null 值时，具体 TTL 的 Map 状态当前才支持 null 值，如果序列化器不支持 null 值，则可以使用 NullableSerializer 来包装它（代价是需要一个额外的字节）。</li></ul><h4 id="清除过期-state"><a href="#清除过期-state" class="headerlink" title="清除过期 state"></a>清除过期 state</h4><p>默认情况下，过期值只有在显式读出时才会被删除，例如通过调用 ValueState.value()。</p><p>注意：这意味着默认情况下，如果未读取过期状态，则不会删除它，这可能导致状态不断增长，这个特性在 Flink 未来的版本可能会发生变化。</p><p>此外，你可以在获取完整状态快照时激活清理状态，这样就可以减少状态的大小。在当前实现下不清除本地状态，但是在从上一个快照恢复的情况下，它不会包括已删除的过期状态，你可以在 StateTtlConfig 中这样配置：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.state.StateTtlConfig;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.time.Time;</span><br><span class="line"></span><br><span class="line">StateTtlConfig ttlConfig = StateTtlConfig</span><br><span class="line">    .newBuilder(Time.seconds(<span class="number">1</span>))</span><br><span class="line">    .cleanupFullSnapshot()</span><br><span class="line">    .build();</span><br></pre></td></tr></table></figure><p>此配置不适用于 RocksDB 状态后端中的增量 checkpoint。对于现有的 Job，可以在 StateTtlConfig 中随时激活或停用此清理策略，例如，从保存点重启后。</p><p>除了在完整快照中清理外，你还可以在后台激活清理。如果使用的后端支持以下选项，则会激活 StateTtlConfig 中的默认后台清理：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.state.StateTtlConfig;</span><br><span class="line">StateTtlConfig ttlConfig = StateTtlConfig</span><br><span class="line">    .newBuilder(Time.seconds(<span class="number">1</span>))</span><br><span class="line">    .cleanupInBackground()</span><br><span class="line">    .build();</span><br></pre></td></tr></table></figure><p>要在后台对某些特殊清理进行更精细的控制，可以按照下面的说明单独配置它。目前，堆状态后端依赖于增量清理，RocksDB 后端使用压缩过滤器进行后台清理。</p><p>我们再来看看 TTL 对应着的类 StateTtlConfig 类中的具体实现，这样我们才能更加的理解其使用方式。</p><p>在该类中的属性有如下：</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-23-143816.png" alt="img"></p><ul><li>DISABLED：它默认创建了一个 UpdateType 为 Disabled 的 StateTtlConfig</li><li>UpdateType：这个是一个枚举，包含 Disabled（代表 TTL 是禁用的，状态不会过期）、OnCreateAndWrite、OnReadAndWrite 可选</li><li>StateVisibility：这也是一个枚举，包含了 ReturnExpiredIfNotCleanedUp、NeverReturnExpired</li><li>TimeCharacteristic：这是时间特征，其实是只有 ProcessingTime 可选</li><li>Time：设置 TTL 的时间，这里有两个参数 unit 和 size</li><li>CleanupStrategies：TTL 清理策略，在该类中又有字段 isCleanupInBackground（是否在后台清理） 和相关的清理 strategies（包含 FULL<em>STATE</em>SCAN<em>SNAPSHOT、INCREMENTAL</em>CLEANUP 和 ROCKSDB<em>COMPACTION</em>FILTER），同时该类中还有 CleanupStrategy 接口，它的实现类有 EmptyCleanupStrategy（不清理，为空）、IncrementalCleanupStrategy（增量的清除）、RocksdbCompactFilterCleanupStrategy（在 RocksDB 中自定义压缩过滤器）。</li></ul><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-23-144111.png" alt="img"></p><h3 id="如何使用托管-Operator-State"><a href="#如何使用托管-Operator-State" class="headerlink" title="如何使用托管 Operator State"></a>如何使用托管 Operator State</h3><p>为了使用托管的 Operator State，必须要有一个有状态的函数，这个函数可以实现 CheckpointedFunction 或者 ListCheckpointed 接口。</p><p>下面分别讲一下如何使用：</p><h4 id="CheckpointedFunction"><a href="#CheckpointedFunction" class="headerlink" title="CheckpointedFunction"></a>CheckpointedFunction</h4><p>如果是实现 CheckpointedFunction 接口的话，那么我们先来看下这个接口里面有什么方法呢：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//当请求 checkpoint 快照时，将调用此方法</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">snapshotState</span><span class="params">(FunctionSnapshotContext context)</span> <span class="keyword">throws</span> Exception</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">//在分布式执行期间创建并行功能实例时，将调用此方法。 函数通常在此方法中设置其状态存储数据结构</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">initializeState</span><span class="params">(FunctionInitializationContext context)</span> <span class="keyword">throws</span> Exception</span>;</span><br></pre></td></tr></table></figure><p>当有请求执行 checkpoint 的时候，snapshotState() 方法就会被调用，initializeState() 方法会在每次初始化用户定义的函数时或者从更早的 checkpoint 恢复的时候被调用，因此 initializeState() 不仅是不同类型的状态被初始化的地方，而且还是 state 恢复逻辑的地方。</p><p>目前，List 类型的托管状态是支持的，状态被期望是一个可序列化的对象的 List，彼此独立，这样便于重分配，换句话说，这些对象是可以重新分配的 non-keyed state 的最小粒度，根据状态的访问方法，定义了重新分配的方案：</p><ul><li>Even-split redistribution：每个算子会返回一个状态元素列表，整个状态在逻辑上是所有列表的连接。在重新分配或者恢复的时候，这个状态元素列表会被按照并行度分为子列表，每个算子会得到一个子列表。这个子列表可能为空，或包含一个或多个元素。举个例子，如果使用并行性 1，算子的检查点状态包含元素 element1 和 element2，当将并行性增加到 2 时，element1 可能最终在算子实例 0 中，而 element2 将转到算子实例 1 中。</li><li>Union redistribution：每个算子会返回一个状态元素列表，整个状态在逻辑上是所有列表的连接。在重新分配或恢复的时候，每个算子都会获得完整的状态元素列表。</li></ul><p>如下示例是一个有状态的 SinkFunction 使用 CheckpointedFunction 来发送到外部之前缓存数据，使用了Even-split策略。</p><p>下面是一个有状态的 SinkFunction 的示例，它使用 CheckpointedFunction 来缓存数据，然后再将这些数据发送到外部系统，使用了 Even-split 策略：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">BufferingSink</span> <span class="keyword">implements</span> <span class="title">SinkFunction</span>&lt;<span class="title">Tuple2</span>&lt;<span class="title">String</span>, <span class="title">Integer</span>&gt;&gt;, <span class="title">CheckpointedFunction</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">int</span> threshold;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">transient</span> ListState&lt;Tuple2&lt;String, Integer&gt;&gt; checkpointedState;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> List&lt;Tuple2&lt;String, Integer&gt;&gt; bufferedElements;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">BufferingSink</span><span class="params">(<span class="keyword">int</span> threshold)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.threshold = threshold;</span><br><span class="line">        <span class="keyword">this</span>.bufferedElements = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">invoke</span><span class="params">(Tuple2&lt;String, Integer&gt; value, Context contex)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        bufferedElements.add(value);</span><br><span class="line">        <span class="keyword">if</span> (bufferedElements.size() == threshold) &#123;</span><br><span class="line">            <span class="keyword">for</span> (Tuple2&lt;String, Integer&gt; element: bufferedElements) &#123;</span><br><span class="line">                <span class="comment">//将数据发到外部系统</span></span><br><span class="line">            &#125;</span><br><span class="line">            bufferedElements.clear();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">snapshotState</span><span class="params">(FunctionSnapshotContext context)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        checkpointedState.clear();</span><br><span class="line">        <span class="keyword">for</span> (Tuple2&lt;String, Integer&gt; element : bufferedElements) &#123;</span><br><span class="line">            checkpointedState.add(element);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">initializeState</span><span class="params">(FunctionInitializationContext context)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        ListStateDescriptor&lt;Tuple2&lt;String, Integer&gt;&gt; descriptor =</span><br><span class="line">            <span class="keyword">new</span> ListStateDescriptor&lt;&gt;(</span><br><span class="line">                <span class="string">&quot;buffered-elements&quot;</span>,</span><br><span class="line">                TypeInformation.of(<span class="keyword">new</span> TypeHint&lt;Tuple2&lt;String, Integer&gt;&gt;() &#123;&#125;));</span><br><span class="line"></span><br><span class="line">        checkpointedState = context.getOperatorStateStore().getListState(descriptor);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (context.isRestored()) &#123;</span><br><span class="line">            <span class="keyword">for</span> (Tuple2&lt;String, Integer&gt; element : checkpointedState.get()) &#123;</span><br><span class="line">                bufferedElements.add(element);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>initializeState 方法将 FunctionInitializationContext 作为参数，它用来初始化 non-keyed 状态。注意状态是如何初始化的，类似于 Keyed state，StateDescriptor 包含状态名称和有关状态值的类型的信息：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">ListStateDescriptor&lt;Tuple2&lt;String, Integer&gt;&gt; descriptor =</span><br><span class="line">    <span class="keyword">new</span> ListStateDescriptor&lt;&gt;(</span><br><span class="line">        <span class="string">&quot;buffered-elements&quot;</span>,</span><br><span class="line">        TypeInformation.of(<span class="keyword">new</span> TypeHint&lt;Tuple2&lt;Long, Long&gt;&gt;() &#123;&#125;));</span><br><span class="line"></span><br><span class="line">checkpointedState = context.getOperatorStateStore().getListState(descriptor);</span><br></pre></td></tr></table></figure><h4 id="ListCheckpointed"><a href="#ListCheckpointed" class="headerlink" title="ListCheckpointed"></a>ListCheckpointed</h4><p>是一种受限的 CheckpointedFunction，只支持 List 风格的状态和 even-spit 的重分配策略。该接口里面的方法有：</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-23-144503.png" alt="img"></p><ul><li>snapshotState(): 获取函数的当前状态。状态必须返回此函数先前所有的调用结果。</li><li>restoreState(): 将函数或算子的状态恢复到先前 checkpoint 的状态。此方法在故障恢复后执行函数时调用。如果函数的特定并行实例无法恢复到任何状态，则状态列表可能为空。</li></ul><h3 id="Stateful-Source-Functions"><a href="#Stateful-Source-Functions" class="headerlink" title="Stateful Source Functions"></a>Stateful Source Functions</h3><p>与其他算子相比，有状态的 source 函数需要注意的地方更多，比如为了保证状态的更新和结果的输出原子性，用户必须在 source 的 context 上加锁。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">CounterSource</span> <span class="keyword">extends</span> <span class="title">RichParallelSourceFunction</span>&lt;<span class="title">Long</span>&gt; <span class="keyword">implements</span> <span class="title">ListCheckpointed</span>&lt;<span class="title">Long</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//一次语义的当前偏移量</span></span><br><span class="line">    <span class="keyword">private</span> Long offset = <span class="number">0L</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//作业取消标志</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">volatile</span> <span class="keyword">boolean</span> isRunning = <span class="keyword">true</span>;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">(SourceContext&lt;Long&gt; ctx)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">final</span> Object lock = ctx.getCheckpointLock();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> (isRunning) &#123;</span><br><span class="line">            <span class="comment">//输出和状态更新是原子性的</span></span><br><span class="line">            <span class="keyword">synchronized</span> (lock) &#123;</span><br><span class="line">                ctx.collect(offset);</span><br><span class="line">                offset += <span class="number">1</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">cancel</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        isRunning = <span class="keyword">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> List&lt;Long&gt; <span class="title">snapshotState</span><span class="params">(<span class="keyword">long</span> checkpointId, <span class="keyword">long</span> checkpointTimestamp)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> Collections.singletonList(offset);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">restoreState</span><span class="params">(List&lt;Long&gt; state)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">for</span> (Long s : state)</span><br><span class="line">            offset = s;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>或许有些算子想知道什么时候 checkpoint 全部做完了，可以参考使用 org.apache.flink.runtime.state.CheckpointListener 接口来实现，在该接口里面有 notifyCheckpointComplete 方法。</p><hr><h3 id="Broadcast-State"><a href="#Broadcast-State" class="headerlink" title="Broadcast State"></a>Broadcast State</h3><h4 id="Broadcast-State-如何使用"><a href="#Broadcast-State-如何使用" class="headerlink" title="Broadcast State 如何使用"></a>Broadcast State 如何使用</h4><p>前面提到了两种 Operator state 支持的动态扩展方法：even-split redistribution 和 union redistribution。Broadcast State 是 Flink 支持的另一种扩展方式，它用来支持将某一个流的数据广播到下游所有的 Task 中，数据都会存储在下游 Task 内存中，接收到广播的数据流后就可以在操作中利用这些数据，一般我们会将一些规则数据进行这样广播下去，然后其他的 Task 也都能根据这些规则数据做配置，更常见的就是规则动态的更新，然后下游还能够动态的感知。</p><p>Broadcast state 的特点是：</p><ul><li>使用 Map 类型的数据结构</li><li>仅适用于同时具有广播流和非广播流作为数据输入的特定算子</li><li>可以具有多个不同名称的 Broadcast state</li></ul><p>那么我们该如何使用 Broadcast State 呢？下面通过一个例子来讲解一下，在这个例子中，我要广播的数据是监控告警的通知策略规则，然后下游拿到我这个告警通知策略去判断哪种类型的告警发到哪里去，该使用哪种方式来发，静默时间多长等。</p><p>第一个数据流是要处理的数据源，流中的对象具有告警或者恢复的事件，其中用一个 type 字段来标识哪个事件是告警，哪个事件是恢复，然后还有其他的字段标明是哪个集群的或者哪个项目的，简单代码如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">DataStreamSource&lt;AlertEvent&gt; alertData = env.addSource(<span class="keyword">new</span> FlinkKafkaConsumer011&lt;&gt;(<span class="string">&quot;alert&quot;</span>,</span><br><span class="line">        <span class="keyword">new</span> AlertEventSchema(),</span><br><span class="line">        parameterTool.getProperties()));</span><br></pre></td></tr></table></figure><p>然后第二个数据流是要广播的数据流，它是告警通知策略数据（定时从 MySQL 中读取的规则表），简单代码如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">DataStreamSource&lt;Rule&gt; alarmdata = env.addSource(<span class="keyword">new</span> GetAlarmNotifyData());</span><br><span class="line"></span><br><span class="line"><span class="comment">// MapState 中保存 (RuleName, Rule) ，在描述类中指定 State name</span></span><br><span class="line">MapStateDescriptor&lt;String, Rule&gt; ruleStateDescriptor = <span class="keyword">new</span> MapStateDescriptor&lt;&gt;(</span><br><span class="line">            <span class="string">&quot;RulesBroadcastState&quot;</span>,</span><br><span class="line">            BasicTypeInfo.STRING_TYPE_INFO,</span><br><span class="line">            TypeInformation.of(<span class="keyword">new</span> TypeHint&lt;Rule&gt;() &#123;&#125;));</span><br><span class="line"></span><br><span class="line"><span class="comment">// alarmdata 使用 MapStateDescriptor 作为参数广播，得到广播流</span></span><br><span class="line">BroadcastStream&lt;Rule&gt; ruleBroadcastStream = alarmdata.broadcast(ruleStateDescriptor);</span><br></pre></td></tr></table></figure><p>然后你要做的是将两个数据流进行连接，连接后再根据告警规则数据流的规则数据进行处理（这个告警的逻辑很复杂，我们这里就不再深入讲），伪代码大概如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">alertData.connect(ruleBroadcastStream)</span><br><span class="line">    .process(</span><br><span class="line">        <span class="keyword">new</span> KeyedBroadcastProcessFunction&lt;AlertEvent, Rule&gt;() &#123;</span><br><span class="line">            <span class="comment">//根据告警规则的数据进行处理告警事件</span></span><br><span class="line">        &#125;</span><br><span class="line">    )</span><br><span class="line">    <span class="comment">//可能还有更多的操作</span></span><br></pre></td></tr></table></figure><p><code>alertData.connect(ruleBroadcastStream)</code> 该 connect 方法将两个流连接起来后返回一个 BroadcastConnectedStream 对象，如果对 BroadcastConnectedStream 不太清楚的可以回看下文章 <a href="https://gitbook.cn/gitchat/column/undefined/topic/5db6a754f6a6211cb9616526">4如何使用 DataStream API 来处理数据？</a> 再次复习一下。BroadcastConnectedStream 调用 process() 方法执行处理逻辑，需要指定一个逻辑实现类作为参数，具体是哪种实现类取决于非广播流的类型：</p><ul><li>如果非广播流是 keyed stream，需要实现 KeyedBroadcastProcessFunction</li><li>如果非广播流是 non-keyed stream，需要实现 BroadcastProcessFunction</li></ul><p>那么该怎么获取这个 Broadcast state 呢，它需要通过上下文来获取:</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ctx.getBroadcastState(ruleStateDescriptor)</span><br></pre></td></tr></table></figure><h4 id="BroadcastProcessFunction-和-KeyedBroadcastProcessFunction"><a href="#BroadcastProcessFunction-和-KeyedBroadcastProcessFunction" class="headerlink" title="BroadcastProcessFunction 和 KeyedBroadcastProcessFunction"></a>BroadcastProcessFunction 和 KeyedBroadcastProcessFunction</h4><p>这两个抽象函数有两个相同的需要实现的接口:</p><ul><li>processBroadcastElement()：处理广播流中接收的数据元</li><li>processElement()：处理非广播流数据的方法</li></ul><p>用于处理非广播流是 non-keyed stream 的情况:</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">BroadcastProcessFunction</span>&lt;<span class="title">IN1</span>, <span class="title">IN2</span>, <span class="title">OUT</span>&gt; <span class="keyword">extends</span> <span class="title">BaseBroadcastProcessFunction</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="keyword">void</span> <span class="title">processElement</span><span class="params">(IN1 value, ReadOnlyContext ctx, Collector&lt;OUT&gt; out)</span> <span class="keyword">throws</span> Exception</span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="keyword">void</span> <span class="title">processBroadcastElement</span><span class="params">(IN2 value, Context ctx, Collector&lt;OUT&gt; out)</span> <span class="keyword">throws</span> Exception</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>用于处理非广播流是 keyed stream 的情况</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">KeyedBroadcastProcessFunction</span>&lt;<span class="title">KS</span>, <span class="title">IN1</span>, <span class="title">IN2</span>, <span class="title">OUT</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="keyword">void</span> <span class="title">processElement</span><span class="params">(IN1 value, ReadOnlyContext ctx, Collector&lt;OUT&gt; out)</span> <span class="keyword">throws</span> Exception</span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="keyword">void</span> <span class="title">processBroadcastElement</span><span class="params">(IN2 value, Context ctx, Collector&lt;OUT&gt; out)</span> <span class="keyword">throws</span> Exception</span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onTimer</span><span class="params">(<span class="keyword">long</span> timestamp, OnTimerContext ctx, Collector&lt;OUT&gt; out)</span> <span class="keyword">throws</span> Exception</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>可以看到这两个接口提供的上下文对象有所不同。非广播方（processElement）使用 ReadOnlyContext，而广播方（processBroadcastElement）使用 Context。这两个上下文对象（简称 ctx）通用的方法接口有：</p><ul><li>访问 Broadcast state：ctx.getBroadcastState(MapStateDescriptorstateDescriptor)</li><li>查询数据元的时间戳：ctx.timestamp()</li><li>获取当前水印：ctx.currentWatermark()</li><li>获取当前处理时间：ctx.currentProcessingTime()</li><li>向旁侧输出（side-outputs）发送数据：ctx.output(OutputTag outputTag, X value)</li></ul><p>这两者不同之处在于对 Broadcast state 的访问限制：广播方对其具有读和写的权限（read-write），非广播方只有读的权限（read-only），为什么要这么设计呢，主要是为了保证 Broadcast state 在算子的所有并行实例中是相同的。由于 Flink 中没有跨任务的通信机制，在一个任务实例中的修改不能在并行任务间传递，而广播端在所有并行任务中都能看到相同的数据元，只对广播端提供可写的权限。同时要求在广播端的每个并行任务中，对接收数据的处理是相同的。如果忽略此规则会破坏 State 的一致性保证，从而导致不一致且难以诊断的结果。也就是说，processBroadcast() 的实现逻辑必须在所有并行实例中具有相同的确定性行为。</p><h4 id="使用-Broadcast-state-需要注意"><a href="#使用-Broadcast-state-需要注意" class="headerlink" title="使用 Broadcast state 需要注意"></a>使用 Broadcast state 需要注意</h4><p>前面介绍了 Broadcast state，并将 BroadcastProcessFunction 和 KeyedBroadcastProcessFunction 做了个对比，那么接下来强调一下使用 Broadcast state 时需要注意的事项：</p><ul><li>没有跨任务的通信，这就是为什么只有广播方可以修改 Broadcast state 的原因。</li><li>用户必须确保所有任务以相同的方式为每个传入的数据元更新 Broadcast state，否则可能导致结果不一致。</li><li>跨任务的 Broadcast state 中的事件顺序可能不同，虽然广播的元素可以保证所有元素都将转到所有下游任务，但元素到达的顺序可能不一致。因此，Broadcast state 更新不能依赖于传入事件的顺序。</li><li>所有任务都会把 Broadcast state 存入 checkpoint，虽然 checkpoint 发生时所有任务都具有相同的 Broadcast state。这是为了避免在恢复期间所有任务从同一文件中进行恢复（避免热点），然而代价是 state 在 checkpoint 时的大小成倍数（并行度数量）增加。</li><li>Flink 确保在恢复或改变并行度时不会有重复数据，也不会丢失数据。在具有相同或改小并行度后恢复的情况下，每个任务读取其状态 checkpoint。在并行度增大时，原先的每个任务都会读取自己的状态，新增的任务以循环方式读取前面任务的检查点。</li><li>不支持 RocksDB state backend，Broadcast state 在运行时保存在内存中。</li></ul><h3 id="Queryable-State"><a href="#Queryable-State" class="headerlink" title="Queryable State"></a>Queryable State</h3><p>Queryable State，顾名思义，就是可查询的状态。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-06-29-075631.jpg" alt="img"></p><p>传统管理这些状态的方式是通过将计算后的状态结果存储在第三方 KV 存储中，然后由第三方应用去获取这些 KV 状态，但是在 Flink 种，现在有了 Queryable State，意味着允许用户对流的内部状态进行实时查询。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-06-29-091521.jpg" alt="img"></p><p>那么就不再像其他流计算框架，需要将结果存储到其他外部存储系统才能够被查询到，这样我们就可以不再需要等待状态写入外部存储（这块可能是其他系统的主要瓶颈之一），甚至可以做到无需任何数据库就可以让用户直接查询到数据，这使得数据获取到的时间会更短，更及时，如果你有这块的需求（需要将某些状态数据进行展示，比如数字大屏），那么就强烈推荐使用 Queryable State。目前可查询的 state 主要针对可分区的 state，如 keyed state 等。</p><p>在 Flink 源码中，为此还专门有一个 module 来讲 Queryable State 呢！</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-23-144649.png" alt="img"></p><p>那么我们该如何使用 Queryable State 呢？有如下两种方式 ：</p><ul><li>QueryableStateStream, 将 KeyedStream 转换为 QueryableStateStream，类似于 Sink，后续不能进行任何转换操作</li><li>StateDescriptor#setQueryable(String queryableStateName)，将 Keyed State 设置为可查询的 （不支持 Operator State）</li></ul><p>外部应用在查询 Flink 应用程序内部状态的时候要使用 QueryableStateClient, 提交异步查询请求来获取状态。如何使状态可查询呢，假如已经创建了一个状态可查询的 Job，并通过 JobClient 提交 Job，那么它在 Flink 内部的具体实现如下图（图片来自 <a href="http://vishnuviswanath.com/flink_queryable_state1.html">Queryable States in ApacheFlink - How it works</a>）所示：</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-06-29-073842.jpg" alt="img"></p><p>上面讲解了让 State 可查询的原理，如果要在 Flink 集群中使用的话，首先得将 Flink 安装目录下 opt 里面的 <code>flink-queryable-state-runtime_2.11-1.9.0.jar</code> 复制到 lib 目录下，默认 lib 目录是不包含这个 jar 的。</p><p>然后你可以像下面这样操作让状态可查询：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Reducing state</span></span><br><span class="line">ReducingStateDescriptor&lt;Tuple2&lt;Integer, Long&gt;&gt; reducingState = <span class="keyword">new</span> ReducingStateDescriptor&lt;&gt;(</span><br><span class="line">        <span class="string">&quot;zhisheng&quot;</span>,</span><br><span class="line">        <span class="keyword">new</span> SumReduce(),</span><br><span class="line">        source.getType());</span><br><span class="line"></span><br><span class="line"><span class="keyword">final</span> String queryName = <span class="string">&quot;zhisheng&quot;</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">final</span> QueryableStateStream&lt;Integer, Tuple2&lt;Integer, Long&gt;&gt; queryableState =</span><br><span class="line">        dataStream.keyBy(<span class="keyword">new</span> KeySelector&lt;Tuple2&lt;Integer, Long&gt;, Integer&gt;() &#123;</span><br><span class="line">            <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = -<span class="number">4126824763829132959L</span>;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> Integer <span class="title">getKey</span><span class="params">(Tuple2&lt;Integer, Long&gt; value)</span> </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> value.f0;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;).asQueryableState(queryName, reducingState);</span><br></pre></td></tr></table></figure><p>除了上面的 Reducing，你还可以使用 ValueState、FoldingState，还可以直接通过asQueryableState(queryName），注意不支持 ListState，调用 asQueryableState 方法后会返回 QueryableStateStream，接着无需再做其他操作。</p><p>那么用户如果定义了 Queryable State 的话，该怎么来查询对应的状态呢？下面来看看具体逻辑：</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-06-29-074814.jpg" alt="img"></p><p>简单来说，当用户在 Job 中定义了 queryable state 之后，就可以在外部通过QueryableStateClient 来查询对应的状态实时值，你可以创建如下方法：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//创建 Queryable State Client</span></span><br><span class="line">QueryableStateClient client = <span class="keyword">new</span> QueryableStateClient(host, port);</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">QueryableStateClient</span><span class="params">(<span class="keyword">final</span> InetAddress remoteAddress, <span class="keyword">final</span> <span class="keyword">int</span> remotePort)</span> </span>&#123;</span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">this</span>.client = <span class="keyword">new</span> Client&lt;&gt;(</span><br><span class="line">            <span class="string">&quot;Queryable State Client&quot;</span>, <span class="number">1</span>,</span><br><span class="line">            messageSerializer, <span class="keyword">new</span> DisabledKvStateRequestStats());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在 QueryableStateClient 中有几个不同参数的 getKvState 方法，参数可有 JobID、queryableStateName、key、namespace、keyTypeInfo、namespaceTypeInfo、StateDescriptor，其实内部最后调用的是一个私有的 getKvState 方法：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> CompletableFuture&lt;KvStateResponse&gt; <span class="title">getKvState</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">final</span> JobID jobId, <span class="keyword">final</span> String queryableStateName,</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">final</span> <span class="keyword">int</span> keyHashCode, <span class="keyword">final</span> <span class="keyword">byte</span>[] serializedKeyAndNamespace)</span> </span>&#123;</span><br><span class="line">    ...</span><br><span class="line">    <span class="comment">//构造 KV state 查询的请求</span></span><br><span class="line">    KvStateRequest request = <span class="keyword">new</span> KvStateRequest(jobId, queryableStateName, keyHashCode, serializedKeyAndNamespace);</span><br><span class="line">    <span class="comment">//这个 client 是在构造 QueryableStateClient 中赋值的，这个 client 是 Client&lt;KvStateRequest, KvStateResponse&gt;，发送请求后会返回 CompletableFuture&lt;KvStateResponse&gt;</span></span><br><span class="line">    <span class="keyword">return</span> client.sendRequest(remoteAddress, request);</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在 Flink 源码中专门有一个 QueryableStateOptions 类来设置可查询状态相关的配置，有如下这些配置。</p><p>服务器端：</p><ul><li>queryable-state.proxy.ports：可查询状态代理的服务器端口范围的配置参数，默认是 9069</li><li>queryable-state.proxy.network-threads：客户端代理的网络线程数，默认是 0</li><li>queryable-state.proxy.query-threads：客户端代理的异步查询线程数，默认是 0</li><li>queryable-state.server.ports：可查询状态服务器的端口范围，默认是 9067</li><li>queryable-state.server.network-threads：KvState 服务器的网络线程数</li><li>queryable-state.server.query-threads：KvStateServerHandler 的异步查询线程数</li><li>queryable-state.enable：是否启用可查询状态代理和服务器</li></ul><p>客户端：</p><ul><li>queryable-state.client.network-threads：KvState 客户端的网络线程数</li></ul><p><strong>注意</strong>：</p><p>可查询状态的生命周期受限于 Job 的生命周期，例如，任务在启动时注册可查询状态，在清理的时候会注销它。在未来的版本中，可能会将其解耦，以便在任务完成后仍可以允许查询到任务的状态。</p>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
            <tag> Flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark 任务调度机制</title>
      <link href="2019/12/02/Spark%20%E4%BB%BB%E5%8A%A1%E8%B0%83%E5%BA%A6%E6%9C%BA%E5%88%B6/"/>
      <url>2019/12/02/Spark%20%E4%BB%BB%E5%8A%A1%E8%B0%83%E5%BA%A6%E6%9C%BA%E5%88%B6/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p>在生产环境下， Spark 集群的部署方式一般为 YARN-Cluster 模式，因此本文基于YARN-Cluster 模式</p><a id="more"></a><h1 id="Spark-任务提交流程"><a href="#Spark-任务提交流程" class="headerlink" title="Spark 任务提交流程"></a><strong>Spark 任务提交流程</strong></h1><p>YARN-Cluster 模式中提交 Spark 应用程序</p><p>首先通过 Client 向 ResourceManager 请求启动一个 Application，同时检查是否有足够的资源满足 Application 的需求，如果资源条件满足，则准备 ApplicationMaster 的启动上下文，交给ResourceManager，并循环监控 Application 状态。</p><p>当提交的资源队列中有资源时，ResourceManager 会在某个 NodeManager 上启动 ApplicationMaster 进程，ApplicationMaster 会单独启动 Driver 后台线程，当 Driver 启动后，ApplicationMaster 会通过本地的 RPC 连接 Driver ，并开始向 ResourceManager 申请 Container 资源运行 Executor 进程, 当 ResourceManager 返回 Container 资源，ApplicationMaster 则在对应的 Container 上启动 Executor 。</p><p>Driver 线程主要是初始化 SparkContext 对象，准备运行所需的上下文，然后一方面保持与 ApplicationMaster 的 RPC 连接，通过 ApplicationMaster 申请资源，另一方面根据用户业务逻辑开始调度任务，将任务下发到已有的空闲 Executor 上。</p><p>当 ResourceManager 向 ApplicationMaster 返 回 Container 资源时 ， ApplicationMaster 就尝试在对应的 Container 上启动 Executor 进程，Executor 进程起来后， 会向 Driver 反向注册， 注册成功后保持与 Driver 的心跳，同时等待 Driver 分发任务，当分发的任务执行完毕后，将任务状态上报给 Driver。</p><h1 id="Spark-任务调度概述"><a href="#Spark-任务调度概述" class="headerlink" title="Spark 任务调度概述"></a>Spark 任务调度概述</h1><p>Driver 线程初始化 SparkContext 对象，准备运行所需的上下文，一方面保持与 ApplicationMaster 的 RPC 连接，通过 ApplicationMaster 申请资源，另一方面根据用户业务逻辑开始调度任务，将任务下发到已有的空闲 Executor 上。</p><p><strong>Driver 会根据用户程序逻辑准备任务，并根据 Executor 资源情况逐步分发任务。</strong> 在详细阐述任务调度前，首先说明下 Spark 里的几个概念。一个 Spark 应用程序包括Job、Stage 以及 Task 三个概念：</p><ul><li>Job 是以 Action 方法为界，遇到一个 Action 方法则触发一个Job</li><li>Stage 是 Job 的子集，以宽依赖为界。遇到 Shuffle 做一次划分</li><li>Task 是 Stage 的子集，以并行度(分区数)来衡量,分区数是多少，则有多少个 task</li></ul><p>Spark 的任务调度总体来说分两路进行，一路是 Stage 级的调度， 一路是 Task 级的调度，总体调度流程如下图所示：</p><img src="../images/spark/6.png" alt="" style="zoom:50%;" /><p>Spark RDD 通过其 Transactions 操作，形成了 RDD 血缘关系图，即 DAG ，最后通过 Action 的调用，触发 Job 并调度执行。</p><p><strong>DAGScheduler 负责 Stage 级的调度，主要是将 Job 切分成若干 Stages，并将每个 Stage 打包成 TaskSet 交给 TaskScheduler 调度。</strong></p><p><strong>TaskScheduler 负责 Task 级的调度，将 DAGScheduler 给过来的 TaskSet 按照指定的调度策略分发到 Executor 上执行，调度过程中 SchedulerBackend 负责提供可用资源， 其中 SchedulerBackend 有多种实现， 分别对接不同的资源管理系统。</strong></p><h1 id="Spark-Stage-级调度"><a href="#Spark-Stage-级调度" class="headerlink" title="Spark Stage 级调度"></a>Spark Stage 级调度</h1><p>DAGScheduler 是实现了面向 stage 的调度，它可以为每个 Job 计算出一个 DAG，追踪 RDD 和 stage 的输出是否被持久化，并且寻找到一个最优调度机制来运行 Job.</p><ol><li><p>接收用户提交的 Job；</p></li><li><p>将 Job 划分为不同 stage 的 DAG图，记录哪些 RDD、Stage 被物化存储，并在每一个 stage 内产生一系列的 task，并封装成 TaskSet；</p></li><li><p>要保证相互依赖的 Job/stage 能够得到顺利的调度执行，DAGScheduler 必然需要监控当前Job / Stage乃至Task的完成情况。</p></li><li><p>结合当前的缓存情况，决定每个 Task 的最佳位置(移动计算而不是移动数据，任务在数据所在的节点上运行)，将 TaskSet 提交给 TaskScheduler;</p><p>DAGScheduler 找到哪些 RDDs 已经被 cache 了来避免重计算它们，而且同样地记住哪些ShuffleMapStages 已经生成了输出文件来避免重建一个 shuffle 的 map 侧计算任务。</p></li><li><p>重新提交 Shuffle 输出丢失的 Stage 给 TaskScheduler</p><p>处理由于 shuffle 输出文件丢失导致的失败，在这种情况下，旧的 stage 可能会被重新提交。一个 stage 内部的失败，如果不是由于 shuffle 文件丢失导致的，会被 TaskScheduler 处理，它会被多次重试每一个 task，直到最后一个。实在不行，才会被取消整个 stage。</p></li></ol><h3 id="Stage-划分"><a href="#Stage-划分" class="headerlink" title="Stage 划分"></a>Stage 划分</h3><p>SparkContext 将 Job 提交给 DAGScheduler，DAGScheduler 将一个 Job 划分为若干 Stages ，<strong>具体划分策略是，以 Shuffle 为界，划分 Stage ,由最终的 RDD 不断通过依赖回溯判断父依赖是否是宽依赖，窄依赖的 RDD 被划分到同一个 Stage 中，进行 pipeline 式的计算，划分的 Stages 分两类，一类叫做 ResultStage 为 DAG 下游的 Stage，由 Action 方法决定； 另一类叫做 ShuffleMapStage，其为下游 Stage 准备数据。</strong></p><h3 id="生成-Job，提交-Stage"><a href="#生成-Job，提交-Stage" class="headerlink" title="生成 Job，提交 Stage"></a><strong>生成 Job，提交 Stage</strong></h3><p><strong>一个 Stage 是否被提交，需要判断它的父 Stage 是否执行，只有在父 Stage 执行完毕才能提交当前 Stage，如果一个 Stage 没有父 Stage，那么从该 Stage 开始提交。Stage 提交时会将 Task 信息[分区信息以及方法等]序列化并被打包成 TaskSet 交给 TaskScheduler，一个 Partition 对应一个 Task。</strong></p><h1 id="Spark-Task-级调度"><a href="#Spark-Task-级调度" class="headerlink" title="Spark Task 级调度"></a>Spark Task 级调度</h1><p>Spark Task 的调度是由 TaskScheduler 来完成。DAGScheduler 将 Stage 打包到 TaskSet 交给 TaskScheduler，TaskScheduler 会将 TaskSet 封装为 TaskSetManager 加入到调度队列中。</p><img src="../images/spark/7.png" alt="" style="zoom:50%;" /><p><strong>TaskSetManager 负责监控管理同一个 Stage 中的 Tasks，TaskScheduler 就是以 TaskSetManager 为单元来调度任务 。</strong></p><p>TaskScheduler 初始化后会启动 SchedulerBackend，它负责跟外界打交道，接收 Executor 的注册信息，并维护 Executor 的状态，同时它在启动后会定期地去询问 TaskScheduler 是否有任务要运行，也就是说， 它会定期地问 TaskScheduler “我有这么余量，你要不要啊”，TaskScheduler 在  SchedulerBackend 问它的时候，会从调度队列中按照指定的调度策略选择 TaskSetManager 去运行。</p><p>SchedulerBackend负责与Cluster Manager交互，取得分配给 Application 的资源，并将资源传给TaskScheduler，由 TaskScheduler 为 Task 最终分配计算资源</p><h3 id="调度策略"><a href="#调度策略" class="headerlink" title="调度策略"></a>调度策略</h3><p>TaskScheduler 会先把 DAGScheduler 提交过来的 TaskSet 封装成 TaskSetManager 放到任务队列里，然后再从任务队列里按照一定的规则把它们取出来放在 SchedulerBackend 给过来的 Executor 上运行。这个调度过程实际上还是比较粗粒度的，是面向 TaskSetManager 的。</p><p>调度队列的层次结构如下图所示</p><img src="../images/spark/8.png" alt="" style="zoom:50%;" /><p>TaskScheduler 支持两种调度策略，一种是 FIFO，也是默认的调度策略，另一种是 FAIR。</p><p>在 TaskScheduler 初始化过程中会实例化 rootPool，表示树的根节点， 是 Pool 类型。</p><ul><li><p><strong>FIFO 调度策略</strong></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span>[spark] <span class="class"><span class="keyword">class</span> <span class="title">FIFOSchedulingAlgorithm</span> <span class="keyword">extends</span> <span class="title">SchedulingAlgorithm</span> </span>&#123;</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">comparator</span></span>(s1: <span class="type">Schedulable</span>, s2: <span class="type">Schedulable</span>): <span class="type">Boolean</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> priority1 = s1.priority <span class="comment">// jobId</span></span><br><span class="line">    <span class="keyword">val</span> priority2 = s2.priority</span><br><span class="line">    <span class="keyword">var</span> res = math.signum(priority1 - priority2)</span><br><span class="line">    <span class="keyword">if</span> (res == <span class="number">0</span>) &#123;</span><br><span class="line">      <span class="keyword">val</span> stageId1 = s1.stageId</span><br><span class="line">      <span class="keyword">val</span> stageId2 = s2.stageId</span><br><span class="line">      res = math.signum(stageId1 - stageId2)</span><br><span class="line">    &#125;</span><br><span class="line">    res &lt; <span class="number">0</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>比较 s1和 s2 所属的 JobId，值越小，优先级越高</li><li>如果两个 JobId 的优先级相同， 则对 s1，s2所属的 StageId 进行比，值越小，优先级越高</li></ul></li><li><p><strong>Fair 调度策略</strong></p><p>FAIR 模式中有一个 Root Pool 和多个子 Pool，各个子 Pool 中 存储着所有待分配的 TaskSetManager 。</p><img src="../images/spark/9.png" alt="" style="zoom:50%;" /><p>可以通过在 Properties 中指定 spark.scheduler.pool 属性，指定某个调度池作为 TaskSetManager 的父调度池，如果根调度池不存在此属性值对应的调度池，会创建以此属性值为名称的调度池作为 TaskSetManager 的父调度池，并将此调度池作为根调度池的子调度池。</p><p>在 FAIR 模式中，需要先对 子Pool 进行排序，再对 子Pool 里面的 TaskSetManager 进行排序，因为 Pool 和 TaskSetManager 都继承了 Schedulable 特质，因此使用相同的排序算法 。</p><p>每个要排序的对象包含三个属性 : runningTasks 值[正在运行的 Task 数]、 minShare 值、 weight 值，比较时会综合考量三个属性值。</p><p>注意，minShare 、weight 的值均在公平调度配置文件 fairscheduler.xml 中被指定， 调度池在构建阶段会读取此文件的相关配置。</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">  <span class="tag">&lt;<span class="name">allocations</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">pool</span> <span class="attr">name</span>=<span class="string">&quot;production&quot;</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">schedulingMode</span>&gt;</span>FAIR<span class="tag">&lt;/<span class="name">schedulingMode</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">weight</span>&gt;</span>1<span class="tag">&lt;/<span class="name">weight</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">minShare</span>&gt;</span>2<span class="tag">&lt;/<span class="name">minShare</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">pool</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">pool</span> <span class="attr">name</span>=<span class="string">&quot;test&quot;</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">schedulingMode</span>&gt;</span>FIFO<span class="tag">&lt;/<span class="name">schedulingMode</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">weight</span>&gt;</span>2<span class="tag">&lt;/<span class="name">weight</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">minShare</span>&gt;</span>3<span class="tag">&lt;/<span class="name">minShare</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">pool</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">allocations</span>&gt;</span></span><br></pre></td></tr></table></figure><ul><li><p><strong>runningTasks 比 minShare 小的先执行</strong></p><blockquote><p>如果 A 对象的 runningTasks 大于它的 minShare，B 对象的 runningTasks 小于它的 minShare,那么 B 排在 A 前面</p></blockquote></li><li><p><strong>minShare 使用率低的先执行</strong></p><blockquote><p>如果A，B 对象的 runningTasks 都小于它的 minShare ，那么就比较 runningTasks 和 minShare 的比值 [minShare使用率]谁小谁排前面</p></blockquote></li><li><p><strong>权重使用率低的先执行</strong></p><blockquote><p>如果A、B 对象 的 runningTasks 都大于它们的 minShare ，那么就比较 runningTasks 与 weight 的比值(权重使用率),谁小谁排前面。</p></blockquote></li><li><p><strong>如果上述比较均相等，则比较名字</strong></p></li></ul><p>FAIR 模式排序完成后，所有的 TaskSetManager 被放入一个 ArrayBuffer 里，之后依次被取出并发送给 Executor 执行 。</p></li></ul><p>从调度队列中拿到 TaskSetManager 后，由于 TaskSetManager 封装了一个 Stage 的所有 Task， 并负责管理调度这些 Task，接下来 TaskSetManager 按照一定的规则逐个取出 Task 给 TaskScheduler，TaskScheduler 提交给 SchedulerBackend 去发到 Executor 执行。</p><h3 id="本地化调度"><a href="#本地化调度" class="headerlink" title="本地化调度"></a>本地化调度</h3><p>DAGScheduler 划分 Stage, 通过调用 submitStage 来提交一个 Stage 对应的 tasks， submitStage 会调用 submitMissingTasks，submitMissingTasks 确定每个需要计算的 task 的 preferredLocations，通过调用 getPreferrdeLocations() 得到 partition 的优先位置，由于一个 partition 对应一个task， 此 partition 的优先位置就是 task 的优先位置，对于要提交到 TaskScheduler 的 TaskSet 中的每一个 task ，该 task 优先位置与其对应的 partition 对应的优先位置一致 </p><p>根据每个 task 的优先位置，确定 task 的 Locality 级别，Locality一共有五种，优先级由高到低顺序</p><table><thead><tr><th>PROCESS_LOCAL</th><th>进程本地化，task 和数据在同一个 Executor 中，性能最好。</th></tr></thead><tbody><tr><td><strong>NODE_LOCAL</strong></td><td>节点本地化，task 和数据在同一个节点中，但是 task 和数据不在同一个 Executor 中，数据需要在进程间进行传输。</td></tr><tr><td><strong>RACK_LOCAL</strong></td><td>机架本地化，task 和数据在同一个机架的两个节点上，数据需要通过网络在节点之间进行传输。</td></tr><tr><td><strong>NO_PREF</strong></td><td>数据从哪里访问都一样快，不需要位置优先</td></tr><tr><td><strong>ANY</strong></td><td>task 和数据不在一个机架中，性能最差。</td></tr></tbody></table><p>在调度执行时，Spark 总是会尽量让每个 task 以最高的本地性级别来启动，当一个 task 以X本地性级别启动，但是该本地性级别对应的所有节点都没有空闲资源而启动失败，此时并不会马上降低本地性级别启动而是在某个时间长度内再次以 X 本地性级别来启动该 task，若超过限时时间则降级启动，去尝试下一个本地性级别，依次类推。</p><p>可以通过调大每个类别的最大容忍延迟时间，在等待阶段对应的 Executor 可能 就会有相应的资源去执行此 task，这就在在一定程度上提到了运行性能。</p><h3 id="失败重试与黑名单机制"><a href="#失败重试与黑名单机制" class="headerlink" title="失败重试与黑名单机制"></a>失败重试与黑名单机制</h3><p>除了选择合适的 Task 调度机制外，还需要监控 Task 的执行状态，与外部通信的是 SchedulerBackend。</p><p><strong>Task 被提交到 Executor 启动执行后，Executor 会将执行状态上报给 SchedulerBackend， SchedulerBackend 则通知该 Task 对应的 TaskSetManager，TaskSetManager 获取得知 Task 的执行状态，对于失败的 Task，TaskSetManager 会记录失败次数，如果失败次数还没有超过最大重试次数，则把该 Task 放回待调度的 Task 池子中，否则整个 Application 失败。</strong></p><p>在记录 Task 失败次数过程中，会记录其上一次失败所在的 ExecutorId 和 Host，下次调度该 Task 时，会使用黑名单机制，避免再次被调度到上一次失败的节点上，起到一定的容错作用。</p><p><strong>黑名单记录 Task 上一次失败所在的 ExecutorId 和 Host，以及其对应的 “拉黑时间”.</strong></p><p><strong>“拉黑时间”是指这段时间内不要再往这个节点上调度这个 Task 了。</strong></p>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark Shuffle 机制</title>
      <link href="2019/11/28/Spark%20Shuffle/"/>
      <url>2019/11/28/Spark%20Shuffle/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p>shuffle 的性能高低直接影响了整个程序的性能和吞吐量。因为在分布式情况下，reduce task 需要跨节点去拉取其它节点上的 map task 结果。这一过程将会产生网络资源消耗和内存，磁盘 IO 的消耗。</p><a id="more"></a><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a><strong>概述</strong></h2><p>Shuffle 描述着数据从 map task 输出到 reduce task 输入的这段过程。</p><p>Shuffle 是连接 Map 和 Reduce 之间的桥梁， Map 的输出要用到 Reduce 中必须经过 shuffle 这个环节.</p><p>shuffle 的性能高低直接影响了整个程序的性能和吞吐量。因为在分布式情况下，reduce task 需要跨节点去拉取其它节点上的 map task 结果。这一过程将会产生网络资源消耗和内存，磁盘 IO 的消耗。</p><p>通常 shuffle 分为两部分：Map阶段的数据准备和  Reduce 阶段的数据拷贝处理。</p><p>一般将在 map 端的 shuffle 称之为 Shuffle Write， 在Reduce 端的 Shuffle 称之为 Shuffle Read</p><h2 id="导致-Shuffle-操作算子"><a href="#导致-Shuffle-操作算子" class="headerlink" title="导致 Shuffle 操作算子"></a><strong>导致 Shuffle 操作算子</strong></h2><h3 id="重分区类的操作"><a href="#重分区类的操作" class="headerlink" title="重分区类的操作"></a><strong>重分区类的操作</strong></h3><p>重分区类算子一般会 shuffle，因为需要在整个集群中，对之前所有的分区的数据进行随机，均匀的打乱，然后把数据放入下游新的指定数量的分区内。</p><p>比如 repartition、repartitionAndSortWithinPartitions等</p><h3 id="byKey-类的操作"><a href="#byKey-类的操作" class="headerlink" title="byKey 类的操作"></a><strong>byKey 类的操作</strong></h3><p>比如 reduceByKey、groupByKey、sortByKey 等，对一个 key 进行聚合操作时要保证集群中，所有节点上相同的 key 分配到同一个节点上进行处理</p><h3 id="Join-类的操作"><a href="#Join-类的操作" class="headerlink" title="Join 类的操作"></a><strong>Join 类的操作</strong></h3><p>比如 join、cogroup 等。两个 rdd 进行 join，就必须将相同 key 的数据，shuffle 到同一个节点上，然后进行相同 key 的两个 rdd 数据操作。</p><h2 id="Shuffle-原理"><a href="#Shuffle-原理" class="headerlink" title="Shuffle 原理"></a><strong>Shuffle 原理</strong></h2><p><strong>ShuffleMapStage 的结束伴随着 shuffle 文件的写磁盘</strong></p><p><strong>ResultStage基本上对应代码中的 action 算子， 即将一个函数应用在 RDD 的各个 partition 的数据集上，意味着一个 job 的运行结束</strong></p><p>在划分 stage 时， 最后一个 stage 称为 finalStage， 它本质上是一个 ResultStage</p><h3 id="HashShuffle"><a href="#HashShuffle" class="headerlink" title="HashShuffle"></a><strong>HashShuffle</strong></h3><p>通常 shuffle 分为两部分：write 阶段的数据准备和 read 阶段的数据拷贝处理。</p><h4 id="shuffle-write"><a href="#shuffle-write" class="headerlink" title="shuffle write"></a>shuffle write</h4><blockquote><p>shuffle write 阶段，ShuffleMapStage 结束后，每一个 task 中的数据按照 key 进行分类，根据 <strong>hash 算法</strong>，<strong>将相同的 key 写入到一个磁盘文件中，而每一个磁盘文件都只属于下游 stage 的一个 task。在将数据写入磁盘之前，会先将数据写入到内存缓冲，当内存缓冲填满之后，溢写到磁盘文件中。</strong></p></blockquote><h4 id="shuffle-read"><a href="#shuffle-read" class="headerlink" title="shuffle read"></a>shuffle read</h4><blockquote><p>shuffle read，通常就是一个 stage 刚开始时要做的事情。此时该 stage 的每一个 task 需要将上一个 stage 的计算结果中的所有相同 key，从各个节点上通过网络都拉取到自己所在的节点上，然后进行 key 的聚合或连接等操作。由于 shuffle write 的过程中，task 给下游 stage 的每个 task 都创建了一个磁盘文件，因此 shuffle read 的过程中，每个 task 只要从上游 stage 的所有 task 所在节点上，拉取属于自己的那一个磁盘文件即可.</p></blockquote><blockquote><p>shuffle read 的拉取过程是一边拉取一边进行聚合的。每个 shuffle read task 都会有一个自己的buffer 缓冲，每次都只能拉取与 buffer 缓冲相同大小的数据，然后通过内存中的一个 Map 进行聚合等操作。聚合完一批数据后，再拉取下一批数据，并放到 buffer 缓冲中进行聚合操作。以此类推，直到最后将所有数据到拉取完，并得到最终的结果。</p></blockquote><p>这种策略的不足在于，<strong>下游有几个 task，上游的每一个 task 都就都需要创建几个临时文件</strong>，每个文件中只存储 key 取 hash 之后相同的数据，导致了当下游的 task 任务过多的时候，上游会堆积大量的小文件.</p><p><img src="../images/spark/2.png" alt="屏幕快照 2020-02-21 下午8.24.28"></p><ol><li>Shuffle 前在磁盘上会产生海量的小文件，此时会产生大量耗时低效的 IO 操作</li><li>内存不够用，由于内存中需要保存海量文件操作信息和临时信息，如果数据处理的规模比较庞大的话，内存不可承受，会出现 OOM 等问题。</li></ol><h3 id="优化之后的-HashShuffle"><a href="#优化之后的-HashShuffle" class="headerlink" title="优化之后的 HashShuffle"></a>优化之后的 HashShuffle</h3><p>这里说的优化，是指我们可以设置一个参数，spark.shuffle.consolidateFiles。该参数默认值为false，将其设置为 true 即可开启优化机制。通常来说，如果我们使用HashShuffleManager，那么都建议开启这个选项。</p><p>开启这个机制之后，在 shuffle write 过程中，task 就不是为下游 stage 的每个 task 创建一个磁盘文件。出现 shuffleFileGroup 的概念。一个 Executor 上有多少个 CPU core ，就可以并行执行多少个 task。第一批并行执行的每个 task 都会创建一个 shuffleFileGroup，并将数据写入对应的磁盘文件内。<strong>当 Executor 的 CPU core 接着执行下一批 task 时，下一批 task 就会复用之前已有的 shuffleFileGroup ，包括其中的磁盘文件。</strong>而不会写入新的磁盘文件中。</p><p>consolidate 机制允许不同的 task 复用同一批磁盘文件，这样就可以有效将多个 task 的磁盘文件进行一定程度上的合并，从而大幅度减少磁盘文件的数量，进而提升 shuffle write 的性能。</p><img src="../images/spark/3.png" alt="" style="zoom:50%;" /><h2 id="SortShuffle"><a href="#SortShuffle" class="headerlink" title="SortShuffle"></a><strong>SortShuffle</strong></h2><p>SortShuffleManager 的运行机制主要分成两种，一种是普通运行机制，另一种是 bypass 运 行 机 制 。 当 shuffle read task 的 数量小于等于 spark.shuffle.sort.bypassMergeThreshold 参数的值时[默认为 200 ]， 就会启用 bypass 机制。</p><h3 id="普通-SortShuffle"><a href="#普通-SortShuffle" class="headerlink" title="普通 SortShuffle"></a><strong>普通 SortShuffle</strong></h3><p>Task 将数据会先写入一个内存数据结构。</p><p><font color='blue'>[根据不同的 shuffle 算子，可能选用不同的数据结构。如果是 reduceByKey 这种聚合类的 shuffle 算子，那么会选用 Map 数据结构，一边通过 Map 进行聚合，一边写入内存；如果是 join 这种普通的 shuffle 算子，那么会选用 Array 数据结构，直接写入内存。]</font></p><p>每写一条数据进入内存数据结构之后，就会判断是否达到了某个临界值,<strong>如果达到了临界值的话，就会尝试的将内存数据结构中的数据溢写到磁盘</strong>,然后清空内存数据结构。</p><p>注：此时的临界值为动态变化的，并非固定值</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">maybeSpill</span></span>(collection: <span class="type">C</span>, currentMemory: <span class="type">Long</span>): <span class="type">Boolean</span> = &#123;</span><br><span class="line">  <span class="keyword">var</span> shouldSpill = <span class="literal">false</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (elementsRead % <span class="number">32</span> == <span class="number">0</span> &amp;&amp; currentMemory &gt;= myMemoryThreshold) &#123;</span><br><span class="line">    <span class="comment">// Claim up to double our current memory from the shuffle memory pool</span></span><br><span class="line">    <span class="keyword">val</span> amountToRequest = <span class="number">2</span> * currentMemory - myMemoryThreshold</span><br><span class="line">    <span class="keyword">val</span> granted = acquireMemory(amountToRequest)</span><br><span class="line">    myMemoryThreshold += granted</span><br><span class="line">    <span class="comment">// If we were granted too little memory to grow further (either tryToAcquire returned 0,</span></span><br><span class="line">    <span class="comment">// or we already had more memory than myMemoryThreshold), spill the current collection</span></span><br><span class="line">    shouldSpill = currentMemory &gt;= myMemoryThreshold</span><br><span class="line">  &#125;</span><br><span class="line">  shouldSpill = shouldSpill || _elementsRead &gt; numElementsForceSpillThreshold</span><br><span class="line">  <span class="comment">// Actually spill</span></span><br><span class="line">  <span class="keyword">if</span> (shouldSpill) &#123;</span><br><span class="line">    _spillCount += <span class="number">1</span></span><br><span class="line">    logSpillage(currentMemory)</span><br><span class="line">    spill(collection)</span><br><span class="line">    _elementsRead = <span class="number">0</span></span><br><span class="line">    _memoryBytesSpilled += currentMemory</span><br><span class="line">    releaseMemory()</span><br><span class="line">  &#125;</span><br><span class="line">  shouldSpill</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>在溢写到磁盘文件之前，会先根据 key 对内存数据结构中已有的数据进行排序</strong>，排序之后，会分批将数据写入磁盘文件。</p><p><font color='grey'>默认的 batch 数量是 10000 条，也就是说，排序好的数据，会以每批次 1 万条数据的形式分批写入磁盘文件，写入磁盘文件是通过Java 的 BufferedOutputStream 实现的。BufferedOutputStream 是 Java 的缓冲输出流，首先会将数据缓冲在内存中，当内存缓冲满溢之后再一次写入磁盘文件中，这样可以减少磁盘IO次数，提升性能。</font></p><p>一个 Task 将所有数据写入内存数据结构的过程中，会发生多次磁盘溢写，产生多个临时文件，最后会将之前所有的临时文件都进行合并，最后会合并成为一个大文件。<strong>最终只剩下两个文件，一个是合并之后的数据文件，一个是索引文件</strong>,索引文件标识了下游各个 Task 的数据在文件中的 start offset 与 end offset。最终再由下游的 task 根据索引文件读取相应的数据文件。</p><img src="../images/spark/5.png" alt="" style="zoom:50%;" /><p><font color = 'blue'> <strong>SortShuffleManager 由于有一个磁盘文件 merge 的过程，因此大大减少了文件数量。 比如第一个 stage 有 50 个 task ， 总共有 10 个 Executor ， 每个 Executor 执行 5个 task ，而第二个 stage 有 100 个 task 。由于每个 task 最终只有一个磁盘 文件，因此 此时每个 Executor 上只有 5 个磁盘文件， 所有 Executor 只有 50 个磁盘文件。</strong></font></p><h3 id="bypassSortShuffle"><a href="#bypassSortShuffle" class="headerlink" title="bypassSortShuffle"></a>bypassSortShuffle</h3><p>此时 Task 会为每个下游 Task 都创建一个临时磁盘文件，并将数据按 key 进行 hash 然后根据 key 的hash 值，将 key 写入对应的磁盘文件之中。当然，写入磁盘文件时也是先写入内存缓冲，缓冲写满之后再溢写到磁盘文件的。最后，同样会将所有临时磁盘文件都合并成一个磁盘文件，并创建一个单独的索引文件。</p><p>该过程的磁盘写机制其实跟未经优化的 HashShuffleManager 是一模一样的,因为都要创建数量惊人的磁盘文件， 只是在最后会做一个磁盘文件的合并而已,因此产生少量的最终磁盘文件,也让该机制相对未经优化的 HashShuffleManager 来说，shuffle read 的性能会更好。</p><p>而该机制与普通 SortShuffleManager 运行机制的不同在于</p><ol><li>磁盘写机制不同 </li><li>不会进行排序 </li></ol><p>也就是说,启用该机制的最大好处在于,shuffle write 过程中,不需要进行数据的排序操作,也就节省掉了这部分的性能开销。</p><p><strong>触发条件</strong></p><ol><li><p>shuffle map task 的数量小于 spark.shuffle.sort.bypassMergeThreshold 参数的值[默认200]</p></li><li><p>不是聚合类的shuffle 算子[比如groupByKey]</p></li></ol><h2 id="Spark-Shuffle-vs-MR-Shuffle"><a href="#Spark-Shuffle-vs-MR-Shuffle" class="headerlink" title="Spark Shuffle vs MR Shuffle"></a>Spark Shuffle vs MR Shuffle</h2><h3 id="Shuffle-管理器"><a href="#Shuffle-管理器" class="headerlink" title="Shuffle 管理器"></a>Shuffle 管理器</h3><p>Hadoop 2.7.x Shuffle 过程是 sort-based 过程，在 shuffle 过程中会发生排序行为</p><p>Spark 2.2.x Spark ShuffleManager 分为HashShuffleManager和 SortShuffleManager。Spark 1.2 后 默认为SortShuffleManager，在普通模式下，shuffle 过程中会发生排序行为；Spark 可以根据业务场景需要进行ShuffleManager 选择Hash Shuffle Manager / Sort ShuffleManager[普通模式和bypass模式]。</p><h3 id="Shuffle-过程排序次数"><a href="#Shuffle-过程排序次数" class="headerlink" title="Shuffle 过程排序次数"></a>Shuffle 过程排序次数</h3><ul><li>Hadoop Shuffle 过程总共会发生 3 次排序行为，详细分别如下：<ul><li>第一次排序行为：在 map 阶段，由环形缓冲区溢出到磁盘上时，落地磁盘的文件会按照 key 进行分区和排序，属于分区内有序，排序算法为快速排序</li><li>第二次排序行为：在 map 阶段，对溢出的文件进行 combiner合并过程中，需要对溢出的小文件进行归并排序、合并，排序算法为归并排序；</li><li>第三次排序行为：在 reduce 阶段，reduce task 将不同 maptask 端文件拉去到同一个reduce 分区后，对文件进行合并，归并排序，排序算法为归并排序；</li></ul></li><li>Spark Shuffle 过程在满足Shuffle Manager 为 SortShuffleManager ，且运行模式为普通模式的情况下才会发生排序行为，排序行为发生在数据结构中保存数据内存达到阈值，在溢出磁盘文件之前会对内存数据结构中数据进行排序；<ul><li>Spark 中 Sorted-Based Shuffle 在 Mapper 端是进行排序的，包括 partition 的排序和每个partition 内部元素进行排序。但是在 Reducer 端没有进行排序，所以 job 的结果默认情况下不是排序的。</li><li>Sorted-Based Shuffle  采用 Tim-Sort 排序算法，好处是可以极为高效的使用 Mapper 端的排序成果完成全局排序。</li></ul></li></ul><h3 id="Shuffle-逻辑流划分"><a href="#Shuffle-逻辑流划分" class="headerlink" title="Shuffle 逻辑流划分"></a>Shuffle 逻辑流划分</h3><ul><li><p>Hadoop Shuffle 过程可以划分为：map()，spill，merge，shuffle，sort，reduce()等，是按照流程顺次执行的，属于Push类型；</p></li><li><p>Spark Shuffle过程是由算子进行驱动，由于Spark的算子懒加载特性，属于Pull类型，整个Shuffle过程可以划分为Shuffle Write 和Shuffle Read两个阶段；</p></li></ul><h3 id="数据结构不同"><a href="#数据结构不同" class="headerlink" title="数据结构不同"></a>数据结构不同</h3><ul><li>Hadoop 是基于文件的数据结构</li><li>Spark是基于RDD的数据结构，计算性能要比 Hadoop 要高</li></ul><h3 id="Shuffle-Fetch-后数据存放位置"><a href="#Shuffle-Fetch-后数据存放位置" class="headerlink" title="Shuffle Fetch 后数据存放位置"></a>Shuffle Fetch 后数据存放位置</h3><ul><li>Hadoop reduce 端将 map task 的文件拉去到同一个 reduce 分区，是将文件进行归并排序、合并，将文件直接保存在磁盘上</li><li>Spark Shuffle Read 拉取来的数据首先肯定是放在 Reducer 端的内存缓存区中的，实现是内存+磁盘的方式，当然也可以通过 Spark.shuffle.spill=false 来设置只能使用内存。使用 ExternalAppendOnlyMap的方式时候如果内存使用达到一定临界值，会首先尝试在内存中扩大 ExternalAppendOnlyMap，如果不能扩容的话才会spill到磁盘。</li></ul><h3 id="Fetch-操作与数据计算粒度"><a href="#Fetch-操作与数据计算粒度" class="headerlink" title="Fetch 操作与数据计算粒度"></a>Fetch 操作与数据计算粒度</h3><ul><li>Hadoop 的 MapReduce 是粗粒度的，Hadoop Shuffle Reducer Fetch到的数据 record先暂时被存放到Buffer 中，当 Buffer 快满时才进行 combine() 操作</li><li>Spark 的 Shuffle Fetch 是细粒度的，Reducer 是对 Map 端数据 Record 边拉去边聚合</li></ul><h2 id="Spark-Shuffle-调优"><a href="#Spark-Shuffle-调优" class="headerlink" title="Spark Shuffle 调优"></a>Spark Shuffle 调优</h2><p>大多数 Spark 作业的性能主要就是消耗在了 shuffle 环节，因为该环节包含了大量的磁盘IO、序列化、网络数据传输等操作。因此，如果要让作业的性能更上一层楼，就有必要对 shuffle 过程进行调优。</p><h3 id="shuffle-相关参数调优"><a href="#shuffle-相关参数调优" class="headerlink" title="shuffle 相关参数调优"></a>shuffle 相关参数调优</h3><ol><li><p><strong>spark.shuffle.file.buffer</strong></p><p>默认值：32k</p><p>参数说明：该参数用于设置 shuffle write task 的 BufferedOutputStream 的 buffer 缓冲大小。将数据写到磁盘文件之前，会先写入 buffer 缓冲中，待缓冲写满之后，才会溢写到磁盘。<br>调优建议：如果作业可用的内存资源较为充足的话，可以适当增加这个参数的大小（比如64k），从而减少shuffle write过程中溢写磁盘文件的次数，也就可以减少磁盘IO次数，进而提升性能。在实践中发现，合理调节该参数，性能会有1%~5%的提升。</p></li><li><p><strong>spark.reducer.maxSizeInFlight</strong></p><p>默认值：48m<br>参数说明：该参数用于设置shuffle read task的buffer缓冲大小，而这个buffer缓冲决定了每次能够拉取多少数据。<br>调优建议：如果作业可用的内存资源较为充足的话，可以适当增加这个参数的大小（比如96m），从而减少拉取数据的次数，也就可以减少网络传输的次数，进而提升性能。在实践中发现，合理调节该参数，性能会有1%~5%的提升。</p></li><li><p><strong>spark.shuffle.io.maxRetries</strong></p><p>默认值：3<br>参数说明：shuffle read task 从 shuffle write task 所在节点拉取属于自己的数据时，如果因为网络异常导致拉取失败，是会自动进行重试的。该参数就代表了可以重试的最大次数。如果在指定次数之内拉取还是没有成功，就可能会导致作业执行失败。<br>调优建议：对于那些包含了特别耗时的 shuffle 操作的作业，建议增加重试最大次数（比如60次），以避免由于 JVM 的 full gc 或者网络不稳定等因素导致的数据拉取失败。在实践中发现，对于针对超大数据量（数十亿~上百亿）的 shuffle 过程，调节该参数可以大幅度提升稳定性。</p></li><li><p><strong>spark.shuffle.io.retryWait</strong></p><p>默认值：5s<br>参数说明：该参数代表了每次重试拉取数据的等待间隔，默认是 5s。<br>调优建议：建议加大间隔时长（比如60s），以增加 shuffle 操作的稳定性。</p></li></ol>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark 数据倾斜</title>
      <link href="2019/11/23/Spark%20%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C/"/>
      <url>2019/11/23/Spark%20%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p>Spark 中的数据倾斜问题主要指 shuffle 过程中由于不同的 key 对应的数据量不同导致的不同 task 所处理的数据量不同的问题。</p><a id="more"></a><h2 id="表现"><a href="#表现" class="headerlink" title="表现"></a>表现</h2><ol><li>Spark 作业的大部分 task 都执行迅速，只有有限的几个 task 执行的非常慢，此时可能出现了数据倾斜，作业可以运行，但是运行的非常慢。</li><li>原本能够正常执行的 Spark 作业，突然出现 OOM[内存溢出] 异常</li></ol><h2 id="定位数据倾斜"><a href="#定位数据倾斜" class="headerlink" title="定位数据倾斜"></a>定位数据倾斜</h2><p>在 Spark 中，同一个 Stage 的不同 Partition 可以并行处理，而具有依赖关系的不同 Stage 之间是串行处理的。假设某个 Spark Job 分为 Stage 0 和 Stage 1 两个 Stage，且 Stage 1 依赖于 Stage 0，那 Stage 0 完全处理结束之前不会处理 Stage 1。而 Stage 0 可能包含 N 个Task，这 N 个Task可以并行进行。如果其中 N-1 个 Task 都在10秒内完成，而另外一个 Task 却耗时1分钟，那该 Stage 的总时间至少为1分钟。换句话说，一个 Stage 所耗费的时间，主要由最慢的那个 Task 决定。<br>由于同一个 Stage 内的所有 Task 执行相同的计算，在排除不同计算节点计算能力差异的前提下，不同Task之间耗时的差异主要由该 Task 所处理的数据量决定。<br>Stage 的数据来源主要分为如下两类</p><ol><li>从数据源直接读取。如读取 HDFS，Kafka</li><li>读取上一个 Stage 的 Shuffle 数据</li></ol><p>常用并且可能会触发 shuffle 操作的算子有：distinct，groupByKey，reduceByKey，aggregateByKey，join 等。出现数据倾斜，很有可能就是使用了这些算子中的某一个导致的。</p><ol><li><p>如果我们是 yarn-client 模式提交，我们可以在本地直接查看 log，在 log 中定位到当前运行到了哪个 stage</p></li><li><p>如果用的 yarn-cluster 模式提交的话，我们可以通过 spark web UI 来查看当前运行到了哪个 stage。</p></li></ol><p>无论用的哪种模式我们都可以在 Spark web UI 上面查看到当前这个 stage 的各个 task 的数据量和运行时间，从而能够进一步确定是不是 task 的数据分配不均导致的数据倾斜。</p><p>当确定了发生数据倾斜的 stage 后，我们可以找出会触发 shuffle 的算子，推算出发生倾斜的那个 stage 对应代码。触发 shuffle 操作的除了上面提到的那些算子外，还要注意使用 spark sql 的某些 sql 语句，比如 group by 等。</p><h2 id="解决策略"><a href="#解决策略" class="headerlink" title="解决策略"></a>解决策略</h2><h4 id="数据源的数据倾斜"><a href="#数据源的数据倾斜" class="headerlink" title="数据源的数据倾斜"></a>数据源的数据倾斜</h4><p>尽量避免数据源的数据倾斜，以 Spark Stream 通过 DirectStream 方式读取 Kafka 数据为例。由于Kafka 的每一个 Partition 对应 Spark 的一个 Task（Partition），所以 Kafka 内相关 Topic 的各 Partition 之间数据是否平衡，直接决定 Spark 处理该数据时是否会产生数据倾斜。</p><p>Kafka 某一 Topic 内消息在不同 Partition 之间的分布，主要由 Producer 端所使用的 Partition 实现类决定。如果使用随机 Partitioner，则每条消息会随机发送到一个 Partition 中，从而从概率上来讲，各 Partition 间的数据会达到平衡。此时源直接读取 Kafka 数据的 Stage 不会产生数据倾斜。<br>但很多时候，业务场景可能会要求将具备同一特征的数据顺序消费，此时就需要将具有相同特征的数据放于同一个Partition中。一个典型的场景是，需要将同一个用户相关的 PV 信息置于同一个 Partition 中。此时，如果产生了数据倾斜，则需要通过其它方式处理。</p><h4 id="过滤异常数据"><a href="#过滤异常数据" class="headerlink" title="过滤异常数据"></a><strong>过滤异常数据</strong></h4><p>如果导致数据倾斜的 key 是异常数据，那么简单的过滤掉就可以了。</p><p>首先要对 key 进行分析，判断是哪些 key 造成数据倾斜。然后对这些 key 对应的记录进行分析:</p><ol><li>空值或者异常值之类的，大多是这个原因引起</li><li>无效数据，大量重复的测试数据或是对结果影响不大的有效数据</li><li>有效数据，业务导致的正常数据分布</li></ol><p>解决方案</p><p>对于第 1，2 种情况，直接对数据进行过滤即可。第3种情况则需要特殊的处理，具体我们下面详细介绍</p><h3 id="调整并行度分散同一个-Task-的不同-Key"><a href="#调整并行度分散同一个-Task-的不同-Key" class="headerlink" title="调整并行度分散同一个 Task 的不同 Key"></a>调整并行度分散同一个 Task 的不同 Key</h3><p>Spark 在做 Shuffle 时，默认使用 HashPartitioner 对数据进行分区。如果并行度设置的不合适，可能造成大量不相同的 Key 对应的数据被分配到了同一个 Task 上，造成该 Task 所处理的数据远大于其它 Task，从而造成数据倾斜。<br>如果调整 Shuffle 时的并行度，使得原本被分配到同一 Task 的不同 Key 发配到不同 Task 上处理，则可降低原 Task 所需处理的数据量，从而缓解数据倾斜问题造成的短板效应。</p><p><strong>优势</strong><br>实现简单，可在需要 Shuffle 的操作算子上直接设置并行度或者使用<code>spark.default.parallelism</code>设置。如果是Spark SQL，还可通过<code>SET spark.sql.shuffle.partitions=[num_tasks]</code>设置并行度。可用最小的代价解决问题。一般如果出现数据倾斜，都可以通过这种方法先试验几次，如果问题未解决，再尝试其它方法。</p><p><img src="https://www.iteblog.com/pic/spark/changeparallelism.png" alt="spark change parallelism"></p><p><strong>劣势</strong><br>适用场景少，只能将分配到同一Task 的不同 Key 分散开，但对于同一 Key 倾斜严重的情况该方法并不适用。并且该方法一般只能缓解数据倾斜，没有彻底消除问题。从实践经验来看，其效果一般。</p><p><strong>方案实践经验</strong></p><p>该方案通常无法彻底解决数据倾斜，因为如果出现一些极端情况，比如某个key对应的数据量有100万，那么无论你的task数量增加到多少，这个对应着100万数据的key肯定还是会分配到一个task中去处理，因此注定还是会发生数据倾斜的。所以这种方案只能说是在发现数据倾斜时尝试使用的第一种手段，尝试去用嘴简单的方法缓解数据倾斜而已，或者是和其他方案结合起来使用。</p><h3 id="自定义-Partitioner"><a href="#自定义-Partitioner" class="headerlink" title="自定义 Partitioner"></a>自定义 Partitioner</h3><p><strong>适用场景</strong><br>大量不同的 Key 被分配到了相同的 Task 造成该 Task 数据量过大。</p><p><strong>解决方案</strong><br>使用自定义的 Partitioner 实现类代替默认的 HashPartitioner，尽量将所有不同的 Key 均匀分配到不同的Task中。</p><p><strong>优势</strong><br>不影响原有的并行度设计。如果改变并行度，后续 Stage 的并行度也会默认改变，可能会影响后续 Stage。</p><p><strong>劣势</strong><br>适用场景有限，只能将不同 Key 分散开，对于同一 Key 对应数据集非常大的场景不适用。效果与调整并行度类似，只能缓解数据倾斜而不能完全消除数据倾斜。而且需要根据数据特点自定义专用的 Partitioner，不够灵活。</p><h3 id="两阶段聚合（局部聚合-全局聚合）"><a href="#两阶段聚合（局部聚合-全局聚合）" class="headerlink" title="两阶段聚合（局部聚合+全局聚合）"></a>两阶段聚合（局部聚合+全局聚合）</h3><p>在 Spark 中使用 groupByKey 和 reduceByKey 这两个算子会进行 shuffle 操作。这时候如果 map 端的文件每个 key 的数据量偏差很大，很容易会造成数据倾斜。</p><p>我们可以先对需要操作的数据中的 key 拼接上随机数进行打散分组，这样原来是一个 key 的数据可能会被分到多个 key 上，然后进行一次聚合，聚合完之后将原来拼在 key 上的随机数去掉，再进行聚合，这样对数据倾斜会有比较好的效果。</p><p>这个方案的核心实现思路就是进行两阶段聚合。第一次是局部聚合，先给每个 key 都打上一个随机数，比如10以内的随机数，此时原先一样的key就变成不一样的了，比如(hello, 1) (hello, 1) (hello, 1) (hello, 1)，就会变成(1_hello, 1) (1_hello, 1) (2_hello, 1) (2_hello, 1)。接着对打上随机数后的数据，执行 reduceByKey 等聚合操作，进行局部聚合，那么局部聚合结果，就会变成了(1_hello, 2) (2_hello, 2)。然后将各个key的前缀给去掉，就会变成(hello,2)(hello,2)，再次进行全局聚合操作，就可以得到最终结果了，比如(hello, 4)。</p><p><strong>方案缺点：</strong>仅仅适用于聚合类的 shuffle 操作，适用范围相对较窄。如果是 join 类的 shuffle 操作，还得用其他的解决方案。</p><h3 id="将-reduce-join-转换为-map-join"><a href="#将-reduce-join-转换为-map-join" class="headerlink" title="将 reduce join 转换为 map join"></a>将 reduce join 转换为 map join</h3><p>通过 Spark 的 Broadcast 机制，将 Reduce 侧 Join 转化为 Map 侧 Join，避免 Shuffle 从而完全消除 Shuffle 带来的数据倾斜。</p><p>两个 RDD 在进行 join 时会有 shuffle 操作，如果每个 key 对应的数据分布不均匀也会有数据倾斜发生。</p><p>这种情况下，如果两个 RDD 中某个 RDD 的数据量不大，可以将该 RDD 的数据提取出来，然后做成广播变量，将数据量大的那个 RDD 做 map 算子操作，然后在 map 算子内和广播变量进行 join，这样可以避免了 join 过程中的 shuffle，也就避免了 shuffle 过程中可能会出现的数据倾斜现象。</p><p><strong>适用场景</strong><br>参与 Join 的一边数据集足够小，可被加载进 Driver 并通过 Broadcast 方法广播到各个 Executor 中。</p><p><strong>解决方案</strong><br>在 Java/Scala 代码中将小数据集数据拉取到 Driver，然后通过 broadcast 方案将小数据集的数据广播到各 Executor。或者在使用 SQL 前，将 broadcast 的阈值调整得足够多，从而使用 broadcast 生效。进而将 Reduce 侧 Join 替换为 Map 侧 Join。</p><p><a href="https://www.iteblog.com/pic/spark/mapjoin.png"><img src="https://www.iteblog.com/pic/spark/mapjoin.png" alt="spark map join"></a></p><h4 id="案例"><a href="#案例" class="headerlink" title="案例"></a>案例</h4><p><strong>优势</strong><br>避免了 Shuffle，彻底消除了数据倾斜产生的条件，可极大提升性能。</p><p><strong>劣势</strong><br>要求参与 Join 的一侧数据集足够小，并且主要适用于 Join 的场景，不适合聚合的场景，适用条件有限。</p><h3 id="为-skew-的-key-增加随机前-后缀，拆分-join-再-union"><a href="#为-skew-的-key-增加随机前-后缀，拆分-join-再-union" class="headerlink" title="为 skew 的 key 增加随机前/后缀，拆分 join 再 union"></a>为 skew 的 key 增加随机前/后缀，<strong>拆分 join 再 union</strong></h3><p>为数据量特别大的 Key 增加随机前/后缀，使得原来 Key 相同的数据变为 Key 不相同的数据，从而使倾斜的数据集分散到不同的 Task 中，彻底解决数据倾斜问题。Join 另一则的数据中，与倾斜 Key 对应的部分数据，与随机前缀集作笛卡尔乘积，从而保证无论数据倾斜侧倾斜Key如何加前缀，都能与之正常 Join。</p><p><strong>适用场景</strong><br>两张表都比较大，无法使用Map则Join。其中一个RDD有少数几个Key的数据量过大，另外一个RDD的Key分布较为均匀。</p><p><strong>解决方案</strong><br>将有数据倾斜的RDD中倾斜Key对应的数据集单独抽取出来加上随机前缀，另外一个RDD每条数据分别与随机前缀结合形成新的RDD（相当于将其数据增到到原来的N倍，N即为随机前缀的总个数），然后将二者Join并去掉前缀。然后将不包含倾斜Key的剩余数据进行Join。最后将两次Join的结果集通过union合并，即可得到全部Join结果。</p><p><img src="https://www.iteblog.com/pic/spark/randomprefix.png" alt="spark random prefix"></p><p><strong>优势</strong><br>相对于 Map 则Join，更能适应大数据集的 Join。如果资源充足，倾斜部分数据集与非倾斜部分数据集可并行进行，效率提升明显。且只针对倾斜部分的数据做数据扩展，增加的资源消耗有限。</p><p><strong>劣势</strong><br>如果倾斜 Key 非常多，则另一侧数据膨胀非常大，此方案不适用。而且此时对倾斜Key与非倾斜Key分开处理，需要扫描数据集两遍，增加了开销。</p><h3 id="大表-key-加盐，小表扩大-N-倍-join"><a href="#大表-key-加盐，小表扩大-N-倍-join" class="headerlink" title="大表 key 加盐，小表扩大 N 倍 join"></a><strong>大表 key 加盐，小表扩大 N 倍 join</strong></h3><h4 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h4><p>如果出现数据倾斜的 Key 比较多，上一种方法将这些大量的倾斜 Key 分拆出来，意义不大。此时更适合直接对存在数据倾斜的数据集全部加上随机前缀，然后对另外一个不存在严重数据倾斜的数据集整体与随机前缀集作笛卡尔乘积（即将数据量扩大N倍）。</p><p><img src="https://www.iteblog.com/pic/spark/randomprefixandenlargesmalltable.png" alt="spark random prefix"></p><p><strong>适用场景</strong><br>一个数据集存在的倾斜Key比较多，另外一个数据集数据分布比较均匀。</p><p><strong>优势</strong><br>对大部分场景都适用，效果不错。</p><p><strong>劣势</strong><br>需要将一个数据集整体扩大N倍，会增加资源消耗。</p>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark 存储模块</title>
      <link href="2019/11/22/Spark%20%E5%85%B1%E4%BA%AB%E5%8F%98%E9%87%8F/"/>
      <url>2019/11/22/Spark%20%E5%85%B1%E4%BA%AB%E5%8F%98%E9%87%8F/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p>在执行 Spark 的应用程序时，Spark 集群会启动 Driver 和 Executor 两种 JVM 进程，前者为主控进程，负责创建 Spark 上下文，提交 Spark 作业[Job]，并将作业转化为计算任务[Task]，在各个 Executor 进程间协调任务的调度；后者负责在工作节点上执行具体的计算任务，并将结果返回给 Driver， 同时为需要持久化的 RDD 提供存储功能。由于 Driver 的内存管理相对来说较为简单，本节主要对 Executor 的内存管理进行分析，下文中的 Spark 内存均特指 Executor 的内存。</p><a id="more"></a><h2 id="Execuor-内存模型"><a href="#Execuor-内存模型" class="headerlink" title="Execuor 内存模型"></a>Execuor 内存模型</h2><h3 id="堆内和堆外内存"><a href="#堆内和堆外内存" class="headerlink" title="堆内和堆外内存"></a>堆内和堆外内存</h3><p>作为一个 JVM 进程，Executor 的内存管理建立在 JVM 的内存管理之上，Spark 对 JVM 的堆内 [On-heap]空间进行了更为详细的分配，以充分利用内存。</p><p>同时，Spark 引入了堆外[Off-heap]内存，使之可以直接在工作节点的系统内存中开辟空间，进一步优化了内存的使用。</p><p><strong>堆内内存受到 JVM 统一管理，堆外内存是直接向操作系统进行内存的申请和释放。</strong></p><h3 id="堆内内存"><a href="#堆内内存" class="headerlink" title="堆内内存"></a>堆内内存</h3><p>堆内内存的大小，由 Spark 应用程序启动时的 –executor-memory 或 spark.executor.memory 参数配置。 </p><p>Executor 内运行的并发任务共享 JVM 堆内内存， 这些任务在缓存 RDD 数据和广播 Broadcast 数据时占用的内存被规划为存储 [Storage] 内存， 而这些任务在执行 Shuffle 时占用的内存被规划为执行 [Execution] 内存，剩余的部分不做特殊规划，Spark 内部的对象实例，或者用户定义的 Spark 应用程序中的对象实例，均占用剩余的空间 。不同的管理模式下， 这三部分占用的空间大小各不相同 </p><h3 id="堆外内存"><a href="#堆外内存" class="headerlink" title="堆外内存"></a>堆外内存</h3><p>JVM 对于内存的清理无法准确指定时间点，因此无法实现精确的释放。<strong>为了进一步优化内存的使用以及提高 Shuffle 时排序的效率，Spark 引入了堆外[Off-heap]内存，使之可以直接在工作节点的系统内存中开辟空间，存储经过序列化的二进制数据。</strong>由于内存的申请和释放不再通过 JVM 机制，而是直接向操作系统申请，减少了不必要的内存开销，以及频繁的 GC 扫描和回收，提升了处理性能。而且序列化的数据占用的空间可以被精确计算，所以相比堆内内存来说，堆外内存可以被精确地申请和释放，降低了管理的难度，也降低了误差 </p><p>在默认情况下堆外内存并不启用，可通过配置 spark.memory.offHeap.enabled 参数启用，并由 spark.memory.offHeap.size 参数设定堆外空间的大小。除了没有 other 空间，堆外内存与堆内内存的划分方式相同，所有运行中的并发任务共享存储内存和执行内存 。</p><h2 id="内存空间分配"><a href="#内存空间分配" class="headerlink" title="内存空间分配"></a>内存空间分配</h2><h3 id="静态内存管理"><a href="#静态内存管理" class="headerlink" title="静态内存管理"></a>静态内存管理</h3><p>在 Spark 最初采用的静态内存管理机制下，**[存储内存]<strong>、</strong>[执行内存]<strong>和</strong>[其他内存]**的大小在 Spark 应用程序运行期间均为固定的，但用户可以应用程序启动前进行配置。</p><img src="/images/spark/009.png" alt="" style="zoom:90%;" /><ul><li><p>可用的存储内存</p><p> systemMaxMemory * spark.storage.memoryFraction * spark.storage.safetyFraction</p></li><li><p>可用的执行内存</p><p>systemMaxMemory * spark.shuffle.memoryFraction * spark.shuffle.safetyFraction</p></li></ul><p>其中 systemMaxMemory 取决于当前 JVM 堆内内存的大小，最后可用的执行内存或者存储内存要在此基础上与各自的 memoryFraction 参数和 safetyFraction 参数相乘得出。 </p><p>上述计算公式中的两个 safetyFraction 参数，其意义在于在逻辑上预留出 [1-safetyFraction] 这么一块保险区域，降低因实际内存超出当前预设范围而导致 OOM 的风险。</p><p>值得注意的是，这个预留的保险区域仅仅是一种逻辑上的规划，在具体使用时 Spark 并没有区别对待，和”其它内存”一样交给了 JVM 去管理。</p><p><strong>Storage 内存和 Execution 内存都有预留空间，目的是防止 OOM ，因为 Spark 堆内内存大小的记录是不准确的，需要留出保险区域。</strong></p><p>堆外的空间分配较为简单，只有存储内存和执行内存。可用的执行内存和存储内存占用的空间大小直接由参数 spark.memory.storageFraction 决定，由于堆外内存占用的空间可以被精确计算，所以无需再设定保险区域 </p><img src="/images/spark/12.png" alt="" style="zoom:90%;" /><p>静态内存管理机制实现起来较为简单，但如果用户不熟悉 Spark 的存储机制，或没有根据具体的数据规模和计算任务或做相应的配置，很容易造成存储内存和执行内存中的一方剩余大量的空间，而另一方却早早被占满，不得不淘汰或移出旧的内容以存储新的内容。由于新的内存管理机制的出现，这种方式目前已经很少有开发者使用，出于兼容旧版本的应用程序的目的，Spark 仍然保留了它的实现。</p><h3 id="统一内存管理"><a href="#统一内存管理" class="headerlink" title="统一内存管理"></a>统一内存管理</h3><p><strong>Spark1.6 之后引入的统一内存管理机制，与静态内存管理的区别在于<font color='blue'>存储内存和执行内存共享同一块空间，可以动态占用对方的空闲区域</font></strong> </p><img src="/images/spark/10.png" alt="" style="zoom:90%;" /><p>其中最重要的优化在于动态占用机制， 其规则如下：</p><img src="/images/spark/20.png" alt="" style="zoom:90%;" /><ul><li><p>设定基本的存储内存和执行内存区域（spark.storage.storageFraction参数），该设定确定了双方各自拥有的空间的范围；</p></li><li><p>双方的空间都不足时，则存储到硬盘</p><p>若己方空间不足而对方空余时，可借用对方的空间; [注：存储空间不足是指不足以放下一个完整的Block]</p></li><li><p>执行内存的空间被对方占用后，可让对方将占用的部分转存到磁盘，然后”归还”借用的空间；</p></li><li><p>存储内存的空间被对方占用后，无法让对方 “归还”，因为需要考虑 Shuffle 过程中的很多因素，实现起来较为复杂。</p></li></ul><h2 id="存储内存管理"><a href="#存储内存管理" class="headerlink" title="存储内存管理"></a>存储内存管理</h2><h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><p>Storage 管理着 Spark 应用在运行过程中产生的各种数据。比如 RDD 缓存，shuffle 过程中缓存及写入磁盘的数据，广播变量等。</p><p>Storage 模块在逻辑上以 Block 为基本存储单位，RDD 的每个 Partition 经过处理后唯一对应一个 Block。Driver 端 BlockManager 负责整个 Spark 应用程序的 Block 的元数据信息的管理和维护，而 Executor 端的 BlockManager 需要将 Block 的更新等状态上报到 Master，同时接收 Master 的命令， 例如新增或删除一个 RDD。</p><ul><li><p>BlockManager</p><blockquote><p>BlockManager 是 整个 Spark 底层负责数据存储与管理的一个组件 ， Driver 和 Executor 的所有数据都由对应的 BlockManager 进行管理。</p><p>Driver 上有 BlockManager Master ，负责对各个节点上的 BlockManager 内部管理的数据的元数据进行维护， 比如 block 的增删改等操作， 都会在这里维护好元数据 的变更。</p><p>每个节点都有一个 BlockManager，每个 BlockManager 创建之后， 第一件事即使去向 BlockManag erMaster 进行注册。</p></blockquote></li><li><p>CacheManager</p><blockquote><p>CacheManager 管理 spark 的缓存，而缓存可以基于内存的缓存，也可以是基于磁盘的缓存；<br>CacheManager 需要通过 BlockManager 来操作数据</p></blockquote></li></ul><h3 id="RDD-的持久化机制"><a href="#RDD-的持久化机制" class="headerlink" title="RDD 的持久化机制"></a>RDD 的持久化机制</h3><p>弹性分布式数据集 RDD 作为 Spark 最根本的数据抽象，是只读的分区记录的集合，基于在稳定物理存储中的数据集上创建，或者在其他已有的 RDD 上执行转换[Transformation]操作产生一个新的 RDD。转换后的 RDD 与 原始的 RDD 之间产生的依赖关系构成了血统[Lineag]。凭借血统，Spark 保证了每一个 RDD 都可以被重新恢复。</p><p>Task 在启动之初读取一个分区时，会先判断这个分区是否已经被持久化，如果没有则需要检查 Checkpoint 或按照血统重新计算。所以如果一个 RDD 要执行多次 action 操作， 可以在第一次 action 操作中使用 persist 或 cache 方法，在内存或磁盘中持久化或缓存这个 RDD，从而在后面的行动时提升计算速度。</p><blockquote><p>其中 cache 这个方法是个 Tranformation ,当第一次遇到 action 算子的时才会进行持久化</p><p>cache 内部调用了 persist(StorageLevel.MEMORY_ONLY)方法，所以执行 cache 算子其实就是执行了 persist 算子且持久化级别为 MEMORY_ONLY。 故缓存是一种特殊的持久化。堆内和堆外存储内存的设计，便可以对缓存 RDD 时使用的内存做统一的规划和管理。</p></blockquote><p>RDD 的持久化由 Spark 的 Storage 模块负责，实现了 RDD 与物理存储的解耦合。 </p><p>在对 RDD 持久化时，Spark 规定了 MEMORY_ONLY 、MEMORY_AND_DISK 等 7 种不同的存储级别 ， 而存储级别是以下 5 个变量的组合：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">StorageLevel</span> <span class="title">private</span>(<span class="params"></span></span></span><br><span class="line"><span class="class"><span class="params">  private var _useDisk: <span class="type">Boolean</span>, //磁盘</span></span></span><br><span class="line"><span class="class"><span class="params">  private var _useMemory: <span class="type">Boolean</span>, //这里其实是指堆内内存</span></span></span><br><span class="line"><span class="class"><span class="params">  private var _useOffHeap: <span class="type">Boolean</span>, //堆外内存</span></span></span><br><span class="line"><span class="class"><span class="params">  private var _deserialized: <span class="type">Boolean</span>, //是否为非序列化</span></span></span><br><span class="line"><span class="class"><span class="params">  private var _replication: <span class="type">Int</span> = 1 //副本个数</span></span></span><br><span class="line"><span class="class"><span class="params"></span>)</span></span><br></pre></td></tr></table></figure><table><thead><tr><th>存储级别</th><th>含义</th></tr></thead><tbody><tr><td>MEMORY_ONLY</td><td>以非序列化的 Java 对象的方式持久化在 JVM 内存中。如果内存无法完全存储 RDD 所有的 partition，那么那些没有持久化的 partition 就会在下一次需要使用它们的时候，重新被计算</td></tr><tr><td>MEMORY_AND_DISK</td><td>同上，但是当 RDD 某些 partition 无法存储在内存中时，会持久化到磁盘中。下次需要使用这些 partition 时，需要从磁盘上读取</td></tr><tr><td>MEMORY_ONLY_SER</td><td>同 MEMORY_ONLY，但是会使用 Java 序列化方式，将 Java 对象序列化后进行持久化。可以减少内存开销，但是需要进行反序列化，因此会加大 CPU 开销</td></tr><tr><td>MEMORY_AND_DISK_SER</td><td>同 MEMORY_AND_DISK，但是使用序列化方式持久化 Java 对象</td></tr><tr><td>DISK_ONLY</td><td>使用非序列化 Java 对象的方式持久化，完全存储到磁盘上</td></tr><tr><td>MEMORY_ONLY_2  MEMORY_AND_DISK_2</td><td>如果是尾部加了 2 的持久化级别，表示将持久化数据复用一份，保存到其他节点，从而在数据丢失时，不需要再次计算，只需要使用备份数据即可</td></tr></tbody></table><h3 id="RDD的缓存过程"><a href="#RDD的缓存过程" class="headerlink" title="RDD的缓存过程"></a>RDD的缓存过程</h3><p>RDD 在缓存到存储内存之前，数据项 [Record]的对象实例在逻辑上占用了 JVM 堆内内存的 other 部分的空间，同一 Partition 的不同数据项的存储空间并不连续。</p><p>缓存到存储内存之后， Partition 被转换成 Block，Record 在堆内或堆外存储内存中占用一块连续的空间。将 Partition 由不连续的存储空间转换为连续存储空间的过程，Spark 称之为”展开” [Unroll] </p><p>Block 有序列化和非序列化两种存储格式，具体以哪种方式取决于该 RDD 的存储级别。非序列化的 Block 用一个数组存储所有的对象实例<strong>，序列化的 Block 则用字节缓冲区 ByteBuffer 来存储二进制数据。</strong></p><p>每个 Executor 的 Storage 模块用一个 LinkedHashMap 来管理堆内和堆外存储内存中所有的 Block ，对这个 LinkedHashMap 新增和删除间接记录了内存的申请和释放。</p><p>因为不能保证存储空间可以一次容纳 Iterator 中的所有数据，当前的计算任务在 Unroll 时要向 MemoryManager 申请足够的 Unroll 空间来临时占位，空间不足则 Unroll 失败，空间足够时可以继续进行。在静态内存管理时，Spark 在存储内存中专门划分了一块 Unroll 空间，其大小是固定的，统一内存管理时则没有对 Unroll 空间进行特别区分，对于序列化的 Partition ，其所需的 Unroll 空间可以直接累加计算，一次申请。</p><blockquote><p>对于序列化的 Partition，其所需的 Unroll 空间可以直接累加计算，一次申请。</p></blockquote><blockquote><p>对于非序列化的 Partition 则要在遍历 Record 的过程中依次申请，即每读取一条 Record，采样估算其所需的 Unroll 空间并进行申请，空间不足时可以中断，释放已占用的 Unroll 空间。</p></blockquote><p>如果最终 Unroll 成功， 当前 Partition 所占用的 Unroll 空间被转换为正常的缓存 RDD 的存储空间 </p><img src="/images/spark/30.png" alt="" style="zoom:90%;" /><h3 id="淘汰与落盘"><a href="#淘汰与落盘" class="headerlink" title="淘汰与落盘"></a>淘汰与落盘</h3><p><strong>由于同一个 Executor 的所有的计算任务共享有限的存储内存空间，当有新的 Block 需要缓存但是剩余空间不足且无法动态占用时，就要对 LinkedHashMap 中 的旧 Block 进行淘汰，而被淘汰的 Block 如果其存储级别中同时包含存储到磁盘的要求，则要对其进行落盘，否则直接删除该 Block。</strong></p><ul><li>被淘汰的旧 Block 要与新 Block 的 MemoryMode 相同，即同属于堆外或堆内内存</li><li>新旧 Block 不能属于同一个RDD，避免循环淘汰</li><li>旧 Block 所属 RDD 不能处于被读状态，避免引发一致性问题</li><li>遍历 LinkedHashMap中 Block，按照最近最少使用 LRU 的顺序淘汰，直到满足新 Block 所需的空间。 其中 LRU 是 LinkedHashMap 的特性。</li></ul><h3 id="执行内存管理"><a href="#执行内存管理" class="headerlink" title="执行内存管理"></a>执行内存管理</h3><p>执行内存主要用来存储任务在执行 Shuffle 时占用的内存，Shuffle 是按照一定规则对 RDD 数据重新分区的过程， Shuffle 的 Write 和 Read 两阶段对执行内存的使用.</p><h3 id="Shuffle-Write"><a href="#Shuffle-Write" class="headerlink" title="Shuffle Write"></a>Shuffle Write</h3><p>在 map 端会采用 ExternalSorter 进行外排，在内存中存储数据时主要占用堆内执行空间。</p><h3 id="Shuffle-Read"><a href="#Shuffle-Read" class="headerlink" title="Shuffle Read"></a>Shuffle Read</h3><ul><li>在对 reduce端的数据进行聚合时， 要将数据交给 Aggregator处理， 在内存中存储数据时占用堆内执行空间。</li><li>如果需要进行最终结果排序，则要将再次将数据交给 ExternalSorter处理，占用堆内执行空间</li></ul><p>在 ExternalSorter 和 Aggregator 中， Spark 会使用一种叫 AppendOnlyMap 的哈希表在堆内执行内存中存储数据 ， 但在 Shuffle 过程中所有数据并不能都保存到该哈希表中， 当这个哈希表占用的内存会进行周期性地采样估算， 当其大到一定程度， 无法再从 MemoryManager 申请到新的执行内存时， Spark 就会将其全部内容存储到磁盘文件中， 这 个过程被称为溢存 [Spill] ， 溢存到磁盘的文件最后会被归 并 [Merge] </p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>Spark 的存储内存和执行内存有着截然不同的管理方式</p><ul><li><p>对于存储内存来说，<strong>Spark</strong> 用一个 **LinkedHashMap **来集中管理所有的 Block，Block 由需要缓存的 RDD 的 Partition 转化而成；</p></li><li><p>对于执行内存，Spark 用 <strong>AppendOnlyMap</strong> 来存储 Shuffle 过程中的数据， 在 Tungsten 排序中甚至抽象成为页式内存管理，开辟了全新的 JVM 内存管理机制 。</p></li></ul><h2 id="Spark-Shuffle-内存使用"><a href="#Spark-Shuffle-内存使用" class="headerlink" title="Spark Shuffle 内存使用"></a>Spark Shuffle 内存使用</h2><p>在使用 Spark 进行计算时，我们经常会碰到作业 (Job) Out Of Memory(OOM) 的情况，而且很大一部分情况是发生在 Shuffle 阶段。那么在 Spark Shuffle 中具体是哪些地方会使用比较多的内存而有可能导致 OOM 呢？ 为此，本文将围绕以上问题梳理 Spark 内存管理和 Shuffle 过程中与内存使用相关的知识；然后，简要分析下在 Spark Shuffle 中有可能导致 OOM 的原因。</p><h2 id="OOM"><a href="#OOM" class="headerlink" title="OOM"></a>OOM</h2><p>内存不够，数据太多就会抛出 OOM的 Exeception，主要有driver OOM和 executor OOM两种</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">java</span><span class="selector-class">.lang</span><span class="selector-class">.OutOfMemoryError</span>: <span class="selector-tag">Java</span> <span class="selector-tag">heap</span> <span class="selector-tag">space</span></span><br></pre></td></tr></table></figure><h3 id="driver-OOM"><a href="#driver-OOM" class="headerlink" title="driver OOM"></a><strong>driver OOM</strong></h3><ul><li><strong>用户在 Driver 端口生成大对象, 比如创建了一个大的集合数据结构</strong></li><li><strong>使用了collect 等操作，将所有 executor 的数据聚合到 driver 导致</strong></li></ul><p>一般是使用了collect 等操作，将所有 executor 的数据聚合到 driver 导致。尽量不要使用 collect操作即可。</p><h3 id="executor-OOM"><a href="#executor-OOM" class="headerlink" title="executor OOM"></a><strong>executor OOM</strong></h3><h3 id="数据倾斜导致内存溢出"><a href="#数据倾斜导致内存溢出" class="headerlink" title="数据倾斜导致内存溢出"></a><strong>数据倾斜导致内存溢出</strong></h3><p>数据不平衡除了有可能导致内存溢出外，也有可能导致性能的问题，调用 repartition 重新分区</p><h3 id="Reduce-OOM"><a href="#Reduce-OOM" class="headerlink" title="Reduce OOM"></a>Reduce OOM</h3><p>reduce task 去 map 端获取数据，reduce一边拉取数据一边聚合，reduce端有一块聚合内存[executor memory * 0.2],也就是这块内存不够<br><strong>解决方法</strong></p><ul><li>增加 reduce 聚合操作的内存的比例</li><li>增加 Executor memory 的大小 <strong>–executor-memory 5G</strong></li><li>减少 reduce task 每次拉取的数据量 设置 spak.reducer.maxSizeInFlight 24m, 拉取的次数就多了，因此建立连接的次数增多，有可能会连接不上[正好赶上 map task 端进行GC]</li></ul><h3 id="shuffle-后内存溢出"><a href="#shuffle-后内存溢出" class="headerlink" title="shuffle 后内存溢出"></a><strong>shuffle 后内存溢出</strong></h3><p> shuffle 后单个文件过大导致内存溢出。在 Spark 中，join，reduceByKey 这一类型的过程，都会有shuffle的过程，在shuffle的使用，需要传入一个partitioner，大部分 Spark 中的 shuffle 操作，默认的 partitioner 都是 HashPatitioner，默认值是父 RDD 中最大的分区数,这个参数通过spark.default.parallelism 控制 [在spark-sql中用spark.sql.shuffle.partitions] </p><p>spark.default.parallelism 参数只对 HashPartitioner 有效，所以如果是别的 Partitioner 或者自己实现的 Partitioner 就不能使用 spark.default.parallelism 这个参数来控制 shuffle 的并发量了。如果是别的partitioner 导致的 shuffle 内存溢出，就需要从 partitioner 的代码增加 partitions 的数量</p><h3 id="coalesce-调用导致内存溢出"><a href="#coalesce-调用导致内存溢出" class="headerlink" title="coalesce 调用导致内存溢出"></a><strong>coalesce 调用导致内存溢出</strong></h3><p>因为 hdfs 中不适合存小问题，所以 Spark 计算后如果产生的文件太小，调用 coalesce 合并文件再存入 hdfs中。但会导致一个问题，例如在 coalesce 之前有100个文件，这也意味着能够有100个 Task，现在调用coalesce(10)，最后只产生10个文件，因为 coalesce 并不是 shuffle 操作，这意味着 coalesce并不是先执行100个 Task，再将 Task 的执行结果合并成10个，而是从头到位只有10个 Task 在执行，原本100个文件是分开执行的，现在每个 Task 同时一次读取10个文件，使用的内存是原来的10倍，这导致了OOM。</p><p>解决这个问题的方法是令程序按照我们想的先执行100个 Task 再将结果合并成10个文件，这个问题同样可以通过repartition 解决，调用 repartition(10)</p><h3 id="standalone-模式下资源分配不均匀导致内存溢出"><a href="#standalone-模式下资源分配不均匀导致内存溢出" class="headerlink" title="standalone 模式下资源分配不均匀导致内存溢出"></a><strong>standalone 模式下资源分配不均匀导致内存溢出</strong></h3><p>在 standalone 的模式下如果配置了 –total-executor-cores 和 –executor-memory 这两个参数，但是没有配置 –executor-cores 参数，有可能导致，每个 Executor 的 memory 是一样的，但是 cores 的数量不同，那么在 cores 数量多的 Executor 中，由于能够同时执行多个Task，就容易导致内存溢出的情况。</p><p>这种情况的解决方法就是同时配置–executor-cores或者spark.executor.cores参数，确保Executor资源分配均匀。</p><h3 id="map-过程产生大量对象导致内存溢出"><a href="#map-过程产生大量对象导致内存溢出" class="headerlink" title="map 过程产生大量对象导致内存溢出"></a><strong>map 过程产生大量对象导致内存溢出</strong></h3><p>这种溢出的原因是在单个 map 中产生了大量的对象导致的</p><p>例如：rdd.map(x=&gt;for(i &lt;- 1 to 10000) yield i.toString)，这个操作在rdd中，每个对象都产生了10000 个对象，这肯定很容易产生内存溢出的问题。</p><p>针对这种问题，在不增加内存的情况下，可以通过减少每个 Task 的大小，以便达到每个 Task 即使产生大量的对象 Executor 的内存也能够装得下。具体做法可以在会产生大量对象的 map 操作之前调用 repartition方法，分区成更小的块传入map。</p><p>例如：rdd.repartition(10000).map(x=&gt;for(i &lt;- 1 to 10000) yield i.toString)</p><h2 id="参数"><a href="#参数" class="headerlink" title="参数"></a><strong>参数</strong></h2><h3 id="spark-driver-memory"><a href="#spark-driver-memory" class="headerlink" title="spark.driver.memory"></a>spark.driver.memory</h3><p>用来设置 Driver 的内存。在 Spark 程序中，SparkContext，DAGScheduler 都是运行在Driver端的。对应Stage 切分也是在 Driver 端运行，如果用户自己写的程序有过多的步骤，切分出过多的 Stage，这部分信息消耗的是 Driver 的内存，这个时候就需要调大 Driver 的内存</p>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark 存储模块</title>
      <link href="2019/11/22/Spark%20%E5%AE%B9%E9%94%99%E6%9C%BA%E5%88%B6/"/>
      <url>2019/11/22/Spark%20%E5%AE%B9%E9%94%99%E6%9C%BA%E5%88%B6/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p>在执行 Spark 的应用程序时，Spark 集群会启动 Driver 和 Executor 两种 JVM 进程，前者为主控进程，负责创建 Spark 上下文，提交 Spark 作业[Job]，并将作业转化为计算任务[Task]，在各个 Executor 进程间协调任务的调度；后者负责在工作节点上执行具体的计算任务，并将结果返回给 Driver， 同时为需要持久化的 RDD 提供存储功能。由于 Driver 的内存管理相对来说较为简单，本节主要对 Executor 的内存管理进行分析，下文中的 Spark 内存均特指 Executor 的内存。</p><a id="more"></a><h2 id="Execuor-内存模型"><a href="#Execuor-内存模型" class="headerlink" title="Execuor 内存模型"></a>Execuor 内存模型</h2><h3 id="堆内和堆外内存"><a href="#堆内和堆外内存" class="headerlink" title="堆内和堆外内存"></a>堆内和堆外内存</h3><p>作为一个 JVM 进程，Executor 的内存管理建立在 JVM 的内存管理之上，Spark 对 JVM 的堆内 [On-heap]空间进行了更为详细的分配，以充分利用内存。</p><p>同时，Spark 引入了堆外[Off-heap]内存，使之可以直接在工作节点的系统内存中开辟空间，进一步优化了内存的使用。</p><p><strong>堆内内存受到 JVM 统一管理，堆外内存是直接向操作系统进行内存的申请和释放。</strong></p><h3 id="堆内内存"><a href="#堆内内存" class="headerlink" title="堆内内存"></a>堆内内存</h3><p>堆内内存的大小，由 Spark 应用程序启动时的 –executor-memory 或 spark.executor.memory 参数配置。 </p><p>Executor 内运行的并发任务共享 JVM 堆内内存， 这些任务在缓存 RDD 数据和广播 Broadcast 数据时占用的内存被规划为存储 [Storage] 内存， 而这些任务在执行 Shuffle 时占用的内存被规划为执行 [Execution] 内存，剩余的部分不做特殊规划，Spark 内部的对象实例，或者用户定义的 Spark 应用程序中的对象实例，均占用剩余的空间 。不同的管理模式下， 这三部分占用的空间大小各不相同 </p><h3 id="堆外内存"><a href="#堆外内存" class="headerlink" title="堆外内存"></a>堆外内存</h3><p>JVM 对于内存的清理无法准确指定时间点，因此无法实现精确的释放。<strong>为了进一步优化内存的使用以及提高 Shuffle 时排序的效率，Spark 引入了堆外[Off-heap]内存，使之可以直接在工作节点的系统内存中开辟空间，存储经过序列化的二进制数据。</strong>由于内存的申请和释放不再通过 JVM 机制，而是直接向操作系统申请，减少了不必要的内存开销，以及频繁的 GC 扫描和回收，提升了处理性能。而且序列化的数据占用的空间可以被精确计算，所以相比堆内内存来说，堆外内存可以被精确地申请和释放，降低了管理的难度，也降低了误差 </p><p>在默认情况下堆外内存并不启用，可通过配置 spark.memory.offHeap.enabled 参数启用，并由 spark.memory.offHeap.size 参数设定堆外空间的大小。除了没有 other 空间，堆外内存与堆内内存的划分方式相同，所有运行中的并发任务共享存储内存和执行内存 。</p><h2 id="内存空间分配"><a href="#内存空间分配" class="headerlink" title="内存空间分配"></a>内存空间分配</h2><h3 id="静态内存管理"><a href="#静态内存管理" class="headerlink" title="静态内存管理"></a>静态内存管理</h3><p>在 Spark 最初采用的静态内存管理机制下，**[存储内存]<strong>、</strong>[执行内存]<strong>和</strong>[其他内存]**的大小在 Spark 应用程序运行期间均为固定的，但用户可以应用程序启动前进行配置。</p><img src="/images/spark/009.png" alt="" style="zoom:90%;" /><ul><li><p>可用的存储内存</p><p> systemMaxMemory * spark.storage.memoryFraction * spark.storage.safetyFraction</p></li><li><p>可用的执行内存</p><p>systemMaxMemory * spark.shuffle.memoryFraction * spark.shuffle.safetyFraction</p></li></ul><p>其中 systemMaxMemory 取决于当前 JVM 堆内内存的大小，最后可用的执行内存或者存储内存要在此基础上与各自的 memoryFraction 参数和 safetyFraction 参数相乘得出。 </p><p>上述计算公式中的两个 safetyFraction 参数，其意义在于在逻辑上预留出 [1-safetyFraction] 这么一块保险区域，降低因实际内存超出当前预设范围而导致 OOM 的风险。</p><p>值得注意的是，这个预留的保险区域仅仅是一种逻辑上的规划，在具体使用时 Spark 并没有区别对待，和”其它内存”一样交给了 JVM 去管理。</p><p><strong>Storage 内存和 Execution 内存都有预留空间，目的是防止 OOM ，因为 Spark 堆内内存大小的记录是不准确的，需要留出保险区域。</strong></p><p>堆外的空间分配较为简单，只有存储内存和执行内存。可用的执行内存和存储内存占用的空间大小直接由参数 spark.memory.storageFraction 决定，由于堆外内存占用的空间可以被精确计算，所以无需再设定保险区域 </p><img src="/images/spark/12.png" alt="" style="zoom:90%;" /><p>静态内存管理机制实现起来较为简单，但如果用户不熟悉 Spark 的存储机制，或没有根据具体的数据规模和计算任务或做相应的配置，很容易造成存储内存和执行内存中的一方剩余大量的空间，而另一方却早早被占满，不得不淘汰或移出旧的内容以存储新的内容。由于新的内存管理机制的出现，这种方式目前已经很少有开发者使用，出于兼容旧版本的应用程序的目的，Spark 仍然保留了它的实现。</p><h3 id="统一内存管理"><a href="#统一内存管理" class="headerlink" title="统一内存管理"></a>统一内存管理</h3><p><strong>Spark1.6 之后引入的统一内存管理机制，与静态内存管理的区别在于<font color='blue'>存储内存和执行内存共享同一块空间，可以动态占用对方的空闲区域</font></strong> </p><img src="/images/spark/10.png" alt="" style="zoom:90%;" /><p>其中最重要的优化在于动态占用机制， 其规则如下：</p><img src="/images/spark/20.png" alt="" style="zoom:90%;" /><ul><li><p>设定基本的存储内存和执行内存区域（spark.storage.storageFraction参数），该设定确定了双方各自拥有的空间的范围；</p></li><li><p>双方的空间都不足时，则存储到硬盘</p><p>若己方空间不足而对方空余时，可借用对方的空间; [注：存储空间不足是指不足以放下一个完整的Block]</p></li><li><p>执行内存的空间被对方占用后，可让对方将占用的部分转存到磁盘，然后”归还”借用的空间；</p></li><li><p>存储内存的空间被对方占用后，无法让对方 “归还”，因为需要考虑 Shuffle 过程中的很多因素，实现起来较为复杂。</p></li></ul><h2 id="存储内存管理"><a href="#存储内存管理" class="headerlink" title="存储内存管理"></a>存储内存管理</h2><h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><p>Storage 管理着 Spark 应用在运行过程中产生的各种数据。比如 RDD 缓存，shuffle 过程中缓存及写入磁盘的数据，广播变量等。</p><p>Storage 模块在逻辑上以 Block 为基本存储单位，RDD 的每个 Partition 经过处理后唯一对应一个 Block。Driver 端 BlockManager 负责整个 Spark 应用程序的 Block 的元数据信息的管理和维护，而 Executor 端的 BlockManager 需要将 Block 的更新等状态上报到 Master，同时接收 Master 的命令， 例如新增或删除一个 RDD。</p><ul><li><p>BlockManager</p><blockquote><p>BlockManager 是 整个 Spark 底层负责数据存储与管理的一个组件 ， Driver 和 Executor 的所有数据都由对应的 BlockManager 进行管理。</p><p>Driver 上有 BlockManager Master ，负责对各个节点上的 BlockManager 内部管理的数据的元数据进行维护， 比如 block 的增删改等操作， 都会在这里维护好元数据 的变更。</p><p>每个节点都有一个 BlockManager，每个 BlockManager 创建之后， 第一件事即使去向 BlockManag erMaster 进行注册。</p></blockquote></li><li><p>CacheManager</p><blockquote><p>CacheManager 管理 spark 的缓存，而缓存可以基于内存的缓存，也可以是基于磁盘的缓存；<br>CacheManager 需要通过 BlockManager 来操作数据</p></blockquote></li></ul><h3 id="RDD-的持久化机制"><a href="#RDD-的持久化机制" class="headerlink" title="RDD 的持久化机制"></a>RDD 的持久化机制</h3><p>弹性分布式数据集 RDD 作为 Spark 最根本的数据抽象，是只读的分区记录的集合，基于在稳定物理存储中的数据集上创建，或者在其他已有的 RDD 上执行转换[Transformation]操作产生一个新的 RDD。转换后的 RDD 与 原始的 RDD 之间产生的依赖关系构成了血统[Lineag]。凭借血统，Spark 保证了每一个 RDD 都可以被重新恢复。</p><p>Task 在启动之初读取一个分区时，会先判断这个分区是否已经被持久化，如果没有则需要检查 Checkpoint 或按照血统重新计算。所以如果一个 RDD 要执行多次 action 操作， 可以在第一次 action 操作中使用 persist 或 cache 方法，在内存或磁盘中持久化或缓存这个 RDD，从而在后面的行动时提升计算速度。</p><blockquote><p>其中 cache 这个方法是个 Tranformation ,当第一次遇到 action 算子的时才会进行持久化</p><p>cache 内部调用了 persist(StorageLevel.MEMORY_ONLY)方法，所以执行 cache 算子其实就是执行了 persist 算子且持久化级别为 MEMORY_ONLY。 故缓存是一种特殊的持久化。堆内和堆外存储内存的设计，便可以对缓存 RDD 时使用的内存做统一的规划和管理。</p></blockquote><p>RDD 的持久化由 Spark 的 Storage 模块负责，实现了 RDD 与物理存储的解耦合。 </p><p>在对 RDD 持久化时，Spark 规定了 MEMORY_ONLY 、MEMORY_AND_DISK 等 7 种不同的存储级别 ， 而存储级别是以下 5 个变量的组合：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">StorageLevel</span> <span class="title">private</span>(<span class="params"></span></span></span><br><span class="line"><span class="class"><span class="params">  private var _useDisk: <span class="type">Boolean</span>, //磁盘</span></span></span><br><span class="line"><span class="class"><span class="params">  private var _useMemory: <span class="type">Boolean</span>, //这里其实是指堆内内存</span></span></span><br><span class="line"><span class="class"><span class="params">  private var _useOffHeap: <span class="type">Boolean</span>, //堆外内存</span></span></span><br><span class="line"><span class="class"><span class="params">  private var _deserialized: <span class="type">Boolean</span>, //是否为非序列化</span></span></span><br><span class="line"><span class="class"><span class="params">  private var _replication: <span class="type">Int</span> = 1 //副本个数</span></span></span><br><span class="line"><span class="class"><span class="params"></span>)</span></span><br></pre></td></tr></table></figure><table><thead><tr><th>存储级别</th><th>含义</th></tr></thead><tbody><tr><td>MEMORY_ONLY</td><td>以非序列化的 Java 对象的方式持久化在 JVM 内存中。如果内存无法完全存储 RDD 所有的 partition，那么那些没有持久化的 partition 就会在下一次需要使用它们的时候，重新被计算</td></tr><tr><td>MEMORY_AND_DISK</td><td>同上，但是当 RDD 某些 partition 无法存储在内存中时，会持久化到磁盘中。下次需要使用这些 partition 时，需要从磁盘上读取</td></tr><tr><td>MEMORY_ONLY_SER</td><td>同 MEMORY_ONLY，但是会使用 Java 序列化方式，将 Java 对象序列化后进行持久化。可以减少内存开销，但是需要进行反序列化，因此会加大 CPU 开销</td></tr><tr><td>MEMORY_AND_DISK_SER</td><td>同 MEMORY_AND_DISK，但是使用序列化方式持久化 Java 对象</td></tr><tr><td>DISK_ONLY</td><td>使用非序列化 Java 对象的方式持久化，完全存储到磁盘上</td></tr><tr><td>MEMORY_ONLY_2  MEMORY_AND_DISK_2</td><td>如果是尾部加了 2 的持久化级别，表示将持久化数据复用一份，保存到其他节点，从而在数据丢失时，不需要再次计算，只需要使用备份数据即可</td></tr></tbody></table><h3 id="RDD的缓存过程"><a href="#RDD的缓存过程" class="headerlink" title="RDD的缓存过程"></a>RDD的缓存过程</h3><p>RDD 在缓存到存储内存之前，数据项 [Record]的对象实例在逻辑上占用了 JVM 堆内内存的 other 部分的空间，同一 Partition 的不同数据项的存储空间并不连续。</p><p>缓存到存储内存之后， Partition 被转换成 Block，Record 在堆内或堆外存储内存中占用一块连续的空间。将 Partition 由不连续的存储空间转换为连续存储空间的过程，Spark 称之为”展开” [Unroll] </p><p>Block 有序列化和非序列化两种存储格式，具体以哪种方式取决于该 RDD 的存储级别。非序列化的 Block 用一个数组存储所有的对象实例<strong>，序列化的 Block 则用字节缓冲区 ByteBuffer 来存储二进制数据。</strong></p><p>每个 Executor 的 Storage 模块用一个 LinkedHashMap 来管理堆内和堆外存储内存中所有的 Block ，对这个 LinkedHashMap 新增和删除间接记录了内存的申请和释放。</p><p>因为不能保证存储空间可以一次容纳 Iterator 中的所有数据，当前的计算任务在 Unroll 时要向 MemoryManager 申请足够的 Unroll 空间来临时占位，空间不足则 Unroll 失败，空间足够时可以继续进行。在静态内存管理时，Spark 在存储内存中专门划分了一块 Unroll 空间，其大小是固定的，统一内存管理时则没有对 Unroll 空间进行特别区分，对于序列化的 Partition ，其所需的 Unroll 空间可以直接累加计算，一次申请。</p><blockquote><p>对于序列化的 Partition，其所需的 Unroll 空间可以直接累加计算，一次申请。</p></blockquote><blockquote><p>对于非序列化的 Partition 则要在遍历 Record 的过程中依次申请，即每读取一条 Record，采样估算其所需的 Unroll 空间并进行申请，空间不足时可以中断，释放已占用的 Unroll 空间。</p></blockquote><p>如果最终 Unroll 成功， 当前 Partition 所占用的 Unroll 空间被转换为正常的缓存 RDD 的存储空间 </p><img src="/images/spark/30.png" alt="" style="zoom:90%;" /><h3 id="淘汰与落盘"><a href="#淘汰与落盘" class="headerlink" title="淘汰与落盘"></a>淘汰与落盘</h3><p><strong>由于同一个 Executor 的所有的计算任务共享有限的存储内存空间，当有新的 Block 需要缓存但是剩余空间不足且无法动态占用时，就要对 LinkedHashMap 中 的旧 Block 进行淘汰，而被淘汰的 Block 如果其存储级别中同时包含存储到磁盘的要求，则要对其进行落盘，否则直接删除该 Block。</strong></p><ul><li>被淘汰的旧 Block 要与新 Block 的 MemoryMode 相同，即同属于堆外或堆内内存</li><li>新旧 Block 不能属于同一个RDD，避免循环淘汰</li><li>旧 Block 所属 RDD 不能处于被读状态，避免引发一致性问题</li><li>遍历 LinkedHashMap中 Block，按照最近最少使用 LRU 的顺序淘汰，直到满足新 Block 所需的空间。 其中 LRU 是 LinkedHashMap 的特性。</li></ul><h3 id="执行内存管理"><a href="#执行内存管理" class="headerlink" title="执行内存管理"></a>执行内存管理</h3><p>执行内存主要用来存储任务在执行 Shuffle 时占用的内存，Shuffle 是按照一定规则对 RDD 数据重新分区的过程， Shuffle 的 Write 和 Read 两阶段对执行内存的使用.</p><h3 id="Shuffle-Write"><a href="#Shuffle-Write" class="headerlink" title="Shuffle Write"></a>Shuffle Write</h3><p>在 map 端会采用 ExternalSorter 进行外排，在内存中存储数据时主要占用堆内执行空间。</p><h3 id="Shuffle-Read"><a href="#Shuffle-Read" class="headerlink" title="Shuffle Read"></a>Shuffle Read</h3><ul><li>在对 reduce端的数据进行聚合时， 要将数据交给 Aggregator处理， 在内存中存储数据时占用堆内执行空间。</li><li>如果需要进行最终结果排序，则要将再次将数据交给 ExternalSorter处理，占用堆内执行空间</li></ul><p>在 ExternalSorter 和 Aggregator 中， Spark 会使用一种叫 AppendOnlyMap 的哈希表在堆内执行内存中存储数据 ， 但在 Shuffle 过程中所有数据并不能都保存到该哈希表中， 当这个哈希表占用的内存会进行周期性地采样估算， 当其大到一定程度， 无法再从 MemoryManager 申请到新的执行内存时， Spark 就会将其全部内容存储到磁盘文件中， 这 个过程被称为溢存 [Spill] ， 溢存到磁盘的文件最后会被归 并 [Merge] </p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>Spark 的存储内存和执行内存有着截然不同的管理方式</p><ul><li><p>对于存储内存来说，<strong>Spark</strong> 用一个 **LinkedHashMap **来集中管理所有的 Block，Block 由需要缓存的 RDD 的 Partition 转化而成；</p></li><li><p>对于执行内存，Spark 用 <strong>AppendOnlyMap</strong> 来存储 Shuffle 过程中的数据， 在 Tungsten 排序中甚至抽象成为页式内存管理，开辟了全新的 JVM 内存管理机制 。</p></li></ul><h2 id="Spark-Shuffle-内存使用"><a href="#Spark-Shuffle-内存使用" class="headerlink" title="Spark Shuffle 内存使用"></a>Spark Shuffle 内存使用</h2><p>在使用 Spark 进行计算时，我们经常会碰到作业 (Job) Out Of Memory(OOM) 的情况，而且很大一部分情况是发生在 Shuffle 阶段。那么在 Spark Shuffle 中具体是哪些地方会使用比较多的内存而有可能导致 OOM 呢？ 为此，本文将围绕以上问题梳理 Spark 内存管理和 Shuffle 过程中与内存使用相关的知识；然后，简要分析下在 Spark Shuffle 中有可能导致 OOM 的原因。</p><h2 id="OOM"><a href="#OOM" class="headerlink" title="OOM"></a>OOM</h2><p>内存不够，数据太多就会抛出 OOM的 Exeception，主要有driver OOM和 executor OOM两种</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">java</span><span class="selector-class">.lang</span><span class="selector-class">.OutOfMemoryError</span>: <span class="selector-tag">Java</span> <span class="selector-tag">heap</span> <span class="selector-tag">space</span></span><br></pre></td></tr></table></figure><h3 id="driver-OOM"><a href="#driver-OOM" class="headerlink" title="driver OOM"></a><strong>driver OOM</strong></h3><ul><li><strong>用户在 Driver 端口生成大对象, 比如创建了一个大的集合数据结构</strong></li><li><strong>使用了collect 等操作，将所有 executor 的数据聚合到 driver 导致</strong></li></ul><p>一般是使用了collect 等操作，将所有 executor 的数据聚合到 driver 导致。尽量不要使用 collect操作即可。</p><h3 id="executor-OOM"><a href="#executor-OOM" class="headerlink" title="executor OOM"></a><strong>executor OOM</strong></h3><h3 id="数据倾斜导致内存溢出"><a href="#数据倾斜导致内存溢出" class="headerlink" title="数据倾斜导致内存溢出"></a><strong>数据倾斜导致内存溢出</strong></h3><p>数据不平衡除了有可能导致内存溢出外，也有可能导致性能的问题，调用 repartition 重新分区</p><h3 id="Reduce-OOM"><a href="#Reduce-OOM" class="headerlink" title="Reduce OOM"></a>Reduce OOM</h3><p>reduce task 去 map 端获取数据，reduce一边拉取数据一边聚合，reduce端有一块聚合内存[executor memory * 0.2],也就是这块内存不够<br><strong>解决方法</strong></p><ul><li>增加 reduce 聚合操作的内存的比例</li><li>增加 Executor memory 的大小 <strong>–executor-memory 5G</strong></li><li>减少 reduce task 每次拉取的数据量 设置 spak.reducer.maxSizeInFlight 24m, 拉取的次数就多了，因此建立连接的次数增多，有可能会连接不上[正好赶上 map task 端进行GC]</li></ul><h3 id="shuffle-后内存溢出"><a href="#shuffle-后内存溢出" class="headerlink" title="shuffle 后内存溢出"></a><strong>shuffle 后内存溢出</strong></h3><p> shuffle 后单个文件过大导致内存溢出。在 Spark 中，join，reduceByKey 这一类型的过程，都会有shuffle的过程，在shuffle的使用，需要传入一个partitioner，大部分 Spark 中的 shuffle 操作，默认的 partitioner 都是 HashPatitioner，默认值是父 RDD 中最大的分区数,这个参数通过spark.default.parallelism 控制 [在spark-sql中用spark.sql.shuffle.partitions] </p><p>spark.default.parallelism 参数只对 HashPartitioner 有效，所以如果是别的 Partitioner 或者自己实现的 Partitioner 就不能使用 spark.default.parallelism 这个参数来控制 shuffle 的并发量了。如果是别的partitioner 导致的 shuffle 内存溢出，就需要从 partitioner 的代码增加 partitions 的数量</p><h3 id="coalesce-调用导致内存溢出"><a href="#coalesce-调用导致内存溢出" class="headerlink" title="coalesce 调用导致内存溢出"></a><strong>coalesce 调用导致内存溢出</strong></h3><p>因为 hdfs 中不适合存小问题，所以 Spark 计算后如果产生的文件太小，调用 coalesce 合并文件再存入 hdfs中。但会导致一个问题，例如在 coalesce 之前有100个文件，这也意味着能够有100个 Task，现在调用coalesce(10)，最后只产生10个文件，因为 coalesce 并不是 shuffle 操作，这意味着 coalesce并不是先执行100个 Task，再将 Task 的执行结果合并成10个，而是从头到位只有10个 Task 在执行，原本100个文件是分开执行的，现在每个 Task 同时一次读取10个文件，使用的内存是原来的10倍，这导致了OOM。</p><p>解决这个问题的方法是令程序按照我们想的先执行100个 Task 再将结果合并成10个文件，这个问题同样可以通过repartition 解决，调用 repartition(10)</p><h3 id="standalone-模式下资源分配不均匀导致内存溢出"><a href="#standalone-模式下资源分配不均匀导致内存溢出" class="headerlink" title="standalone 模式下资源分配不均匀导致内存溢出"></a><strong>standalone 模式下资源分配不均匀导致内存溢出</strong></h3><p>在 standalone 的模式下如果配置了 –total-executor-cores 和 –executor-memory 这两个参数，但是没有配置 –executor-cores 参数，有可能导致，每个 Executor 的 memory 是一样的，但是 cores 的数量不同，那么在 cores 数量多的 Executor 中，由于能够同时执行多个Task，就容易导致内存溢出的情况。</p><p>这种情况的解决方法就是同时配置–executor-cores或者spark.executor.cores参数，确保Executor资源分配均匀。</p><h3 id="map-过程产生大量对象导致内存溢出"><a href="#map-过程产生大量对象导致内存溢出" class="headerlink" title="map 过程产生大量对象导致内存溢出"></a><strong>map 过程产生大量对象导致内存溢出</strong></h3><p>这种溢出的原因是在单个 map 中产生了大量的对象导致的</p><p>例如：rdd.map(x=&gt;for(i &lt;- 1 to 10000) yield i.toString)，这个操作在rdd中，每个对象都产生了10000 个对象，这肯定很容易产生内存溢出的问题。</p><p>针对这种问题，在不增加内存的情况下，可以通过减少每个 Task 的大小，以便达到每个 Task 即使产生大量的对象 Executor 的内存也能够装得下。具体做法可以在会产生大量对象的 map 操作之前调用 repartition方法，分区成更小的块传入map。</p><p>例如：rdd.repartition(10000).map(x=&gt;for(i &lt;- 1 to 10000) yield i.toString)</p><h2 id="参数"><a href="#参数" class="headerlink" title="参数"></a><strong>参数</strong></h2><h3 id="spark-driver-memory"><a href="#spark-driver-memory" class="headerlink" title="spark.driver.memory"></a>spark.driver.memory</h3><p>用来设置 Driver 的内存。在 Spark 程序中，SparkContext，DAGScheduler 都是运行在Driver端的。对应Stage 切分也是在 Driver 端运行，如果用户自己写的程序有过多的步骤，切分出过多的 Stage，这部分信息消耗的是 Driver 的内存，这个时候就需要调大 Driver 的内存</p>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark 存储模块</title>
      <link href="2019/11/22/Spark%20%E5%AD%98%E5%82%A8%E6%A8%A1%E5%9D%97/"/>
      <url>2019/11/22/Spark%20%E5%AD%98%E5%82%A8%E6%A8%A1%E5%9D%97/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p>在执行 Spark 的应用程序时，Spark 集群会启动 Driver 和 Executor 两种 JVM 进程，前者为主控进程，负责创建 Spark 上下文，提交 Spark 作业[Job]，并将作业转化为计算任务[Task]，在各个 Executor 进程间协调任务的调度；后者负责在工作节点上执行具体的计算任务，并将结果返回给 Driver， 同时为需要持久化的 RDD 提供存储功能。由于 Driver 的内存管理相对来说较为简单，本节主要对 Executor 的内存管理进行分析，下文中的 Spark 内存均特指 Executor 的内存。</p><a id="more"></a><h2 id="Execuor-内存模型"><a href="#Execuor-内存模型" class="headerlink" title="Execuor 内存模型"></a>Execuor 内存模型</h2><h3 id="堆内和堆外内存"><a href="#堆内和堆外内存" class="headerlink" title="堆内和堆外内存"></a>堆内和堆外内存</h3><p>作为一个 JVM 进程，Executor 的内存管理建立在 JVM 的内存管理之上，Spark 对 JVM 的堆内 [On-heap]空间进行了更为详细的分配，以充分利用内存。</p><p>同时，Spark 引入了堆外[Off-heap]内存，使之可以直接在工作节点的系统内存中开辟空间，进一步优化了内存的使用。</p><p><strong>堆内内存受到 JVM 统一管理，堆外内存是直接向操作系统进行内存的申请和释放。</strong></p><h3 id="堆内内存"><a href="#堆内内存" class="headerlink" title="堆内内存"></a>堆内内存</h3><p>堆内内存的大小，由 Spark 应用程序启动时的 –executor-memory 或 spark.executor.memory 参数配置。 </p><p>Executor 内运行的并发任务共享 JVM 堆内内存， Spark 对堆内内存的管理是一种逻辑上的“规划式”的管理，Executor 端的堆内内存区域在逻辑上被划分为以下四个区域。</p><ol><li><p>执行内存 (Execution Memory) : 主要用于存放 Shuffle、Join、Sort、Aggregation 等计算过程中的临时数据</p></li><li><p>存储内存 (Storage Memory) : 主要用于存储 spark 的 cache 数据，例如 RDD 的缓存、unroll 数据</p><p>主要用于存储 spark 的 cache 数据，例如 RDD 的缓存、广播（Broadcast）数据、和 unroll 数据。内存占比为 UsableMemory * spark.memory.fraction * spark.memory.storageFraction，Spark 2+ 中，默认初始状态下 Storage Memory 和 Execution Memory 均约占系统总内存的30%（1 * 0.6 * 0.5 = 0.3）。</p></li><li><p>用户内存（User Memory）: 主要用于存储 RDD 转换操作所需要的数据，例如 RDD 依赖等信息；</p></li><li><p>预留内存（Reserved Memory）: 系统预留内存，会用来存储 Spark 内部对象。</p><p>系统预留内存，用来存储 Spark 内部对象。其大小在代码中是写死的，其值等于 300MB，这个值是不能修改的</p></li></ol><h3 id="堆外内存"><a href="#堆外内存" class="headerlink" title="堆外内存"></a>堆外内存</h3><p>Spark 对于堆内内存的清理无法准确指定时间点，因此无法实现精确的释放。<strong>为了进一步优化内存的使用 Spark 引入了堆外内存，使之可以直接在工作节点的系统内存中开辟空间，存储经过序列化的二进制数据。</strong>由于内存的申请和释放不再通过 JVM 机制，而是直接向操作系统申请，减少了不必要的内存开销，以及频繁的 GC 扫描和回收，提升了处理性能。而且序列化的数据占用的空间可以被精确计算，所以相比堆内内存来说，堆外内存可以被精降低了管理的难度，也降低了误差。 </p><p>在默认情况下堆外内存并不启用，可通过配置 spark.memory.offHeap.enabled 参数启用，并由 spark.memory.offHeap.size 参数设定堆外空间的大小。除了没有 other 空间，堆外内存与堆内内存的划分方式相同，所有运行中的并发任务共享存储内存和执行内存。</p><h2 id="内存空间分配"><a href="#内存空间分配" class="headerlink" title="内存空间分配"></a>内存空间分配</h2><h3 id="静态内存管理"><a href="#静态内存管理" class="headerlink" title="静态内存管理"></a>静态内存管理</h3><p>在 Spark 最初采用的静态内存管理机制下，**[存储内存]<strong>、</strong>[执行内存]<strong>和</strong>[其他内存]**的大小在 Spark 应用程序运行期间均为固定的，但用户可以应用程序启动前进行配置。</p><img src="../images/spark/009.png" alt="" style="zoom:90%;" /><ul><li><p>可用的存储内存</p><p> systemMaxMemory * spark.storage.memoryFraction * spark.storage.safetyFraction</p></li><li><p>可用的执行内存</p><p>systemMaxMemory * spark.shuffle.memoryFraction * spark.shuffle.safetyFraction</p></li></ul><p>其中 systemMaxMemory 取决于当前 JVM 堆内内存的大小，最后可用的执行内存或者存储内存要在此基础上与各自的 memoryFraction 参数和 safetyFraction 参数相乘得出。 </p><p>上述计算公式中的两个 safetyFraction 参数，其意义在于在逻辑上预留出 [1-safetyFraction] 这么一块保险区域，降低因实际内存超出当前预设范围而导致 OOM 的风险。</p><p>值得注意的是，这个预留的保险区域仅仅是一种逻辑上的规划，在具体使用时 Spark 并没有区别对待，和”其它内存”一样交给了 JVM 去管理。</p><p><strong>Storage 内存和 Execution 内存都有预留空间，目的是防止 OOM ，因为 Spark 堆内内存大小的记录是不准确的，需要留出保险区域。</strong></p><p>堆外的空间分配较为简单，只有存储内存和执行内存。可用的执行内存和存储内存占用的空间大小直接由参数 spark.memory.storageFraction 决定，由于堆外内存占用的空间可以被精确计算，所以无需再设定保险区域 </p><img src="../images/spark/12.png" alt="" style="zoom:90%;" /><p>静态内存管理机制实现起来较为简单，但如果用户不熟悉 Spark 的存储机制，或没有根据具体的数据规模和计算任务或做相应的配置，很容易造成存储内存和执行内存中的一方剩余大量的空间，而另一方却早早被占满，不得不淘汰或移出旧的内容以存储新的内容。由于新的内存管理机制的出现，这种方式目前已经很少有开发者使用，出于兼容旧版本的应用程序的目的，Spark 仍然保留了它的实现。</p><h3 id="统一内存管理"><a href="#统一内存管理" class="headerlink" title="统一内存管理"></a>统一内存管理</h3><p><strong>Spark1.6 之后引入的统一内存管理机制，与静态内存管理的区别在于<font color='blue'>存储内存和执行内存共享同一块空间，可以动态占用对方的空闲区域</font></strong> </p><img src="../images/spark/10.png" alt="" style="zoom:90%;" /><p>其中最重要的优化在于动态占用机制， 其规则如下：</p><img src="../images/spark/20.png" alt="" style="zoom:90%;" /><ul><li><p>设定基本的存储内存和执行内存区域（spark.storage.storageFraction参数），该设定确定了双方各自拥有的空间的范围；</p></li><li><p>双方的空间都不足时，则存储到硬盘</p><p>若己方空间不足而对方空余时，可借用对方的空间; [注：存储空间不足是指不足以放下一个完整的Block]</p></li><li><p>执行内存的空间被对方占用后，可让对方将占用的部分转存到磁盘，然后”归还”借用的空间；</p></li><li><p>存储内存的空间被对方占用后，无法让对方 “归还”，因为需要考虑 Shuffle 过程中的很多因素，实现起来较为复杂。</p><p>Storage 内存的空间被对方占用后，目前的实现是无法让对方”归还”，因为需要考虑 Shuffle 过程中的很多因素，实现起来较为复杂；而且 Shuffle 过程产生的文件在后面一定会被使用到，而 Cache 在内存的数据不一定在后面使用。在 <a href="http://www.linuxprobe.com/wp-content/uploads/2017/04/unified-memory-management-spark-10000.pdf">Unified Memory Management in Spark 1.6</a> 中详细讲解了为何选择这种策略，简单总结如下:</p><ul><li>数据清除的开销 : 驱逐 storage 内存的开销取决于 storage level，MEMORY_ONLY 可能是最昂贵的，因为需要重新计算，MEMORY_AND_DISK_SER 正好相反，只涉及到磁盘IO。溢写 execution 内存到磁盘的开销并不昂贵，因为 execution 存储的数据格式紧凑(compact format)，序列化开销低。并且，清除的 storage 内存可能不会被用到，但是，可以预见的是，驱逐的 execution 内存是必然会再被读到内存的，频繁的驱除重读 execution 内存将导致昂贵的开销。</li><li>实现的复杂度 : storage 内存的驱逐是容易实现的，只需要使用已有的方法，drop 掉 block。execution 则复杂的多，首先，execution 以 page 为单位管理这部分内存，并且确保相应的操作至少有 one page ，如果把这 one page 内存驱逐了，对应的操作就会处于饥饿状态。此外，还需要考虑 execution 内存被驱逐的情况下，等待 cache 的 block 如何处理。</li></ul></li></ul><h2 id="存储内存管理"><a href="#存储内存管理" class="headerlink" title="存储内存管理"></a>存储内存管理</h2><h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><p>Storage 模块在逻辑上以 Block 为基本存储单位，RDD 的每个 Partition 经过处理后唯一对应一个 Block。</p><p>BlockManager 是 整个 Spark 底层负责数据存储与管理的一个组件 ， Driver 和 Executor 的所有数据都由对应的 BlockManager 进行管理。</p><p>Driver 端 BlockManager 负责整个 Spark 应用程序的 Block 的元数据信息的管理和维护，而 Executor 端的 BlockManager 需要将 Block 的更新等状态上报到 Master，同时接收 Master 的命令， 例如新增或删除一个 RDD。</p><h3 id="RDD-的持久化机制"><a href="#RDD-的持久化机制" class="headerlink" title="RDD 的持久化机制"></a>RDD 的持久化机制</h3><p>Task 在启动之初读取一个分区时，会先判断这个分区是否已经被持久化，如果没有则需要检查 Checkpoint 或按照血统重新计算。所以如果一个 RDD 要执行多次 action 操作， 可以在第一次 action 操作中使用 persist 或 cache 方法，在内存或磁盘中持久化或缓存这个 RDD，从而在后面的行动时提升计算速度。</p><blockquote><p>其中 cache 这个方法是个 Tranformation ,当第一次遇到 action 算子的时才会进行持久化</p><p>cache 内部调用了 persist(StorageLevel.MEMORY_ONLY)方法，所以执行 cache 算子其实就是执行了 persist 算子且持久化级别为 MEMORY_ONLY。 故缓存是一种特殊的持久化。堆内和堆外存储内存的设计，便可以对缓存 RDD 时使用的内存做统一的规划和管理。</p></blockquote><p>RDD 的持久化由 Spark 的 Storage 模块负责，实现了 RDD 与物理存储的解耦合。 </p><p>在对 RDD 持久化时，Spark 规定了 MEMORY_ONLY 、MEMORY_AND_DISK 等 7 种不同的存储级别 ， 而存储级别是以下 5 个变量的组合：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">StorageLevel</span> <span class="title">private</span>(<span class="params"></span></span></span><br><span class="line"><span class="class"><span class="params">  private var _useDisk: <span class="type">Boolean</span>, //磁盘</span></span></span><br><span class="line"><span class="class"><span class="params">  private var _useMemory: <span class="type">Boolean</span>, //这里其实是指堆内内存</span></span></span><br><span class="line"><span class="class"><span class="params">  private var _useOffHeap: <span class="type">Boolean</span>, //堆外内存</span></span></span><br><span class="line"><span class="class"><span class="params">  private var _deserialized: <span class="type">Boolean</span>, //是否为非序列化</span></span></span><br><span class="line"><span class="class"><span class="params">  private var _replication: <span class="type">Int</span> = 1 //副本个数</span></span></span><br><span class="line"><span class="class"><span class="params"></span>)</span></span><br></pre></td></tr></table></figure><table><thead><tr><th>存储级别</th><th>含义</th></tr></thead><tbody><tr><td>MEMORY_ONLY</td><td>以非序列化的 Java 对象的方式持久化在 JVM 内存中。如果内存无法完全存储 RDD 所有的 partition，那么那些没有持久化的 partition 就会在下一次需要使用它们的时候，重新被计算</td></tr><tr><td>MEMORY_AND_DISK</td><td>同上，但是当 RDD 某些 partition 无法存储在内存中时，会持久化到磁盘中。下次需要使用这些 partition 时，需要从磁盘上读取</td></tr><tr><td>MEMORY_ONLY_SER</td><td>同 MEMORY_ONLY，但是会使用 Java 序列化方式，将 Java 对象序列化后进行持久化。可以减少内存开销，但是需要进行反序列化，因此会加大 CPU 开销</td></tr><tr><td>MEMORY_AND_DISK_SER</td><td>同 MEMORY_AND_DISK，但是使用序列化方式持久化 Java 对象</td></tr><tr><td>DISK_ONLY</td><td>使用非序列化 Java 对象的方式持久化，完全存储到磁盘上</td></tr><tr><td>MEMORY_ONLY_2  MEMORY_AND_DISK_2</td><td>如果是尾部加了 2 的持久化级别，表示将持久化数据复用一份，保存到其他节点，从而在数据丢失时，不需要再次计算，只需要使用备份数据即可</td></tr></tbody></table><h3 id="RDD的缓存过程"><a href="#RDD的缓存过程" class="headerlink" title="RDD的缓存过程"></a>RDD的缓存过程</h3><p>RDD 在缓存之前，数据项 [Record]的对象实例在逻辑上占用了 JVM 堆内内存的 other 部分的空间，同一Partition 的不同数据项的存储空间并不连续。缓存到存储内存之后， Partition 被转换成 Block，Record 在堆内或堆外存储内存中占用一块连续的空间。将 Partition 由不连续的存储空间转换为连续存储空间的过程，Spark 称之为”展开” [Unroll] </p><p>Block 有序列化和非序列化两种存储格式，具体以哪种方式取决于该 RDD 的存储级别。非序列化的 Block 用一个数组存储所有的对象实例<strong>，序列化的 Block 则用字节缓冲区 ByteBuffer 来存储二进制数据。</strong></p><p>每个 Executor 的 Storage 模块用一个 LinkedHashMap 来管理堆内和堆外存储内存中所有的 Block ，对这个 LinkedHashMap 新增和删除间接记录了内存的申请和释放。</p><p>因为不能保证存储空间可以一次容纳 Iterator 中的所有数据，当前的计算任务在 Unroll 时要向 MemoryManager 申请足够的 Unroll 空间来临时占位，空间不足则 Unroll 失败，空间足够时可以继续进行。在静态内存管理时，Spark 在存储内存中专门划分了一块 Unroll 空间，其大小是固定的，统一内存管理时则没有对 Unroll 空间进行特别区分，对于序列化的 Partition ，其所需的 Unroll 空间可以直接累加计算，一次申请。</p><blockquote><p>对于序列化的 Partition，其所需的 Unroll 空间可以直接累加计算，一次申请。</p></blockquote><blockquote><p>对于非序列化的 Partition 则要在遍历 Record 的过程中依次申请，即每读取一条 Record，采样估算其所需的 Unroll 空间并进行申请，空间不足时可以中断，释放已占用的 Unroll 空间。</p></blockquote><p>如果最终 Unroll 成功， 当前 Partition 所占用的 Unroll 空间被转换为正常的缓存 RDD 的存储空间 </p><img src="../images/spark/30.png" alt="" style="zoom:50%;" /><h3 id="淘汰与落盘"><a href="#淘汰与落盘" class="headerlink" title="淘汰与落盘"></a>淘汰与落盘</h3><p><strong>由于同一个 Executor 的所有的计算任务共享有限的存储内存空间，当有新的 Block 需要缓存但是剩余空间不足且无法动态占用时，就要对 LinkedHashMap 中 的旧 Block 进行淘汰，而被淘汰的 Block 如果其存储级别中同时包含存储到磁盘的要求，则要对其进行落盘，否则直接删除该 Block。</strong></p><ul><li>被淘汰的旧 Block 要与新 Block 的 MemoryMode 相同，即同属于堆外或堆内内存</li><li>新旧 Block 不能属于同一个RDD，避免循环淘汰</li><li>旧 Block 所属 RDD 不能处于被读状态，避免引发一致性问题</li><li>遍历 LinkedHashMap中 Block，按照最近最少使用 LRU 的顺序淘汰，直到满足新 Block 所需的空间。 其中 LRU 是 LinkedHashMap 的特性。</li></ul><h3 id="执行内存管理"><a href="#执行内存管理" class="headerlink" title="执行内存管理"></a>执行内存管理</h3><p>执行内存主要用来存储任务在执行 Shuffle 时占用的内存，Shuffle 是按照一定规则对 RDD 数据重新分区的过程， Shuffle 的 Write 和 Read 两阶段对执行内存的使用.</p><h3 id="Shuffle-Write"><a href="#Shuffle-Write" class="headerlink" title="Shuffle Write"></a>Shuffle Write</h3><p>在 map 端会采用 ExternalSorter 进行外排，在内存中存储数据时主要占用堆内执行空间。</p><h3 id="Shuffle-Read"><a href="#Shuffle-Read" class="headerlink" title="Shuffle Read"></a>Shuffle Read</h3><ul><li>在对 reduce端的数据进行聚合时， 要将数据交给 Aggregator处理， 在内存中存储数据时占用堆内执行空间。</li><li>如果需要进行最终结果排序，则要将再次将数据交给 ExternalSorter处理，占用堆内执行空间</li></ul><p>在 ExternalSorter 和 Aggregator 中， Spark 会使用一种叫 AppendOnlyMap 的哈希表在堆内执行内存中存储数据 ， 但在 Shuffle 过程中所有数据并不能都保存到该哈希表中， 当这个哈希表占用的内存会进行周期性地采样估算， 当其大到一定程度， 无法再从 MemoryManager 申请到新的执行内存时， Spark 就会将其全部内容存储到磁盘文件中， 这 个过程被称为溢存 [Spill] ， 溢存到磁盘的文件最后会被归 并 [Merge] </p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>Spark 的存储内存和执行内存有着截然不同的管理方式</p><ul><li><p>对于存储内存来说，<strong>Spark</strong> 用一个 **LinkedHashMap **来集中管理所有的 Block，Block 由需要缓存的 RDD 的 Partition 转化而成；</p></li><li><p>对于执行内存，Spark 用 <strong>AppendOnlyMap</strong> 来存储 Shuffle 过程中的数据， 在 Tungsten 排序中甚至抽象成为页式内存管理，开辟了全新的 JVM 内存管理机制 。</p></li></ul><h2 id="Spark-Shuffle-内存使用"><a href="#Spark-Shuffle-内存使用" class="headerlink" title="Spark Shuffle 内存使用"></a>Spark Shuffle 内存使用</h2><p>在使用 Spark 进行计算时，我们经常会碰到作业 (Job) Out Of Memory(OOM) 的情况，而且很大一部分情况是发生在 Shuffle 阶段。那么在 Spark Shuffle 中具体是哪些地方会使用比较多的内存而有可能导致 OOM 呢？ 为此，本文将围绕以上问题梳理 Spark 内存管理和 Shuffle 过程中与内存使用相关的知识；然后，简要分析下在 Spark Shuffle 中有可能导致 OOM 的原因。</p><h2 id="OOM"><a href="#OOM" class="headerlink" title="OOM"></a>OOM</h2><p>内存不够，数据太多就会抛出 OOM 的 Exeception，主要有 driver OOM 和 executor OOM两种</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">java</span><span class="selector-class">.lang</span><span class="selector-class">.OutOfMemoryError</span>: <span class="selector-tag">Java</span> <span class="selector-tag">heap</span> <span class="selector-tag">space</span></span><br></pre></td></tr></table></figure><h3 id="driver-OOM"><a href="#driver-OOM" class="headerlink" title="driver OOM"></a><strong>driver OOM</strong></h3><ul><li><strong>用户在 Driver 端口生成大对象, 比如创建了一个大的集合数据结构</strong></li><li><strong>使用了collect 等操作，将所有 executor 的数据聚合到 driver 导致</strong></li></ul><p>一般是使用了collect 等操作，将所有 executor 的数据聚合到 driver 导致。尽量不要使用 collect 操作即可。</p><h3 id="executor-OOM"><a href="#executor-OOM" class="headerlink" title="executor OOM"></a><strong>executor OOM</strong></h3><h3 id="数据倾斜导致内存溢出"><a href="#数据倾斜导致内存溢出" class="headerlink" title="数据倾斜导致内存溢出"></a><strong>数据倾斜导致内存溢出</strong></h3><p>数据不平衡除了有可能导致内存溢出外，也有可能导致性能的问题，调用 repartition 重新分区</p><h3 id="Reduce-OOM"><a href="#Reduce-OOM" class="headerlink" title="Reduce OOM"></a>Reduce OOM</h3><p>reduce task 去 map 端获取数据，reduce一边拉取数据一边聚合，reduce端有一块聚合内存[executor memory * 0.2],也就是这块内存不够<br><strong>解决方法</strong></p><ul><li>增加 reduce 聚合操作的内存的比例</li><li>增加 Executor memory 的大小 <strong>–executor-memory 5G</strong></li><li>减少 reduce task 每次拉取的数据量 设置 spak.reducer.maxSizeInFlight 24m, 拉取的次数就多了，因此建立连接的次数增多，有可能会连接不上[正好赶上 map task 端进行GC]</li></ul><h3 id="shuffle-后内存溢出"><a href="#shuffle-后内存溢出" class="headerlink" title="shuffle 后内存溢出"></a><strong>shuffle 后内存溢出</strong></h3><p>shuffle 后单个文件过大导致内存溢出。在 Spark 中，join，reduceByKey 这一类型的过程，都会有shuffle 的过程，在 shuffle 的使用，需要传入一个 partitioner，大部分 Spark 中的 shuffle 操作，默认的 partitioner 都是 HashPatitioner，默认值是父 RDD 中最大的分区数,这个参数通过spark.default.parallelism 控制 [在spark-sql中用spark.sql.shuffle.partitions] </p><p>spark.default.parallelism 参数只对 HashPartitioner 有效，所以如果是别的 Partitioner 或者自己实现的 Partitioner 就不能使用 spark.default.parallelism 这个参数来控制 shuffle 的并发量了。如果是别的partitioner 导致的 shuffle 内存溢出，就需要从 partitioner 的代码增加 partitions 的数量</p><h3 id="coalesce-调用导致内存溢出"><a href="#coalesce-调用导致内存溢出" class="headerlink" title="coalesce 调用导致内存溢出"></a><strong>coalesce 调用导致内存溢出</strong></h3><p>因为 hdfs 中不适合存小问题，所以 Spark 计算后如果产生的文件太小，调用 coalesce 合并文件再存入 hdfs中。但会导致一个问题，例如在 coalesce 之前有100个文件，这也意味着能够有100个 Task，现在调用coalesce(10)，最后只产生10个文件，因为 coalesce 并不是 shuffle 操作，这意味着 coalesce并不是先执行100个 Task，再将 Task 的执行结果合并成10个，而是从头到位只有10个 Task 在执行，原本100个文件是分开执行的，现在每个 Task 同时一次读取10个文件，使用的内存是原来的10倍，这导致了OOM。</p><p>解决这个问题的方法是令程序按照我们想的先执行100个 Task 再将结果合并成10个文件，这个问题同样可以通过repartition 解决，调用 repartition(10)</p><h3 id="standalone-模式下资源分配不均匀导致内存溢出"><a href="#standalone-模式下资源分配不均匀导致内存溢出" class="headerlink" title="standalone 模式下资源分配不均匀导致内存溢出"></a><strong>standalone 模式下资源分配不均匀导致内存溢出</strong></h3><p>在 standalone 的模式下如果配置了 –total-executor-cores 和 –executor-memory 这两个参数，但是没有配置 –executor-cores 参数，有可能导致，每个 Executor 的 memory 是一样的，但是 cores 的数量不同，那么在 cores 数量多的 Executor 中，由于能够同时执行多个Task，就容易导致内存溢出的情况。</p><p>这种情况的解决方法就是同时配置–executor-cores或者spark.executor.cores参数，确保Executor资源分配均匀。</p><h3 id="map-过程产生大量对象导致内存溢出"><a href="#map-过程产生大量对象导致内存溢出" class="headerlink" title="map 过程产生大量对象导致内存溢出"></a><strong>map 过程产生大量对象导致内存溢出</strong></h3><p>这种溢出的原因是在单个 map 中产生了大量的对象导致的</p><p>例如：rdd.map(x=&gt;for(i &lt;- 1 to 10000) yield i.toString)，这个操作在rdd中，每个对象都产生了10000 个对象，这肯定很容易产生内存溢出的问题。</p><p>针对这种问题，在不增加内存的情况下，可以通过减少每个 Task 的大小，以便达到每个 Task 即使产生大量的对象 Executor 的内存也能够装得下。具体做法可以在会产生大量对象的 map 操作之前调用 repartition方法，分区成更小的块传入map。</p><p>例如：rdd.repartition(10000).map(x=&gt;for(i &lt;- 1 to 10000) yield i.toString)</p><h2 id="参数"><a href="#参数" class="headerlink" title="参数"></a><strong>参数</strong></h2><h3 id="spark-driver-memory"><a href="#spark-driver-memory" class="headerlink" title="spark.driver.memory"></a>spark.driver.memory</h3><p>用来设置 Driver 的内存。在 Spark 程序中，SparkContext，DAGScheduler 都是运行在Driver端的。对应Stage 切分也是在 Driver 端运行，如果用户自己写的程序有过多的步骤，切分出过多的 Stage，这部分信息消耗的是 Driver 的内存，这个时候就需要调大 Driver 的内存</p>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark 调优</title>
      <link href="2019/11/19/Spark%20%E4%BC%98%E5%8C%96/"/>
      <url>2019/11/19/Spark%20%E4%BC%98%E5%8C%96/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p>在执行 Spark 的应用程序时，Spark集群会启动 Driver 和 Executor 两种 JVM 进程，前者为主控进程，负责创建 Spark 上下文，提交 Spark 作业[Job]，并将作业转化为计算任务[Task]，在各个 Executor 进程间协调任务的调度；后者负责在工作节点上执行具体的计算任务，并将结果返回给 Driver， 同时为需要持久化的 RDD 提供存储功能。由于 Driver 的内存管理相对来说较为简单，本节主要对 Executor 的内存管理进行分析，下文中的 Spark 内存均特指 Executor 的内存。</p><a id="more"></a><h3 id=""><a href="#" class="headerlink" title=""></a></h3>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>智慧出行服务体系建设的开发之订单和轨迹监控</title>
      <link href="2019/11/13/%E6%99%BA%E6%85%A7%E5%87%BA%E8%A1%8C%E6%9C%8D%E5%8A%A1%E4%BD%93%E7%B3%BB%E5%BB%BA%E8%AE%BE%E7%9A%84%E5%BC%80%E5%8F%91%E4%B9%8B%E8%AE%A2%E5%8D%95%E5%92%8C%E8%BD%A8%E8%BF%B9%E7%9B%91%E6%8E%A7/"/>
      <url>2019/11/13/%E6%99%BA%E6%85%A7%E5%87%BA%E8%A1%8C%E6%9C%8D%E5%8A%A1%E4%BD%93%E7%B3%BB%E5%BB%BA%E8%AE%BE%E7%9A%84%E5%BC%80%E5%8F%91%E4%B9%8B%E8%AE%A2%E5%8D%95%E5%92%8C%E8%BD%A8%E8%BF%B9%E7%9B%91%E6%8E%A7/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><h2 id="1、实时订单统计"><a href="#1、实时订单统计" class="headerlink" title="1、实时订单统计"></a>1、实时订单统计</h2><p>订单数据处理流程:</p><p>1.shell播放脚本读取订单数据到指定订单文件中.</p><p>2.使用flume监听订单文件，实时将订单数据发送到Kafka.</p><p>3.使用Spark streaming处理统计订单数据和乘车人数保存到redis中.</p><p>4.页面请求Java中台相应restful接口，restful接口查询redis中的数据返回页面，然后页面渲染显示.</p><p><img src="/images/didi/1571121383981.png" alt="1571121383981"></p><h3 id="1-1-订单数据回放"><a href="#1-1-订单数据回放" class="headerlink" title="1.1 订单数据回放"></a>1.1 订单数据回放</h3><p> 1.安装配置 Flume</p><p>flume agent配置:</p><p>代理名称：配置如下:a1(按照业务功能自定义一个名称即可)</p><p><img src="/images/didi/1571053796134.png" alt="1571053796134"></p><p>配置文件内容如下:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">a1.sources=r1</span><br><span class="line">a1.sinks=k1</span><br><span class="line">a1.channels=c1</span><br><span class="line">a1.sources.r1.type=exec</span><br><span class="line"><span class="meta">#</span><span class="bash">先使用tail -F的方式，随后做优化</span></span><br><span class="line">a1.sources.r1.command=tail -F /root/order/order</span><br><span class="line">a1.sources.r1.fileHeader=true</span><br><span class="line">a1.sinks.k1.type=org.apache.flume.sink.kafka.KafkaSink</span><br><span class="line">a1.sinks.k1.topic=hai_kou_order_topic</span><br><span class="line">a1.sinks.k1.brokerList=cdh-node01:9092,cdh-node02:9092,cdh-node03:9092</span><br><span class="line">a1.sinks.k1.batchSize=20</span><br><span class="line">a1.sinks.k1.requiredAcks=1</span><br><span class="line">a1.sinks.k1.producer.linger.ms=1</span><br><span class="line">a1.sinks.k1.producer.compression.type=snappy</span><br><span class="line">a1.channels.c1.type=memory</span><br><span class="line">a1.channels.c1.capacity=1000</span><br><span class="line">a1.channels.c1.transactionCapacity=100</span><br><span class="line">a1.sources.r1.channels=c1</span><br><span class="line">a1.sinks.k1.channel=c1</span><br></pre></td></tr></table></figure><p>2.kafka manager工具安装</p><p>此工具主要用作kafka主题消息的监控，主题增加，删除等操作.</p><p>3.消费kafka中的订单数据数据代码实现.</p><h3 id="1-2-数据回放的断点续传解决方案"><a href="#1-2-数据回放的断点续传解决方案" class="headerlink" title="1.2 数据回放的断点续传解决方案"></a>1.2 数据回放的断点续传解决方案</h3><p>问题背景:</p><p> 通常我们使用flume和kafka集成，都是使用flume监控文件,会在配置source时的命令，例如:tail -F 文件名,这种方式依然会存在一个问题，但flume的agent进程由于各种原因挂掉一段时间之后，</p><p>解决方案:</p><p>1.第一种方案,是在使用tail -F命令的地方修改</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a1.sources.r2.command=</span><br><span class="line">tail  -n +$(tail -n1 /root/log) -F /root/data/nginx.log | awk &#x27;ARGIND==1&#123;i=$0;next&#125;&#123;i++;if($0~/^tail/)&#123;i=0&#125;;print $0;print i &gt;&gt; &quot;/root/log&quot;;fflush(&quot;&quot;)&#125;&#x27; /root/log</span><br></pre></td></tr></table></figure><p>2.第二种方案,高版本的flume可以使用tailDir Souce</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">a1.sources.s1.type = TAILDIR</span><br><span class="line">a1.sources.s1.positionFile = /home/dev/flume/flume-1.8.0/log/taildir_position.json</span><br><span class="line">a1.sources.s1.filegroups = f1</span><br><span class="line">a1.sources.s1.filegroups.f1 = /home/dev/log/moercredit/logstash.log</span><br><span class="line">a1.sources.s1.headers.f1.headerKey1 = aaa</span><br><span class="line">a1.sources.s1.fileHeader = true</span><br></pre></td></tr></table></figure><h3 id="1-3-实时订单数据统计（订单情况、乘车人数情况）"><a href="#1-3-实时订单数据统计（订单情况、乘车人数情况）" class="headerlink" title="1.3 实时订单数据统计（订单情况、乘车人数情况）"></a>1.3 实时订单数据统计（订单情况、乘车人数情况）</h3><p>OrderStreamingProcessor类</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.cartravel.spark</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.cartravel.common.&#123;<span class="type">Constants</span>, <span class="type">TopicName</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.serialization.<span class="type">StringDeserializer</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.kafka010._</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.kafka010.<span class="type">LocationStrategies</span>.<span class="type">PreferConsistent</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.kafka010.<span class="type">ConsumerStrategies</span>.<span class="type">Subscribe</span></span><br><span class="line"><span class="keyword">import</span> org.apache.log4j.<span class="type">Level</span></span><br><span class="line"><span class="keyword">import</span> org.apache.log4j.<span class="type">Logger</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> scala.collection.<span class="type">JavaConversions</span>._</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * 订单数据流处理程</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">OrderStreamingProcessor</span> </span>&#123;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Order</span>(<span class="params">oderId: <span class="type">String</span></span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">object</span> <span class="title">OrderStreamingProcessor</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">import</span> org.apache.spark._</span><br><span class="line">    <span class="keyword">import</span> org.apache.spark.streaming._</span><br><span class="line">    <span class="comment">//设置Spark程序在控制台中的日志打印级别</span></span><br><span class="line">    <span class="type">Logger</span>.getLogger(<span class="string">&quot;org&quot;</span>).setLevel(<span class="type">Level</span>.<span class="type">WARN</span>)</span><br><span class="line">    <span class="comment">//local[*]使用本地模式运行，*表示内部会自动计算CPU核数，也可以直接指定运行线程数比如2，就是local[2]</span></span><br><span class="line">    <span class="comment">//表示使用两个线程来模拟spark集群</span></span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">&quot;OrderMonitor&quot;</span>).setMaster(<span class="string">&quot;local[1]&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//初始化Spark Streaming环境</span></span><br><span class="line">    <span class="keyword">val</span> streamingContext = <span class="keyword">new</span> <span class="type">StreamingContext</span>(conf, <span class="type">Seconds</span>(<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment">//设置检查点</span></span><br><span class="line">    streamingContext.checkpoint(<span class="string">&quot;/sparkapp/tmp&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//&quot;auto.offset.reset&quot; -&gt; &quot;earliest&quot;</span></span><br><span class="line">    <span class="comment">//</span></span><br><span class="line">    <span class="keyword">val</span> kafkaParams = <span class="type">Map</span>[<span class="type">String</span>, <span class="type">Object</span>](</span><br><span class="line"><span class="comment">//      &quot;bootstrap.servers&quot; -&gt; &quot;192.168.21.173:6667,192.168.21.174:6667,192.168.21.175:6667&quot;,</span></span><br><span class="line">      <span class="string">&quot;bootstrap.servers&quot;</span> -&gt; <span class="type">Constants</span>.<span class="type">KAFKA_BOOTSTRAP_SERVERS</span>,</span><br><span class="line">      <span class="string">&quot;key.deserializer&quot;</span> -&gt; classOf[<span class="type">StringDeserializer</span>],</span><br><span class="line">      <span class="string">&quot;value.deserializer&quot;</span> -&gt; classOf[<span class="type">StringDeserializer</span>],</span><br><span class="line">      <span class="string">&quot;group.id&quot;</span> -&gt; <span class="string">&quot;test0001&quot;</span>,</span><br><span class="line">      <span class="string">&quot;auto.offset.reset&quot;</span> -&gt; <span class="string">&quot;latest&quot;</span>,</span><br><span class="line">      <span class="string">&quot;enable.auto.commit&quot;</span> -&gt; (<span class="literal">false</span>: java.lang.<span class="type">Boolean</span>)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> topics = <span class="type">Array</span>(</span><br><span class="line">      <span class="type">TopicName</span>.<span class="type">HAI_KOU_ORDER_TOPIC</span>.getTopicName,</span><br><span class="line">      <span class="type">TopicName</span>.<span class="type">CHENG_DU_ORDER_TOPIC</span>.getTopicName,</span><br><span class="line">      <span class="type">TopicName</span>.<span class="type">XI_AN_ORDER_TOPIC</span>.getTopicName</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    topics.foreach(println(_))</span><br><span class="line">    println(<span class="string">&quot;topics:&quot;</span> + topics)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> stream = <span class="type">KafkaUtils</span>.createDirectStream[<span class="type">String</span>, <span class="type">String</span>](</span><br><span class="line">      streamingContext,</span><br><span class="line">      <span class="type">PreferConsistent</span>,</span><br><span class="line">      <span class="type">Subscribe</span>[<span class="type">String</span>, <span class="type">String</span>](topics, kafkaParams)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    stream.count().print();</span><br><span class="line"></span><br><span class="line">    <span class="comment">//实时统计订单总数</span></span><br><span class="line">    <span class="keyword">val</span> ordersDs = stream.map(record =&gt; &#123;</span><br><span class="line">      <span class="comment">//主题名称</span></span><br><span class="line">      <span class="keyword">val</span> topicName = record.topic()</span><br><span class="line">      <span class="keyword">val</span> orderInfo = record.value()</span><br><span class="line"></span><br><span class="line">      <span class="comment">//订单信息解析器</span></span><br><span class="line">      <span class="keyword">var</span> orderParser: <span class="type">OrderParser</span> = <span class="literal">null</span>;</span><br><span class="line"></span><br><span class="line">      <span class="comment">//不同主题的订单进行不同的处理</span></span><br><span class="line">      topicName <span class="keyword">match</span> &#123;</span><br><span class="line">        <span class="keyword">case</span> <span class="string">&quot;hai_kou_order_topic&quot;</span> =&gt; &#123;</span><br><span class="line">          orderParser = <span class="keyword">new</span> <span class="type">HaiKouOrderParser</span>();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">case</span> <span class="string">&quot;cheng_du_order_topic&quot;</span> =&gt; &#123;</span><br><span class="line">          orderParser = <span class="keyword">new</span> <span class="type">ChengDuOrderParser</span>()</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">case</span> <span class="string">&quot;xi_an_order_topic&quot;</span> =&gt; &#123;</span><br><span class="line">          orderParser = <span class="keyword">new</span> <span class="type">XiAnOrderParser</span>()</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">case</span> _ =&gt; &#123;</span><br><span class="line">          orderParser = <span class="literal">null</span>;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      println(<span class="string">&quot;orderParser:&quot;</span> + orderParser)</span><br><span class="line">      <span class="keyword">if</span> (<span class="literal">null</span> != orderParser) &#123;</span><br><span class="line">        <span class="keyword">val</span> order = orderParser.parser(orderInfo)</span><br><span class="line">        println(<span class="string">&quot;parser order:&quot;</span> + order)</span><br><span class="line">        order</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="literal">null</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">//订单计数,对于每个订单出现一次计数1</span></span><br><span class="line">    <span class="keyword">val</span> orderCountRest = ordersDs.map(order =&gt; &#123;</span><br><span class="line">      <span class="keyword">if</span> (<span class="literal">null</span> == order) &#123;</span><br><span class="line">        (<span class="string">&quot;&quot;</span>, <span class="number">0</span>)</span><br><span class="line">      &#125; <span class="keyword">else</span> <span class="keyword">if</span> (order.getClass == classOf[<span class="type">ChengDuTravelOrder</span>]) &#123;</span><br><span class="line">        (<span class="type">Constants</span>.<span class="type">CITY_CODE_CHENG_DU</span> + <span class="string">&quot;_&quot;</span> + order.createDay, <span class="number">1</span>)</span><br><span class="line">      &#125; <span class="keyword">else</span> <span class="keyword">if</span> (order.getClass == classOf[<span class="type">XiAnTravelOrder</span>]) &#123;</span><br><span class="line">        (<span class="type">Constants</span>.<span class="type">CITY_CODE_XI_AN</span> + <span class="string">&quot;_&quot;</span> + order.createDay, <span class="number">1</span>)</span><br><span class="line">      &#125; <span class="keyword">else</span> <span class="keyword">if</span> (order.getClass == classOf[<span class="type">HaiKouTravelOrder</span>]) &#123;</span><br><span class="line">        (<span class="type">Constants</span>.<span class="type">CITY_CODE_HAI_KOU</span> + <span class="string">&quot;_&quot;</span> + order.createDay, <span class="number">1</span>)</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        (<span class="string">&quot;&quot;</span>, <span class="number">0</span>)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;).updateStateByKey((currValues: <span class="type">Seq</span>[<span class="type">Int</span>], state: <span class="type">Option</span>[<span class="type">Int</span>]) =&gt; &#123;</span><br><span class="line">      <span class="keyword">var</span> count = currValues.sum + state.getOrElse(<span class="number">0</span>);</span><br><span class="line">      <span class="type">Some</span>(count)</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">      * 乘车人数统计</span></span><br><span class="line"><span class="comment">      * 如果是成都或者西安的订单，数据中没有乘车人数字段，所有按照默认一单一人的方式进行统计</span></span><br><span class="line"><span class="comment">      * 海口的订单数据中有乘车人数字段，就按照具体数进行统计</span></span><br><span class="line"><span class="comment">      */</span></span><br><span class="line">    <span class="keyword">val</span> passengerCountRest = ordersDs.map(order =&gt; &#123;</span><br><span class="line">      <span class="keyword">if</span> (<span class="literal">null</span> == order) &#123;</span><br><span class="line">        (<span class="string">&quot;&quot;</span>, <span class="number">0</span>)</span><br><span class="line">      &#125; <span class="keyword">else</span> <span class="keyword">if</span> (order.getClass == classOf[<span class="type">ChengDuTravelOrder</span>]) &#123;</span><br><span class="line">        (<span class="type">Constants</span>.<span class="type">CITY_CODE_CHENG_DU</span> + <span class="string">&quot;_&quot;</span> + order.createDay, <span class="number">1</span>)</span><br><span class="line">      &#125; <span class="keyword">else</span> <span class="keyword">if</span> (order.getClass == classOf[<span class="type">XiAnTravelOrder</span>]) &#123;</span><br><span class="line">        (<span class="type">Constants</span>.<span class="type">CITY_CODE_XI_AN</span> + <span class="string">&quot;_&quot;</span> + order.createDay, <span class="number">1</span>)</span><br><span class="line">      &#125; <span class="keyword">else</span> <span class="keyword">if</span> (order.getClass == classOf[<span class="type">HaiKouTravelOrder</span>]) &#123;</span><br><span class="line">        <span class="keyword">var</span> passengerCount = order.asInstanceOf[<span class="type">HaiKouTravelOrder</span>].passengerCount.toInt</span><br><span class="line">        <span class="comment">//scala不支持类似java中的三目运算符，可以使用下面的操作方式</span></span><br><span class="line">        passengerCount = <span class="keyword">if</span>(passengerCount&gt;<span class="number">0</span>) passengerCount <span class="keyword">else</span> <span class="number">1</span></span><br><span class="line">        (<span class="type">Constants</span>.<span class="type">CITY_CODE_HAI_KOU</span> + <span class="string">&quot;_&quot;</span> + order.createDay,passengerCount)</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        (<span class="string">&quot;&quot;</span>, <span class="number">0</span>)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;).updateStateByKey((currValues: <span class="type">Seq</span>[<span class="type">Int</span>], state: <span class="type">Option</span>[<span class="type">Int</span>]) =&gt; &#123;</span><br><span class="line">      <span class="keyword">var</span> count = currValues.sum + state.getOrElse(<span class="number">0</span>);</span><br><span class="line">      <span class="type">Some</span>(count)</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    orderCountRest.foreachRDD(orderCountRDD=&gt;&#123;</span><br><span class="line">      <span class="keyword">import</span> com.cartravel.util.<span class="type">JedisUtil</span></span><br><span class="line">      <span class="keyword">val</span> jedisUtil = <span class="type">JedisUtil</span>.getInstance()</span><br><span class="line">      <span class="keyword">val</span> jedis = jedisUtil.getJedis</span><br><span class="line">      <span class="keyword">val</span> orderCountRest = orderCountRDD.collect()</span><br><span class="line">      println(<span class="string">&quot;orderCountRest:&quot;</span>+orderCountRest)</span><br><span class="line">      orderCountRest.foreach(countrest=&gt;&#123;</span><br><span class="line">        println(<span class="string">&quot;countrest:&quot;</span>+countrest._1+<span class="string">&quot;,&quot;</span>+countrest._2)</span><br><span class="line">        <span class="keyword">if</span>(<span class="literal">null</span>!=countrest)&#123;</span><br><span class="line">          jedis.hset(<span class="type">Constants</span>.<span class="type">ORDER_COUNT</span>, countrest._1, countrest._2 + <span class="string">&quot;&quot;</span>)</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;)</span><br><span class="line">      jedisUtil.returnJedis(jedis)</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    passengerCountRest.foreachRDD(passengerCountRdd=&gt;&#123;</span><br><span class="line">      <span class="keyword">import</span> com.cartravel.util.<span class="type">JedisUtil</span></span><br><span class="line">      <span class="keyword">val</span> jedisUtil = <span class="type">JedisUtil</span>.getInstance()</span><br><span class="line">      <span class="keyword">val</span> jedis = jedisUtil.getJedis</span><br><span class="line"></span><br><span class="line">      <span class="keyword">val</span> passengerCountRest = passengerCountRdd.collect()</span><br><span class="line">      passengerCountRest.foreach(countrest=&gt;&#123;</span><br><span class="line">        jedis.hset(<span class="type">Constants</span>.<span class="type">PASSENGER_COUNT</span>, countrest._1, countrest._2 + <span class="string">&quot;&quot;</span>)</span><br><span class="line">      &#125;)</span><br><span class="line">      jedisUtil.returnJedis(jedis)</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//启动sparkstreaming程序</span></span><br><span class="line">    streamingContext.start();</span><br><span class="line">    streamingContext.awaitTermination();</span><br><span class="line">    streamingContext.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="2、全域订单轨迹监控"><a href="#2、全域订单轨迹监控" class="headerlink" title="2、全域订单轨迹监控"></a>2、全域订单轨迹监控</h2><h3 id="2-1-实时订单轨迹监控"><a href="#2-1-实时订单轨迹监控" class="headerlink" title="2.1 实时订单轨迹监控"></a>2.1 实时订单轨迹监控</h3><p>​    盖亚数据计划开放的开源数据集中是已经生成的订单轨迹数据所以是不知道订单什么时候结束，真实的业务场景中是有开始和技术的标志位，但是我们可以在数据中认为的设置开始和技术标记，可以这么做在数据的开始开可以设置start字符串在数据的技术可以设置end技术的字符串，使用start和end字符串作为订单轨迹数据的开始和结束.</p><p>实现流程:</p><p><img src="/images/didi/1571215041738.png" alt="1571215041738"></p><p>消费轨迹数据</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br></pre></td><td class="code"><pre><span class="line">com.cartravel.kafka.<span class="keyword">package</span> com.cartravel.kafka;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.cartravel.common.Constants;</span><br><span class="line"><span class="keyword">import</span> com.cartravel.common.Order;</span><br><span class="line"><span class="keyword">import</span> com.cartravel.common.TopicName;</span><br><span class="line"><span class="keyword">import</span> com.cartravel.util.HBaseUtil;</span><br><span class="line"><span class="keyword">import</span> com.cartravel.util.JedisUtil;</span><br><span class="line"><span class="keyword">import</span> com.cartravel.util.ObjUtil;</span><br><span class="line"><span class="keyword">import</span> org.apache.commons.lang.StringUtils;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.client.Put;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.client.Table;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.Consumer;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecord;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecords;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.KafkaConsumer;</span><br><span class="line"><span class="keyword">import</span> org.apache.log4j.Level;</span><br><span class="line"><span class="keyword">import</span> org.apache.log4j.Logger;</span><br><span class="line"><span class="keyword">import</span> redis.clients.jedis.Jedis;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.BufferedOutputStream;</span><br><span class="line"><span class="keyword">import</span> java.io.FileOutputStream;</span><br><span class="line"><span class="keyword">import</span> java.io.PrintWriter;</span><br><span class="line"><span class="keyword">import</span> java.text.SimpleDateFormat;</span><br><span class="line"><span class="keyword">import</span> java.util.*;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">GpsConsumer</span> <span class="keyword">implements</span> <span class="title">Runnable</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> Logger log = Logger.getLogger(GpsConsumer.class);</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span>  KafkaConsumer&lt;String, String&gt; consumer;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> String topic;</span><br><span class="line">    <span class="comment">//计数消费到的消息条数</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">int</span> count = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">private</span> FileOutputStream file = <span class="keyword">null</span>;</span><br><span class="line">    <span class="keyword">private</span> BufferedOutputStream out = <span class="keyword">null</span>;</span><br><span class="line">    <span class="keyword">private</span> PrintWriter printWriter = <span class="keyword">null</span>;</span><br><span class="line">    <span class="keyword">private</span> String lineSeparator = <span class="keyword">null</span>;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> batchNum = <span class="number">0</span>;</span><br><span class="line">    JedisUtil instance = <span class="keyword">null</span>;</span><br><span class="line">    Jedis jedis = <span class="keyword">null</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> String cityCode = <span class="string">&quot;&quot;</span>;</span><br><span class="line">    <span class="keyword">private</span> Map&lt;String, String&gt; gpsMap = <span class="keyword">new</span> HashMap&lt;String, String&gt;();</span><br><span class="line">    SimpleDateFormat sdf = <span class="keyword">new</span> SimpleDateFormat(<span class="string">&quot;yyyy-MM-dd HH:mm:ss&quot;</span>);</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">GpsConsumer</span><span class="params">(String topic, String groupId)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (topic.equalsIgnoreCase(TopicName.CHENG_DU_GPS_TOPIC.getTopicName())) &#123;</span><br><span class="line">            cityCode = Constants.CITY_CODE_CHENG_DU;</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (topic.equalsIgnoreCase(TopicName.XI_AN_GPS_TOPIC.getTopicName())) &#123;</span><br><span class="line">            cityCode = Constants.CITY_CODE_XI_AN;</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (topic.equalsIgnoreCase(TopicName.HAI_KOU_ORDER_TOPIC.getTopicName())) &#123;</span><br><span class="line">            cityCode = Constants.CITY_CODE_HAI_KOU;</span><br><span class="line">        &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> IllegalArgumentException(topic+<span class="string">&quot;,主题名称不合法!&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line"></span><br><span class="line">        <span class="comment">//dev-hdp</span></span><br><span class="line"><span class="comment">//        props.put(&quot;bootstrap.servers&quot;, &quot;192.168.21.173:6667,192.168.21.174:6667,192.168.21.175:6667&quot;);</span></span><br><span class="line">        <span class="comment">//dev-cdh</span></span><br><span class="line"><span class="comment">//        props.put(&quot;bootstrap.servers&quot;, &quot;192.168.21.177:9092,192.168.21.178:9092,192.168.21.179:9092&quot;);</span></span><br><span class="line">        <span class="comment">//pro-cdh</span></span><br><span class="line">        props.put(<span class="string">&quot;bootstrap.servers&quot;</span>, Constants.KAFKA_BOOTSTRAP_SERVERS);</span><br><span class="line"></span><br><span class="line"><span class="comment">//        props.put(&quot;bootstrap.servers&quot;, &quot;192.168.21.178:9092&quot;);</span></span><br><span class="line"></span><br><span class="line">        props.put(<span class="string">&quot;group.id&quot;</span>, groupId);</span><br><span class="line">        props.put(<span class="string">&quot;enable.auto.commit&quot;</span>, <span class="string">&quot;true&quot;</span>);</span><br><span class="line">        props.put(<span class="string">&quot;auto.offset.reset&quot;</span>, <span class="string">&quot;earliest&quot;</span>);</span><br><span class="line"><span class="comment">//        props.put(&quot;auto.offset.reset&quot;, &quot;latest&quot;);</span></span><br><span class="line">        props.put(<span class="string">&quot;session.timeout.ms&quot;</span>, <span class="string">&quot;30000&quot;</span>);</span><br><span class="line">        props.put(<span class="string">&quot;key.deserializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;</span>);</span><br><span class="line">        props.put(<span class="string">&quot;value.deserializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;</span>);</span><br><span class="line"></span><br><span class="line">        consumer = <span class="keyword">new</span> KafkaConsumer&lt;String,String&gt;(props);</span><br><span class="line">        <span class="keyword">this</span>.topic = topic;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">                doWork();</span><br><span class="line">            &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">                e.printStackTrace();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">doWork</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        batchNum++;</span><br><span class="line">        consumer.subscribe(Collections.singletonList(<span class="keyword">this</span>.topic));</span><br><span class="line">        ConsumerRecords&lt;String, String&gt; records = consumer.poll(<span class="number">1000</span>);</span><br><span class="line">        System.out.println(<span class="string">&quot;第&quot;</span> + batchNum + <span class="string">&quot;批次,&quot;</span> + records.count());</span><br><span class="line">        <span class="comment">//司机ID</span></span><br><span class="line">        String driverId = <span class="string">&quot;&quot;</span>;</span><br><span class="line">        <span class="comment">//订单ID</span></span><br><span class="line">        String orderId = <span class="string">&quot;&quot;</span>;</span><br><span class="line">        <span class="comment">//经度</span></span><br><span class="line">        String lng = <span class="string">&quot;&quot;</span>;</span><br><span class="line">        <span class="comment">//维度</span></span><br><span class="line">        String lat = <span class="string">&quot;&quot;</span>;</span><br><span class="line">        <span class="comment">//时间戳</span></span><br><span class="line">        String timestamp = <span class="string">&quot;&quot;</span>;</span><br><span class="line">        Order order = <span class="keyword">null</span>;</span><br><span class="line">        Order startEndTimeOrder = <span class="keyword">null</span>;</span><br><span class="line">        Object tmpOrderObj = <span class="keyword">null</span>;</span><br><span class="line">        <span class="keyword">if</span> (records.count() &gt; <span class="number">0</span>) &#123;</span><br><span class="line">            Table table = HBaseUtil.getTable(Constants.HTAB_GPS);</span><br><span class="line">            JedisUtil instance = JedisUtil.getInstance();</span><br><span class="line">            jedis = instance.getJedis();</span><br><span class="line">            List&lt;Put&gt; puts = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">            String rowkey = <span class="string">&quot;&quot;</span>;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (gpsMap.size() &gt; <span class="number">0</span>) &#123;</span><br><span class="line">                gpsMap.clear();</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="comment">//表不存在时创建表</span></span><br><span class="line">            <span class="keyword">if</span> (!HBaseUtil.tableExists(Constants.HTAB_GPS)) &#123;</span><br><span class="line">                HBaseUtil.createTable(HBaseUtil.getConnection(), Constants.HTAB_GPS, Constants.DEFAULT_FAMILY);</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; record : records) &#123;</span><br><span class="line">                count++;</span><br><span class="line">                log.warn(<span class="string">&quot;Received message: (&quot;</span> + record.key() + <span class="string">&quot;, &quot;</span> + record.value() + <span class="string">&quot;) at offset &quot;</span> +</span><br><span class="line">                        record.offset() + <span class="string">&quot;,count:&quot;</span> + count);</span><br><span class="line">                String value = record.value();</span><br><span class="line">                <span class="keyword">if</span> (value.contains(<span class="string">&quot;,&quot;</span>)) &#123;</span><br><span class="line">                    order = <span class="keyword">new</span> Order();</span><br><span class="line">                    String[] split = value.split(<span class="string">&quot;,&quot;</span>);</span><br><span class="line">                    driverId = split[<span class="number">0</span>];</span><br><span class="line">                    orderId = split[<span class="number">1</span>];</span><br><span class="line">                    timestamp = split[<span class="number">2</span>];</span><br><span class="line">                    lng = split[<span class="number">3</span>];</span><br><span class="line">                    lat = split[<span class="number">4</span>];</span><br><span class="line"></span><br><span class="line">                    rowkey = orderId + <span class="string">&quot;_&quot;</span> + timestamp;</span><br><span class="line">                    gpsMap.put(<span class="string">&quot;CITYCODE&quot;</span>, cityCode);</span><br><span class="line">                    gpsMap.put(<span class="string">&quot;DRIVERID&quot;</span>, driverId);</span><br><span class="line">                    gpsMap.put(<span class="string">&quot;ORDERID&quot;</span>, orderId);</span><br><span class="line">                    gpsMap.put(<span class="string">&quot;TIMESTAMP&quot;</span>, timestamp + <span class="string">&quot;&quot;</span>);</span><br><span class="line">                    gpsMap.put(<span class="string">&quot;TIME&quot;</span>, sdf.format(<span class="keyword">new</span> Date(Long.parseLong(timestamp+<span class="string">&quot;000&quot;</span>))));</span><br><span class="line">                    gpsMap.put(<span class="string">&quot;LNG&quot;</span>, lng);</span><br><span class="line">                    gpsMap.put(<span class="string">&quot;LAT&quot;</span>, lat);</span><br><span class="line"></span><br><span class="line">                    order.setOrderId(orderId);</span><br><span class="line"></span><br><span class="line">                    puts.add(HBaseUtil.createPut(rowkey, Constants.DEFAULT_FAMILY.getBytes(), gpsMap));</span><br><span class="line"></span><br><span class="line">                    <span class="comment">//1.存入实时订单单号</span></span><br><span class="line">                    jedis.sadd(Constants.REALTIME_ORDERS, cityCode + <span class="string">&quot;_&quot;</span> + orderId);</span><br><span class="line">                    <span class="comment">//2.存入实时订单的经纬度信息</span></span><br><span class="line">                    jedis.lpush(cityCode + <span class="string">&quot;_&quot;</span> + orderId, lng + <span class="string">&quot;,&quot;</span> + lat);</span><br><span class="line">                    <span class="comment">//3.存入订单的开始结束时间信息</span></span><br><span class="line"></span><br><span class="line">                    <span class="keyword">byte</span>[] orderBytes = jedis.hget(Constants.ORDER_START_ENT_TIME.getBytes()</span><br><span class="line">                            , orderId.getBytes());</span><br><span class="line"></span><br><span class="line">                    <span class="keyword">if</span> (orderBytes != <span class="keyword">null</span>) &#123;</span><br><span class="line">                        tmpOrderObj = ObjUtil.deserialize(orderBytes);</span><br><span class="line">                    &#125;</span><br><span class="line"></span><br><span class="line">                    <span class="keyword">if</span> (<span class="keyword">null</span> != tmpOrderObj) &#123;</span><br><span class="line">                        startEndTimeOrder = (Order) tmpOrderObj;</span><br><span class="line">                        startEndTimeOrder.setEndTime(Long.parseLong(timestamp+<span class="string">&quot;000&quot;</span>));</span><br><span class="line">                        jedis.hset(Constants.ORDER_START_ENT_TIME.getBytes(), orderId.getBytes(),</span><br><span class="line">                                ObjUtil.serialize(startEndTimeOrder));</span><br><span class="line">                    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                        <span class="comment">//第一次写入订单的开始时间,开始时间和结束时间一样</span></span><br><span class="line">                        order.setStartTime(Long.parseLong(timestamp));</span><br><span class="line">                        order.setEndTime(Long.parseLong(timestamp));</span><br><span class="line">                        jedis.hset(Constants.ORDER_START_ENT_TIME.getBytes(), orderId.getBytes(),</span><br><span class="line">                                ObjUtil.serialize(order));</span><br><span class="line">                    &#125;</span><br><span class="line">                    hourOrderInfoGather(jedis,gpsMap);</span><br><span class="line">                &#125; <span class="keyword">else</span> <span class="keyword">if</span> (value.contains(<span class="string">&quot;end&quot;</span>)) &#123;</span><br><span class="line">                    jedis.lpush(cityCode + <span class="string">&quot;_&quot;</span> + orderId, value);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            table.put(puts);</span><br><span class="line">            instance.returnJedis(jedis);</span><br><span class="line">        &#125;</span><br><span class="line">        log.warn(<span class="string">&quot;正常结束...&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 统计城市的每小时的订单信息和订单数</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> Exception</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span>  <span class="keyword">void</span> <span class="title">hourOrderInfoGather</span><span class="params">(Jedis jedis,Map&lt;String, String&gt; gpsMap)</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">        String time = gpsMap.get(<span class="string">&quot;TIME&quot;</span>);</span><br><span class="line">        String orderId = gpsMap.get(<span class="string">&quot;ORDERID&quot;</span>);</span><br><span class="line">        String day = time.substring(<span class="number">0</span>,time.indexOf(<span class="string">&quot; &quot;</span>));</span><br><span class="line">        String hour = time.split(<span class="string">&quot; &quot;</span>)[<span class="number">1</span>].substring(<span class="number">0</span>,<span class="number">2</span>);</span><br><span class="line">        <span class="comment">//redis表名,小时订单统计</span></span><br><span class="line">        String hourOrderCountTab = cityCode+<span class="string">&quot;_&quot;</span>+day+<span class="string">&quot;_hour_order_count&quot;</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//redis表名,小时订单ID</span></span><br><span class="line">        String hourOrderField = cityCode+<span class="string">&quot;_&quot;</span>+day+<span class="string">&quot;_&quot;</span>+hour;</span><br><span class="line">        String hourOrder = cityCode+<span class="string">&quot;_order&quot;</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">int</span> hourOrderCount = <span class="number">0</span>;</span><br><span class="line">        <span class="comment">//redis set集合中存放每小时内的所有订单id</span></span><br><span class="line">        <span class="keyword">if</span>(!jedis.sismember(hourOrder,orderId))&#123;</span><br><span class="line">            <span class="comment">//使用set存储小时订单id</span></span><br><span class="line">            jedis.sadd(hourOrder,orderId);</span><br><span class="line">            String hourOrdernum = jedis.hget(hourOrderCountTab, hourOrderField);</span><br><span class="line">            <span class="keyword">if</span>(StringUtils.isEmpty(hourOrdernum))&#123;</span><br><span class="line">                hourOrderCount = <span class="number">1</span>;</span><br><span class="line">            &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">                hourOrderCount = Integer.parseInt(hourOrdernum)+<span class="number">1</span>;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="comment">//HashMap 存储每个小时的订单总数</span></span><br><span class="line">            jedis.hset(hourOrderCountTab,hourOrderField,hourOrderCount+<span class="string">&quot;&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        Logger.getLogger(<span class="string">&quot;org.apache.kafka&quot;</span>).setLevel(Level.INFO);</span><br><span class="line">        <span class="comment">//kafka主题</span></span><br><span class="line">        String topic = <span class="string">&quot;cheng_du_gps_topic&quot;</span>;</span><br><span class="line">        <span class="comment">//消费组id</span></span><br><span class="line">        String groupId = <span class="string">&quot;cheng_du_gps_consumer_01&quot;</span>;</span><br><span class="line"></span><br><span class="line">        GpsConsumer gpsConsumer = <span class="keyword">new</span> GpsConsumer(topic, groupId);</span><br><span class="line">        Thread start = <span class="keyword">new</span> Thread(gpsConsumer);</span><br><span class="line">        start.start();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="2-2-历史订单轨迹回放"><a href="#2-2-历史订单轨迹回放" class="headerlink" title="2.2 历史订单轨迹回放"></a>2.2 历史订单轨迹回放</h3><p><a href="https://lbs.amap.com/api/javascript-api/example/marker/replaying-historical-running-data">高德轨迹回放示例</a></p><p>功能实现流程:</p><p><img src="/images/didi/1571216562760.png" alt="1571216562760"></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line">com.cartravel.ordermonitor.TrackMonitorController</span><br><span class="line"></span><br><span class="line"> <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 查询订单历史轨迹点</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> wrapper</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@PostMapping(&quot;/historyTrackPoints&quot;)</span></span><br><span class="line">    <span class="keyword">public</span> ResultModel&lt;List&lt;TrackPoint&gt;&gt; historyTrackPoints(<span class="meta">@RequestBody</span> QueryWrapper wrapper) &#123;</span><br><span class="line">        <span class="keyword">long</span> startTime = System.currentTimeMillis();</span><br><span class="line">        logger.info(<span class="string">&quot;【查询图形(点线面)】&quot;</span>);</span><br><span class="line">        ResultModel&lt;List&lt;TrackPoint&gt;&gt; result = <span class="keyword">new</span> ResultModel&lt;List&lt;TrackPoint&gt;&gt;();</span><br><span class="line">        Object tmpOrderObj = <span class="keyword">null</span>;</span><br><span class="line">        Order startEndTimeOrder = <span class="keyword">null</span>;</span><br><span class="line">        List&lt;TrackPoint&gt; list = <span class="keyword">null</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            String orderId = wrapper.getOrderId();</span><br><span class="line">            JedisUtil instance = JedisUtil.getInstance();</span><br><span class="line">            Jedis jedis = instance.getJedis();</span><br><span class="line"></span><br><span class="line">            <span class="keyword">byte</span>[] orderBytes = jedis.hget(Constants.ORDER_START_ENT_TIME.getBytes()</span><br><span class="line">                    , orderId.getBytes());</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (orderBytes != <span class="keyword">null</span>) &#123;</span><br><span class="line">                tmpOrderObj = ObjUtil.deserialize(orderBytes);</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (<span class="keyword">null</span> != tmpOrderObj) &#123;</span><br><span class="line">                startEndTimeOrder = (Order) tmpOrderObj;</span><br><span class="line">                String starttime = startEndTimeOrder.getStartTime() + <span class="string">&quot;&quot;</span>;</span><br><span class="line">                String enttime = startEndTimeOrder.getEndTime() + <span class="string">&quot;&quot;</span>;</span><br><span class="line"></span><br><span class="line">                String tableName = Constants.HTAB_GPS;</span><br><span class="line">                list = HBaseUtil.getRest(tableName, wrapper.getOrderId(),</span><br><span class="line">                        starttime, enttime, TrackPoint.class);</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            result.setSuccess(<span class="keyword">true</span>);</span><br><span class="line">            result.setData(list);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            result.setMsg(e.getMessage());</span><br><span class="line">        &#125;</span><br><span class="line">        logger.info(<span class="string">&quot;【查询订单历史轨迹点】msg:&#123;&#125;,time:&#123;&#125;&quot;</span>, result.getMsg(),</span><br><span class="line">                System.currentTimeMillis() - startTime);</span><br><span class="line">        <span class="keyword">return</span> result;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><h1 id=""><a href="#" class="headerlink" title=""></a></h1>]]></content>
      
      
      <categories>
          
          <category> 滴滴智慧出行 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
            <tag> Kafka </tag>
            
            <tag> 滴滴智慧出行 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hadoop Federation</title>
      <link href="2019/10/15/Federation%20/"/>
      <url>2019/10/15/Federation%20/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p>HDFS Federation 是解决 NameNode 内存瓶颈问题的水平横向扩展方案。</p><a id="more"></a><h1 id="1-概述"><a href="#1-概述" class="headerlink" title="1. 概述"></a>1. 概述</h1><p>HDFS 主要有两大模块：</p><blockquote><ul><li><strong>Namespace</strong></li></ul><blockquote><p>包括目录、文件和块。</p></blockquote><blockquote><p>它支持所有和命名空间相关的文件操作，如创建、删除、修改，查看所有文件和目录。</p></blockquote><ul><li><p>**Block Storage Service ** **[块存储服务]**包括两部分</p><ul><li><p>在 namenode 中的块的管理</p><ul><li>提供 datanode 集群的注册、心跳检测等功能。</li><li>处理块的报告信息和维护块的位置信息。</li><li>支持块相关的操作，如创建、删除、修改、获取块的位置信息。</li><li>管理块的冗余信息、创建副本、删除多余的副本等。</li></ul></li><li><p>存储</p><blockquote><p>datanode 提供本地文件系统上块的存储、读写、访问等。</p></blockquote></li></ul></li></ul><p>以前的 HDFS 框架整个集群只允许有一个namenode，一个 namenode管理所有的命名空间，HDFS 联邦通过增加多个 namenode 来打破这种限制。</p><p>单 NameNode 的架构使得 HDFS 在集群扩展性和性能上都有潜在的问题。当集群大到一定程度后，NameNode 进程使用的内存可能会达到上百 G，NameNode 成为了性能的瓶颈。因而提出了 namenode 水平扩展方案– Federation</p><p>hdfs federation 即 hdfs 的联邦，可以简单理解为多个 hdfs 集群聚合到一起，更准确的理解是有多个namenode节点的 hdfs 集群</p></blockquote><h1 id="2-架构"><a href="#2-架构" class="headerlink" title="2. 架构"></a>2. 架构</h1><h2 id="2-1-概述"><a href="#2-1-概述" class="headerlink" title="2.1. 概述"></a>2.1. 概述</h2><p>HDFS Federation 是解决 NameNode 内存瓶颈问题的水平横向扩展方案。</p><p>NameNode 之间相互独立，各自管理自己的区域，且不需要互相协调，一个 NameNode 挂掉了不会影响其他的 NameNode</p><p>DataNode 被用作通用的数据存储设备，每个 DataNode 要向集群中所有的 NameNode注册，且周期性的向所有NameNode 发送心跳和报告，并执行来自所有 NameNode 的命令.</p><p>一个Block Pool由属于同一个 NameSpace 的数据块组成，每个 DataNode 可能会存储集群中所有 Block Pool 数据块，每个Block Pool内部自治，各自管理各自的Block，不会与其他 Block Pool交流<br>NameNode 和 Block Pool 一起被称作 Namespace Volume，它是管理的基本单位，当一个 namespace 被删除后，所有 datanode 上与其对应的 block pool 也会被删除。当集群升级时，每个 namespace volume 作为一个基本单元进行升级</p><h2 id="2-2-Block-Pool"><a href="#2-2-Block-Pool" class="headerlink" title="2.2.  Block Pool"></a>2.2.  Block Pool</h2><p>一个块池就是属于一个namespace的一组块。datanodes存储集群中所有的块池，它独立于其它块池进行管理。这允许namespace在不与其它namespace交互的情况下生成块的ID，有故障的namenode不影响datanode继续为集群中的其它namenode服务。一个namespace和它的blockpool一起叫做namespace volume，这是一个自己的管理单位，当一个namenode被删除，那么在datanode上的相应的block pool也会被删除。在集群进行升级的时候，每一个namespace volume独立的进行升级。</p><h2 id="2-3-ClusterID"><a href="#2-3-ClusterID" class="headerlink" title="2.3. ClusterID"></a>2.3. ClusterID</h2><p>增加一个新的ClusterID标识来在集群中所有的节点。当一个namenode被格式化的时候，这个标识被指定或自动生成，这个ID会用于格式化集群中的其它namenode。</p><h2 id="2-5-不足"><a href="#2-5-不足" class="headerlink" title="2.5. 不足"></a>2.5. 不足</h2><p>HDFS Federation 并没有完全解决单点故障问题。虽然 NameNode 存在多个，但是从单个NameNode 看，仍然存在单点故障：<br>如果某个 NameNode 挂掉了，其管理的相应的文件便不可以访问。 Federation 中每个NameNode 仍然像之前 HDFS 上实现一样，配有一个 Secondary NameNode，以便主 NameNode 挂掉，用于还原元数据信息。<br>所以一般集群规模很大的时，会采用 HA+Federation 的部署方案</p><h1 id="3-配置"><a href="#3-配置" class="headerlink" title="3. 配置"></a>3. 配置</h1>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
            <tag> 大数据 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark 架构设计</title>
      <link href="2019/10/10/Spark%E6%9E%B6%E6%9E%84/"/>
      <url>2019/10/10/Spark%E6%9E%B6%E6%9E%84/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><h1 id="Spark-核心组件"><a href="#Spark-核心组件" class="headerlink" title="Spark 核心组件"></a>Spark 核心组件</h1><h3 id="Driver"><a href="#Driver" class="headerlink" title="Driver"></a>Driver</h3><p>Spark 驱动器节点，用于执行 Spark 任务中的 main 方法， 负责实际代码的执行工作。Driver 在 Spark 作业执行时主要负责：</p><ol><li><p>将用户程序转化为任务[job]</p></li><li><p>Executor 之间调度任务task</p></li><li><p>跟踪 Executor 的执行情况；</p></li><li><p>通过 UI 展示查询运行情况</p></li></ol><h3 id="Executor"><a href="#Executor" class="headerlink" title="Executor"></a>Executor</h3><p>Spark Executor 节点是一个 JVM 进程，负责在 Spark 作业中运行 具体 任务，任 务 彼此之 间相互独立。 Spark 应用启动时， Executor 节点被同时启动， 并且 始终伴 随着整个 Spark 应用的生命周期而存在。 如果有 Executor 节点发生了故障或崩溃 ， Spark 应用也可以继续执行， 会将出错节点上的任务调度到其他 Executor 节点上继 续运行。</p><p>Executor 有两个核心功能：</p><ol><li><p>负责运行组成 Spark 应用的任务，并将结果返回给 Driver 进程；</p></li><li><p>它们通过自身的块管理器（ Block Manager ）为用户程序中要求缓存的 RDD</p></li></ol><h2 id="运行流程概述"><a href="#运行流程概述" class="headerlink" title="运行流程概述"></a>运行流程概述</h2><p>![image-20200131155431461](/Users/zxc/Library/Application Support/typora-user-images/image-20200131155431461.png)</p><h2 id="Spark-部署模式"><a href="#Spark-部署模式" class="headerlink" title="Spark 部署模式"></a>Spark 部署模式</h2><h3 id="本地模式"><a href="#本地模式" class="headerlink" title="本地模式"></a>本地模式</h3><p>Local[N] 模式，用单机的多个线程来模拟 Spark 分布式计算，直接运行在本地，便于调试，通常用来验证开发出来的应用程序逻辑上有没有问题。</p><p>其中 N 代表可以使用 N 个线程，每个线程拥有一个 core。如果不指定 N，则默认是1个线程，该线程有1个core。</p><ul><li>local 只启动一个 executor</li><li>local[k] 启动 k 个executor</li><li>local[*] 启动 和 cpu 数目相同的 executor</li></ul><h3 id="Standalone-模式"><a href="#Standalone-模式" class="headerlink" title="Standalone 模式"></a>Standalone 模式</h3><p>独立模式，自带完整的服务，可单独部署到一个集群中，无需依赖任何其他资源管理系统。</p><h3 id="Spark-On-Mesos-模式"><a href="#Spark-On-Mesos-模式" class="headerlink" title="Spark On Mesos 模式"></a>Spark On Mesos 模式</h3><p>Spark 运行在 Mesos 上会比运行在 YARN 上更加灵活，更加自然。目前在 Spark On Mesos 环境中，用户可选择两种调度模式之一运行自己的应用程序。 </p><ul><li><p>粗粒度模式</p><blockquote><p>每个应用程序的运行环境由一个 Dirver 和若干个 Executor 组成，其中，每个 Executor 占用若干资源，内部可运行多个Task。应用程序的各个任务正式运行之前，需要将运行环境中的资源全部申请好，且运行过程中要一直占用这些资源，即使不用，最后程序运行结束后，回收这些资源。</p></blockquote></li><li><p>细粒度模式</p><blockquote><p>鉴于粗粒度模式会造成大量资源浪费，Spark On Mesos 还提供了另外一种调度模式：细粒度模式，这种模式类似于现在的云计算，思想是按需分配。与粗粒度模式一样，应用程序启动时，先会启动executor，但每个 executor 占用资源仅仅是自己运行所需的资源，不需要考虑将来要运行的任务，之后，mesos 会为每个 executor 动态分配资源，每分配一些，便可以运行一个新任务，单个 Task 运行完之后可以马上释放对应的资源。每个 Task 会汇报状态给 Mesos slave 和 Mesos Master ，便于更加细粒度管理和容错，这种调度模式类似于 MapReduce 调度模式，每个 Task 完全独立，优点是便于资源控制和隔离，但缺点也很明显，短作业运行延迟大。</p></blockquote></li></ul><h3 id="Spark-On-YARN-模式"><a href="#Spark-On-YARN-模式" class="headerlink" title="Spark On YARN 模式"></a>Spark On YARN 模式</h3><p>目前仅支持粗粒度模式。这是由于 YARN 上的 Container 资源是不可以动态伸缩的，一旦 Container 启动之后，可使用的资源不能再发生变化，不过这个已经在 YARN 计划中了。 </p><p>spark on yarn 的支持两种模式： </p><ol><li>yarn-cluster：适用于生产环境； </li><li>yarn-client：适用于交互、调试，希望立即看到 app 的输出 </li></ol><p>yarn-cluster 和 yarn-client 的区别在于 yarn appMaster，每个 yarn app 实例有一个 appMaster进程，是为 app 启动的第一个 container</p><p>负责从 ResourceManager 请求资源，获取到资源后，告诉 NodeManager 为其启动 container。</p><h2 id="运行机制"><a href="#运行机制" class="headerlink" title="运行机制"></a>运行机制</h2><h3 id="Standalone-模式运行机制"><a href="#Standalone-模式运行机制" class="headerlink" title="Standalone 模式运行机制"></a>Standalone 模式运行机制</h3><p>在 Standalone Client 模式下，Driver 在任务提交的本地机器上运行，Driver 启动后向 Master 注册应用程序，Master 根据 submit 脚本的资源需求找到内部资源至少可以启动一个 Executor 的所有 Worker ，然后在这些 Worker 之间分配 Executor ，Worker 上的 Executor 启动后会向 Driver 反向注册，所有的 Executor 注册完成后，Driver 开 始执行 main 函数， 之后执行到 Action 算子时 ， 开始划分 stage ， 每个 stage 生成对 应 的 taskSet ， 之后将task 分发到各个 Executor 上执行。</p><h3 id="YARN-模式运行机制"><a href="#YARN-模式运行机制" class="headerlink" title="YARN 模式运行机制"></a>YARN 模式运行机制</h3><h5 id="YARN-Client-模式"><a href="#YARN-Client-模式" class="headerlink" title="YARN  Client 模式"></a><font color='blue'>YARN  Client 模式</font></h5><p>在 YARN Client 模式下， Driver 在任务提交的本地机器上运行， Driver 启动后会和 ResourceManager 通讯申请启动 Application Master，随后 ResourceManager 分配 container ， 在合适的 NodeManager 上启动 ApplicationMaster，此时的 ApplicationMaster 的功能相当于一个 Executor Laucher，只 负责向 ResourceManager 申请 Executor 内存 。</p><p>ResourceManager 接到 ApplicationMaster 的资源申请后会分配 container，然后 ApplicationMaster 在资源分配指定的 NodeManager 上启动 Executor 进程， Executor 进程启动后会向 Driver 反向注册， Executor 全部注册完成后 Driver 开始执行 main 函数，之后执行到 Action 算子时， 触发一个job，并根据宽依赖开始划分 stage ， 每个 stage 生成对应的 taskSet，之后将 task 分发到各个 Executor 上执行 。</p><p>![屏幕快照 2020-03-23 上午12.33.41](/Users/zxc/Documents/hexo/source/_posts/Spark架构.assets/屏幕快照 2020-03-23 上午12.33.41.png)</p><p><font color='blue'><strong>YARN Cluster模式</strong></font></p><p>在 Yarn-Cluster 模式中，当用户向 Yarn 中提交一个应用程序后， Yarn 将分两个阶段运行该应用程序：第一个阶段是把 Spark 的 Driver 作为一个 ApplicationMaster 在 Yarn 集群中先启动；第二个阶段是由 ApplicationMaster 创建应用程序，然后为它向 ResourceManager 申请资源，并启动 Executor 来运行 Task，同时监控它的整个运行过程，直到运行完成。</p><p>![屏幕快照 2020-03-23 上午12.27.59](/Users/zxc/Documents/hexo/source/_posts/Spark架构.assets/屏幕快照 2020-03-23 上午12.27.59.png)</p><p>在 YARNCluster 模式下，任务提交后会和 ResourceManager 通讯申请启动 Application Master ， 随后 ResourceManager 分配 container， 在合适的 NodeManager 上启动 ApplicationMaster ， 此时的 ApplicationMaster 就是 Driver 。</p><p>Driver 启动后 向 ResourceManager 申请 Executor 内存， ResourceManager 接到 ApplicationMaster 的资源申请后会分配 container， 然后在合适的 NodeManager 上启动 Executor 进程， Executor 进程启动后会向 Driver 反向注册， Executor 全部注册完 成后 Driver 开始执行 main 函数，之后执行到 Action 算子时，触发一个 job ，并根据 宽依赖 开始划分 stage ， 每个 stage 生成对应 的 taskSet ， 之后将 task 分发到各个 Executor 上执行。</p>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>JVM 字节码[1]: 数据类型</title>
      <link href="2019/07/27/JVM-%E5%AD%97%E8%8A%82%E7%A0%81-1-%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B/"/>
      <url>2019/07/27/JVM-%E5%AD%97%E8%8A%82%E7%A0%81-1-%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p>Class字节码中有两种数据类型：</p><p>（1）字节数据直接量：这是基本的数据类型。共细分为u1、u2、u4、u8四种，分别代表连续的1个字节、2个字节、4个字节、8个字节组成的整体数据。<br>（2）表/数组：表是由多个基本数据或其他表，按照既定顺序组成的大的数据集合。表是有结构的，它的结构体：组成表的成分所在的位置和顺序都是已经严格定义好的。</p><p>Access Falgs：<br>访问标志信息包括了该class文件是类还是接口，是否被定义成public，是否是abstract，如果是类，是否被定义成final。</p><p><img src="./images/re.png"></p><p><img src="./images/hff.png"></p><ul><li><p>0x0021是0x0020和0x0001的并集，表示ACC_PUBLIC和ACC_SUPER<br>0x0002:private</p></li><li><p>字段表（Fields）：<br>字段表用于描述类和接口中声明的变量。这里的字段包含了类级别变量和实例变量，但是不包括方法内部声明的局部变量。</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> 深入理解 JVM </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
            <tag> JVM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>消息系统-Kafka(2)：事务机制</title>
      <link href="2019/07/12/%E6%B6%88%E6%81%AF%E7%B3%BB%E7%BB%9F-Kafka(4)%EF%BC%9A%E4%BA%8B%E5%8A%A1/"/>
      <url>2019/07/12/%E6%B6%88%E6%81%AF%E7%B3%BB%E7%BB%9F-Kafka(4)%EF%BC%9A%E4%BA%8B%E5%8A%A1/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p>Kafka 在 0.11 版本中除了引入了 <code>Exactly Once</code> 语义，还引入了事务特性。<strong>Kafka 事务特性是指一系列的生产者生产消息和消费者提交偏移量的操作在一个事务中，或者说是一个原子操作，生产消息和提交偏移量同时成功或者失败。</strong></p>]]></content>
      
      
      <categories>
          
          <category> Kafka </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kafka </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>JVM 指令集</title>
      <link href="2019/07/03/JVM-%E6%8C%87%E4%BB%A4%E9%9B%86/"/>
      <url>2019/07/03/JVM-%E6%8C%87%E4%BB%A4%E9%9B%86/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><table><thead><tr><th align="center"></th><th></th><th></th></tr></thead><tbody><tr><td align="center"><strong>指令码</strong></td><td><strong>助记符</strong></td><td><strong>功能描述</strong></td></tr><tr><td align="center">0x00</td><td>nop</td><td>无操作</td></tr><tr><td align="center">0x01</td><td>aconst_null</td><td>aconst_null 功能描述 null进栈指令执行前指令执行后栈底…… null栈顶     注意：JVM并没有为null指派一个具体的值。</td></tr><tr><td align="center">0x02</td><td>iconst_m1</td><td>int型常量值-1进栈</td></tr><tr><td align="center">0x03</td><td>iconst_0</td><td>int型常量值0进栈</td></tr><tr><td align="center">0x04</td><td>iconst_1</td><td>int型常量值1进栈</td></tr><tr><td align="center">0x05</td><td>iconst_2</td><td>int型常量值2进栈</td></tr><tr><td align="center">0x06</td><td>iconst_3</td><td>int型常量值3进栈</td></tr><tr><td align="center">0x07</td><td>iconst_4</td><td>int型常量值4进栈</td></tr><tr><td align="center">0x08</td><td>iconst_5</td><td>int型常量值5进栈</td></tr><tr><td align="center"></td><td></td><td></td></tr><tr><td align="center">0x09</td><td>lconst_0</td><td>long型常量值0进栈</td></tr><tr><td align="center">0x0A</td><td>lconst_1</td><td>long型常量值1进栈</td></tr><tr><td align="center"></td><td></td><td></td></tr><tr><td align="center">0x0B</td><td>fconst_0</td><td>float型常量值0进栈</td></tr><tr><td align="center">0x0C</td><td>fconst_1</td><td>float型常量值1进栈</td></tr><tr><td align="center">0x0D</td><td>fconst_2</td><td>float型常量值2进栈</td></tr><tr><td align="center"></td><td></td><td></td></tr><tr><td align="center">0x0E</td><td>dconst_0</td><td>double型常量值0进栈</td></tr><tr><td align="center">0x0F</td><td>dconst_1</td><td>double型常量值1进栈</td></tr><tr><td align="center"></td><td></td><td></td></tr><tr><td align="center">0x10</td><td>bipush</td><td>将一个byte型常量值推送至栈顶</td></tr><tr><td align="center">0x11</td><td>sipush</td><td>将一个short型常量值推送至栈顶</td></tr><tr><td align="center"></td><td></td><td></td></tr><tr><td align="center">0x12</td><td>ldc</td><td>将int、float或String型常量值从常量池中推送至栈顶</td></tr><tr><td align="center">0x13</td><td>ldc_w</td><td>将int、float或String型常量值从常量池中推送至栈顶（宽索引）</td></tr><tr><td align="center">0x14</td><td>ldc2_w</td><td>将long或double型常量值从常量池中推送至栈顶（宽索引）</td></tr><tr><td align="center"></td><td></td><td></td></tr><tr><td align="center">0x15</td><td>iload</td><td>指定的int型局部变量进栈</td></tr><tr><td align="center">0x16</td><td>lload</td><td>指定的long型局部变量进栈</td></tr><tr><td align="center">0x17</td><td>fload</td><td>指定的float型局部变量进栈</td></tr><tr><td align="center">0x18</td><td>dload</td><td>指定的double型局部变量进栈</td></tr><tr><td align="center">0x19</td><td>aload</td><td>指令格式： aload index功能描述： 当前frame的局部变量数组中下标为index的引用型局部变量进栈指令执行前指令执行后栈底…… objectref栈顶     index ： 无符号一byte整型。和wide指令联用， 可以使index为两byte</td></tr><tr><td align="center"></td><td></td><td></td></tr><tr><td align="center">0x1A</td><td>iload_0</td><td>第一个int型局部变量进栈</td></tr><tr><td align="center">0x1B</td><td>iload_1</td><td>第二个int型局部变量进栈</td></tr><tr><td align="center">0x1C</td><td>iload_2</td><td>第三个int型局部变量进栈</td></tr><tr><td align="center">0x1D</td><td>iload_3</td><td>第四个int型局部变量进栈</td></tr><tr><td align="center"></td><td></td><td></td></tr><tr><td align="center">0x1E</td><td>lload_0</td><td>第一个long型局部变量进栈</td></tr><tr><td align="center">0x1F</td><td>lload_1</td><td>第二个long型局部变量进栈</td></tr><tr><td align="center">0x20</td><td>lload_2</td><td>第三个long型局部变量进栈</td></tr><tr><td align="center">0x21</td><td>lload_3</td><td>第四个long型局部变量进栈</td></tr><tr><td align="center"></td><td></td><td></td></tr><tr><td align="center">0x22</td><td>fload_0</td><td>第一个float型局部变量进栈</td></tr><tr><td align="center">0x23</td><td>fload_1</td><td>第二个float型局部变量进栈</td></tr><tr><td align="center">0x24</td><td>fload_2</td><td>第三个float型局部变量进栈</td></tr><tr><td align="center">0x25</td><td>fload_3</td><td>第四个float型局部变量进栈</td></tr><tr><td align="center"></td><td></td><td></td></tr><tr><td align="center">0x26</td><td>dload_0</td><td>第一个double型局部变量进栈</td></tr><tr><td align="center">0x27</td><td>dload_1</td><td>第二个double型局部变量进栈</td></tr><tr><td align="center">0x28</td><td>dload_2</td><td>第三个double型局部变量进栈</td></tr><tr><td align="center">0x29</td><td>dload_3</td><td>第四个double型局部变量进栈</td></tr><tr><td align="center"></td><td></td><td></td></tr><tr><td align="center">0x2A</td><td>aload_0</td><td>指令格式：aload_0该指令的行为类似于aload指令index为0的情况。</td></tr><tr><td align="center">0x2B</td><td>aload_1</td><td>同上</td></tr><tr><td align="center">0x2C</td><td>aload_2</td><td>同上</td></tr><tr><td align="center">0x2D</td><td>aload_3</td><td>同上</td></tr><tr><td align="center"></td><td></td><td></td></tr><tr><td align="center">0x2E</td><td>iaload</td><td>指定的int型数组的指定下标处的值进栈</td></tr><tr><td align="center">0x2F</td><td>laload</td><td>指定的long型数组的指定下标处的值进栈</td></tr><tr><td align="center">0x30</td><td>faload</td><td>指定的float型数组的指定下标处的值进栈</td></tr><tr><td align="center">0x31</td><td>daload</td><td>指定的double型数组的指定下标处的值进栈</td></tr><tr><td align="center">0x32</td><td>aaload</td><td>指令格式： aaload 功能描述： 栈顶的数组下标（index）、数组引用（arrayref）出栈，并根据这两个数值取出对应的数组元素值（value）进栈。 抛出异常： 如果arrayref的值为null，会抛出NullPointerException。如果index造成数组越界，会抛出ArrayIndexOutOfBoundsException。指令执行前指令执行后栈底……arrayrefvalueindex 栈顶      index： int类型arrayref： 数组的引用</td></tr><tr><td align="center">0x33</td><td>baload</td><td>指定的boolean或byte型数组的指定下标处的值进栈</td></tr><tr><td align="center">0x34</td><td>caload</td><td>指定的char型数组的指定下标处的值进栈</td></tr><tr><td align="center">0x35</td><td>saload</td><td>指定的short型数组的指定下标处的值进栈</td></tr><tr><td align="center"></td><td></td><td></td></tr><tr><td align="center">0x36</td><td>istore</td><td>将栈顶int型数值存入指定的局部变量</td></tr><tr><td align="center">0x37</td><td>lstore</td><td>将栈顶long型数值存入指定的局部变量</td></tr><tr><td align="center">0x38</td><td>fstore</td><td>将栈顶float型数值存入指定的局部变量</td></tr><tr><td align="center">0x39</td><td>dstore</td><td>将栈顶double型数值存入指定的局部变量</td></tr><tr><td align="center">0x3A</td><td>astore</td><td>astore index 功能描述： 将栈顶数值（objectref）存入当前frame的局部变量数组中指定下标（index）处的变量中，栈顶数值出栈。指令执行前指令执行后栈底……objectref 栈顶     index ： 无符号一byte整数。该指令和wide联用，index可以为无符号两byte整数</td></tr><tr><td align="center"></td><td></td><td></td></tr><tr><td align="center">0x3B</td><td>istore_0</td><td>将栈顶int型数值存入第一个局部变量</td></tr><tr><td align="center">0x3C</td><td>istore_1</td><td>将栈顶int型数值存入第二个局部变量</td></tr><tr><td align="center">0x3D</td><td>istore_2</td><td>将栈顶int型数值存入第三个局部变量</td></tr><tr><td align="center">0x3E</td><td>istore_3</td><td>将栈顶int型数值存入第四个局部变量</td></tr><tr><td align="center"></td><td></td><td></td></tr><tr><td align="center">0x3F</td><td>lstore_0</td><td>将栈顶long型数值存入第一个局部变量</td></tr><tr><td align="center">0x40</td><td>lstore_1</td><td>将栈顶long型数值存入第二个局部变量</td></tr><tr><td align="center">0x41</td><td>lstore_2</td><td>将栈顶long型数值存入第三个局部变量</td></tr><tr><td align="center">0x42</td><td>lstore_3</td><td>将栈顶long型数值存入第四个局部变量</td></tr><tr><td align="center"></td><td></td><td></td></tr><tr><td align="center">0x43</td><td>fstore_0</td><td>将栈顶float型数值存入第一个局部变量</td></tr><tr><td align="center">0x44</td><td>fstore_1</td><td>将栈顶float型数值存入第二个局部变量</td></tr><tr><td align="center">0x45</td><td>fstore_2</td><td>将栈顶float型数值存入第三个局部变量</td></tr><tr><td align="center">0x46</td><td>fstore_3</td><td>将栈顶float型数值存入第四个局部变量</td></tr><tr><td align="center"></td><td></td><td></td></tr><tr><td align="center">0x47</td><td>dstore_0</td><td>将栈顶double型数值存入第一个局部变量</td></tr><tr><td align="center">0x48</td><td>dstore_1</td><td>将栈顶double型数值存入第二个局部变量</td></tr><tr><td align="center">0x49</td><td>dstore_2</td><td>将栈顶double型数值存入第三个局部变量</td></tr><tr><td align="center">0x4A</td><td>dstore_3</td><td>将栈顶double型数值存入第四个局部变量</td></tr><tr><td align="center"></td><td></td><td></td></tr><tr><td align="center">0x4B</td><td>astore_0</td><td>指令格式： astore_0功能描述： 该指令的行为类似于astore指令index为0的情况。</td></tr><tr><td align="center">0x4C</td><td>astore_1</td><td>同上</td></tr><tr><td align="center">0x4D</td><td>astore_2</td><td>同上</td></tr><tr><td align="center">0x4E</td><td>astore_3</td><td>同上</td></tr><tr><td align="center"></td><td></td><td></td></tr><tr><td align="center">0x4F</td><td>iastore</td><td>将栈顶int型数值存入指定数组的指定下标处</td></tr><tr><td align="center">0x50</td><td>lastore</td><td>将栈顶long型数值存入指定数组的指定下标处</td></tr><tr><td align="center">0x51</td><td>fastore</td><td>将栈顶float型数值存入指定数组的指定下标处</td></tr><tr><td align="center">0x52</td><td>dastore</td><td>将栈顶double型数值存入指定数组的指定下标处</td></tr><tr><td align="center">0x53</td><td>aastore</td><td>指令格式： aastore 功能描述： 根据栈顶的引用型数值（value）、数组下标（index）、数组引用（arrayref）出栈，将数值存入对应的数组元素中抛出异常： 如果value的类型和arrayref所引用的数组的元素类型不兼容，会抛出抛出ArrayStoreException。如果index造成数组越界，会抛出ArrayIndexOutOfBoundsException。 如果arrayref值为null，会抛出NullPointerException。指令执行前指令执行后栈底……arrayref index value 栈顶       arrayref ： 必须是对数组的引用index ： int类型value ： 引用类型</td></tr><tr><td align="center">0x54</td><td>bastore</td><td>将栈顶boolean或byte型数值存入指定数组的指定下标处</td></tr><tr><td align="center">0x55</td><td>castore</td><td>将栈顶char型数值存入指定数组的指定下标处</td></tr><tr><td align="center">0x56</td><td>sastore</td><td>将栈顶short型数值存入指定数组的指定下标处</td></tr><tr><td align="center"></td><td></td><td></td></tr><tr><td align="center">0x57</td><td>pop</td><td>栈顶数值出栈 (该栈顶数值不能是long或double型)</td></tr><tr><td align="center">0x58</td><td>pop2</td><td>栈顶的一个（如果是long、double型的)或两个（其它类型的）数值出栈</td></tr><tr><td align="center"></td><td></td><td></td></tr><tr><td align="center">0x59</td><td>dup</td><td>复制栈顶数值，并且复制值进栈</td></tr><tr><td align="center">0x5A</td><td>dup_x1</td><td>复制栈顶数值，并且复制值进栈2次</td></tr><tr><td align="center">0x5B</td><td>dup_x2</td><td>复制栈顶数值，并且复制值进栈2次或3次</td></tr><tr><td align="center">0x5C</td><td>dup2</td><td>复制栈顶一个（long、double型的)或两个（其它类型的）数值，并且复制值进栈</td></tr><tr><td align="center">0x5D</td><td>dup2_x1</td><td></td></tr><tr><td align="center">0x5E</td><td>dup2_x2</td><td></td></tr><tr><td align="center"></td><td></td><td></td></tr><tr><td align="center">0x5F</td><td>swap</td><td>栈顶的两个数值互换(要求栈顶的两个数值不能是long或double型的)</td></tr><tr><td align="center"></td><td></td><td></td></tr><tr><td align="center">0x60</td><td>iadd</td><td>栈顶两int型数值相加，并且结果进栈</td></tr><tr><td align="center">0x61</td><td>ladd</td><td>栈顶两long型数值相加，并且结果进栈</td></tr><tr><td align="center">0x62</td><td>fadd</td><td>栈顶两float型数值相加，并且结果进栈</td></tr><tr><td align="center">0x63</td><td>dadd</td><td>栈顶两double型数值相加，并且结果进栈</td></tr><tr><td align="center"></td><td></td><td></td></tr><tr><td align="center">0x64</td><td>isub</td><td>栈顶两int型数值相减，并且结果进栈</td></tr><tr><td align="center">0x65</td><td>lsub</td><td>栈顶两long型数值相减，并且结果进栈</td></tr><tr><td align="center">0x66</td><td>fsub</td><td>栈顶两float型数值相减，并且结果进栈</td></tr><tr><td align="center">0x67</td><td>dsub</td><td>栈顶两double型数值相减，并且结果进栈</td></tr><tr><td align="center"></td><td></td><td></td></tr><tr><td align="center">0x68</td><td>imul</td><td>栈顶两int型数值相乘，并且结果进栈</td></tr><tr><td align="center">0x69</td><td>lmul</td><td>栈顶两long型数值相乘，并且结果进栈</td></tr><tr><td align="center">0x6A</td><td>fmul</td><td>栈顶两float型数值相乘，并且结果进栈</td></tr><tr><td align="center">0x6B</td><td>dmul</td><td>栈顶两double型数值相乘，并且结果进栈</td></tr><tr><td align="center"></td><td></td><td></td></tr><tr><td align="center">0x6C</td><td>idiv</td><td>栈顶两int型数值相除，并且结果进栈</td></tr><tr><td align="center">0x6D</td><td>ldiv</td><td>栈顶两long型数值相除，并且结果进栈</td></tr><tr><td align="center">0x6E</td><td>fdiv</td><td>栈顶两float型数值相除，并且结果进栈</td></tr><tr><td align="center">0x6F</td><td>ddiv</td><td>栈顶两double型数值相除，并且结果进栈</td></tr><tr><td align="center"></td><td></td><td></td></tr><tr><td align="center">0x70</td><td>irem</td><td>栈顶两int型数值作取模运算，并且结果进栈</td></tr><tr><td align="center">0x71</td><td>lrem</td><td>栈顶两long型数值作取模运算，并且结果进栈</td></tr><tr><td align="center">0x72</td><td>frem</td><td>栈顶两float型数值作取模运算，并且结果进栈</td></tr><tr><td align="center">0x73</td><td>drem</td><td>栈顶两double型数值作取模运算，并且结果进栈</td></tr><tr><td align="center"></td><td></td><td></td></tr><tr><td align="center">0x74</td><td>ineg</td><td>栈顶int型数值取负，并且结果进栈</td></tr><tr><td align="center">0x75</td><td>lneg</td><td>栈顶long型数值取负，并且结果进栈</td></tr><tr><td align="center">0x76</td><td>fneg</td><td>栈顶float型数值取负，并且结果进栈</td></tr><tr><td align="center">0x77</td><td>dneg</td><td>栈顶double型数值取负，并且结果进栈</td></tr><tr><td align="center"></td><td></td><td></td></tr><tr><td align="center">0x78</td><td>ishl</td><td>int型数值左移指定位数，并且结果进栈</td></tr><tr><td align="center">0x79</td><td>lshl</td><td>long型数值左移指定位数，并且结果进栈</td></tr><tr><td align="center"></td><td></td><td></td></tr><tr><td align="center">0x7A</td><td>ishr</td><td>int型数值带符号右移指定位数，并且结果进栈</td></tr><tr><td align="center">0x7B</td><td>lshr</td><td>long型数值带符号右移指定位数，并且结果进栈</td></tr><tr><td align="center">0x7C</td><td>iushr</td><td>int型数值无符号右移指定位数，并且结果进栈</td></tr><tr><td align="center">0x7D</td><td>lushr</td><td>long型数值无符号右移指定位数，并且结果进栈</td></tr><tr><td align="center"></td><td></td><td></td></tr><tr><td align="center">0x7E</td><td>iand</td><td>栈顶两int型数值按位与，并且结果进栈</td></tr><tr><td align="center">0x7F</td><td>land</td><td>栈顶两long型数值按位与，并且结果进栈</td></tr><tr><td align="center"></td><td></td><td></td></tr><tr><td align="center">0x80</td><td>ior</td><td>栈顶两int型数值按位或，并且结果进栈</td></tr><tr><td align="center">0x81</td><td>lor</td><td>栈顶两long型数值按位或，并且结果进栈</td></tr><tr><td align="center"></td><td></td><td></td></tr><tr><td align="center">0x82</td><td>ixor</td><td>栈顶两int型数值按位异或，并且结果进栈</td></tr><tr><td align="center">0x83</td><td>lxor</td><td>栈顶两long型数值按位异或，并且结果进栈</td></tr><tr><td align="center"></td><td></td><td></td></tr><tr><td align="center">0x84</td><td>iinc</td><td>指定int型变量增加指定值</td></tr><tr><td align="center"></td><td></td><td></td></tr><tr><td align="center">0x85</td><td>i2l</td><td>栈顶int值强转long值，并且结果进栈</td></tr><tr><td align="center">0x86</td><td>i2f</td><td>栈顶int值强转float值，并且结果进栈</td></tr><tr><td align="center">0x87</td><td>i2d</td><td>栈顶int值强转double值，并且结果进栈</td></tr><tr><td align="center">0x88</td><td>l2i</td><td>栈顶long值强转int值，并且结果进栈</td></tr><tr><td align="center">0x89</td><td>l2f</td><td>栈顶long值强转float值，并且结果进栈</td></tr><tr><td align="center">0x8A</td><td>l2d</td><td>栈顶long值强转double值，并且结果进栈</td></tr><tr><td align="center">0x8B</td><td>f2i</td><td>栈顶float值强转int值，并且结果进栈</td></tr><tr><td align="center">0x8C</td><td>f2l</td><td>栈顶float值强转long值，并且结果进栈</td></tr><tr><td align="center">0x8D</td><td>f2d</td><td>栈顶float值强转double值，并且结果进栈</td></tr><tr><td align="center">0x8E</td><td>d2i</td><td>栈顶double值强转int值，并且结果进栈</td></tr><tr><td align="center">0x8F</td><td>d2l</td><td>栈顶double值强转long值，并且结果进栈</td></tr><tr><td align="center">0x90</td><td>d2f</td><td>栈顶double值强转float值，并且结果进栈</td></tr><tr><td align="center">0x91</td><td>i2b</td><td>栈顶int值强转byte值，并且结果进栈</td></tr><tr><td align="center">0x92</td><td>i2c</td><td>栈顶int值强转char值，并且结果进栈</td></tr><tr><td align="center">0x93</td><td>i2s</td><td>栈顶int值强转short值，并且结果进栈</td></tr><tr><td align="center"></td><td></td><td></td></tr><tr><td align="center">0x94</td><td>lcmp</td><td>比较栈顶两long型数值大小，并且结果（1，0，-1）进栈</td></tr><tr><td align="center">0x95</td><td>fcmpl</td><td>比较栈顶两float型数值大小，并且结果（1，0，-1）进栈；当其中一个数值为NaN时， -1进栈</td></tr><tr><td align="center">0x96</td><td>fcmpg</td><td>比较栈顶两float型数值大小，并且结果（1，0，-1）进栈；当其中一个数值为NaN时，1进栈</td></tr><tr><td align="center">0x97</td><td>dcmpl</td><td>比较栈顶两double型数值大小，并且结果（1，0，-1）进栈；当其中一个数值为NaN时，-1进栈</td></tr><tr><td align="center">0x98</td><td>dcmpg</td><td>比较栈顶两double型数值大小，并且结果（1，0，-1）进栈；当其中一个数值为NaN时，1进栈</td></tr><tr><td align="center"></td><td></td><td></td></tr><tr><td align="center">0x99</td><td>ifeq</td><td>当栈顶int型数值等于0时跳转</td></tr><tr><td align="center">0x9A</td><td>ifne</td><td>当栈顶int型数值不等于0时跳转</td></tr><tr><td align="center">0x9B</td><td>iflt</td><td>当栈顶int型数值小于0时跳转</td></tr><tr><td align="center">0x9C</td><td>ifge</td><td>当栈顶int型数值大于等于0时跳转</td></tr><tr><td align="center">0x9D</td><td>ifgt</td><td>当栈顶int型数值大于0时跳转</td></tr><tr><td align="center">0x9E</td><td>ifle</td><td>当栈顶int型数值小于等于0时跳转</td></tr><tr><td align="center">0x9F</td><td>if_icmpeq</td><td>比较栈顶两int型数值大小，当结果等于0时跳转</td></tr><tr><td align="center">0xA0</td><td>if_icmpne</td><td>比较栈顶两int型数值大小，当结果不等于0时跳转</td></tr><tr><td align="center">0xA1</td><td>if_icmplt</td><td>比较栈顶两int型数值大小，当结果小于0时跳转</td></tr><tr><td align="center">0xA2</td><td>if_icmpge</td><td>比较栈顶两int型数值大小，当结果大于等于0时跳转</td></tr><tr><td align="center">0xA3</td><td>if_icmpgt</td><td>比较栈顶两int型数值大小，当结果大于0时跳转</td></tr><tr><td align="center">0xA4</td><td>if_icmple</td><td>比较栈顶两int型数值大小，当结果小于等于0时跳转</td></tr><tr><td align="center">0xA5</td><td>if_acmpeq</td><td>比较栈顶两引用型数值，当结果相等时跳转</td></tr><tr><td align="center">0xA6</td><td>if_acmpne</td><td>比较栈顶两引用型数值，当结果不相等时跳转</td></tr><tr><td align="center"></td><td></td><td></td></tr><tr><td align="center">0xA7</td><td>goto</td><td>无条件跳转</td></tr><tr><td align="center"></td><td></td><td></td></tr><tr><td align="center">0xA8</td><td>jsr</td><td>跳转至指定16位offset位置，并将jsr下一条指令地址压入栈顶</td></tr><tr><td align="center">0xA9</td><td>ret</td><td>返回至局部变量指定的index的指令位置（通常与jsr、jsr_w联合使用）</td></tr><tr><td align="center">0xAA</td><td>tableswitch</td><td>用于switch条件跳转，case值连续（可变长度指令）</td></tr><tr><td align="center">0xAB</td><td>lookupswitch</td><td>用于switch条件跳转，case值不连续（可变长度指令）</td></tr><tr><td align="center"></td><td></td><td></td></tr><tr><td align="center">0xAC</td><td>ireturn</td><td>当前方法返回int</td></tr><tr><td align="center">0xAD</td><td>lreturn</td><td>当前方法返回long</td></tr><tr><td align="center">0xAE</td><td>freturn</td><td>当前方法返回float</td></tr><tr><td align="center">0xAF</td><td>dreturn</td><td>当前方法返回double</td></tr><tr><td align="center">0xB0</td><td>areturn</td><td>指令格式： areturn功能描述： 从方法中返回一个对象的引用。抛出异常： 如果当前方法是<code>synchronized</code>方法，并且当前线程不是改方法的锁的拥有者，会抛出IllegalMonitorStateException指令执行前指令执行后栈底… objectref 栈顶     objectref ： 被返回的对象引用</td></tr><tr><td align="center">0xB1</td><td>return</td><td>当前方法返回void</td></tr><tr><td align="center"></td><td></td><td></td></tr><tr><td align="center">0xB2</td><td>getstatic</td><td>获取指定类的静态域，并将其值压入栈顶</td></tr><tr><td align="center">0xB3</td><td>putstatic</td><td>为指定的类的静态域赋值</td></tr><tr><td align="center">0xB4</td><td>getfield</td><td>获取指定类的实例域，并将其值压入栈顶</td></tr><tr><td align="center">0xB5</td><td>putfield</td><td>为指定的类的实例域赋值</td></tr><tr><td align="center"></td><td></td><td></td></tr><tr><td align="center">0xB6</td><td>invokevirtual</td><td>调用实例方法</td></tr><tr><td align="center">0xB7</td><td>invokespecial</td><td>调用超类构造方法、实例初始化方法、私有方法</td></tr><tr><td align="center">0xB8</td><td>invokestatic</td><td>调用静态方法</td></tr><tr><td align="center">0xb9</td><td>invokeinterface</td><td>调用接口方法</td></tr><tr><td align="center"></td><td></td><td></td></tr><tr><td align="center">0xBA</td><td>—</td><td>因为历史原因，该码点为未使用的保留码点</td></tr><tr><td align="center"></td><td></td><td></td></tr><tr><td align="center">0xBB</td><td>new</td><td>创建一个对象，并且其引用进栈</td></tr><tr><td align="center">0xBC</td><td>newarray</td><td>创建一个基本类型数组，并且其引用进栈</td></tr><tr><td align="center">0xBD</td><td>anewarray</td><td>指令格式： anewarray index1 index2 功能描述： 栈顶数值（count）作为数组长度，创建 一个引用 型数组。栈顶数值出栈，数组引用进栈。 抛出异常： 如果count小于0，会抛出 NegativeArraySizeException指令执行前指令执行后栈底……countarrayref栈顶     count： int类型。arrayref： 对所创建的数组的引用。</td></tr><tr><td align="center">0xBE</td><td>arraylength</td><td>指令格式： arraylength功能描述： 栈顶的数组引用（arrayref）出栈，该 数组的长度进栈。抛出异常： 如果arrayref的值为null，会抛出NullPointerException。指令执行前指令执行后栈底……arrayreflength栈顶     arrayref： 数组引用length： 数组长度</td></tr><tr><td align="center"></td><td></td><td></td></tr><tr><td align="center">0xBF</td><td>athrow</td><td>指令格式： athrow功能描述： 将栈顶的数值作为异常或错误抛出抛出异常： 如果栈顶数值为null，则使用NullPointerException代替栈顶数值抛出。如果方法是的，则有可能抛出 IllegalMonitorStateException。指令执行前指令执行后栈底…objectrefobjectref 栈顶     objectref： Throwable或其子类的实例的引用。</td></tr><tr><td align="center">0xC0</td><td>checkcast</td><td>类型转换检查，如果该检查未通过将会抛出ClassCastException异常</td></tr><tr><td align="center">0xc1</td><td>instanceof</td><td>检查对象是否是指定的类的实例。如果是，1进栈；否则，0进栈</td></tr><tr><td align="center"></td><td></td><td></td></tr><tr><td align="center">0xC2</td><td>monitorenter</td><td>获得对象锁</td></tr><tr><td align="center">0xC3</td><td>monitorexit</td><td>释放对象锁</td></tr><tr><td align="center"></td><td></td><td></td></tr><tr><td align="center">0xC4</td><td>wide</td><td>用于修改其他指令的行为</td></tr><tr><td align="center"></td><td></td><td></td></tr><tr><td align="center">0xC5</td><td>multianewarray</td><td>创建指定类型和维度的多维数组（执行该指令时，栈中必须包含各维度的长度值），并且其引用值进栈</td></tr><tr><td align="center"></td><td></td><td></td></tr><tr><td align="center">0xC6</td><td>ifnull</td><td>为null时跳转</td></tr><tr><td align="center">0xC7</td><td>ifnonnull</td><td>不为null时跳转</td></tr><tr><td align="center">0xC8</td><td>goto_w</td><td>无条件跳转（宽索引）</td></tr><tr><td align="center">0xC9</td><td>jsr_w</td><td>跳转至指定32位offset位置，并且jsr_w下一条指令地址进栈</td></tr><tr><td align="center"></td><td></td><td></td></tr><tr><td align="center">0xCA</td><td>breakpoint</td><td></td></tr><tr><td align="center"></td><td></td><td></td></tr><tr><td align="center">0xFE</td><td>impdep1</td><td></td></tr><tr><td align="center">0xFF</td><td>impdep2</td><td></td></tr></tbody></table>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>JVM 垃圾收集器[1]:垃圾回收算法</title>
      <link href="2019/07/01/JVM-%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E5%99%A8-1-%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6%E7%AE%97%E6%B3%95/"/>
      <url>2019/07/01/JVM-%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E5%99%A8-1-%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6%E7%AE%97%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script>]]></content>
      
      
      <categories>
          
          <category> 深入理解 JVM </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java </tag>
            
            <tag> JVM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>JVM 类加载器</title>
      <link href="2019/06/25/JVM%20%E7%B1%BB%E5%8A%A0%E8%BD%BD%E6%9C%BA%E5%88%B6%E5%AD%A6%E4%B9%A0/"/>
      <url>2019/06/25/JVM%20%E7%B1%BB%E5%8A%A0%E8%BD%BD%E6%9C%BA%E5%88%B6%E5%AD%A6%E4%B9%A0/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p>类的加载指的是将类的 .class 文件中的二进制数据读入到内存中，将其放在运行时数据区的**[方法区]<strong>内，然后在</strong>[堆区]**创建一个 <code>java.lang.Class</code> 对象，用来封装类在方法区内的数据结构。类的加载的最终产品是位于堆区中的 Class 对象，Class对象封装了类在方法区内的数据结构，并且向Java程序员提供了访问方法区内的数据结构的接口。 </p><a id="more"></a><h1 id="1-简介"><a href="#1-简介" class="headerlink" title="1.简介"></a>1.简介</h1><p>Sun 公司开发了 Java 语言，但任何人都可以在遵循 JVM 规范的前提下开发和提供 JVM 实现。所以目前业界有多种不同的JVM 实现，包括 Oracle Hostpot JVM、IBM JVM 和 Taobao JVM。<br>JRE 由 Java API 和 JVM 组成，JVM 通过类加载器(Class Loader)加类 Java 应用，并通过 Java API 进行执行。<br>最初 Java 语言被设计为基于虚拟机器在而非物理机器，重而实现 WORA(一次编写，到处运行)的目的，尽管这个目标几乎被世人所遗忘。所以，JVM 可以在所有的硬件环境上执行 Java 字节码而无须调整 Java 的执行模式。<img src="/Users/joker/Documents/Learnning/JVM/resources/images/3.png" alt="3" style="zoom:50%;" /></p><h1 id="2-基本特性"><a href="#2-基本特性" class="headerlink" title="2.基本特性"></a>2.基本特性</h1><ul><li><strong>基于栈 (Stack-based) 的虚拟机</strong></li><li><strong>符号引用 (Symbolic reference)</strong><br>  除基本类型外的所有 Java 类型 (类和接口) 都是通过符号引用取得关联的，而非显式的基于内存地址的引用。</li><li><strong>垃圾回收机制</strong><br>  类的实例通过用户代码进行显式创建，但却通过垃圾回收机制自动销毁。</li><li><strong>平台无关性</strong><br>  JVM 却通过明确的定义基本类型的字节长度来维持代码的平台兼容性，从而做到平台无关。</li><li><strong>网络字节序(Network byte order)</strong><br>  Java class文件的二进制表示使用的是基于网络的字节序(network byte order)。为了在使用小端(little endian)的Intel x86平台和在使用了大端(big endian)的RISC系列平台之间保持平台无关，必须要定义一个固定的字节序。JVM选择了网络传输协议中使用的网络字节序，即基于大端(big endian)的字节序。</li></ul><h1 id="3-Java-虚拟机与程序的生命周期"><a href="#3-Java-虚拟机与程序的生命周期" class="headerlink" title="3.Java 虚拟机与程序的生命周期"></a>3.Java 虚拟机与程序的生命周期</h1><p>在如下几种情况下。Java 虚拟机将结束生命周期</p><ol><li>执行了 System.exit()方法</li><li>程序正常执行结束</li><li>程序在执行过程中遇到了异常或错误而异常结束</li><li>由于操作系统出现错误而导致Java虚拟机进程终止<h1 id="4-类加载机制"><a href="#4-类加载机制" class="headerlink" title="4.类加载机制"></a>4.类加载机制</h1></li></ol><p>类的加载指的是将类的 <code>.class</code> 文件中的二进制数据读入到内存中，将其放在运行时数据区的方法区内，然后在内存区创建一个 <code>java.lang.Class</code> 对象，用来封装类在方法区内的数据结构<br><font color = 'red'> <strong>注意：</strong></font>规范并未说明 <em>Class</em> 对象位于哪里，<em>HotSpot</em> 虚拟机将其放在了方法区内。</p><img src="/Users/joker/Library/Application Support/typora-user-images/截屏2020-09-29 下午2.24.15.png" alt="截屏2020-09-29 下午2.24.15" style="zoom:50%;" /><h2 id="4-1-类加载时机"><a href="#4-1-类加载时机" class="headerlink" title="4.1. 类加载时机"></a>4.1. 类加载时机</h2><p>JVM 规范允许类加载器在预料某个类将要被使用时就预先加载它，如果在预先加载的过程中遇到了.class文件缺失或存在错误，类加载器必须在<strong>程序首次主动</strong>使用该类才报告错误（LinkageError错误），如果这个类没有被程序主动使用，那么类加载器就不会报告错误。</p><h2 id="4-2-类加载过程"><a href="#4-2-类加载过程" class="headerlink" title="4.2. 类加载过程"></a>4.2. 类加载过程</h2><img src="/Users/joker/Documents/Learnning/JVM/resources/images/2.png" alt="2" style="zoom:50%;" /><h3 id="4-2-1-加载"><a href="#4-2-1-加载" class="headerlink" title="4.2.1. 加载"></a>4.2.1. 加载</h3><p>加载是类加载过程的第一个阶段，在加载阶段，虚拟机需要完成以下三件事情：</p><ul><li>通过一个类的全限定名来获取其定义的二进制字节流。</li><li>将这个字节流所代表的静态存储结构转化为方法区的运行时数据结构。</li><li>在 Java 堆中生成一个代表这个类的 java.lang.Class 对象，作为对方法区中这些数据的访问入口。</li></ul><p>相对于类加载的其他阶段而言，加载阶段（准确地说，是加载阶段获取类的二进制字节流的动作）是可控性最强的阶段，因为开发人员既可以使用系统提供的类加载器来完成加载，也可以自定义自己的类加载器来完成加载。</p><p>加载阶段完成后，虚拟机外部的 二进制字节流就按照虚拟机所需的格式存储在方法区之中，而且在 Java 堆中也创建一个java.lang.Class 类的对象，这样便可以通过该对象访问方法区中的这些数据。</p><h4 id="gt-加载-class-文件的方式"><a href="#gt-加载-class-文件的方式" class="headerlink" title="-&gt; 加载 .class 文件的方式"></a><strong>-&gt; 加载 .class 文件的方式</strong></h4><ul><li>从本地系统中直接加载</li><li>通过网络下载 .class 文件</li><li>从 zip, jar 等归档文件中加载 .class 文件</li><li>从专有数据库中提取 .class 文件，比较少见</li><li>运行时计算生成: 动态代理</li><li>将 Java 源文件动态编译为 .class 文件</li></ul><h3 id="4-2-2-连接"><a href="#4-2-2-连接" class="headerlink" title="4.2.2. 连接"></a>4.2.2. 连接</h3><p><strong>类被加载之后，进入连接阶段。连接阶段就是将已经读入到内存的类的二进制数据合并到虚拟机的运行时环境中去。</strong></p><h4 id="gt-验证"><a href="#gt-验证" class="headerlink" title="-&gt; 验证"></a><strong>-&gt; 验证</strong></h4><p>验证是连接阶段的第一步，这一阶段的目的是为了确保 Class 文件的字节流中包含的信息符合当前虚拟机的要求，并且不会危害虚拟机自身的安全。验证阶段大致会完成4个阶段的检验动作：</p><ol><li><p><strong>文件格式验证</strong></p><p>验证字节流是否符合 Class 文件格式的规范；例如：是否以 <code>0xCAFEBABE</code> 开头、主次版本号是否在当前虚拟机的处理范围之内、常量池中的常量是否有不被支持的类型。</p></li><li><p><strong>元数据验证</strong></p><p>对字节码描述的信息进行语义分析（注意：对比javac编译阶段的语义分析），以保证其描述的信息符合Java语言规范的要求；例如：这个类是否有父类，除了java.lang.Object之外。</p></li><li><p><strong>字节码验证</strong></p><p>通过数据流和控制流分析，确定程序语义是合法的、符合逻辑的。</p></li><li><p><strong>符号引用验证</strong></p><p>确保解析动作能正确执行。</p></li></ol><p>验证阶段是非常重要的，但不是必须的，它对程序运行期没有影响，如果所引用的类经过反复验证，那么可以考虑采用-Xverifynone 参数来关闭大部分的类验证措施，以缩短虚拟机类加载的时间。</p><h4 id="gt-准备"><a href="#gt-准备" class="headerlink" title="-&gt; 准备"></a>-&gt; 准备</h4><p><strong>在准备阶段，Java 虚拟机为类的静态变量分配内存，并设置默认的初始值。</strong></p><blockquote><p>准备阶段是正式为类变量分配内存并设置类变量初始值的阶段，这些内存都将在方法区中分配。对于该阶段有以下几点需要注意：</p></blockquote><ol><li><p>这时候进行内存分配的仅包括类变量（static），而不包括实例变量，实例变量会在对象实例化时随着对象一块分配在Java堆中。</p></li><li><p>这里所设置的初始值通常情况下是数据类型默认的零值（如0、0L、null、false等），而不是被在Java代码中被显式地赋予的值。</p><p>![截屏2020-09-29 下午4.25.03](/Users/joker/Library/Application Support/typora-user-images/截屏2020-09-29 下午4.25.03.png)</p></li></ol><p>假设一个类变量的定义为：public static int value = 3；</p><p>那么变量 value 在准备阶段过后的初始值为0，而不是3，因为这时候尚未开始执行任何 Java 方法，而把 value 赋值为3的 putstatic 指令是在程序编译后，存放于类构造器 <clinit>() 方法之中的，所以把 value 赋值为 3 的动作将在初始化阶段才会执行。</p><p>例如对于以下 Sample 类，在准备阶段，将为 int 类型的静态变量 a 分配4个字节的内存空间，并且赋予默认值0，为long类型的静态变量b分配8个字节的内存空间，并且赋予默认值0</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Sample</span></span>&#123;</span><br><span class="line">       <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">int</span> a=<span class="number">1</span>;</span><br><span class="line">       <span class="keyword">public</span>  <span class="keyword">static</span> <span class="keyword">long</span> b;</span><br><span class="line">       <span class="keyword">public</span>  <span class="keyword">static</span> <span class="keyword">long</span> c;</span><br><span class="line">       <span class="keyword">static</span> &#123;</span><br><span class="line">           b=<span class="number">2</span>;</span><br><span class="line">       &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="gt-解析"><a href="#gt-解析" class="headerlink" title="-&gt; 解析"></a>-&gt; 解析</h4><p>解析阶段是虚拟机将常量池内的符号引用替换为直接引用的过程，解析动作主要针对类或接口、字段、类方法、接口方法、方法类型、方法句柄和调用点限定符7类符号引用进行。<strong>符号引用</strong> 就是一组符号来描述目标，可以是任何字面量。</p><p><strong>直接引用</strong>就是直接指向目标的指针、相对偏移量或一个间接定位到目标的句柄。</p><h3 id="4-2-3-初始化"><a href="#4-2-3-初始化" class="headerlink" title="4.2.3. 初始化"></a>4.2.3. 初始化</h3><p><strong>在初始化阶段，Java 虚拟机执行类的初始化语句，为类的静态变量赋予初始值。</strong>JVM 负责对类进行初始化，主要对类变量进行初始化在程序中，静态变量的初始化有两种途径</p><p>初始化阶段就是执行类构造器方法<clinit>() 的过程</p><p>![截屏2020-09-29 下午4.30.01](/Users/joker/Library/Application Support/typora-user-images/截屏2020-09-29 下午4.30.01.png)</p><ol><li>在静态变量的声明处进行初始化</li><li>在静态代码块中进行初始化。</li></ol><p><strong><font color='blue'>准备阶段即使我们为静态变量赋值为任意的数值，但是该静态变量还是会被初始化为他的默认值，最后的初始化时才会把我们赋予的值设为该静态变量的值</font></strong></p><hr><h4 id="gt-初始化时机"><a href="#gt-初始化时机" class="headerlink" title="-&gt; 初始化时机"></a>-&gt; 初始化时机</h4><p><strong>Java 虚拟机实现必须在每个类或接口被 Java 程序 “首次主动使用” 时初始化他们</strong></p><ol><li>主动使用<ul><li>创建类的实例</li><li>访问某个类或接口的静态变量，或者对该静态变量赋值</li><li>调用该类的静态方法</li><li>反射</li><li>初始化一个类的子类</li><li>Java虚拟机启动时被标为启动类的类（Java Test）</li></ul></li><li>被动使用</li></ol><hr><h4 id="gt-类的初始化步骤"><a href="#gt-类的初始化步骤" class="headerlink" title="-&gt; 类的初始化步骤"></a>-&gt; <strong>类的初始化步骤</strong></h4><ol><li>假如这个类还没有被加载和连接，那就先进行加载和连接</li><li>假如类存在直接父类，并且这个父类还没有被初始化，那就先初始化直接父类</li><li>假如类中存在初始化语句，那就依次执行这些初始化语句</li></ol><p>当 Java 虚拟机初始化一个类时，要求它的所有父类都已经被初始化，<strong>但是这条规则不适用于接口</strong>。因此，一个父接口并不会因为它的子接口或者实现类的初始化而初始化。只有当程序首次使用特定的接口的静态变量时，才会导致该接口的初始化。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ClassLoadTest9</span> </span>&#123;</span><br><span class="line">    <span class="keyword">static</span> &#123;</span><br><span class="line">        System.out.println(<span class="string">&quot;ClassLoadTest9&quot;</span>);</span><br><span class="line">    &#125; </span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        System.out.println(Child1.a);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Parent1</span> </span>&#123;</span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">int</span> a = <span class="number">9</span>;</span><br><span class="line">    <span class="keyword">static</span> &#123;</span><br><span class="line">        System.out.println(<span class="string">&quot;Parent1&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Child1</span> <span class="keyword">extends</span> <span class="title">Parent1</span> </span>&#123;</span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">int</span> b = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">static</span> &#123;</span><br><span class="line">        System.out.println(<span class="string">&quot;Child1&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 最后输出顺序</span></span><br><span class="line"><span class="comment">// ClassLoadTest9</span></span><br><span class="line"><span class="comment">// Parent1</span></span><br><span class="line"><span class="comment">// 9</span></span><br></pre></td></tr></table></figure><hr><p><strong>接口初始化</strong></p><ol><li>当一个接口在初始化时，并不要求其父接口都完成了初始化只有在真正使用到父接口的时候（如引用接口中定义的常量），才会初始化</li></ol><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * 当一个接口在初始化时，并不要求其父接口都完成了初始化</span></span><br><span class="line"><span class="comment"> * 只有在真正使用到父接口的时候（如引用接口中定义的常量），才会初始化</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ClassLoadTest5</span> </span>&#123;</span><br><span class="line">   <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">       System.out.println(MyChild.b);</span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">interface</span> <span class="title">Student5</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> a = <span class="number">9</span>; <span class="comment">//前面省了public static final</span></span><br><span class="line">    Thread thread = <span class="keyword">new</span> Thread() &#123;</span><br><span class="line">        &#123;</span><br><span class="line">            System.out.println(<span class="string">&quot;thread 初始化了&quot;</span>);<span class="comment">//如果父接口初始化了这句应该输出</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"><span class="class"><span class="keyword">interface</span> <span class="title">MyChild</span> <span class="keyword">extends</span> <span class="title">Student5</span> </span>&#123;     <span class="comment">//接口属性默认是 public static final</span></span><br><span class="line">    String b = LocalDateTime.now().toString();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><hr><h2 id="类加载器"><a href="#类加载器" class="headerlink" title="类加载器"></a>类加载器</h2><p>类的加载由类加载器完成，JVM 提供的类加载器叫做系统类加载器，此外还可以通过继承 ClassLoader 基类来自定义类加载器。</p><h3 id="系统类加载器"><a href="#系统类加载器" class="headerlink" title="系统类加载器"></a>系统类加载器</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.poplar.classload;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ClassLoadTest18</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        System.out.println(System.getProperty(<span class="string">&quot;sun.boot.class.path&quot;</span>));<span class="comment">//根加载器路径</span></span><br><span class="line">        System.out.println(System.getProperty(<span class="string">&quot;java.ext.dirs&quot;</span>));<span class="comment">//扩展类加载器路径</span></span><br><span class="line">        System.out.println(System.getProperty(<span class="string">&quot;java.class.path&quot;</span>));<span class="comment">//应用类加载器路径</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><ul><li><p><strong>根类加载器</strong>（Bootstrap）</p><blockquote><p>该加载器没有父加载器，它负责加载虚拟机中的核心类库。根类加载器从系统属性 <code>sun.boot.class.path</code> 所指定的目录中加载类库。类加载器的实现依赖于底层操作系统，属于虚拟机的实现的一部分，它并没有集成<code>java.lang.ClassLoader</code> 类。</p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.poplar.classload;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ClassLoadTest7</span> </span>&#123;</span><br><span class="line">   <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">     <span class="comment">//null 由于String是由根加载器加载，在rt.jar包下</span></span><br><span class="line">     System.out.println(String.class.getClassLoader());</span><br><span class="line">     <span class="comment">//sun.misc.Launcher$AppClassLoader@73d16e93 </span></span><br><span class="line">     System.out.println(C.class.getClassLoader());</span><br><span class="line">   &#125;</span><br><span class="line"> &#125;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">C</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul><ul><li><p><strong>扩展类加载器</strong>（Extension）</p><blockquote><p>它的父加载器为根类加载器。它从 <code>java.ext.dirs</code> 系统属性所指定的目录中加载类库，或者从JDK的安装目录的jre\lib\ext子目录（扩展目录）下加载类库，如果把用户创建的jar文件放在这个目录下，也会自动由扩展类加载器加载，扩展类加载器是纯java类，是java.lang.ClassLoader的子类。</p></blockquote></li><li><p><strong>系统应用类加载器</strong>（AppClassLoader/System）</p><blockquote><p>也称为应用类加载器，它的父加载器为扩展类加载器，它从环境变量classpath或者系统属性java.class.path所指定的目录中加载类，他是用户自定义的类加载器的默认父加载器。系统类加载器时纯java类，是java.lang.ClassLoader的子类。</p></blockquote></li></ul><h3 id="用户自定义的类加载器"><a href="#用户自定义的类加载器" class="headerlink" title="用户自定义的类加载器"></a><strong>用户自定义的类加载器</strong></h3><ul><li>java.lang.ClassLoader的子类</li><li>用户可以定制类的加载方式</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.poplar.classload;</span><br><span class="line"><span class="keyword">import</span> java.io.*;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**  </span></span><br><span class="line"><span class="comment"> * 自定义类加载器</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CustomClassLoader</span> <span class="keyword">extends</span> <span class="title">ClassLoader</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> String classLoaderName;</span><br><span class="line">  <span class="keyword">private</span> String path;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setPath</span><span class="params">(String path)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">this</span>.path = path;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String filePost = <span class="string">&quot;.class&quot;</span>;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="title">CustomClassLoader</span><span class="params">(ClassLoader parent, String classLoaderName)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">super</span>(parent);<span class="comment">//显示指定该类的父类加载器</span></span><br><span class="line">    <span class="keyword">this</span>.classLoaderName = classLoaderName;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="title">CustomClassLoader</span><span class="params">(String classLoaderName)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">super</span>();<span class="comment">//将系统类加载器当作该类的父类加载器</span></span><br><span class="line">    <span class="keyword">this</span>.classLoaderName = classLoaderName;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> Class <span class="title">findClass</span><span class="params">(String name)</span> </span>&#123;</span><br><span class="line">    System.out.println(<span class="string">&quot;findClass,输出这句话说明我们自己的类加载器加载了指定的类&quot;</span>);</span><br><span class="line">    <span class="keyword">byte</span>[] b = loadClassData(name);</span><br><span class="line">    <span class="keyword">return</span> defineClass(name, b, <span class="number">0</span>, b.length);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">byte</span>[] loadClassData(String name) &#123;</span><br><span class="line">    InputStream is = <span class="keyword">null</span>;</span><br><span class="line">    <span class="keyword">byte</span>[] data = <span class="keyword">null</span>;</span><br><span class="line">    ByteArrayOutputStream byteArrayOutputStream = <span class="keyword">null</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      name = name.replace(<span class="string">&quot;.&quot;</span>, File.separator);<span class="comment">//File.separator根据操作系统而变化</span></span><br><span class="line">      is = <span class="keyword">new</span> FileInputStream(<span class="keyword">new</span> File(path + name + filePost));</span><br><span class="line">      byteArrayOutputStream = <span class="keyword">new</span> ByteArrayOutputStream();</span><br><span class="line">      <span class="keyword">int</span> len = <span class="number">0</span>;</span><br><span class="line">      <span class="keyword">while</span> (-<span class="number">1</span> != (len = is.read())) &#123;</span><br><span class="line">        byteArrayOutputStream.write(len);</span><br><span class="line">      &#125;</span><br><span class="line">      data = byteArrayOutputStream.toByteArray();</span><br><span class="line"></span><br><span class="line">    &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">      e.printStackTrace();</span><br><span class="line">    &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">      <span class="keyword">try</span> &#123;</span><br><span class="line">        is.close();</span><br><span class="line">        byteArrayOutputStream.close();</span><br><span class="line">      &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">        e.printStackTrace();</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">return</span> data;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/*</span></span><br><span class="line"><span class="comment">   执行结果： </span></span><br><span class="line"><span class="comment">   findClass,输出这句话说明我们自己的类加载器加载了指定的类</span></span><br><span class="line"><span class="comment">   com.poplar.classload.CustomClassLoader2@15db97422018699554</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">    CustomClassLoader2 Loader2 = <span class="keyword">new</span> CustomClassLoader2(<span class="string">&quot;load2&quot;</span>);</span><br><span class="line">    test1(loader2) </span><br><span class="line">    CustomClassLoader2 Loader3 = <span class="keyword">new</span> CustomClassLoader2(<span class="string">&quot;load3&quot;</span>);</span><br><span class="line">    test1(loader3)    </span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">test1</span><span class="params">(CustomClassLoader2 loader2)</span> <span class="keyword">throws</span> ClassNotFoundException, InstantiationException, IllegalAccessException </span>&#123;</span><br><span class="line">          loader2.setPath(<span class="string">&quot;C:\\Users\\poplar\\Desktop\\&quot;</span>);</span><br><span class="line">          Class&lt;?&gt; clazz = loader2.loadClass(<span class="string">&quot;com.poplar.classload.ClassLoadTest&quot;</span>);</span><br><span class="line">          Object instance = clazz.newInstance();</span><br><span class="line">          System.out.println(instance.getClass().getClassLoader());</span><br><span class="line">          System.out.println(instance.hashCode());</span><br><span class="line">          System.out.println(<span class="string">&quot;-------------------------------------&quot;</span>);</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><hr><p><strong>命名空间</strong></p><ul><li>每个类加载器都有自己的命名空间，<strong>命名空间由该加载器及所有父加载器所加载的类构成</strong>；</li><li>在同一个命名空间中，不会出现类的完整名字（包括类的包名）相同的两个类；</li><li>在不同的命名空间中，有可能会出现类的完整名字（包括类的包名）相同的两个类；</li><li>同一命名空间内的类是互相可见的，<strong>非同一命名空间内的类是不可见的</strong>；</li><li>子加载器可以见到父加载器加载的类，<strong>父加载器也不能见到子加载器加载的类</strong>。</li></ul><hr><p><strong>注：</strong></p><ul><li>类加载器本身也是类加载器，类加载器又是谁加载的呢？？（先有鸡还是现有蛋）<br>类加载器是由启动类加载器去加载的，启动类加载器是C++写的，内嵌在JVM中。</li><li>内嵌于JVM中的启动类加载器会加载java.lang.ClassLoader以及其他的Java平台类。当JVM启动时，一块特殊的机器码会运行，它会加载扩展类加载器以及系统类加载器，这块特殊的机器码叫做启动类加载器。</li><li>启动类加载器并不是java类，其他的加载器都是java类。</li><li>启动类加载器是特定于平台的机器指令，它负责开启整个加载过程。</li></ul><hr><h3 id="双亲委派机制"><a href="#双亲委派机制" class="headerlink" title="双亲委派机制"></a>双亲委派机制</h3><p>类加载器用来把类加载到 Java 虚拟机中。从 JDK1.2 版本开始，类的加载过程采用双亲委托机制，这种机制能更好地保证 Java 平台的安全。在此委托机制中，除了 Java 虚拟机自带的根类加载器以外，其余的类加载器都有且只有一个父加载器。当 Java 程序请求加载器 loader1 加载 Sample 类时，loader1 首先委托自己的父加载器去加载 Sample 类，若父加载器能加载，则有父加载器完成加载任务，否则才由加载器loader1 本身加载 Sample 类。</p><p>在双亲委托机制中，各个加载器按照父子关系形成了树形结构，除了根加载器之外，其余的类加载器都有一个父加载器若有一个类能够成功加载 Test 类，那么这个类加载器被称为<strong>定义类加载器</strong>，所有能成功返回Class对象引用的类加载器（包括定义类加载器）称为<strong>初始类加载器</strong>。</p><h4 id="gt-工作过程"><a href="#gt-工作过程" class="headerlink" title="-&gt; 工作过程"></a>-&gt; 工作过程</h4><p>如果一个类加载器收到了类加载的请求，他首先不会自己去尝试加载这个类，而是把这个请求委派父类加载器去完成。每一个层次的类加载器都是如此，因此所有的加载请求最终都传送到顶层的启动类加载器中，只有当父加载器反馈自己无法完成这个请求[加载器的搜索范围中没有找到所需的类]时，子加载器才会尝试自己去加载。</p><hr><h4 id="gt-优点"><a href="#gt-优点" class="headerlink" title="-&gt; 优点"></a>-&gt; 优点</h4><ol><li><p>可以确保 Java 和核心库的安全</p><blockquote><p>所有的 Java 应用都会引用 java.lang 中的类，也就是说在运行期java.lang中的类会被加载到虚拟机中，如果这个加载过程如果是由自己的类加载器所加载，那么很可能就会在JVM中存在多个版本的java.lang中的类，而且这些类是相互不可见的（命名空间的作用）。借助于双亲委托机制，Java核心类库中的类的加载工作都是由启动根加载器去加载，从而确保了Java应用所使用的的都是同一个版本的Java核心类库，他们之间是相互兼容的</p></blockquote></li><li><p>确保 Java 核心类库中的类不会被自定义的类所替代</p></li><li><p>不同的类加载器可以为相同名称的类（binary name）创建额外的命名空间。相同名称的类可以并存在Java虚拟机中，只需要用不同的类加载器去加载即可。相当于在 Java 虚拟机内部建立了一个又一个相互隔离的 Java 类空间。</p></li><li><p><strong>避免重复加载</strong>，父类已经加载过的类，子类无需重新加载。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.poplar.classload;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ClassLoadTest10</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">static</span> &#123;</span><br><span class="line">    System.out.println(<span class="string">&quot;ClassLoadTest10&quot;</span>);</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  <span class="comment">/*执行结果：由于父类已经初始化过了所以Parent2只输出一次</span></span><br><span class="line"><span class="comment">   * ClassLoadTest10</span></span><br><span class="line"><span class="comment">   * Parent2</span></span><br><span class="line"><span class="comment">   * 2</span></span><br><span class="line"><span class="comment">   * Child2</span></span><br><span class="line"><span class="comment">   * 3</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">    Parent2 parent2;</span><br><span class="line">    parent2 = <span class="keyword">new</span> Parent2();</span><br><span class="line">    System.out.println(Parent2.a);</span><br><span class="line">    System.out.println(Child2.b);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Parent2</span> </span>&#123;</span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">int</span> a = <span class="number">2</span>;</span><br><span class="line">    <span class="keyword">static</span> &#123;</span><br><span class="line">        System.out.println(<span class="string">&quot;Parent2&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Child2</span> <span class="keyword">extends</span> <span class="title">Parent2</span> </span>&#123;</span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">int</span> b = <span class="number">3</span>;</span><br><span class="line">    <span class="keyword">static</span> &#123;</span><br><span class="line">        System.out.println(<span class="string">&quot;Child2&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ol><hr><h4 id="gt-双亲委派模型破坏"><a href="#gt-双亲委派模型破坏" class="headerlink" title="-&gt; 双亲委派模型破坏"></a>-&gt; <strong>双亲委派模型破坏</strong></h4><p>在实际的应用中双亲委派解决了java 基础类统一加载的问题，但是却着实存在着一定的缺席。jdk 中的基础类作为用户典型的api 被调用，但是也存在被 api 调用用户的代码的情况，典型的如 SPI 代码。</p><h5 id="SPI-机制简介"><a href="#SPI-机制简介" class="headerlink" title="SPI 机制简介"></a>SPI 机制简介</h5><hr><h3 id="类的卸载"><a href="#类的卸载" class="headerlink" title="类的卸载"></a>类的卸载</h3><ul><li>当一个类被加载、连接和初始化之后，它的生命周期就开始了。当此类的Class对象不再被引用，即不可触及时，Class对象就会结束生命周期，类在方法区内的数据也会被卸载。</li><li>一个类何时结束生命周期，取决于代表它的Class对象何时结束生命周期。</li><li>由Java虚拟机自带的类加载器所加载的类，在虚拟机的生命周期中，始终不会被卸载。Java虚拟机本身会始终引用这些加载器，而这些类加载器则会始终引用他们所加载的类的Class对象，因此这些Class对象是可触及的。</li><li>由用户自定义的类加载器所加载的类是可以被卸载的。（<strong>jvisualvm 查看当前java进程 -XX:+TraceClassUnloading这个用于追</strong>）</li></ul><img src="/Users/joker/Documents/Learnning/JVM/resources/images/截屏2020-09-29 下午2.23.49.png" alt="截屏2020-09-29 下午2.23.49" style="zoom:50%;" />]]></content>
      
      
      <categories>
          
          <category> 深入理解 JVM </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java </tag>
            
            <tag> JVM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SparkSQL 基于 HBase 做⾃定义数据源</title>
      <link href="2019/03/02/SparkSQL-%E5%9F%BA%E4%BA%8E-HBase-%E5%81%9A%E2%BE%83%E5%AE%9A%E4%B9%89%E6%95%B0%E6%8D%AE%E6%BA%90/"/>
      <url>2019/03/02/SparkSQL-%E5%9F%BA%E4%BA%8E-HBase-%E5%81%9A%E2%BE%83%E5%AE%9A%E4%B9%89%E6%95%B0%E6%8D%AE%E6%BA%90/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>列族设计</title>
      <link href="2019/02/28/%E6%95%B0%E6%8D%AE%E6%9F%A5%E8%AF%A2%E5%B9%B3%E5%8F%B0/"/>
      <url>2019/02/28/%E6%95%B0%E6%8D%AE%E6%9F%A5%E8%AF%A2%E5%B9%B3%E5%8F%B0/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p>[TOC]</p><h1 id="1、项目需求"><a href="#1、项目需求" class="headerlink" title="1、项目需求"></a>1、项目需求</h1><p>数据类型:csv, orc, excel, word, txt, log</p><p>数据存储位置</p><ul><li><p>不是在一台机器上</p></li><li><p>第三方</p></li><li><p>公司内部多个系统，</p></li></ul><p>因为历史积累原因。数据集存储在了不同的引擎：redis、mysql、hbase、hdfs、elasticsearch、kafka等</p><p>因为早前没有统一规划数据源，导致数据源多达十几种；所以不好规划统一，导致查询关联及其麻烦（要各种预先抽取多个数据源到同一个地方，然后在做统一处理，最后出报表 ，而且查询及其缓慢）</p><h1 id="2、涉及知识点"><a href="#2、涉及知识点" class="headerlink" title="2、涉及知识点"></a>2、涉及知识点</h1><p>1、前后端任务交换指令：Akka</p><p>2、计算引擎：sparkSQL</p><p>3、二次定义sparkSQL语法：Antlr</p><p>4、服务自动发现：zookeeper</p><h1 id="3、项目架构说明"><a href="#3、项目架构说明" class="headerlink" title="3、项目架构说明"></a>3、项目架构说明</h1><h3 id="3-1-逻辑架构"><a href="#3-1-逻辑架构" class="headerlink" title="3.1.逻辑架构"></a>3.1.逻辑架构</h3><p><img src="/Users/zxc/Documents/hexo/source/_posts/%E6%95%B0%E6%8D%AE%E6%9F%A5%E8%AF%A2%E5%B9%B3%E5%8F%B0.assets/image-20200226101743131.png" alt="image-20200226101743131"></p><h3 id="3-2-项目架构"><a href="#3-2-项目架构" class="headerlink" title="3.2.项目架构"></a>3.2.项目架构</h3><p><img src="../images/platform/image-20200229121357540.png" alt="image-20200229121357540"></p><h1 id="4、最终目标"><a href="#4、最终目标" class="headerlink" title="4、最终目标"></a>4、最终目标</h1><h2 id="1、项目的实现细节"><a href="#1、项目的实现细节" class="headerlink" title=".1、项目的实现细节"></a>.1、项目的实现细节</h2><p><img src="/Users/zxc/Documents/hexo/source/_posts/%E6%95%B0%E6%8D%AE%E6%9F%A5%E8%AF%A2%E5%B9%B3%E5%8F%B0.assets/image-20200226205823252.png" alt="image-20200226205823252"></p><h1 id="4、Actor入门"><a href="#4、Actor入门" class="headerlink" title="4、Actor入门"></a>4、Actor入门</h1><p>通讯原理图:</p><p><img src="assets/421906-20190303184058021-556718746.png" alt="img"></p><h2 id="4-1、java和scala在并发编程模型对比："><a href="#4-1、java和scala在并发编程模型对比：" class="headerlink" title="4.1、java和scala在并发编程模型对比："></a>4.1、java和scala在并发编程模型对比：</h2><table><thead><tr><th>Java内置线程模型</th><th>Scala Actor模型</th></tr></thead><tbody><tr><td>“共享数据锁模型”</td><td>share nothing</td></tr><tr><td>每个object有一个monitor，监视多线程对共享数据的访问【线程内部】</td><td>不共享数据，actor之间通过message传递（基于事件驱动）</td></tr><tr><td>加锁的代码通过synchronized标志</td><td></td></tr><tr><td>死锁的问题</td><td></td></tr><tr><td>每个线程内部是顺序执行的</td><td>每个actor内部是顺序执行的</td></tr></tbody></table><h2 id="4-2、Actor的执行顺序"><a href="#4-2、Actor的执行顺序" class="headerlink" title="4.2、Actor的执行顺序"></a>4.2、Actor的执行顺序</h2><p>1、调用start（）方法启动Actor</p><p>2、调用start（）方法后其act()方法会被执行</p><p>3、向Actor发送消息</p><h2 id="4-3、发送消息的方式"><a href="#4-3、发送消息的方式" class="headerlink" title="4.3、发送消息的方式"></a>4.3、发送消息的方式</h2><table><thead><tr><th>!</th><th>发送异步消息，没有返回值。</th></tr></thead><tbody><tr><td>!?</td><td>发送同步消息，等待返回值。</td></tr><tr><td>!!</td><td>发送异步消息，返回值是 Future[Any]。</td></tr></tbody></table><h2 id="4-4、Actor例子"><a href="#4-4、Actor例子" class="headerlink" title="4.4、Actor例子"></a>4.4、Actor例子</h2><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.scala-lang<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>scala-actors<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.11.8<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="4-1-1、Actor可以不断的接收消息"><a href="#4-1-1、Actor可以不断的接收消息" class="headerlink" title="4.1.1、Actor可以不断的接收消息"></a>4.1.1、Actor可以不断的接收消息</h3><p><img src="image/image-20200226112623582.png" alt="image-20200226112623582"></p><p>驱动程序</p><p><img src="image/image-20200226112642299.png" alt="image-20200226112642299"></p><p>说明：在act()方法中加入了while (true) 循环，就可以不停的接收消息</p><p>注意：发送start消息和stop的消息是异步的，但是Actor接收到消息执行的过程是同步的按顺序执行</p><h3 id="4-1-2：结合case-class发送消息"><a href="#4-1-2：结合case-class发送消息" class="headerlink" title="4.1.2：结合case class发送消息"></a>4.1.2：结合case class发送消息</h3><h4 id="1）：写3个case-class"><a href="#1）：写3个case-class" class="headerlink" title="1）：写3个case class"></a>1）：写3个case class</h4><p><img src="/Users/zxc/Documents/hexo/source/_posts/%E6%95%B0%E6%8D%AE%E6%9F%A5%E8%AF%A2%E5%B9%B3%E5%8F%B0.assets/image-20200226132545991.png" alt="image-20200226132545991"></p><h4 id="2）：接收消息的actor"><a href="#2）：接收消息的actor" class="headerlink" title="2）：接收消息的actor"></a>2）：接收消息的actor</h4><p><img src="/Users/zxc/Documents/hexo/source/_posts/%E6%95%B0%E6%8D%AE%E6%9F%A5%E8%AF%A2%E5%B9%B3%E5%8F%B0.assets/image-20200226132614679.png" alt="image-20200226132614679"></p><h4 id="3）：驱动"><a href="#3）：驱动" class="headerlink" title="3）：驱动"></a>3）：驱动</h4><p><img src="image/image-20200226132638126.png" alt="image-20200226132638126"></p><h1 id="5：Akka入门"><a href="#5：Akka入门" class="headerlink" title="5：Akka入门"></a>5：Akka入门</h1><p>akka，一款高性能，高容错，分布式的并行框架</p><p>特点:</p><p>1.并行与并发</p><p>2.异步非阻塞</p><p>3.容错</p><p>4.持久化</p><p>5.轻量级，每个actor占用内存比较小 300byte,1G 内存容纳300w个actor</p><p>场景:</p><p>分布式计算中的分布式通讯，解决的是高并发场景的问题，（消息体比较小），吞吐量不是很高，零拷贝（）</p><p>密集型计算场景</p><p>总结：对高并发和密集型的计算场景，akka都可以使用</p><h2 id="5-1、使用Akka来进行消息传递"><a href="#5-1、使用Akka来进行消息传递" class="headerlink" title="5.1、使用Akka来进行消息传递"></a>5.1、使用Akka来进行消息传递</h2><p><img src="image/image-20200226150503739.png" alt="image-20200226150503739"></p><p>驱动：</p><p><img src="image/image-20200226150523120.png" alt="image-20200226150523120"></p><p>pom里面需要引入akka的配置:</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>com.typesafe.akka<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>akka-remote_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.5.9<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><h1 id="6、工程搭建"><a href="#6、工程搭建" class="headerlink" title="6、工程搭建"></a>6、工程搭建</h1><h2 id="6-1、工程模块的创建"><a href="#6-1、工程模块的创建" class="headerlink" title="6.1、工程模块的创建"></a>6.1、工程模块的创建</h2><p>1）：创建模块名称和工程包名</p><p><img src="image/image-20200226225147512.png" alt="image-20200226225147512"></p><h2 id="6-2、给工程添加依赖关系"><a href="#6-2、给工程添加依赖关系" class="headerlink" title="6.2、给工程添加依赖关系"></a>6.2、给工程添加依赖关系</h2><p>各个模块配置工程以来pom.xml文件</p><h2 id="6-3：编写驱动程序"><a href="#6-3：编写驱动程序" class="headerlink" title="6.3：编写驱动程序"></a>6.3：编写驱动程序</h2><p>本节的目的：</p><p>把驱动程序编写好，并启动起来；</p><p>但是让驱动能够顺利启动，我们需要完成如下操作：</p><p><img src="image/image-20200229212621597.png" alt="image-20200229212621597"></p><h3 id="6-3-1、编写参数的获取和校验操作"><a href="#6-3-1、编写参数的获取和校验操作" class="headerlink" title="6.3.1、编写参数的获取和校验操作"></a>6.3.1、编写参数的获取和校验操作</h3><p><img src="image/image-20200226230550952.png" alt="image-20200226230550952"></p><h3 id="6-3-2、构建解释器基类"><a href="#6-3-2、构建解释器基类" class="headerlink" title="6.3.2、构建解释器基类"></a>6.3.2、构建解释器基类</h3><p><img src="image/image-20200226231204688.png" alt="image-20200226231204688"></p><p>然后写任务状态的基类：</p><p><img src="image/image-20200226165224753.png" alt="image-20200226165224753"></p><p>在提供对外的解释器接口</p><p><img src="image/image-20200227010740434.png" alt="image-20200227010740434"></p><h3 id="6-3-3、创建文本解析标识的代码包"><a href="#6-3-3、创建文本解析标识的代码包" class="headerlink" title="6.3.3、创建文本解析标识的代码包"></a>6.3.3、创建文本解析标识的代码包</h3><p><img src="image/image-20200226231501546.png" alt="image-20200226231501546"></p><h3 id="6-3-4、对解释器基类的增强"><a href="#6-3-4、对解释器基类的增强" class="headerlink" title="6.3.4、对解释器基类的增强"></a>6.3.4、对解释器基类的增强</h3><p><img src="image/image-20200226233012070.png" alt="image-20200226233012070"></p><h4 id="第一步：在伴生类中提供换行符匹配操作（就是个正则表达式）"><a href="#第一步：在伴生类中提供换行符匹配操作（就是个正则表达式）" class="headerlink" title="第一步：在伴生类中提供换行符匹配操作（就是个正则表达式）"></a>第一步：在伴生类中提供换行符匹配操作（就是个正则表达式）</h4><p><img src="image/image-20200227010827958.png" alt="image-20200227010827958"></p><h4 id="第二步：对接口进行功能增强"><a href="#第二步：对接口进行功能增强" class="headerlink" title="第二步：对接口进行功能增强"></a>第二步：对接口进行功能增强</h4><p>问题:对spark-shell绑定变量的作用是什么?</p><p><img src="image/image-20200226233516692.png" alt="image-20200226233516692"></p><h4 id="第三步：spark-shell绑定变量"><a href="#第三步：spark-shell绑定变量" class="headerlink" title="第三步：spark-shell绑定变量"></a>第三步：spark-shell绑定变量</h4><p>spark-shell绑定变量，第一个要绑定的就是sparkSession</p><p>以及：sparkContext下面的内容、隐士转换、sparkSQL、udf函数等内容</p><p>但是现在我们还没有sparkSession ， 所以我们先实现一个sparkSession的构建</p><p>【com.kkb.engine下面构建EngineSession】</p><p><img src="image/image-20200226234148574.png" alt="image-20200226234148574"></p><p>因为执行了enableHiveSupport，所以需要加入服务器的hive-site.xml文件</p><p><img src="image/image-20200226234416770.png" alt="image-20200226234416770"></p><p>然后启动metastore服务</p><p><img src="image/image-20200226234614813.png" alt="image-20200226234614813"></p><p>因为我们参照的是livy代码，本身就是一个rest 服务，用来做spark和web端的一种交互；</p><p>所以livy很好的帮我们解决了，spark-shell从初始化的绑定、到绑定变量错误的处理；</p><p>都已经帮我们解决好了</p><p><img src="image/image-20200227001748520.png" alt="image-20200227001748520"></p><p><img src="image/image-20200227001921833.png" alt="image-20200227001921833"></p><p><img src="image/image-20200227002054594.png" alt="image-20200227002054594"></p><p><img src="image/image-20200227002145504.png" alt="image-20200227002145504"></p><h3 id="6-3-5、实现spark解释器"><a href="#6-3-5、实现spark解释器" class="headerlink" title="6.3.5、实现spark解释器"></a>6.3.5、实现spark解释器</h3><p><img src="image/image-20200227002831717.png" alt="image-20200227002831717"></p><p><img src="image/image-20200227003136968.png" alt="image-20200227003136968"></p><p><img src="image/image-20200227003619514.png" alt="image-20200227003619514"></p><p><img src="image/image-20200227003731607.png" alt="image-20200227003731607"></p><p>这样，我们构建好了spark的解释器，实际就是为了构建一个属于自己的spark-shell；</p><p>好在其他框架实现了，我们只需要把其他框架的源码拿来即可</p><p>所以我们在回到 驱动程序App类：</p><p>【App类】</p><p><img src="image/image-20200227004109191.png" alt="image-20200227004109191"></p><h3 id="6-3-6：获取zk的客户端"><a href="#6-3-6：获取zk的客户端" class="headerlink" title="6.3.6：获取zk的客户端"></a>6.3.6：获取zk的客户端</h3><p>这样，我们继续按照流程往下走 ， 那么此时就要构建zk的客户端了</p><p>因为，我们后面会把 引擎注册到zk里面，并且依赖于zk进行服务的自动发现</p><p>【在common工程下，创建zk的工具类】</p><h4 id="第一步：导包"><a href="#第一步：导包" class="headerlink" title="第一步：导包"></a>第一步：导包</h4><p>我们采用第三方的zk工具，尽量帮我们封装代码</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>com.101tec<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>zkclient<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>0.7<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><p>然后创建ZKUtils工具类</p><p><img src="image/image-20200227004951874.png" alt="image-20200227004951874"></p><h4 id="第二步：编写获取zk客户端代码"><a href="#第二步：编写获取zk客户端代码" class="headerlink" title="第二步：编写获取zk客户端代码"></a>第二步：编写获取zk客户端代码</h4><p>1）：</p><p><img src="image/image-20200227005235931.png" alt="image-20200227005235931"></p><p>2）：</p><p><img src="image/image-20200227005617623.png" alt="image-20200227005617623"></p><p>3）：</p><p><img src="image/image-20200227005931483.png" alt="image-20200227005931483"></p><p>4）：</p><p>把之前写的加载配置文件的工具类，放入common工程下</p><p><img src="image/image-20200227012355971.png" alt="image-20200227012355971"></p><p>然后把common工程打入engine工程里面</p><p>就是在engine的pom文件里面添加common工程包</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>com.kkb.platform<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>common<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.0-SNAPSHOT<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><p><img src="image/image-20200227012602948.png" alt="image-20200227012602948"></p><p>5）：</p><p><img src="image/image-20200227012759219.png" alt="image-20200227012759219"></p><h3 id="6-3-7、注册Akka并发变成模型"><a href="#6-3-7、注册Akka并发变成模型" class="headerlink" title="6.3.7、注册Akka并发变成模型"></a>6.3.7、注册Akka并发变成模型</h3><h4 id="第一步：把基本信息类导入"><a href="#第一步：把基本信息类导入" class="headerlink" title="第一步：把基本信息类导入"></a>第一步：把基本信息类导入</h4><p><img src="image/image-20200227014728761.png" alt="image-20200227014728761"></p><h4 id="第二步：考虑引擎在zk中的情况"><a href="#第二步：考虑引擎在zk中的情况" class="headerlink" title="第二步：考虑引擎在zk中的情况"></a>第二步：考虑引擎在zk中的情况</h4><p>那么，我们如果注册引擎，其实就是把PlatEngine这个类注册到zk里面去；</p><p>那么问题来了：</p><p>最开始运行的时候 ，zk里面肯定没有引擎信息(所谓引擎信息 ， 我们认为其实就是id –&gt; ip:port)；</p><p>1 –&gt; node1:3001</p><p>2—&gt;node2:3002</p><p>但是如果不是最初启动时候，那么引擎肯定是存在的，那么我们就要确保，再次注册引擎的时候 ，</p><p>id—&gt;ip:port</p><p>这里面的id和端口，绝对不能重复</p><p><img src="image/image-20200227015836730.png" alt="image-20200227015836730"></p><p>所以我们需要让id和端口绝对不一致，那么最好的方式就是用zk来维护状态 ， 如果这个状态存在，那么就将id和端口递增</p><p><img src="image/image-20200227021538703.png" alt="image-20200227021538703"></p><h4 id="第三步：编写引擎注册到zk的代码"><a href="#第三步：编写引擎注册到zk的代码" class="headerlink" title="第三步：编写引擎注册到zk的代码"></a>第三步：编写引擎注册到zk的代码</h4><p>注册引擎，大概分成4个步骤：</p><p>1、准备好目录信息</p><p>2、创建引擎的父目录</p><p>3、创建引擎的临时节点（数据写入节点信息）</p><p>4、把前3步合并</p><p>那么接下来，我们来实现这个功能：</p><h5 id="1、准备好目录信息"><a href="#1、准备好目录信息" class="headerlink" title="1、准备好目录信息"></a>1、准备好目录信息</h5><p><img src="image/image-20200228110702511.png" alt="image-20200228110702511"></p><h5 id="2、创建引擎的父目录"><a href="#2、创建引擎的父目录" class="headerlink" title="2、创建引擎的父目录"></a>2、创建引擎的父目录</h5><p>如果上面这个路径在zk里面是不存在的，那么直接创建一个永久节点路径；作为引擎的存储路径</p><p><img src="image/image-20200228111049584.png" alt="image-20200228111049584"></p><h5 id="3、创建引擎的临时节点（数据写入节点信息）"><a href="#3、创建引擎的临时节点（数据写入节点信息）" class="headerlink" title="3、创建引擎的临时节点（数据写入节点信息）"></a>3、创建引擎的临时节点（数据写入节点信息）</h5><p>这样，有了父目录之后，我们就可以把数据信息写入；</p><p><img src="image/image-20200228121027512.png" alt="image-20200228121027512"></p><p>所以健壮的写法是，必须考虑，如果父目录不存在的情况：</p><p><img src="image/image-20200228123230934.png" alt="image-20200228123230934"></p><h5 id="4、合并前3步，完成引擎的注册"><a href="#4、合并前3步，完成引擎的注册" class="headerlink" title="4、合并前3步，完成引擎的注册"></a>4、合并前3步，完成引擎的注册</h5><p><img src="image/image-20200228130021725.png" alt="image-20200228130021725"></p><p>‘</p><h4 id="第四步：封装注册引擎代码，并返回akka配置信息"><a href="#第四步：封装注册引擎代码，并返回akka配置信息" class="headerlink" title="第四步：封装注册引擎代码，并返回akka配置信息"></a>第四步：封装注册引擎代码，并返回akka配置信息</h4><h5 id="1、创建包和类"><a href="#1、创建包和类" class="headerlink" title="1、创建包和类"></a>1、创建包和类</h5><p><img src="image/image-20200228152051356.png" alt="image-20200228152051356"></p><h5 id="2、"><a href="#2、" class="headerlink" title="2、"></a>2、</h5><p><img src="image/image-20200228152619122.png" alt="image-20200228152619122"></p><h5 id="3、利用我们刚刚写的zk方法，进行注册"><a href="#3、利用我们刚刚写的zk方法，进行注册" class="headerlink" title="3、利用我们刚刚写的zk方法，进行注册"></a>3、利用我们刚刚写的zk方法，进行注册</h5><p><img src="image/image-20200228152827421.png" alt="image-20200228152827421"></p><h4 id="第五步：需要考虑，如果后续需要注册多个引擎，怎么办"><a href="#第五步：需要考虑，如果后续需要注册多个引擎，怎么办" class="headerlink" title="第五步：需要考虑，如果后续需要注册多个引擎，怎么办"></a>第五步：需要考虑，如果后续需要注册多个引擎，怎么办</h4><h5 id="1、去zk里面查看是否有已经存在的引擎"><a href="#1、去zk里面查看是否有已经存在的引擎" class="headerlink" title="1、去zk里面查看是否有已经存在的引擎"></a>1、去zk里面查看是否有已经存在的引擎</h5><h6 id="1-1）：在zk里面添加查询子节点的功能"><a href="#1-1）：在zk里面添加查询子节点的功能" class="headerlink" title="1.1）：在zk里面添加查询子节点的功能"></a>1.1）：在zk里面添加查询子节点的功能</h6><p><img src="image/image-20200228163034784.png" alt="image-20200228163034784"></p><p>获取完子节点后，把里面的数据拿出来，就是IP:port</p><h6 id="1-2）：获取子节点里面的数据"><a href="#1-2）：获取子节点里面的数据" class="headerlink" title="1.2）：获取子节点里面的数据"></a>1.2）：获取子节点里面的数据</h6><p><img src="image/image-20200228171702935.png" alt="image-20200228171702935"></p><h6 id="1-3）：将返回的元组：-Option-String-Stat-，-封装成引擎信息"><a href="#1-3）：将返回的元组：-Option-String-Stat-，-封装成引擎信息" class="headerlink" title="1.3）：将返回的元组：(Option[String , Stat]) ， 封装成引擎信息"></a>1.3）：将返回的元组：(Option[String , Stat]) ， 封装成引擎信息</h6><p>【将1.2代码利用到1.3】</p><p><img src="image/image-20200228172106022.png" alt="image-20200228172106022"></p><h6 id="1-4）：合并前三步"><a href="#1-4）：合并前三步" class="headerlink" title="1.4）：合并前三步"></a>1.4）：合并前三步</h6><p>我们第一步获取子节点名称（id：1、2、3）</p><p>然后根据子节点的名称，来通过第第三步，来获取具体的引擎信息</p><p>所以，我们这一步做个合并处理</p><p><img src="image/image-20200228182805326.png" alt="image-20200228182805326"></p><p>这样我们封装好了，如何获取zookeeper里面的引擎信息，那么按照步骤，我们接下来要顺序注册引擎</p><p>比如：</p><p>最开始</p><p>id = 1 ， port = 3000</p><p>顺序增长</p><p>id = 2 ， port = 3001</p><h5 id="2、顺序增长id和port"><a href="#2、顺序增长id和port" class="headerlink" title="2、顺序增长id和port"></a>2、顺序增长id和port</h5><p>那么顺序增长的前提是，当前zookeeper里面已经存在这个引擎了，所以才会顺序增长</p><p><img src="image/image-20200228184339213.png" alt="image-20200228184339213"></p><p>最后在把我们刚刚写好的，注册引擎拿到最下面，顺序增长完port和id以后，开始注册</p><h5 id="3、把注册代码拿下来"><a href="#3、把注册代码拿下来" class="headerlink" title="3、把注册代码拿下来"></a>3、把注册代码拿下来</h5><p><img src="image/image-20200228184540919.png" alt="image-20200228184540919"></p><h3 id="6-3-8、回到驱动类，注册Akka信息"><a href="#6-3-8、回到驱动类，注册Akka信息" class="headerlink" title="6.3.8、回到驱动类，注册Akka信息"></a>6.3.8、回到驱动类，注册Akka信息</h3><p>这样，我们在回到驱动类，注册Akka信息</p><p><img src="image/image-20200228184831942.png" alt="image-20200228184831942"></p><h3 id="6-3-9：测试上面的代码"><a href="#6-3-9：测试上面的代码" class="headerlink" title="6.3.9：测试上面的代码"></a>6.3.9：测试上面的代码</h3><p>然后我们在测试下，上面写的代码是不是达到预期的效果</p><p><img src="image/image-20200228191315865.png" alt="image-20200228191315865"></p><p>然后去zookeeper里面查看</p><p><img src="image/image-20200228191413561.png" alt="image-20200228191413561"></p><h3 id="6-3-10、获取当前akka的参数"><a href="#6-3-10、获取当前akka的参数" class="headerlink" title="6.3.10、获取当前akka的参数"></a>6.3.10、获取当前akka的参数</h3><p>经过测试，我们上面写的代码没有任何问题；</p><p>那么接下来，我们要获取当前akka的地址（也就是引擎的地址）</p><p><img src="image/image-20200228195113493.png" alt="image-20200228195113493"></p><h3 id="6-3-11、把引擎信息，维护到EngineSession里面"><a href="#6-3-11、把引擎信息，维护到EngineSession里面" class="headerlink" title="6.3.11、把引擎信息，维护到EngineSession里面"></a>6.3.11、把引擎信息，维护到EngineSession里面</h3><p>我们在构建spark-shell功能时候，把sparkSession维护到了EngineSession里面了</p><p>那么干脆 ， 我们就把这个类作为任务状态的统计类；</p><p>那么我们把引擎信息，也维护进来</p><p><img src="image/image-20200228200531271.png" alt="image-20200228200531271"></p><p>然后我们回到驱动程序，实例化一个EngineSession</p><p><img src="image/image-20200228200733678.png" alt="image-20200228200733678"></p><h3 id="6-3-12、设置并行度"><a href="#6-3-12、设置并行度" class="headerlink" title="6.3.12、设置并行度"></a>6.3.12、设置并行度</h3><p>那么接下来，我们就要启动Akka了 ；</p><p><img src="image/image-20200229101416642.png" alt="image-20200229101416642"></p><p><img src="image/image-20200229104149185.png" alt="image-20200229104149185"></p><p>这样，我们有了内部参数的维护，那么我们在返回驱动程序 ， 把并行度设置上</p><p><img src="image/image-20200229104215439.png" alt="image-20200229104215439"></p><h3 id="6-3-13、创建Akka模型"><a href="#6-3-13、创建Akka模型" class="headerlink" title="6.3.13、创建Akka模型"></a>6.3.13、创建Akka模型</h3><p>我们之所以设置并行度，最终目标就是要并行的启动Akka任务模型</p><p>所以接下来，我们要创建 一个Akka的任务模型</p><p><img src="image/image-20200229110101464.png" alt="image-20200229110101464"></p><h3 id="6-3-14：启动Akka模型"><a href="#6-3-14：启动Akka模型" class="headerlink" title="6.3.14：启动Akka模型"></a>6.3.14：启动Akka模型</h3><p>现在我们有了akka模型 ， 而且我们也拿到了并行度</p><p>接下来启动所有模型</p><p><img src="image/image-20200229110527970.png" alt="image-20200229110527970"></p><p>这样我们就正常的启动了；</p><p>但是有这样一种可能，就是很可能主线程提前结束了，子线程还在继续运行；</p><p>就是可能会出现僵尸进程！</p><h3 id="6-3-15：避免出现僵尸进程"><a href="#6-3-15：避免出现僵尸进程" class="headerlink" title="6.3.15：避免出现僵尸进程"></a>6.3.15：避免出现僵尸进程</h3><p>所以我们需要让主线程等待子线程结束后，在执行关闭回收操作</p><p>所以我们在EngineSession里面，添加一些功能，让主线程等待子线程</p><p><img src="image/image-20200229114026613.png" alt="image-20200229114026613"></p><p><img src="image/image-20200229114459570.png" alt="image-20200229114459570"></p><p><img src="image/image-20200229150815199.png" alt="image-20200229150815199"></p><p>然后让JobActor继承这个日志功能</p><p><img src="image/image-20200229150935521.png" alt="image-20200229150935521"></p><h1 id="7、编写JobActor"><a href="#7、编写JobActor" class="headerlink" title="7、编写JobActor"></a>7、编写JobActor</h1><h2 id="7-1、编写jobActor的初始化preStart阶段"><a href="#7-1、编写jobActor的初始化preStart阶段" class="headerlink" title="7.1、编写jobActor的初始化preStart阶段"></a>7.1、编写jobActor的初始化preStart阶段</h2><h3 id="7-1-1、将jobActor注册到zookeeper中"><a href="#7-1-1、将jobActor注册到zookeeper中" class="headerlink" title="7.1.1、将jobActor注册到zookeeper中"></a>7.1.1、将jobActor注册到zookeeper中</h3><h4 id="第一步，在ZKUtiils里面添加引擎路径"><a href="#第一步，在ZKUtiils里面添加引擎路径" class="headerlink" title="第一步，在ZKUtiils里面添加引擎路径"></a>第一步，在ZKUtiils里面添加引擎路径</h4><p><img src="image/image-20200301150403124.png" alt="image-20200301150403124"></p><h4 id="第二步：在jobActor里拼接引擎路径"><a href="#第二步：在jobActor里拼接引擎路径" class="headerlink" title="第二步：在jobActor里拼接引擎路径"></a>第二步：在jobActor里拼接引擎路径</h4><p><img src="image/image-20200301151133784.png" alt="image-20200301151133784"></p><h4 id="第三步：初始化zk客户端"><a href="#第三步：初始化zk客户端" class="headerlink" title="第三步：初始化zk客户端"></a>第三步：初始化zk客户端</h4><p><img src="image/image-20200301152207863.png" alt="image-20200301152207863"></p><p>因为我们来初始化zk的客户端</p><p><img src="image/image-20200301152512876.png" alt="image-20200301152512876"></p><h4 id="第四步：将actor的引擎，注册到zk"><a href="#第四步：将actor的引擎，注册到zk" class="headerlink" title="第四步：将actor的引擎，注册到zk"></a>第四步：将actor的引擎，注册到zk</h4><p>首先我们在ZKUtils里面封装个方法，专门来对接jobActor的注册</p><p><img src="image/image-20200301154915611.png" alt="image-20200301154915611"></p><p>然后在jobActor的初始化里面，进行注册jobActor的引擎</p><p><img src="image/image-20200301155245818.png" alt="image-20200301155245818"></p><p>然后启动测试，查看zk里面是否注册进去</p><p><img src="image/image-20200301155925856.png" alt="image-20200301155925856"></p><p>测试结果</p><p><img src="image/image-20200301155944791.png" alt="image-20200301155944791"></p><h2 id="7-2、编写jobActor的结束阶段代码"><a href="#7-2、编写jobActor的结束阶段代码" class="headerlink" title="7.2、编写jobActor的结束阶段代码"></a>7.2、编写jobActor的结束阶段代码</h2><p>首先我们把后续需要的一些变量提前初始化好：</p><p>1、spark的解释器</p><p>2、sparkSession</p><p>上面这俩变量，会在actor的生命周期结束时候进行回收</p><h3 id="第一步：定义成员变量"><a href="#第一步：定义成员变量" class="headerlink" title="第一步：定义成员变量"></a>第一步：定义成员变量</h3><p><img src="image/image-20200301164223196.png" alt="image-20200301164223196"></p><h3 id="第二步：初始化阶段给上面两个变量赋值"><a href="#第二步：初始化阶段给上面两个变量赋值" class="headerlink" title="第二步：初始化阶段给上面两个变量赋值"></a>第二步：初始化阶段给上面两个变量赋值</h3><p><img src="image/image-20200301164328130.png" alt="image-20200301164328130"></p><h3 id="第三步：在postStop，actor的生命周期结束阶段对这俩变量进行回收"><a href="#第三步：在postStop，actor的生命周期结束阶段对这俩变量进行回收" class="headerlink" title="第三步：在postStop，actor的生命周期结束阶段对这俩变量进行回收"></a>第三步：在postStop，actor的生命周期结束阶段对这俩变量进行回收</h3><p><img src="image/image-20200301164416595.png" alt="image-20200301164416595"></p><h2 id="7-3-、编写jobActor的receive"><a href="#7-3-、编写jobActor的receive" class="headerlink" title="7.3 、编写jobActor的receive"></a>7.3 、编写jobActor的receive</h2><p>首先我们要开始编写一个actor的钩子，目的很简单，万一出现了错误，我们可以 catch住这个错误，然后把错误回显给客户端（web端）</p><h3 id="7-3-1、编写一个-actor的钩子"><a href="#7-3-1、编写一个-actor的钩子" class="headerlink" title="7.3.1、编写一个 actor的钩子"></a>7.3.1、编写一个 actor的钩子</h3><p><img src="image/image-20200301180055957.png" alt="image-20200301180055957"></p><h3 id="7-3-2、在receive里匹配指令"><a href="#7-3-2、在receive里匹配指令" class="headerlink" title="7.3.2、在receive里匹配指令"></a>7.3.2、在receive里匹配指令</h3><h4 id="1）：匹配指令-，-添加actorHook，并初始化变量"><a href="#1）：匹配指令-，-添加actorHook，并初始化变量" class="headerlink" title="1）：匹配指令 ， 添加actorHook，并初始化变量"></a>1）：匹配指令 ， 添加actorHook，并初始化变量</h4><p><img src="image/image-20200302101454086.png" alt="image-20200302101454086"></p><h4 id="2）：编写一个获取全局唯一的任务组ID-，-因为后续会基于任务组ID来获取引擎信息"><a href="#2）：编写一个获取全局唯一的任务组ID-，-因为后续会基于任务组ID来获取引擎信息" class="headerlink" title="2）：编写一个获取全局唯一的任务组ID ， 因为后续会基于任务组ID来获取引擎信息"></a>2）：编写一个获取全局唯一的任务组ID ， 因为后续会基于任务组ID来获取引擎信息</h4><p><img src="image/image-20200302102948757.png" alt="image-20200302102948757"></p><h4 id="3）：将job信息（包含任务组ID）-，返回给前端"><a href="#3）：将job信息（包含任务组ID）-，返回给前端" class="headerlink" title="3）：将job信息（包含任务组ID） ，返回给前端"></a>3）：将job信息（包含任务组ID） ，返回给前端</h4><p><img src="image/image-20200302103118286.png" alt="image-20200302103118286"></p><h4 id="4）：更新线程副本里面的作业描述"><a href="#4）：更新线程副本里面的作业描述" class="headerlink" title="4）：更新线程副本里面的作业描述"></a>4）：更新线程副本里面的作业描述</h4><p>之所以这样做：</p><p>1、可以对任务作出描述，方便任务的web端定位</p><p>2、有了任务的描述，那么后续是可以取消已经提交的任务</p><p><img src="image/image-20200302103752873.png" alt="image-20200302103752873"></p><h4 id="5）：基于commandMode进行匹配具体操作"><a href="#5）：基于commandMode进行匹配具体操作" class="headerlink" title="5）：基于commandMode进行匹配具体操作"></a>5）：基于commandMode进行匹配具体操作</h4><p>接下来，我们要匹配具体的操作，就是看传递过来的指令是代码还是SQL；</p><p>然后根据指令的不同，选择不同的操作方式</p><p><img src="image/image-20200302141525959.png" alt="image-20200302141525959"></p><h3 id="7-3-3：接收CODE，然后处理"><a href="#7-3-3：接收CODE，然后处理" class="headerlink" title="7.3.3：接收CODE，然后处理"></a>7.3.3：接收CODE，然后处理</h3><p>定义一个变量assemble_instruction，来组装命令</p><p><img src="image/image-20200307152001994.png" alt="image-20200307152001994"></p><p>接受命令：</p><p><img src="image/image-20200302154945358.png" alt="image-20200302154945358"></p><p><img src="image/image-20200302161956423.png" alt="image-20200302161956423"></p><h4 id="最后处理完后，把结果回显给客户端"><a href="#最后处理完后，把结果回显给客户端" class="headerlink" title="最后处理完后，把结果回显给客户端"></a>最后处理完后，把结果回显给客户端</h4><p>上面处理后，会返回一个response的结果 ， 我们需要把结果回显给客户端</p><p>因此，我们需要接收这个response，然后解析他 ， 然后收集他</p><h5 id="第一步：去EngineSession添加一个记录任务的map集合"><a href="#第一步：去EngineSession添加一个记录任务的map集合" class="headerlink" title="第一步：去EngineSession添加一个记录任务的map集合"></a>第一步：去EngineSession添加一个记录任务的map集合</h5><p>去EngineSession添加一个记录任务的map集合 ， 主要是为了保存批处理的作业信息</p><p><img src="image/image-20200302163508474.png" alt="image-20200302163508474"></p><h5 id="第二步：根据执行的结果，存储job的状态"><a href="#第二步：根据执行的结果，存储job的状态" class="headerlink" title="第二步：根据执行的结果，存储job的状态"></a>第二步：根据执行的结果，存储job的状态</h5><p><img src="image/image-20200302170312756.png" alt="image-20200302170312756"></p><h5 id="第三步：响应任务状态"><a href="#第三步：响应任务状态" class="headerlink" title="第三步：响应任务状态"></a>第三步：响应任务状态</h5><p><img src="image/image-20200302171027572.png" alt="image-20200302171027572"></p><h3 id="7-3-4：接收SQL，然后处理"><a href="#7-3-4：接收SQL，然后处理" class="headerlink" title="7.3.4：接收SQL，然后处理"></a>7.3.4：接收SQL，然后处理</h3><p>SQL：select name from person where age &gt; 18</p><p>sql执行流程(sql生命周期):</p><p><img src="image/image-20200302182348637.png" alt="image-20200302182348637"></p><p>不管解析被划分为几步，在Spark 执行环境中，都要转化成RDD的调用代码，才能被spark core所执行</p><p><img src="image/image-20200302192314945.png" alt="image-20200302192314945"></p><p>那么这里面有个关键的点，就是查询的SQL ， 怎么转化成Unresolved LogicalPlan；</p><p>Unresolved LogicalPlan 这个阶段接收的是抽象的语法树，所以我们需要知道的就是，这个SQL语句是怎么转成抽象语法树的；</p><p>答案就是：antlr4（spark是在2.0以后，开始使用antlr4解析的sql语法）</p><p>spark通过antlr4去解析SQL语句，形成抽象语法树AST；</p><p>也就是说，详细的流程是这样的：</p><p><img src="image/image-20200303140649183.png" alt="image-20200303140649183"></p><h4 id="7-3-4-1：antlr的入门"><a href="#7-3-4-1：antlr的入门" class="headerlink" title="7.3.4.1：antlr的入门"></a>7.3.4.1：antlr的入门</h4><h5 id="1）语法"><a href="#1）语法" class="headerlink" title="1）语法"></a>1）语法</h5><ul><li><code>grammar</code> 名称和文件名要一致</li><li>Parser 规则（即 non-terminal）以小写字母开始</li><li>Lexer 规则（即 terminal）以大写字母开始</li><li>用 <code>&#39;string&#39;</code> 单引号引出字符串</li></ul><h5 id="2）：配置antlr的环境变量"><a href="#2）：配置antlr的环境变量" class="headerlink" title="2）：配置antlr的环境变量"></a>2）：配置antlr的环境变量</h5><p>首先你要有配置antlr的环境变量</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">export CLASSPATH=&quot;.:/usr/local/lib/antlr-4.5.3-complete.jar:$CLASSPATH&quot;</span><br><span class="line">alias antlr4=&#x27;java -Xmx500M -cp &quot;/usr/local/lib/antlr-4.5.3-complete.jar:$CLASSPATH&quot; org.antlr.v4.Tool&#x27;</span><br><span class="line">alias grun=&#x27;java org.antlr.v4.runtime.misc.TestRig&#x27;</span><br></pre></td></tr></table></figure><p>然后我们按照语法格式来写一个hello word的代码；</p><p>【随便打开 一个maven工程，然后导包】</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.antlr<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>antlr4-runtime<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>4.5.3<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><h6 id="第一步：编写antlr文件"><a href="#第一步：编写antlr文件" class="headerlink" title="第一步：编写antlr文件"></a>第一步：编写antlr文件</h6><p><img src="image/image-20200303121223051.png" alt="image-20200303121223051"></p><p>通过上面，我们大概了解，antlr其实主要就是写正则</p><h6 id="第二步：生成java代码"><a href="#第二步：生成java代码" class="headerlink" title="第二步：生成java代码"></a>第二步：生成java代码</h6><p>接下来，通过命令生成java代码</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">antlr4 learnAntlr.g4 </span><br></pre></td></tr></table></figure><p><img src="image/image-20200303121837606.png" alt="image-20200303121837606"></p><h6 id="第三步：重写learnAntlrBaseListener"><a href="#第三步：重写learnAntlrBaseListener" class="headerlink" title="第三步：重写learnAntlrBaseListener"></a>第三步：重写learnAntlrBaseListener</h6><p><img src="image/image-20200303122615079.png" alt="image-20200303122615079"></p><h6 id="第四步：词法和语法解析"><a href="#第四步：词法和语法解析" class="headerlink" title="第四步：词法和语法解析"></a>第四步：词法和语法解析</h6><p><img src="image/image-20200303123221642.png" alt="image-20200303123221642"></p><h6 id="第五步：运行测试"><a href="#第五步：运行测试" class="headerlink" title="第五步：运行测试"></a>第五步：运行测试</h6><p><img src="image/image-20200303123335519.png" alt="image-20200303123335519"></p><h4 id="7-3-4-2：在代码中编写antlr文件"><a href="#7-3-4-2：在代码中编写antlr文件" class="headerlink" title="7.3.4.2：在代码中编写antlr文件"></a>7.3.4.2：在代码中编写antlr文件</h4><p><img src="image/image-20200303141447185.png" alt="image-20200303141447185"></p><h4 id="7-3-4-3：讲解antlr文件里面的意思"><a href="#7-3-4-3：讲解antlr文件里面的意思" class="headerlink" title="7.3.4.3：讲解antlr文件里面的意思"></a>7.3.4.3：讲解antlr文件里面的意思</h4><h5 id="1）：load的列子"><a href="#1）：load的列子" class="headerlink" title="1）：load的列子"></a>1）：load的列子</h5><p>假如我想实现加载文件操作，形成一个表，然后在基于这个表做查询操作；</p><p>以上的动作是：数据源—&gt;load（加载）——&gt;select这个表</p><p>比如：</p><p>那么我们在Engine.g4文件中就是：</p><p><img src="image/image-20200303144307204.png" alt="image-20200303144307204"></p><p>他们的对应关系就是：</p><p><img src="image/image-20200303144216429.png" alt="image-20200303144216429"></p><h5 id="2）：save的列子"><a href="#2）：save的列子" class="headerlink" title="2）：save的列子"></a>2）：save的列子</h5><p>比如我们从kafka记载了数据，形成tb表 ， 然后将数据写入mysql</p><p>简单说就是: kafka –&gt;load –&gt;save —&gt;mysql</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">load</span> kafka.veche4</span><br><span class="line"><span class="keyword">where</span> <span class="string">`spark.job.mode`</span>=<span class="string">&quot;stream&quot;</span> </span><br><span class="line"><span class="keyword">and</span> <span class="string">`kafka.bootstrap.servers`</span>=<span class="string">&quot;39.100.249.234:9092&quot;</span> </span><br><span class="line"><span class="keyword">as</span> tb; </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">save tb as jdbc.`t3`</span><br><span class="line">where </span><br><span class="line">driver=&quot;com.mysql.jdbc.Driver&quot;      </span><br><span class="line">and url=&quot;jdbc:mysql://cdh2:3306/test?useUnicode=true&amp;characterEncoding=utf8&quot;</span><br><span class="line">and user=&quot;root&quot;</span><br><span class="line">and password=&quot;root&quot;</span><br><span class="line">and `failOnDataLoss`=&quot;false&quot; </span><br><span class="line">and `outputMode`=&quot;Append&quot; </span><br><span class="line">and `streamName`=&quot;Stream&quot; </span><br><span class="line">and `duration`=&quot;2&quot;</span><br><span class="line">and `checkpointLocation`=&quot;/Users/angel/Desktop/data/S1_2020080120&quot;</span><br><span class="line">and coalesce=&quot;1&quot;;</span><br></pre></td></tr></table></figure><p>那么对应我们antlr文件就是：</p><p><img src="image/image-20200303153620249.png" alt="image-20200303153620249"></p><p>load操作，我们刚刚讲过了，接下来在说下，怎么save的</p><p>【看图做对比】</p><p><img src="image/image-20200303155109413.png" alt="image-20200303155109413"></p><h4 id="7-3-4-4：最后把剩余的功能统一说下"><a href="#7-3-4-4：最后把剩余的功能统一说下" class="headerlink" title="7.3.4.4：最后把剩余的功能统一说下"></a>7.3.4.4：最后把剩余的功能统一说下</h4><p><img src="image/image-20200303160555795.png" alt="image-20200303160555795"></p><h4 id="7-3-4-5：生成antlr的代码"><a href="#7-3-4-5：生成antlr的代码" class="headerlink" title="7.3.4.5：生成antlr的代码"></a>7.3.4.5：生成antlr的代码</h4><p>命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">antlr4 Engine.g4</span><br></pre></td></tr></table></figure><p><img src="image/image-20200303162739086.png" alt="image-20200303162739086"></p><h3 id="7-3-5：重写EngineBaseListener"><a href="#7-3-5：重写EngineBaseListener" class="headerlink" title="7.3.5：重写EngineBaseListener"></a>7.3.5：重写EngineBaseListener</h3><p>就像我们自己写的例子一样，此时我们要重写一下EngineBaseListener，对这个类的功能做增量，来满足我们的需求</p><h4 id="第一步：创建EngineSQLExecListener类【不带参数】"><a href="#第一步：创建EngineSQLExecListener类【不带参数】" class="headerlink" title="第一步：创建EngineSQLExecListener类【不带参数】"></a>第一步：创建EngineSQLExecListener类【不带参数】</h4><p><img src="image/image-20200303164047018.png" alt="image-20200303164047018"></p><h4 id="第二步：重写EngineSQLExecListener里面的exitSql方法"><a href="#第二步：重写EngineSQLExecListener里面的exitSql方法" class="headerlink" title="第二步：重写EngineSQLExecListener里面的exitSql方法"></a>第二步：重写EngineSQLExecListener里面的exitSql方法</h4><p>重写这个方法的依据：</p><p>antlr4在离开sql的时候，会触发exitSql</p><p><img src="image/image-20200303164252153.png" alt="image-20200303164252153"></p><p>所以我们要重写这块儿，来触发我们的业务逻辑</p><p><img src="image/image-20200303164639965.png" alt="image-20200303164639965"></p><h4 id="第三步：测试上面的步骤"><a href="#第三步：测试上面的步骤" class="headerlink" title="第三步：测试上面的步骤"></a>第三步：测试上面的步骤</h4><h5 id="1）：首先我们要先确保当前的流程一定是通的，所以我们要测试下，确保当前流程绝对没问题"><a href="#1）：首先我们要先确保当前的流程一定是通的，所以我们要测试下，确保当前流程绝对没问题" class="headerlink" title="1）：首先我们要先确保当前的流程一定是通的，所以我们要测试下，确保当前流程绝对没问题"></a>1）：首先我们要先确保当前的流程一定是通的，所以我们要测试下，确保当前流程绝对没问题</h5><p>所以我们写个方法，专门来执行词法 和语法的解析</p><p><img src="image/image-20200303165604622.png" alt="image-20200303165604622"></p><h5 id="2）：然后我们测试下，看看操作是否通过"><a href="#2）：然后我们测试下，看看操作是否通过" class="headerlink" title="2）：然后我们测试下，看看操作是否通过"></a>2）：然后我们测试下，看看操作是否通过</h5><p><img src="image/image-20200303165702911.png" alt="image-20200303165702911"></p><h5 id="3）：然后集成一下接收到命令后，怎么对接这块儿"><a href="#3）：然后集成一下接收到命令后，怎么对接这块儿" class="headerlink" title="3）：然后集成一下接收到命令后，怎么对接这块儿"></a>3）：然后集成一下接收到命令后，怎么对接这块儿</h5><p><img src="image/image-20200303173906741.png" alt="image-20200303173906741"></p><h5 id="4）：启动App驱动，然后在test里面发送命令"><a href="#4）：启动App驱动，然后在test里面发送命令" class="headerlink" title="4）：启动App驱动，然后在test里面发送命令"></a>4）：启动App驱动，然后在test里面发送命令</h5><p><img src="image/image-20200303172308184.png" alt="image-20200303172308184"></p><h4 id="第四步：编写模式匹配的load操作"><a href="#第四步：编写模式匹配的load操作" class="headerlink" title="第四步：编写模式匹配的load操作"></a>第四步：编写模式匹配的load操作</h4><p>接下来，我们来完善load操作：</p><p><img src="image/image-20200303175143737.png" alt="image-20200303175143737"></p><h5 id="1）：创建包"><a href="#1）：创建包" class="headerlink" title="1）：创建包"></a>1）：创建包</h5><p><img src="image/image-20200303183125754.png" alt="image-20200303183125754"></p><h5 id="2）：规范化sql里面的方法"><a href="#2）：规范化sql里面的方法" class="headerlink" title="2）：规范化sql里面的方法"></a>2）：规范化sql里面的方法</h5><p>编写trait来规范化操作，方便后期维护，也是一种面向接口的编程思想</p><p><img src="image/image-20200303184320426.png" alt="image-20200303184320426"></p><h5 id="3）：编写load操作，LoadAdaptor"><a href="#3）：编写load操作，LoadAdaptor" class="headerlink" title="3）：编写load操作，LoadAdaptor"></a>3）：编写load操作，LoadAdaptor</h5><h6 id="第一步：编写整体的大框"><a href="#第一步：编写整体的大框" class="headerlink" title="第一步：编写整体的大框"></a>第一步：编写整体的大框</h6><p><img src="image/image-20200303191438474.png" alt="image-20200303191438474"></p><h6 id="第二步：确定要解析的内容和成员变量"><a href="#第二步：确定要解析的内容和成员变量" class="headerlink" title="第二步：确定要解析的内容和成员变量"></a>第二步：确定要解析的内容和成员变量</h6><p>在这个里面，我们要解析语法：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">LOAD format POINT? path WHERE? expression? booleanExpression* AS tablename</span><br></pre></td></tr></table></figure><p><img src="image/image-20200303192306205.png" alt="image-20200303192306205"></p><h6 id="第三步：遍历语法树的孩子节点"><a href="#第三步：遍历语法树的孩子节点" class="headerlink" title="第三步：遍历语法树的孩子节点"></a>第三步：遍历语法树的孩子节点</h6><p><img src="image/image-20200303193309702.png" alt="image-20200303193309702"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark.job.mode &#x3D; stream 就代表是流处理 ， 否则就是离线处理</span><br></pre></td></tr></table></figure><p><img src="image/image-20200303194249408.png" alt="image-20200303194249408"></p><h5 id="4）：EngineSQLExecListener这个类的模式匹配，将LoadAdaptor做关联"><a href="#4）：EngineSQLExecListener这个类的模式匹配，将LoadAdaptor做关联" class="headerlink" title="4）：EngineSQLExecListener这个类的模式匹配，将LoadAdaptor做关联"></a>4）：EngineSQLExecListener这个类的模式匹配，将LoadAdaptor做关联</h5><p><img src="image/image-20200303195002420.png" alt="image-20200303195002420"></p><h5 id="5）：测试当前的LoadAdaptor"><a href="#5）：测试当前的LoadAdaptor" class="headerlink" title="5）：测试当前的LoadAdaptor"></a>5）：测试当前的LoadAdaptor</h5><p>测试LoadAdaptor是否对当前节点树做了解析，那么直接打印一下这个option信息</p><p><img src="image/image-20200303201523924.png" alt="image-20200303201523924"></p><p>在测试类，编写测试命令</p><p>离线：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">val instruction = &quot;<span class="keyword">load</span> jdbc.testTableName <span class="string">&quot; +</span></span><br><span class="line"><span class="string">    &quot;</span><span class="keyword">where</span> driver=\<span class="string">&quot;com.mysql.jdbc.Driver\&quot;</span> <span class="string">&quot; +</span></span><br><span class="line"><span class="string">    &quot;</span>      <span class="keyword">and</span> <span class="keyword">url</span>=\<span class="string">&quot;jdbc:mysql://cdh1:3306/hive?characterEncoding=utf8\&quot;</span> <span class="string">&quot; +</span></span><br><span class="line"><span class="string">    &quot;</span>      <span class="keyword">and</span> <span class="keyword">user</span>=\<span class="string">&quot;root\&quot;</span> <span class="string">&quot; +</span></span><br><span class="line"><span class="string">    &quot;</span>      <span class="keyword">and</span> <span class="keyword">password</span>=\<span class="string">&quot;root\&quot;</span> <span class="string">&quot; +</span></span><br><span class="line"><span class="string">    &quot;</span><span class="keyword">as</span> tb; &quot; +</span><br><span class="line">    &quot;<span class="keyword">SELECT</span> * <span class="keyword">FROM</span> tb <span class="keyword">LIMIT</span> <span class="number">100</span>;&quot;</span><br></pre></td></tr></table></figure><p>流：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">val instruction = &quot;<span class="keyword">load</span> kafka.<span class="string">`veche4`</span><span class="string">&quot; +</span></span><br><span class="line"><span class="string">      &quot;</span><span class="keyword">where</span> <span class="string">`spark.job.mode`</span>=\<span class="string">&quot;stream\&quot;</span> <span class="string">&quot; +</span></span><br><span class="line"><span class="string">      &quot;</span><span class="keyword">and</span> <span class="string">`kafka.bootstrap.servers`</span>=\<span class="string">&quot;39.100.249.234:9092\&quot;</span> <span class="string">&quot; +</span></span><br><span class="line"><span class="string">      &quot;</span><span class="keyword">as</span> tb; &quot; +</span><br><span class="line">      &quot;save tb as hbase.`TES6`&quot; +</span><br><span class="line">      &quot;where `hbase.zookeeper.quorum`=\&quot;angel_rsong1:2181\&quot; &quot; +</span><br><span class="line">      &quot;and `hbase.table.rowkey.field`=\&quot;id_\&quot; &quot; +</span><br><span class="line">      &quot;and `hbase.table.family`=\&quot;MM\&quot; &quot; +</span><br><span class="line">      &quot;and `failOnDataLoss`=\&quot;false\&quot; &quot; +</span><br><span class="line">      &quot;and `hbase.table.numReg`=\&quot;10\&quot; &quot; +</span><br><span class="line">      &quot;and `hbase.check_table`=\&quot;true\&quot; &quot; +</span><br><span class="line">      &quot;and `outputMode`=\&quot;Append\&quot; &quot; +</span><br><span class="line">      &quot;and `streamName`=\&quot;Stream\&quot; &quot; +</span><br><span class="line">      &quot;and `duration`=\&quot;10\&quot; &quot; +</span><br><span class="line">      &quot;and `sendDingDingOnTerminated`=\&quot;true\&quot; &quot; +</span><br><span class="line">      &quot;and `checkpointLocation`=\&quot;/Users/angel/Desktop/data/ess312ldd\&quot;;&quot;</span><br></pre></td></tr></table></figure><h4 id="第五步：处理匹配离线处理的数据源"><a href="#第五步：处理匹配离线处理的数据源" class="headerlink" title="第五步：处理匹配离线处理的数据源"></a>第五步：处理匹配离线处理的数据源</h4><p>这一步，我们要根据刚刚load操作出来的结果，然后去匹配操作。然后去查询出数据</p><p>也就是;</p><p>load   —&gt;匹配到离线 —&gt;select结果</p><h5 id="1）：构建离线匹配数据源的类（BatchJobLoadAdaptor）"><a href="#1）：构建离线匹配数据源的类（BatchJobLoadAdaptor）" class="headerlink" title="1）：构建离线匹配数据源的类（BatchJobLoadAdaptor）"></a>1）：构建离线匹配数据源的类（BatchJobLoadAdaptor）</h5><p><img src="image/image-20200306111220821.png" alt="image-20200306111220821"></p><p>因为我们对接数据源是通过spark sql去处理的，所以我们先在EngineSQLExecListener这个监听者实现类里面添加sparkSession</p><p>这样我们的BatchJobLoadAdaptor（匹配离线数据源）就可以使用sparkSQL，去查询数据源的数据了</p><h5 id="2）：监听者实现类（EngineSQLExecListener），添加sparkSession"><a href="#2）：监听者实现类（EngineSQLExecListener），添加sparkSession" class="headerlink" title="2）：监听者实现类（EngineSQLExecListener），添加sparkSession"></a>2）：监听者实现类（EngineSQLExecListener），添加sparkSession</h5><p><img src="image/image-20200306112742980.png" alt="image-20200306112742980"></p><h5 id="3）：离线匹配数据源的类（BatchJobLoadAdaptor）开始根据sparkSession查询数据"><a href="#3）：离线匹配数据源的类（BatchJobLoadAdaptor）开始根据sparkSession查询数据" class="headerlink" title="3）：离线匹配数据源的类（BatchJobLoadAdaptor）开始根据sparkSession查询数据"></a>3）：离线匹配数据源的类（BatchJobLoadAdaptor）开始根据sparkSession查询数据</h5><p><img src="image/image-20200306112549529.png" alt="image-20200306112549529"></p><h6 id="匹配数据源"><a href="#匹配数据源" class="headerlink" title="匹配数据源"></a>匹配数据源</h6><p><img src="image/image-20200306113928729.png" alt="image-20200306113928729"></p><p><img src="image/image-20200306114045495.png" alt="image-20200306114045495"></p><p>最后回到JobActor ， 给EngineSQLExecListener添加sparkSession的参数</p><p><img src="image/image-20200306145634422.png" alt="image-20200306145634422"></p><p>然后在LoadAdaptor里面添加上BatchJobLoadAdaptor匹配数据源的操作</p><p><img src="image/image-20200306150202828.png" alt="image-20200306150202828"></p><h5 id="4）：接下来，做个简单的测试，看看能不能查出来数据"><a href="#4）：接下来，做个简单的测试，看看能不能查出来数据" class="headerlink" title="4）：接下来，做个简单的测试，看看能不能查出来数据"></a>4）：接下来，做个简单的测试，看看能不能查出来数据</h5><p>在BatchJobLoadAdaptor里面打印一下结果</p><p><img src="/image/image-20200306150830229.png" alt="image-20200306150830229"></p><h6 id=""><a href="#" class="headerlink" title=""></a></h6><p>最后在测试类执行如下代码</p><p><img src="image/image-20200306150936703.png" alt="image-20200306150936703"></p><p>结果：</p><p><img src="image/image-20200306150955088.png" alt="image-20200306150955088"></p><p>出现如上结果，此时说明流程已经走通</p><h4 id="第六步：完成EngineSQLExecListener里面的select操作"><a href="#第六步：完成EngineSQLExecListener里面的select操作" class="headerlink" title="第六步：完成EngineSQLExecListener里面的select操作"></a>第六步：完成EngineSQLExecListener里面的select操作</h4><p>sparkSession.sql（查询sql）的操作</p><p><img src="image/image-20200306154440394.png" alt="image-20200306154440394"></p><p>所以 我们创建一个selectAdaptor类，来匹配select操作</p><p><img src="image/image-20200306171806403.png" alt="image-20200306171806403"></p><h5 id="1）：然后编写selectAdaptor里面的内容："><a href="#1）：然后编写selectAdaptor里面的内容：" class="headerlink" title="1）：然后编写selectAdaptor里面的内容："></a>1）：然后编写selectAdaptor里面的内容：</h5><p><img src="image/image-20200306173235155.png" alt="image-20200306173235155"></p><h5 id="2）：执行sql语句，创建临时表，然后在把处理结果进行保存"><a href="#2）：执行sql语句，创建临时表，然后在把处理结果进行保存" class="headerlink" title="2）：执行sql语句，创建临时表，然后在把处理结果进行保存"></a>2）：执行sql语句，创建临时表，然后在把处理结果进行保存</h5><p><img src="image/image-20200306181256650.png" alt="image-20200306181256650"></p><p>加载这个结果的原因说明：</p><p>我们后面要将处理的结果返回给客户端 ， 并且还要将处理的结果保存起来；</p><p>比如说，后续公司需要开发一些功能，将处理后的数据结果，可以下载下来，保存成csv文件；</p><p>那么此时对处理结果进行保存，显然是必要的</p><p><img src="image/image-20200306174118213.png" alt="image-20200306174118213"></p><h6 id="最后："><a href="#最后：" class="headerlink" title="最后："></a>最后：</h6><p>我们在JobActor里面封装一下解析SQL的操作，让我们解析并执行完SQL以后，可以拿到结果</p><h4 id="第七步：封装处理结果操作"><a href="#第七步：封装处理结果操作" class="headerlink" title="第七步：封装处理结果操作"></a>第七步：封装处理结果操作</h4><p>我们返回的处理结果是dataFrame ， 所以我们需要将dataFrame转成json</p><p>这样在将结果返回给客户端(因为客户端没法直接解析dataFrame)</p><h5 id="1）：将dataFrame结果，解析成json"><a href="#1）：将dataFrame结果，解析成json" class="headerlink" title="1）：将dataFrame结果，解析成json"></a>1）：将dataFrame结果，解析成json</h5><p>在adaptor工程下，写个方法，专门来将dataFrame转成json</p><p>方法：</p><p>使用SparkSQL内置函数接口开发StructType/Row转Json函数，我们以前肯定使用过：</p><p>【列子】</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="keyword">val</span> spark = getSparkSession(getSparkConf)</span><br><span class="line">  <span class="keyword">val</span> df = spark.createDataFrame(<span class="type">Seq</span>(</span><br><span class="line">    (<span class="string">&quot;ming&quot;</span>, <span class="number">20</span>, <span class="number">15552211521</span>L),</span><br><span class="line">    (<span class="string">&quot;hong&quot;</span>, <span class="number">19</span>, <span class="number">13287994007</span>L),</span><br><span class="line">    (<span class="string">&quot;zhi&quot;</span>, <span class="number">21</span>, <span class="number">15552211523</span>L)</span><br><span class="line">  )) toDF(<span class="string">&quot;name&quot;</span>, <span class="string">&quot;age&quot;</span>, <span class="string">&quot;phone&quot;</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> data: <span class="type">Dataset</span>[<span class="type">String</span>] = df.toJSON</span><br><span class="line">  println(data.collect())</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>跟踪上述，发现最终都会调用Spark源码中的org.apache.spark.sql.execution.datasources.json.JacksonGenerator类，使用Jackson，根据传入的StructType、JsonGenerator和InternalRow，生成Json字符串。</p><p><img src="image/image-20200306184128194.png" alt="image-20200306184128194"></p><p>JacksonGenerator类</p><p><img src="image/image-20200306183801547.png" alt="image-20200306183801547"></p><p>【我们在adaptor工程下使用】</p><p><img src="image/image-20200306185941917.png" alt="image-20200306185941917"></p><h5 id="2）：编写结果的报告处理和结果落地保存"><a href="#2）：编写结果的报告处理和结果落地保存" class="headerlink" title="2）：编写结果的报告处理和结果落地保存"></a>2）：编写结果的报告处理和结果落地保存</h5><p><img src="image/image-20200306190750386.png" alt="image-20200306190750386"></p><h5 id="3）：把解析SQL操作和汇报结果操作进行封装"><a href="#3）：把解析SQL操作和汇报结果操作进行封装" class="headerlink" title="3）：把解析SQL操作和汇报结果操作进行封装"></a>3）：把解析SQL操作和汇报结果操作进行封装</h5><p><img src="image/image-20200306191612342.png" alt="image-20200306191612342"></p><h5 id="4）：最后在匹配SQL处理处-，-使用parseSQL方法"><a href="#4）：最后在匹配SQL处理处-，-使用parseSQL方法" class="headerlink" title="4）：最后在匹配SQL处理处 ， 使用parseSQL方法"></a>4）：最后在匹配SQL处理处 ， 使用parseSQL方法</h5><p><img src="image/image-20200306191838030.png" alt="image-20200306191838030"></p><h5 id="5）：测试上面的所有操作"><a href="#5）：测试上面的所有操作" class="headerlink" title="5）：测试上面的所有操作"></a>5）：测试上面的所有操作</h5><p><img src="image/image-20200306174244108.png" alt="image-20200306174244108"></p><h6 id="在汇报结果的方法reportResult里面，对处理的结果展示一下"><a href="#在汇报结果的方法reportResult里面，对处理的结果展示一下" class="headerlink" title="在汇报结果的方法reportResult里面，对处理的结果展示一下"></a>在汇报结果的方法reportResult里面，对处理的结果展示一下</h6><p><img src="image/image-20200306192441584.png" alt="image-20200306192441584"></p><h6 id="【最后结果展示】"><a href="#【最后结果展示】" class="headerlink" title="【最后结果展示】"></a>【最后结果展示】</h6><p>sql的结果</p><p><img src="image/image-20200306192540469.png" alt="image-20200306192540469"></p><p>hdfs上的数据</p><p><img src="image/image-20200306192605230.png" alt="image-20200306192605230"></p><h3 id="7-3-6、编写存储数据SaveAdaptor"><a href="#7-3-6、编写存储数据SaveAdaptor" class="headerlink" title="7.3.6、编写存储数据SaveAdaptor"></a>7.3.6、编写存储数据SaveAdaptor</h3><p><img src="image/image-20200307101105874.png" alt="image-20200307101105874"></p><p>在存储数据时，我们需要考虑两个场景：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1：考虑场景问题：离线、流式</span><br><span class="line">2：考虑数据格式问题：json、text、hbase、mysql....</span><br></pre></td></tr></table></figure><h4 id="7-3-6-1、创建SaveAdaptor类"><a href="#7-3-6-1、创建SaveAdaptor类" class="headerlink" title="7.3.6.1、创建SaveAdaptor类"></a>7.3.6.1、创建SaveAdaptor类</h4><p><img src="image/image-20200307100950614.png" alt="image-20200307100950614"></p><h4 id="7-3-6-2、分析antlr里面的save操作"><a href="#7-3-6-2、分析antlr里面的save操作" class="headerlink" title="7.3.6.2、分析antlr里面的save操作"></a>7.3.6.2、分析antlr里面的save操作</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(&#x27;save&#x27;|&#x27;SAVE&#x27;) (overwrite | append | errorIfExists | ignore | <span class="keyword">update</span>)* tableName <span class="string">&#x27;as&#x27;</span> <span class="keyword">format</span> <span class="string">&#x27;.&#x27;</span> <span class="keyword">path</span> (<span class="string">&#x27;where&#x27;</span> | <span class="string">&#x27;WHERE&#x27;</span>)? expression? booleanExpression* (<span class="string">&#x27;partitionBy&#x27;</span> <span class="keyword">col</span>)? (<span class="string">&#x27;coalesce&#x27;</span> numPartition)?</span><br></pre></td></tr></table></figure><p>上面的语法解析，实际就是解析类似如下的SQL</p><p><img src="image/image-20200307101919402.png" alt="image-20200307101919402"></p><h4 id="7-3-6-3、写好局部变量，对应解析后的值"><a href="#7-3-6-3、写好局部变量，对应解析后的值" class="headerlink" title="7.3.6.3、写好局部变量，对应解析后的值"></a>7.3.6.3、写好局部变量，对应解析后的值</h4><p>接下来要提供一些局部变量，我们解析出来值后，赋值给这些局部变量</p><p><img src="image/image-20200307134832183.png" alt="image-20200307134832183"></p><h4 id="7-3-6-4、遍历节点树，解析每一个节点"><a href="#7-3-6-4、遍历节点树，解析每一个节点" class="headerlink" title="7.3.6.4、遍历节点树，解析每一个节点"></a>7.3.6.4、遍历节点树，解析每一个节点</h4><p>接下来遍历节点，按照我们自己写的antlr语法规则，开始遍历语法规则</p><p><img src="image/image-20200307141654330.png" alt="image-20200307141654330"></p><h4 id="7-3-6-5、判断当前的save操作是流处理还是批处理"><a href="#7-3-6-5、判断当前的save操作是流处理还是批处理" class="headerlink" title="7.3.6.5、判断当前的save操作是流处理还是批处理"></a>7.3.6.5、判断当前的save操作是流处理还是批处理</h4><h5 id="1）：在EngineSQLExecListener里面添加加载流的配置操作"><a href="#1）：在EngineSQLExecListener里面添加加载流的配置操作" class="headerlink" title="1）：在EngineSQLExecListener里面添加加载流的配置操作"></a>1）：在EngineSQLExecListener里面添加加载流的配置操作</h5><p><img src="image/image-20200307144448650.png" alt="image-20200307144448650"></p><h5 id="2）：在load操作的时候LoadAdaptor，添加流的标志"><a href="#2）：在load操作的时候LoadAdaptor，添加流的标志" class="headerlink" title="2）：在load操作的时候LoadAdaptor，添加流的标志"></a>2）：在load操作的时候LoadAdaptor，添加流的标志</h5><p><img src="image/image-20200307144652111.png" alt="image-20200307144652111"></p><h5 id="3）：在save操作的时候，判断是流处理还是批处理"><a href="#3）：在save操作的时候，判断是流处理还是批处理" class="headerlink" title="3）：在save操作的时候，判断是流处理还是批处理"></a>3）：在save操作的时候，判断是流处理还是批处理</h5><p><img src="image/image-20200307145006039.png" alt="image-20200307145006039"></p><h4 id="7-3-6-6、开发save操作的批处理的sink源匹配"><a href="#7-3-6-6、开发save操作的批处理的sink源匹配" class="headerlink" title="7.3.6.6、开发save操作的批处理的sink源匹配"></a>7.3.6.6、开发save操作的批处理的sink源匹配</h4><h5 id="1）：创建BatchJobSaveAdaptor类"><a href="#1）：创建BatchJobSaveAdaptor类" class="headerlink" title="1）：创建BatchJobSaveAdaptor类"></a>1）：创建BatchJobSaveAdaptor类</h5><p><img src="image/image-20200307150035441.png" alt="image-20200307150035441"></p><h5 id="2）：对format（数据源）进行匹配，根据需求落地到不同的源"><a href="#2）：对format（数据源）进行匹配，根据需求落地到不同的源" class="headerlink" title="2）：对format（数据源）进行匹配，根据需求落地到不同的源"></a>2）：对format（数据源）进行匹配，根据需求落地到不同的源</h5><p><img src="image/image-20200307151423629.png" alt="image-20200307151423629"></p><h5 id="3）：编写一个最简单的数据落地操作text格式数据落地"><a href="#3）：编写一个最简单的数据落地操作text格式数据落地" class="headerlink" title="3）：编写一个最简单的数据落地操作text格式数据落地"></a>3）：编写一个最简单的数据落地操作text格式数据落地</h5><h6 id="第一步：封装写入操作"><a href="#第一步：封装写入操作" class="headerlink" title="第一步：封装写入操作"></a>第一步：封装写入操作</h6><p><img src="image/image-20200307151551096.png" alt="image-20200307151551096"></p><h6 id="第二步：实现text的写操作"><a href="#第二步：实现text的写操作" class="headerlink" title="第二步：实现text的写操作"></a>第二步：实现text的写操作</h6><p><img src="image/image-20200307152325271.png" alt="image-20200307152325271"></p><h5 id="4）：SaveAdaptor中添加批处理的落地流程"><a href="#4）：SaveAdaptor中添加批处理的落地流程" class="headerlink" title="4）：SaveAdaptor中添加批处理的落地流程"></a>4）：SaveAdaptor中添加批处理的落地流程</h5><p><img src="image/image-20200307171844892.png" alt="image-20200307171844892"></p><h5 id="5）：EngineSQLExecListener监听器实现者里面调用SelectAdaptor"><a href="#5）：EngineSQLExecListener监听器实现者里面调用SelectAdaptor" class="headerlink" title="5）：EngineSQLExecListener监听器实现者里面调用SelectAdaptor"></a>5）：EngineSQLExecListener监听器实现者里面调用SelectAdaptor</h5><p><img src="image/image-20200307172014619.png" alt="image-20200307172014619"></p><h5 id="6）：测试"><a href="#6）：测试" class="headerlink" title="6）：测试"></a>6）：测试</h5><p><img src="image/image-20200307174113415.png" alt="image-20200307174113415"></p><p><img src="image/image-20200307174129100.png" alt="image-20200307174129100"></p>]]></content>
      
      
      <categories>
          
          <category> HBase </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
            <tag> HBase </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>DataNode启动流程</title>
      <link href="2019/02/22/DataNode%E5%90%AF%E5%8A%A8%E6%B5%81%E7%A8%8B/"/>
      <url>2019/02/22/DataNode%E5%90%AF%E5%8A%A8%E6%B5%81%E7%A8%8B/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script>]]></content>
      
      
      <categories>
          
          <category> Hadoop 源码阅读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>消息系统-Kafka(5)：可靠性保障</title>
      <link href="2019/02/15/%E6%B6%88%E6%81%AF%E7%B3%BB%E7%BB%9F-Kafka(5)%EF%BC%9A%E5%8F%AF%E9%9D%A0%E6%80%A7%E4%BF%9D%E9%9A%9C/"/>
      <url>2019/02/15/%E6%B6%88%E6%81%AF%E7%B3%BB%E7%BB%9F-Kafka(5)%EF%BC%9A%E5%8F%AF%E9%9D%A0%E6%80%A7%E4%BF%9D%E9%9A%9C/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><h1 id="1-副本"><a href="#1-副本" class="headerlink" title="1. 副本"></a>1. 副本</h1><h2 id="1-1-概述"><a href="#1-1-概述" class="headerlink" title="1.1. 概述"></a>1.1. 概述</h2><h2 id="1-2-失效副本"><a href="#1-2-失效副本" class="headerlink" title="1.2. 失效副本"></a>1.2. 失效副本</h2><blockquote></blockquote><h3 id="1-2-1-失效副本判定"><a href="#1-2-1-失效副本判定" class="headerlink" title="1.2.1.失效副本判定"></a>1.2.1.失效副本判定</h3><p>从 Kafka 0.9.x 版本开始通过唯一的一个参数 replica.lag.time.max.ms[默认大小为10,000]来控制，当 ISR 中的一个 follower 副本滞后 leader 副本的时间超过参数 replica.lag.time.max.ms 指定的值时即判定为副本失效，需要将此 follower副本 剔出除 ISR 之外。</p><ul><li><p><strong><font color = 'red'>注意</font></strong></p><blockquote><p>在 Kafka 0.9.x 版本之前还有另一个 Broker 级别的参数 <code>replica.lag.max.messages</code> 也是用来判定失效副本的，当一个 follower 副本滞后 leader 副本的消息数超过replica.lag.max.messages 的大小时则判定此 follower 副本为失效副本。</p><p>它与 <code>replica.lag.time.max.ms</code> 参数判定出的失败副本去并集组成一个失效副本的集合，从而进一步剥离出ISR。不过这个 replica.lag.max.messages 参数很难给定一个合适的值，若设置的太大则这个参数本身就没有太多意义，若设置的太小则会让 follower 副本反复的处于同步、未同步、同步的死循环中，进而又会造成ISR的频繁变动。而且这个参数是 Broker 级别的，也就是说对 Broker 中的所有 topic 都生效，就以默认的值4000来说，对于消息流入速度很低的topic来说，比如TPS=10，这个参数并无用武之地；而对于消息流入速度很高的topic来说，比如TPS=20,000，这个参数的取值又会引入ISR的频繁变动，所以从0.9.x版本开始就彻底移除了这一参数</p></blockquote></li></ul><p>当follower副本将leader副本的LEO（Log End Offset，每个分区最后一条消息的位置）之前的日志全部同步时，则认为该follower副本已经追赶上leader副本，此时更新该副本的lastCaughtUpTimeMs标识。Kafka的副本管理器（ReplicaManager）启动时会启动一个副本过期检测的定时任务，而这个定时任务会定时检查当前时间与副本的lastCaughtUpTimeMs差值是否大于参数replica.lag.time.max.ms指定的值。千万不要错误的认为follower副本只要拉取leader副本的数据就会更新lastCaughtUpTimeMs，试想当leader副本的消息流入速度大于follower副本的拉取速度时，follower副本一直不断的拉取leader副本的消息也不能与leader副本同步，如果还将此follower副本置于ISR中，那么当leader副本失效，而选取此follower副本为新的leader副本，那么就会有严重的消息丢失。</p><h2 id="1-3-ISR"><a href="#1-3-ISR" class="headerlink" title="1.3. ISR"></a>1.3. ISR</h2><p>分区中所有副本统称为 AR (Assign Replicas).所有和 leader 副本保持一定程度同步的副本[包括leader 副本在内]组成 LSR</p><p>LSR 集合是 AR 集合的一个子集</p><p>消息会首先发送到 leader 副本，然后 follower 副本才能从 leader 副本中拉取消息进行同步，同步期内 follower副本相较 leader副本有一定的滞后，这个滞后在可忍受的滞后范围，这个范围可以通过参数进行配置</p><p>与 leader 副本滞后过多的副本组成 OSR </p><p>在正常情况下，所有的 follower 副本都应该与 leader副本保持一定程度的同步,即 AR=ISR, OR集合为空</p><h2 id="1-3-1-维护"><a href="#1-3-1-维护" class="headerlink" title="1.3.1. 维护"></a>1.3.1. 维护</h2><p><strong>leader 副本负责维护和跟踪</strong> ISR 集合中所有 follower 副本的滞后状态，当follower副本落后太多或失效时，leader副本会把它从 ISR 集合中剔除。如果 OSR 集合中所有follower副本“追上”了leader副本，那么leader副本会把它从 OSR 集合转移至 ISR 集合。默认情况下，当leader副本发生故障时，只有在 ISR 集合中的follower副本才有资格被选举为新的leader，而在 OSR 集合中的副本则没有任何机会（不过这个可以通过配置来改变）。</p><h2 id="1-3-2-ISR-缩减"><a href="#1-3-2-ISR-缩减" class="headerlink" title="1.3.2. ISR 缩减"></a>1.3.2. ISR 缩减</h2><ul><li><p>isr-expiration定时任务会周期性的检测每个分区是否需要缩减其ISR集合，当检测到ISR中有是失效的副本的时候，就会缩减 ISR 集合；</p></li><li><p><strong>将变更后的数据记录到 ZooKeeper 对应 /brokers/topics//partition//state 节点；</strong></p></li><li><p><strong>ISR 集合发生变更时将变更后的数据缓存到 isrChangeSet</strong></p></li><li><p>isr-change-propagation 定时任务会周期性（固定值为2500ms）地检查 isrChangeSet，在 zk 中的 /isr_change_notification 节点下创建 isr_change 开头的持久顺序节点，并保存 isrChangeSet 的数据</p></li><li><p>kafka控制器为 /isr_change_notification 添加了一个 Watcher，当这个节点中有子节点发生变化的时候会触发 Watcher 动作，以此通知控制器<strong>更新相关的元数据信息</strong>并向它管理的 broker 节点发送更新元数据信息的请求。最<strong>后删除 /isr_change_notification 的路径下已经处理过的节点</strong>。</p><ul><li><p><strong><font color='red'>注意</font></strong></p><blockquote><p>频繁的触发 Watcher 会影响 kafka 控制器，zookeeper 甚至其他的 broker 性能。为了避免这种情况，kafka 添加了指定的条件，当检测到分区 ISR 集合发生变化的时候，还需要检查一下两个条件：</p></blockquote><ul><li>上一次 ISR 集合发生变化距离现在已经超过5秒，</li><li>上一次写入zookeeper的时候距离现在已经超过60秒。</li></ul><blockquote><p>满足以上两个条件之一者可以将 ISR 写入集合的变化的目标节点。</p></blockquote></li></ul></li></ul><h3 id="1-3-3-ISR-增加"><a href="#1-3-3-ISR-增加" class="headerlink" title="1.3.3. ISR 增加"></a>1.3.3. ISR 增加</h3><blockquote><p>随着 follower 副本不断进行消息同步，follower 副本 LEO 也会逐渐后移，并且最终赶上 leader 副本，此时 follower 副本就有资格进入 ISR 集合，追赶上leader 副本的判定准侧是<strong>此副本的 LEO 是否大于等于 leader 副本 HW</strong>。更新 ZooKeeper 中的 /broker/topics//partition//state 节点和 isrChangeSet，之后的操作同 ISR  集合的缩减。</p></blockquote><h2 id="1-4-副本同步"><a href="#1-4-副本同步" class="headerlink" title="1.4. 副本同步"></a>1.4. 副本同步</h2><h3 id="1-4-1-相关概念"><a href="#1-4-1-相关概念" class="headerlink" title="1.4.1. 相关概念"></a>1.4.1. 相关概念</h3><p><strong>LEO [Log End Offset]，标识当前日志文件中下一条待写入的消息的 offset</strong>。上图中 offset 为 9 的位置即为当前日志文件的 LEO</p><p><strong>LEO 的大小相当于当前日志分区中最后一条消息的 offset 值加 1</strong></p><p>分区 ISR 集合中的每个副本都会维护自身的 LEO ，而 ISR 集合中最小的 LEO 即为分区的 HW，对消费者而言只能消费 HW 之前的消息。</p><p><strong>HW</strong> 是 High Watermark 的缩写，俗称高水位，水印，它标识了一个特定的消息偏移量[Offset],消费者只能拉取到这个 Offset 之前的消息。LSR 队列中的最小 LEO。</p><p>Leader副本 发生故障之后，从 LSR 中选出一个新的 Leader副本，为保证多个副本之间的数据一致性，其余的 Follower副本会从各自的日志文件中高于 HW 的部分截掉，然后从新的 Leader 副本同步数据。</p><img src="/Users/zxc/Documents/big_data/Kafka/resources/未命名.png" alt="未命名" style="zoom:72%;" /><h3 id="1-4-2-过程"><a href="#1-4-2-过程" class="headerlink" title="1.4.2. 过程"></a>1.4.2. 过程</h3><p>某个分区有3个副本分别位于 broker0、broker1 和 broker2 节点中，假设 broker0 上的副本1为当前分区的 leader 副本，那么副本2和副本3就是 follower 副本，整个消息追加的过程可以概括如下：</p><ul><li>生产者客户端发送消息至 leader 副本中。</li><li>消息被追加到 leader 副本的本地日志，并且会更新日志的偏移量。</li><li>follower 副本（副本2和副本3）向 leader 副本请求同步数据。</li><li>leader 副本所在的服务器读取本地日志，并更新对应拉取的 follower 副本的信息。</li><li>leader 副本所在的服务器将拉取结果返回给 follower 副本。</li><li>follower 副本收到 leader 副本返回的拉取结果，将消息追加到本地日志中，并更新日志的偏移量信息。</li></ul><p>某一时刻，leader 副本的 LEO 增加至5，并且所有副本的 HW 还都为0。</p><p><strong>之后 follower 副本 向 leader 副本 拉取消息，在拉取的请求中会带有自身的 LEO 信息</strong>[这个 LEO 信息对应的是 FetchRequest 请求中的 fetch_offset]。<strong>leader 副本返回给 follower 副本相应的消息，并且还带有自身的 HW 信息</strong></p><p><strong>follower 副本 各自拉取到了消息，并更新各自的 LEO 为3和4。与此同时，follower 副本 还会更新自己的 HW</strong>，更新 HW 的算法是比较当前 LEO 和 leader 副本 中传送过来的 HW 的值，取较小值作为自己的 HW 值。当前两个 follower 副本的 HW 都等于0</p><p><strong>接下来 follower 副本 再次请求拉取 leader 副本中的消息。</strong></p><p><strong>leader 副本 收到来自 follower 副本 的 FetchRequest 请求，其中带有 LEO 的相关信息，选取其中的最小值作为新的 HW即 min(15,3,4)=3，然后连同消息和 HW 一起返回 FetchResponse 给 follower 副本。</strong> leader 副本 的 HW 是一个很重要的东西，因为它直接影响了分区数据对消费者的可见性。</p><p>两个 follower 副本 在收到新的消息之后更新 LEO 并且更新自己的 HW 为<strong>3</strong> [min(LEO,3)=3]。</p><h2 id="1-4-Leader-Epoch"><a href="#1-4-Leader-Epoch" class="headerlink" title="1.4. Leader Epoch"></a>1.4. Leader Epoch</h2><p>leader epoch 代表 leader 的纪元信息（epoch），初始值为0。每当 leader 变更一次，leader epoch 的值就会加1，相当于为 leader 增设了一个版本号。<br>每个副本中还会增设一个矢量 &lt;LeaderEpoch =&gt; StartOffset&gt;，其中 StartOffset 表示当前 LeaderEpoch 下写入的第一条消息的偏移量。</p><h2 id="1-5-为什么不支持读写分离？"><a href="#1-5-为什么不支持读写分离？" class="headerlink" title="1.5. 为什么不支持读写分离？"></a>1.5. 为什么不支持读写分离？</h2><h2 id="1-6-日志同步"><a href="#1-6-日志同步" class="headerlink" title="1.6. 日志同步"></a>1.6. 日志同步</h2>]]></content>
      
      
      <categories>
          
          <category> Kafka </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kafka </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>消息系统-Kafka(3)：消费者</title>
      <link href="2019/02/13/%E6%B6%88%E6%81%AF%E7%B3%BB%E7%BB%9F-Kafka(3)%EF%BC%9A%E6%B6%88%E8%B4%B9%E8%80%85/"/>
      <url>2019/02/13/%E6%B6%88%E6%81%AF%E7%B3%BB%E7%BB%9F-Kafka(3)%EF%BC%9A%E6%B6%88%E8%B4%B9%E8%80%85/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><h1 id="1-消费者和消费者组"><a href="#1-消费者和消费者组" class="headerlink" title="1.消费者和消费者组"></a>1.消费者和消费者组</h1><h2 id="1-1-概述"><a href="#1-1-概述" class="headerlink" title="1.1. 概述"></a>1.1. 概述</h2><p>Kafka 消费者从属于消费者组。一个消费者组里的消费者订阅的为同一主题。每个消费者接受主题一部分分区的消息。</p><p><strong>向消费者组里增加消费者是横向伸缩消费能力的主要方式</strong>。Kafka 消费者在做一些高延迟的操作，如向HDFS或数据库中写入数据，或者使用数据进行一些耗时的计算操作。在这些情况下，单个消费者无法跟上数据的生成速度，此时我们可以增加消费者，以分担负载，每个消费者只处理部分分区上的消息，这就是横向伸缩的主要手段。我们有必要为主题创建大量分区，负载增长时可以增加更多的消费者</p><p><font color = 'red'><strong>注意：</strong> 一个主题的同一个分区同时只能供同一个消费者组里的一个消费者消费数据，因此不要让消费者的数量超过主题分区的数量，多于的消费者只会被闲置。</font></p><h2 id="1-2-消费方式"><a href="#1-2-消费方式" class="headerlink" title="1.2. 消费方式"></a>1.2. 消费方式</h2><p>由于 Push 模式 很难适应消费速率不同的消费者，因此消息发送速率是由 broker 决定的。它的目标是尽可能的以最快的速度传递消息，但是这样容易造成 consumer 来不及处理消息，典型的表现就是拒绝服务以及网络拥塞。 所以 kafka 采用 pull 模式，根据消费者的能力以适当的速率消费消息。==</p><p><strong>Pull 模式的不足之处是如果 kafka 中没有数据，消费者可能会陷入循环中，一直返回空数据，针对这一点，Kafka 的消费者在消费数据时会传入一个时长参数 timeout，如果当前没有数据可供消费，consumer 会等待一段时间在返回，这段时长即为 timeout。</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> ConsumerRecords&lt;K, V&gt; <span class="title">poll</span><span class="params">(<span class="keyword">final</span> Duration timeout)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> poll(timeout.toMillis(), <span class="keyword">true</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="1-3-特定偏移处消费数据"><a href="#1-3-特定偏移处消费数据" class="headerlink" title="1.3. 特定偏移处消费数据"></a>1.3. 特定偏移处消费数据</h2><p>使用 poll() 开始消费每个分区中最后一个已经发生的偏移量的消息，并继续按顺序处理所有消息。但是，有时我们肯想要以不同的偏移量开始阅读。如果你想从分区的开头读取所有消息，或者你想要一直跳到分区的末尾并开始只消费新消息，那么可以使用有一些专门的API：seekToBeginning(TopicPartition tp)和seekToEnd(TopicPartition tp)。</p><h2 id="1-4-独立消费者"><a href="#1-4-独立消费者" class="headerlink" title="1.4. 独立消费者"></a>1.4. 独立消费者</h2><p>单一的消费者总是需要从主题中的所有分区读取数据，或者从一个主题特定分区读取数据。在这种情况下没有理由需要组或负载均衡，只是订阅特定的主题或分区，偶尔使用消息和提交偏移量。</p><h2 id="1-5-多线程消费者"><a href="#1-5-多线程消费者" class="headerlink" title="1.5. 多线程消费者"></a>1.5. 多线程消费者</h2><p>KafkaProducer是线程安全的，而 KafkaConsumer 是非线程安全的，多线程需要处理好线程同步，多线程的实现方式有多种，这里介绍一种：<strong>每个线程各自实例化一个KakfaConsumer对象</strong>，这种方式的缺点是：当这些线程属于同一个消费组时，线程的数量受限于分区数，当消费者线程的数量大于分区数时，就有一部分消费线程一直处于空闲状态</p><h2 id="1-6-退出"><a href="#1-6-退出" class="headerlink" title="1.6. 退出"></a>1.6. 退出</h2><p>如果确定要退出循环，需要通过另一个线程调用 consumer.wakeup() 方法。</p><p>如果循环运行在主线程里，可以在 ShutdownHook 里调用该方法。consumer.wakeup() 是消费者唯一一个可以从其他线程里安全调用的方法。</p><p>ShutdownHook运行在单独的线程里，所以退出循环最安全的方式只能是调用 consumer.wakeup()</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Runtime.getRuntime().addShutdownHook(<span class="keyword">new</span> Thread() &#123;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    System.out.println(<span class="string">&quot;Starting exit...&quot;</span>);</span><br><span class="line">    consumer.wakeup();</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      mainThread.join();</span><br><span class="line">    &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">      e.printStackTrace();</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure><p>调用 consumer.wakeup() 可以退出 poll() ，并抛出 WakeupException异常，或者如果调用consumer.wakeup() 时线程没有等待轮询，那么异常将在下一轮调用 poll() 时抛出。不需要处理WakeupException,因为它只是用于跳出循环的一种方式。不过，在退出线程之前调用 consumer.close()是很有必要的，它会提交任何还没有提交的东西，并向群组协调器发送消息，告知自己要离开群组，接下来就会触发再均衡，而不需要等待会话超时。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">try</span>&#123;</span><br><span class="line">  <span class="keyword">while</span>(<span class="keyword">true</span>) &#123;</span><br><span class="line">    ConsumerRecords records = movingAvg.consumer.poll(<span class="number">1000</span>);</span><br><span class="line">    System.out.println(System.currentTimeMillis() + <span class="string">&quot; -- waiting for data...&quot;</span>);</span><br><span class="line">    <span class="keyword">for</span>(ConsumerRecord record : records) &#123;</span><br><span class="line">      System.out.printf(<span class="string">&quot;offset = %d, key = %s, value = %s\n&quot;</span>,</span><br><span class="line">                        record.offset(), record.key(), record.value());</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">for</span>(TopicPartition tp: consumer.assignment())&#123;</span><br><span class="line">    System.out.println(<span class="string">&quot;Committing offset at position:&quot;</span> + consumer.position(tp));</span><br><span class="line">    &#125;</span><br><span class="line">    movingAvg.consumer.commitSync();</span><br><span class="line">  &#125;</span><br><span class="line">&#125; <span class="keyword">catch</span>(WakeupException e) &#123;</span><br><span class="line">    <span class="comment">// ignore for shutdown</span></span><br><span class="line">&#125; <span class="keyword">finally</span>&#123;</span><br><span class="line">  <span class="comment">//在退出之前，确保你已经完全关闭了消费者</span></span><br><span class="line">    consumer.close();</span><br><span class="line">    System.out.println(<span class="string">&quot;Closed consumer and we are done&quot;</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="2-分区分配"><a href="#2-分区分配" class="headerlink" title="2. 分区分配"></a>2. 分区分配</h1><p>群组里的消费者共同读取主题的分区，一个新的消费者加入群组时，它读取的是原本由其他消费者读取的消息。</p><p>当一个消费者被关闭或发生崩溃时，它就离开群组，原本由它读取的分区将由群组里的其他消费者来读取。在主题发生变化时，比如管理员添加了新的分区，会发生分区重分配。</p><h2 id="3-1-再均衡"><a href="#3-1-再均衡" class="headerlink" title="3.1. 再均衡"></a>3.1. 再均衡</h2><h3 id="3-1-1-概述"><a href="#3-1-1-概述" class="headerlink" title="3.1.1. 概述"></a>3.1.1. 概述</h3><p><font color = 'blue'><strong>分区的所有权从一个消费者转移到另一个消费者，这样的行为被称为再均衡</strong></font></p><p>再均衡为消费者群组带来了<strong>高可用</strong>和<strong>伸缩性。</strong></p><p>再均衡期间，消费者无法读取消息，造成整个群组一小段时间内的不可用。此外，当分区被重新分配给另一个消费者时，消费者当前的读取状态会丢失，它还有可能还需要去刷新缓存，在它重新恢复状态之前会拖慢应用程序。<br>当消费者要加入群组时，会向群组协调器发送一个JoinGroup请求。第一个加入群组的消费者将成为群主。群主从协调器那里获取群组的成员列表，列表包含了最近发送过心跳的消费者，并负责给每一个消费者分配分区。<br>PartitionAssignor 接口的类决定哪些分区被分配给哪些消费者</p><p>在Kafka中，当有新消费者加入或者订阅的topic数发生变化时，会触发Rebalance(再均衡：在同一个消费者组当中，分区的所有权从一个消费者转移到另外一个消费者)机制，Rebalance顾名思义就是重新均衡消费者消费。Rebalance的过程如下：</p><p>第一步：所有成员都向coordinator发送请求，请求入组。一旦所有成员都发送了请求，coordinator会从中选择一个consumer担任leader的角色，并把组成员信息以及订阅信息发给leader。<br>第二步：leader开始分配消费方案，指明具体哪个consumer负责消费哪些topic的哪些partition。一旦完成分配，leader会将这个方案发给coordinator。coordinator接收到分配方案之后会把方案发给各个consumer，这样组内的所有成员就都知道自己应该消费哪些分区了。</p><h3 id="3-1-2-条件"><a href="#3-1-2-条件" class="headerlink" title="3.1.2. 条件"></a>3.1.2. 条件</h3><ol><li><p>消费者组中新<strong>添加消费者</strong>读取到原本是其他消费者读取的消息</p></li><li><p><strong>消费者关闭或崩溃</strong>之后离开群组，原本由他读取的  partition 将由群组里其他消费者读取</p></li><li><p>当向一个 Topic <strong>添加新的 partition</strong>，会发生 partition 在消费者中的重新分配</p></li></ol><h3 id="3-1-3-再均衡监听器"><a href="#3-1-3-再均衡监听器" class="headerlink" title="3.1.3.  再均衡监听器"></a>3.1.3.  再均衡监听器</h3><p>在为消费者分配新的partition或者移除旧的partition时，可以通过消费者API执行一些应用程序代码，在使用<strong>subscribe</strong>()方法时传入一个<strong>ConsumerRebalanceListener</strong>实例。</p><p><strong>ConsumerRebalanceListener</strong>需要实现的两个方法</p><ol><li><p><strong>public void onPartitionRevoked(Collection<TopicPartition> partitions)</strong></p><p>该方法会在<strong>再均衡开始之前</strong>和<strong>消费者停止读取消息之后</strong>被调用。如果在这里提交偏移量，下一个接管partition的消费者就知道该从哪里开始读取了。</p></li><li><p><strong>public void onPartitionAssigned(Collection<TopicPartition> partitions)</strong></p><p>该方法会在<strong>重新分配partition之后</strong>和<strong>消费者开始读取消息之前</strong>被调用。</p></li></ol><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> Map&lt;TopicPartition, OffsetAndMetadata&gt; currentOffsets = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line"><span class="keyword">private</span> <span class="class"><span class="keyword">class</span> <span class="title">HandleRebalance</span> <span class="keyword">implements</span> <span class="title">ConsumerRebalanceListener</span> </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onPartitionsAssigned</span><span class="params">(Collection&lt;TopicPartition&gt; partitions)</span> </span>&#123;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onPartitionsRevoked</span><span class="params">(Collection&lt;TopicPartition&gt; partitions)</span> </span>&#123;</span><br><span class="line">       <span class="comment">//如果发生再均衡，要在即将失去partition所有权时提交偏移量。</span></span><br><span class="line">       <span class="comment">//调用commitSync方法，确保在再均衡发生之前提交偏移量</span></span><br><span class="line">        consumer.commitSync(currentOffsets);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="keyword">try</span>&#123;</span><br><span class="line">      consumer.subscribe(topics, <span class="keyword">new</span> HandleRebalance());</span><br><span class="line">      <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">          ConsumerRecords&lt;String, String&gt; records = consumer.poll(<span class="number">100</span>);</span><br><span class="line">          <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; record : records) &#123;</span><br><span class="line">              currentOffsets.put(<span class="keyword">new</span> TopicPartition(record.topic(), record.partition()), </span><br><span class="line">                                 <span class="keyword">new</span> OffsetAndMetadata(record.offset() + <span class="number">1</span>, “no matadata”));</span><br><span class="line">          &#125;</span><br><span class="line">          consumer.commitAsync(currentOffsets, <span class="keyword">null</span>);</span><br><span class="line">  &#125; <span class="keyword">catch</span>(WakeupException e) &#123;</span><br><span class="line">      <span class="comment">//忽略异常，正在关闭消费者</span></span><br><span class="line">  &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">      log.error(<span class="string">&quot;unexpected error&quot;</span>, e);</span><br><span class="line">  &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">      <span class="keyword">try</span>&#123;</span><br><span class="line">          consumer.commitSync(currentOffsets);</span><br><span class="line">      &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">          consumer.close();</span><br><span class="line">      &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><h2 id="3-2-分区分配"><a href="#3-2-分区分配" class="headerlink" title="3.2. 分区分配"></a>3.2. 分区分配</h2><h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><p>在 Kafka 中，存在着三种分区分配策略。一种是 RangeAssignor 分配策略(范围分区)，另一种是 RoundRobinAssignor 分配策略(轮询分区)。默认采用 Range 范围分区。</p><p>Kafka 提供了消费者客户端参数 partition.assignment.strategy 用来设置消费者与订阅主题之间的分区分配策略。默认情况下，此参数的值为：org.apache.kafka.clients.consumer.RangeAssignor，即采用 RangeAssignor 分配策略。</p><p><img src="https://img-blog.csdnimg.cn/20200105211408449.png" alt="在这里插入图片描述"></p><ol><li><p><strong>RangeAssignor</strong></p><p><strong>RangeAssignor 策略的原理是按照消费者总数和分区总数进行整除运算来获得一个跨度，然后将分区按照跨度进行平均分配，以保证分区尽可能均匀地分配给所有的消费者。对于每一个topic，RangeAssignor 策略会将消费组内所有订阅这个 topic 的消费者按照名称的字典序排序，然后为每个消费者划分固定的分区范围，如果不够平均分配，那么字典序靠前的消费者会被多分配一个分区。</strong></p><ul><li>topic下的所有有效分区平铺</li><li>消费者按照字典排序</li><li>分区数除以消费者数，得到 n</li><li>分区数对消费者数取余，得到 m</li><li>消费者集合中，前 m 个消费者能够分配到 n+1 个分区，而剩余的消费者只能分配到 n 个分区</li></ul></li><li><p><strong>RoundRobinAssignor</strong></p><p><strong>RoundRobin 轮询分区策略，是把所有的 partition 和所有的 consumer 都列出来，然后按照 hashcode 进行排序，最后通过轮询算法来分配 partition 给到各个消费者。</strong></p><ul><li>消费者按照字典排序，例如C0, C1, C2… …，并构造环形迭代器。</li><li>topic 名称按照字典排序，并得到每个 topic 的所有分区，从而得到所有分区集合。</li><li>遍历第2步所有分区集合，同时轮询消费者。</li><li>如果轮询到的消费者订阅的topic不包括当前遍历的分区所属topic，则跳过；否则分配给当前消费者，并继续第3步。</li></ul></li><li><p><strong>StickyAssignor</strong></p><p>Kafka 从 0.11.x 版本开始引入 StickyAssignor 分配策略，它主要有两个目的：分区的分配要尽可能的均匀和分区的分配尽可能的与上次分配的保持相同。当两者发生冲突时，第一个目标优先于第二个目标。鉴于这两个目标，StickyAssignor 策略的具体实现要比 RangeAssignor 和RoundRobinAssignor 这两种分配策略要复杂很多。</p></li></ol><h2 id="4-提交和偏移量-Offset"><a href="#4-提交和偏移量-Offset" class="headerlink" title="4. 提交和偏移量 Offset"></a>4. 提交和偏移量 Offset</h2><p>调用 poll() 时，返回由生产者写入 Kafka 但还没有被消费者读取过的记录</p><h4 id="Offset-的维护"><a href="#Offset-的维护" class="headerlink" title="Offset 的维护"></a>Offset 的维护</h4><p>由于 consumer 在消费过程中可能会出现断电等故障，consumer恢复之后，需要从故障前的位置继续消费，所以 consumer 需要记录自己消费位置，以便故障恢复后继续消费。 </p><p>Kafka 0.9 版本之前， comsumer 默认将 offset 保存在 Zookeeper中</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 3] ls /</span><br><span class="line">[cluster, controller, brokers, zookeeper, admin, isr_change_notification, dubbo, log_dir_event_notification, controller_epoch, kafka-manager, consumers, hive_zookeeper_namespace_hive, latest_producer_id_block, config, hbase]</span><br><span class="line"></span><br><span class="line">[zk: localhost:2181(CONNECTED) 16] ls /consumers/console-consumer-37662/offsets/test_kafka</span><br><span class="line">[44, 45, 46, 47, 48, 49, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43]</span><br><span class="line"></span><br><span class="line">[zk: localhost:2181(CONNECTED) 19] ls /consumers/console-consumer-37662/offsets/test_kafka/0</span><br><span class="line">[]</span><br></pre></td></tr></table></figure><p>从0.9 版本开始， consumer 默认将 offset 保存在 kafka __consumer_offsets主题中。</p><img src="https://img-blog.csdnimg.cn/20200105165739847.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTA4MDk4NA==,size_13,color_FFFFFF,t_70" alt="在这里插入图片描述" style="zoom:50%;" /><h4 id="2-2-自动提交偏移量"><a href="#2-2-自动提交偏移量" class="headerlink" title="2.2. 自动提交偏移量"></a>2.2. 自动提交偏移量</h4><p>将enable.auto.commit 设为true，则每过5秒，消费者会自动把从 poll() 方法接受到的最大偏移量提交上去。提交时间由auto.commit.interval.ms控制，默认值为5秒</p><ul><li><p>自动提交偏移量不足</p><p>假设我们使用默认的5秒提交时间间隔，在最近一次提交之后的3秒发生了再均衡，再均衡之后，消费者从最后一次提交的偏移量位置开始读取消息。这个时候偏移量已经落后了3秒，所以这3秒内到达的消息将会被重复处理。<br>**<font color='red'>注：可以通过修改提交时间间隔来频繁提交偏移量，减少可能出现重复消息的时间窗，不过这种情况无法完全避免。</font>**</p><h4 id="2-3-手动提交偏移量"><a href="#2-3-手动提交偏移量" class="headerlink" title="2.3. 手动提交偏移量"></a>2.3. 手动提交偏移量</h4><p>把 enable.auto.commit 设为 false，让应用程序决定何时提交偏移量。使用 commitSync() 提交偏移量最简单也最可靠。</p></li></ul><h2 id="5-总结"><a href="#5-总结" class="headerlink" title="5.总结"></a>5.总结</h2><h4 id="Kafka-如何保证消息有序"><a href="#Kafka-如何保证消息有序" class="headerlink" title="Kafka 如何保证消息有序 ?"></a><font color='blue'>Kafka 如何保证消息有序 ?</font></h4><p>kafka 中的每个 partition 中的消息在写入时都是有序的，而且单独一个 partition 只能由一个消费者去消费，可以在里面保证消息的顺序性。但是分区之间的消息是不保证有序的。</p><p>两种方案</p><ol><li><p>kafka topic 只设置一个 partition 分区 </p><p>kafka 默认保证同一个 partition 分区内的消息是有序的，则可以设置topic只使用一个分区，这样消息就是全局有序，缺点是只能被consumer group里的一个消费者消费，降低了性能，不适用高并发的情况</p></li><li><p>producer 将消息发送到指定 partition 分区</p><p>既然 kafka 默认保证同一个 partition 分区内的消息是有序的，则 producer 可以在发送消息时可以指定需要保证顺序的几条消息发送到同一个分区，这样消费者消费时，消息就是有序。</p></li></ol><h5 id="Kafka-缺点-？"><a href="#Kafka-缺点-？" class="headerlink" title="Kafka 缺点 ？"></a><font color='blue'>Kafka 缺点 ？</font></h5><ol><li>由于是批量发送，数据并非真正的实时；</li><li>对于mqtt协议不支持；</li><li>不支持物联网传感数据直接接入；</li><li>仅支持统一分区内消息有序，无法实现全局消息有序；</li><li>监控不完善，需要安装插件；</li><li>依赖zookeeper进行元数据管理；</li></ol>]]></content>
      
      
      <categories>
          
          <category> Kafka </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kafka </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>消息系统-Kafka(1)：初识Kafka</title>
      <link href="2019/02/12/%E6%B6%88%E6%81%AF%E7%B3%BB%E7%BB%9F-Kafka(1)%EF%BC%9A%E5%88%9D%E8%AF%86Kafka/"/>
      <url>2019/02/12/%E6%B6%88%E6%81%AF%E7%B3%BB%E7%BB%9F-Kafka(1)%EF%BC%9A%E5%88%9D%E8%AF%86Kafka/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p>一个消息系统负责将数据从一个应用传递到另外一个应用，应用只需关注于数据，无需关注数据在两个或多个应用间是如何传递的。</p><a id="more"></a><h1 id="1-消息系统"><a href="#1-消息系统" class="headerlink" title="1.消息系统"></a>1.消息系统</h1><p>一个消息系统负责将数据从一个应用传递到另外一个应用，应用只需关注于数据，无需关注数据在两个或多个应用间是如何传递的。</p><ol><li><h4 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h4><p>分布式消息传递基于可靠的消息队列，在客户端应用和消息系统之间异步传递消息。有两种主要的消息传递模式：<strong>点对点传递模式、发布-订阅模式</strong>。大部分的消息系统选用发布-订阅模式。<strong>Kafka就是一种发布-订阅模式</strong>。</p></li><li><h4 id="点对点传递模式"><a href="#点对点传递模式" class="headerlink" title="点对点传递模式"></a>点对点传递模式</h4><p>在点对点消息系统中，消息持久化到一个队列中。此时，将有一个或多个消费者消费队列中的数据。但是一条消息只能被消费一次。当一个消费者消费了队列中的某条数据之后，该条数据则从消息队列中删除。该模式即使有多个消费者同时消费数据，也能保证数据处理的顺序。</p></li><li><h4 id="发布-订阅模式"><a href="#发布-订阅模式" class="headerlink" title="发布-订阅模式"></a>发布-订阅模式</h4><p>在发布-订阅消息系统中，消息被持久化到一个 topic 中。与点对点消息系统不同的是，消费者可以订阅一个或多个 topic，消费者可以消费该topic中所有的数据，同一条数据可以被多个消费者消费，数据被消费后不会立马删除。在发布-订阅消息系统中，消息的生产者称为发布者，消费者称为订阅者。</p></li></ol><h1 id="2-Kafka"><a href="#2-Kafka" class="headerlink" title="2.Kafka"></a>2.Kafka</h1><h2 id="2-1-概述"><a href="#2-1-概述" class="headerlink" title="2.1.概述"></a>2.1.概述</h2><p>Kafka 是<strong>分布式发布-订阅消息系统</strong>，它最初是由 LinkedIn 公司开发的，之后成为 Apache 项目的一部分，<strong>Kafka是一个分布式，可划分的，冗余备份的持久性的日志服务，它主要用于处理流式数据</strong>。</p><h2 id="2-2-特征"><a href="#2-2-特征" class="headerlink" title="2.2. 特征"></a>2.2. 特征</h2><ol><li><p><strong>支持多个生产者</strong></p><p>Kafka 可以无缝地支持多个生产者，不管客户端在使用单个主题还是多个主题。所以它很适合用来从多个前端系统收集数据，并以统一的格式对外提供数据。例如，一个包含了多个微服务的网站，可以为页面视图创建一个单独的主题，所有服务都以相同的消息格式向该主题写入数据。消费者应用程序会获得统一的页面视图，而无需协调来自不同生产者的数据流。</p></li><li><p><strong>支持多个消费者</strong></p><p>Kafka 也支持多个消费者从一个单独的消息流上读取数据，而且消费者之间直不影响。这与其他队列系统不同，其他队列系统的消息一旦被一个客户端读取，其他客户端就无法再读取它。另外，多个消费者可以组成一个群组，它们共享一个悄息流，并保证整个群组对每个给定的消息只处理一次。</p></li><li><p><strong>基于磁盘存储</strong></p><p>消费者可能会因为处理速度慢或突发的流量高峰导致无陆及时读取消息，而持久化数据可以保证数据不会丢失。消费者可以在进行应用程序维护时离线一小段时间，而无需担心消息丢失或堵塞在生产者端。消费者可以被关闭，但消息会继续保留在Kafka 里。消费者可以从上次中断的地方继续处理消息。</p></li><li><p><strong>可伸缩，高性能</strong></p><p>通过横向扩展生产者、消费者和 broker, Kafka 可以轻松处理巨大的消息流。在处理大量数据的同时，它还能保证亚秒级的消息延迟。</p></li></ol><h2 id="2-3-应用场景"><a href="#2-3-应用场景" class="headerlink" title="2.3.应用场景"></a>2.3.应用场景</h2><ul><li><p><strong>日志收集</strong></p><blockquote><p>一个公司可以用 Kafka 可以收集各种服务的 log，通过 Kafka 以统一接口服务的方式开放给各种consumer，例如 Hadoop、Hbase 等</p></blockquote></li><li><p><strong>消息系统</strong></p><blockquote><p>解耦和生产者和消费者、缓存消息等</p></blockquote></li><li><p><strong>用户活动跟踪</strong></p><blockquote><p>Kafka 经常被用来记录 web 用户或者 app 用户的各种活动，如浏览网页、搜索、点击等活动，这些活动信息被各个服务器发布到 Kafka 的 topic 中，然后订阅者通过订阅这些 topic 来做实时的监控分析，或者装载到 Hadoop、数据仓库中做离线分析和挖掘</p></blockquote></li><li><p><strong>运营指标</strong></p><blockquote><p>Kafka 也经常用来记录运营监控数据。包括收集各种分布式应用的数据，生产各种操作的集中反馈，比如报警和报告</p></blockquote></li><li><p><strong>流式处理</strong></p><blockquote><p>比如 spark streaming 和 storm</p></blockquote></li></ul><h1 id="3-架构"><a href="#3-架构" class="headerlink" title="3. 架构"></a>3. 架构</h1><p>一个典型的 Kafka 体系架构包括若干Producer [可以是服务器日志，业务数据，页面前端产生的page view等等]，若干broker [Kafka支持水平扩展，一般broker数量越多，集群吞吐率越高]，若干Consumer (Group)，以及一个Zookeeper集群。Kafka通过Zookeeper管理集群配置，选举leader，以及在consumer group发生变化时进行rebalance。</p><p>Producer 使用 <strong>push[推]</strong> 模式将消息发布到broker</p><p>Consumer 使用 **pull[拉] **模式从broker订阅并消费消息。</p><p>Pull 模式下，consumer 可以自主决定是否批量的从 broker 拉取数据。Push 模式必须在不知道下游consumer 消费能力和消费策略的情况下决定是立即推送每条消息还是缓存之后批量推送。如果为了避免consumer 崩溃而采用较低的推送速率，将可能导致一次只推送较少的消息而造成浪费。</p><p>Pull 模式下，consumer 就可以根据自己的消费能力去决定这些策略</p><p>Pull 模式有个缺点是，如果 broker 没有可供消费的消息，将导致 consumer 不断在循环中轮询，直到新消息到达。为了避免这点，Kafka 有个参数可以让 consumer 阻塞知道新消息到达[当然也可以阻塞知道消息的数量达到某个特定的量这样就可以批量发]</p><h2 id="3-1-Broker"><a href="#3-1-Broker" class="headerlink" title="3.1. Broker"></a>3.1. Broker</h2><p><strong>Kafka 集群包含一个或多个服务器，服务器节点称为 broker。</strong></p><p><strong>broker 存储 topic 的数据。如果某 topic 有 N 个partition，集群有 N 个broker，那么每个broker 存储该 topic 的一个 partition。</strong></p><ol><li><p><strong>Topic</strong></p><p>每条发布到 Kafka 集群的消息都有一个类别，这个类别被称为 Topic。物理上不同 Topic 的消息分开存储，逻辑上一个 Topic 的消息虽然保存于一个或多个 broker 上但用户只需指定消息的Topic即可生产或消费数据而不必关心数据存于何处类似于数据库的表名</p></li><li><p><strong>Partition</strong></p></li></ol><p>topic 中的数据分割为一个或多个 partition。每个 topic 至少有一个 partition。每个partition 中的数据使用多个 segment 文件存储。partition 中的数据是有序的，不同partition 间的数据丢失了数据的顺序。如果 topic 有多个 partition，消费数据时就不能保证数据的顺序。在需要严格保证消息的消费顺序的场景下，需要将 partition 数目设为1。</p><p>如果 Topic 不进行分区，而将 Topic 内的消息存储于一个 broker，那么关于该 Topic 的所有读写请求都将由这一个 broker 处理，吞吐量很容易陷入瓶颈，这显然是不符合高吞吐量应用场景的。</p><p>有了 Partition 概念以后，假设一个 Topic 被分为 10 个 Partitions，Kafka 会根据一定的算法将 10 个 Partition 尽可能均匀的分布到不同的 broker（服务器）上，当 producer 发布消息时，producer 客户端可以采用 random、key-hash 及 轮询 等算法选定目标 partition，若不指定，Kafka 也将根据一定算法将其置于某一分区上。Partiton 机制可以极大的提高吞吐量，并且使得系统具备良好的水平扩展能力。</p><p>如果某topic有N个partition，集群有(N+M)个broker，那么其中有N个broker存储该topic的一个partition，剩下的M个broker不存储该topic的partition数据。</p><p>如果某topic有N个partition，集群中broker数目少于N个，那么一个broker存储该topic的一个或多个partition。在实际生产环境中，尽量避免这种情况的发生，这种情况容易导致Kafka集群数据不均衡。</p><h2 id="3-2-Producer"><a href="#3-2-Producer" class="headerlink" title="3.2.Producer"></a>3.2.Producer</h2><p>生产者即数据的发布者，该角色将消息发布到 Kafka 的 topic 中。broker 接收到生产者发送的消息后，broker 将该消息<strong>追加</strong>到当前用于追加数据的 segment 文件中。生产者发送的消息，存储到一个partition 中，生产者也可以指定数据存储的 partition。</p><h2 id="3-3-Consumer-amp-Consumer-Group"><a href="#3-3-Consumer-amp-Consumer-Group" class="headerlink" title="3.3.Consumer &amp; Consumer Group"></a>3.3.Consumer &amp; Consumer Group</h2><p>消费者可以从 Broker 中读取数据。消费者可以消费多个 topic 中的数据。</p><p>每个 Consumer 属于一个特定的 Consumer Group（可为每个Consumer指定group name，若不指定group name则属于默认的group）。这是 Kafka 用来实现一个 topic 消息的广播（发给所有的consumer）和单播（发给任意一个consumer）的手段。一个 topic 可以有多个 CG。topic 的消息会复制给 consumer。如果需要实现广播，只要每个 consumer 有一个独立的 CG 就可以了。要实现单播只要所有的 consumer 在同一个 CG。用CG还可以将 consumer 进行自由的分组而不需要多次发送消息到不同的topic。</p><h2 id="3-4-Zookeeper"><a href="#3-4-Zookeeper" class="headerlink" title="3.4.Zookeeper"></a>3.4.Zookeeper</h2><h3 id="3-4-1-概述"><a href="#3-4-1-概述" class="headerlink" title="3.4.1. 概述"></a>3.4.1. 概述</h3><p><strong>Zookeeper 是一个开放源码的、高性能的分布式协调服务，它用于 Kafka 的分布式应用。Zookeeper 主要用来跟踪 Kafka 集群中的节点状态, 以及 Kafka Topic, message 等等其他信息. 同时, Kafka 依赖于Zookeeper, 没有Zookeeper 是不能运行起来 Kafka 的.</strong></p><p>Zookeeper 存储了一些关于 consumer 和 broker 的信息，那么就从这两方面说明 zookeeper 的作用。</p><ol><li><p><strong>Broker 注册</strong></p><p>zookeeper 记录了所有 broker 的存活状态，broker 会向 zookeeper 发送心跳请求来上报自己的状态。</p><p>zookeeper 维护了一个正在运行并且属于集群的 broker 列表。</p></li><li><p><strong>Topic 注册</strong></p><p>在 Kafka 中，同一个 <strong>Topic 的消息会被分成多个分区</strong>并将其分布在多个 Broker 上，<strong>这些分区信息及与 Broker 的对应关系</strong>也都是由 Zookeeper 在维护，由专门的节点来记录</p></li><li><p>控制器选举</p><p>Kafka 集群中有多个 Broker，其中有一个会被选举为控制器。</p><p>控制器负责管理整个集群所有分区和副本的状态，例如某个分区的 leader 故障了，控制器会选举新的 leader。</p><p>从多个 broker 中选出控制器，这个工作就是 zookeeper 负责的。</p></li><li><p>记录 ISR 信息</p><p> zookeeper 记录着 ISR 的信息，而且是实时更新的，只要发现其中有成员不正常，马上移除。</p></li><li><p>topic 配置</p><p>zookeeper 保存了 topic 相关配置，例如 topic 列表、每个 topic 的 partition 数量、副本的位置等等。</p></li><li><p>consumer</p><ul><li><p>offset</p><blockquote><p>kafka 老版本中，consumer 的消费偏移量是默认存储在 zookeeper 中的。</p></blockquote><blockquote><p>新版本中，逐渐弱化了 zookeeper 的作用。新的 consumer 使用了 kafka 内部的group coordination 协议，也减少了对zookeeper的依赖，工作由 kafka 自己做了，kafka 专门做了一个 offset manager。</p></blockquote></li><li><p>注册</p><blockquote><p>和 broker 一样，consumer 也需要注册。</p><p>consumer 会自动注册，注册的方式也是创建一个临时节点，consumer down 了之后就会自动销毁。</p></blockquote></li></ul></li><li><p><strong>分区注册</strong></p><blockquote><p>kafka 的每个 partition 只能被消费组中的一个 consumer 消费，kafka 必须知道所有 partition 与 consumer 的关系。</p></blockquote></li></ol><h2 id="4-总结"><a href="#4-总结" class="headerlink" title="4.总结"></a>4.总结</h2><h5 id="Kafka-分区数可以增加或减少吗？为什么？"><a href="#Kafka-分区数可以增加或减少吗？为什么？" class="headerlink" title="Kafka 分区数可以增加或减少吗？为什么？"></a><font color='blue'>Kafka 分区数可以增加或减少吗？为什么？</font></h5><p>我们可以使用 bin/kafka-topics.sh 命令对 Kafka 增加 Kafka 的分区数据，但是 Kafka 不支持减少分区数。<br>Kafka 分区数据不支持减少是由很多原因的，比如减少的分区其数据放到哪里去？是删除，还是保留？删除的话，那么这些没消费的消息不就丢了。如果保留这些消息如何放到其他分区里面？追加到其他分区后面的话那么就破坏了 Kafka 单个分区的有序性。如果要保证删除分区数据插入到其他分区保证有序性，那么实现起来逻辑就会非常复杂。</p>]]></content>
      
      
      <categories>
          
          <category> Kafka </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kafka </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>消息系统-Kafka(2)：生产者</title>
      <link href="2019/02/12/%E6%B6%88%E6%81%AF%E7%B3%BB%E7%BB%9F-Kafka(2)%EF%BC%9A%E7%94%9F%E4%BA%A7%E8%80%85/"/>
      <url>2019/02/12/%E6%B6%88%E6%81%AF%E7%B3%BB%E7%BB%9F-Kafka(2)%EF%BC%9A%E7%94%9F%E4%BA%A7%E8%80%85/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p>生产者即数据的发布者，该角色将消息发布到 Kafka 的 topic 中。broker 接收到生产者发送的消息后，broker 将该消息<strong>追加</strong>到当前用于追加数据的 segment 文件中。生产者发送的消息，存储到一个partition 中，生产者也可以指定数据存储的 partition。</p><a id="more"></a><h1 id="1-生产者概览"><a href="#1-生产者概览" class="headerlink" title="1.生产者概览"></a>1.生产者概览</h1><p>生产者即数据的发布者，该角色将消息发布到 Kafka 的 topic 中。broker 接收到生产者发送的消息后，将该消息<strong>追加</strong>到当前用于追加数据的 segment 文件中。生产者发送的消息，存储到一个partition 中，生产者也可以指定数据存储的 partition。</p><h2 id="1-1-创建生产者"><a href="#1-1-创建生产者" class="headerlink" title="1.1.创建生产者"></a>1.1.创建生产者</h2><p>ProductRecord 对象还可以指定键或分区。</p><p>在发送 ProductRecord 对象时，生产者要把键和值对象序列化为字节数组，这样才可以在网络上进行传输。接下来，数据被传给分区器。如果在 ProductRecord 对象里指定了分区，分区器直接将指定的分区返回。如果没有指定分区，分区器会根据 ProductRecord 对象的键选择一个分区。如果选定分区以后，生产者就知道向哪个主题和分区发送这条记录。<br>紧接着，这条记录被添加到一个记录批次里，这个批次里的所有消息被发送到相同主题和分区。(有一个独立的线程负责把这些记录批次发送到相应的 broker 上)</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Create a record with no key</span></span><br><span class="line"><span class="comment"> * </span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> topic The topic this record should be sent to</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> value The record contents</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">ProducerRecord</span><span class="params">(String topic, V value)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">this</span>(topic, <span class="keyword">null</span>, <span class="keyword">null</span>, <span class="keyword">null</span>, value, <span class="keyword">null</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>服务器在收到这些消息时会返回一个响应。如果消息成功写入 Kafka ，返回一个 RecordMetaData对象，包含了主题和分区信息，以及记录在分区的偏移量。如果写入失败，则会返回一个错误。生产者收到错误之后会尝试重新发送消息，几次之后如果还是失败，就返回错误信息。</p><img src="../images/kafka2.png" alt="" style="zoom:50%;" /><h2 id="2-创建生产者"><a href="#2-创建生产者" class="headerlink" title="2.创建生产者"></a>2.创建生产者</h2><p>向 Kafka 写入消息，首先要创建一个生产者对象，并设置一些属性。Kafka 生产者有3个必选的属性</p><ul><li><p>bootstrap.servers</p><p> 指定 broker 的地址清单</p> <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">properties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">&quot;master:9092,data1:9092,data2:9092,data3:9092,data4:9092,data5:9092&quot;</span>);</span><br></pre></td></tr></table></figure></li><li><p>key.serializer</p><p>broker 希望接受到的消息的键和值都是字节数组。生产者接口允许使用参数化类型。因此可以把 Java 对象作为键和值发送给 broker(这样代码具有良好的可读性)</p></li><li><p>value.serializer </p></li></ul><h1 id="3-拦截器"><a href="#3-拦截器" class="headerlink" title="3.拦截器"></a>3.拦截器</h1><h2 id="3-1-概述"><a href="#3-1-概述" class="headerlink" title="3.1.概述"></a>3.1.概述</h2><p>对于生产者而言，拦截器使用户在消息发送前以及 Producer 回调逻辑前有机会对消息做一些定制化需求，比如修改消息等。同时，生产者允许用户指定多个拦截器按序作用于同一条消息从而形成一个拦截链(interceptor chain)。<br>Intercetpor 的实现接口是org.apache.kafka.clients.producer.ProducerInterceptor，其定义的方法包括：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * A plugin interface that allows you to intercept (and possibly mutate) the records received by the producer before</span></span><br><span class="line"><span class="comment"> * they are published to the Kafka cluster.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">ProducerInterceptor</span>&lt;<span class="title">K</span>, <span class="title">V</span>&gt; <span class="keyword">extends</span> <span class="title">Configurable</span></span></span><br></pre></td></tr></table></figure><ul><li><p><strong>onSend(ProducerRecord)</strong></p><p>生产者确保消息被序列化以及计算分区前调用该方法。用户可以在该方法中对消息做任何操作，但最好保证不要修改消息所属的topic和分区，否则会影响目标分区的计算。</p></li><li><p><strong>onAcknowledgement(RecordMetadata, Exception)</strong></p><p>该方法会在消息被应答之前或消息发送失败时调用，并且通常都是在生产者回调逻辑触发之前。onAcknowledgement运行在生产者的IO线程中，因此不要在该方法中放入很重的逻辑，否则会拖慢生产者的消息发送效率</p></li></ul><ul><li><p><strong>close</strong></p><p>关闭interceptor，主要用于执行一些资源清理工作</p></li></ul><h2 id="3-2-拦截器实现"><a href="#3-2-拦截器实现" class="headerlink" title="3.2. 拦截器实现"></a>3.2. 拦截器实现</h2><p>实现一个简单的双 interceptor组成的拦截链。</p><ul><li>第一个interceptor会在消息发送前将时间戳信息加到消息前面；</li><li>第二个interceptor会在消息发送后更新成功发送消息数或失败发送消息数。</li></ul><h3 id="3-2-1-时间拦截器实现"><a href="#3-2-1-时间拦截器实现" class="headerlink" title="3.2.1.  时间拦截器实现"></a>3.2.1.  时间拦截器实现</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TimerInterceptor</span> <span class="keyword">implements</span> <span class="title">ProducerInterceptor</span>&lt;<span class="title">String</span>, <span class="title">String</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">configure</span><span class="params">(Map&lt;String, ?&gt; configs)</span> </span>&#123;&#125;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> ProducerRecord&lt;String, String&gt; <span class="title">onSend</span><span class="params">(ProducerRecord&lt;String, String&gt; record)</span> </span>&#123;</span><br><span class="line">        String value = System.currentTimeMillis() + <span class="string">&quot;---&quot;</span> + record.value();</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> ProducerRecord&lt;&gt;(record.topic(), record.partition(), record.key(), value);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onAcknowledgement</span><span class="params">(RecordMetadata metadata, Exception exception)</span> </span>&#123;&#125;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> </span>&#123;&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="3-2-2-计数拦截器实现"><a href="#3-2-2-计数拦截器实现" class="headerlink" title="3.2.2.  计数拦截器实现"></a>3.2.2.  计数拦截器实现</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.Map;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CountInterceptor</span> <span class="keyword">implements</span> <span class="title">ProducerInterceptor</span>&lt;<span class="title">String</span>, <span class="title">String</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">int</span> success;</span><br><span class="line">    <span class="keyword">int</span> error;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">configure</span><span class="params">(Map&lt;String, ?&gt; configs)</span> </span>&#123;&#125;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> ProducerRecord&lt;String, String&gt; <span class="title">onSend</span><span class="params">(ProducerRecord&lt;String, String&gt; record)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> record;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onAcknowledgement</span><span class="params">(RecordMetadata metadata, Exception exception)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(metadata != <span class="keyword">null</span>)&#123;</span><br><span class="line">            success ++;</span><br><span class="line">        &#125;<span class="keyword">else</span> &#123;</span><br><span class="line">            error++;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        System.out.println(<span class="string">&quot;success:    &quot;</span> + success);</span><br><span class="line">        System.out.println(<span class="string">&quot;error:    &quot;</span> + error);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="3-2-3-配置拦截器"><a href="#3-2-3-配置拦截器" class="headerlink" title="3.2.3.  配置拦截器"></a>3.2.3.  配置拦截器</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">ArrayList&lt;String&gt; interceptors = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">interceptors.add(<span class="string">&quot;api.interceptor.TimerInterceptor&quot;</span>);</span><br><span class="line">interceptors.add(<span class="string">&quot;api.interceptor.CountInterceptor&quot;</span>);</span><br><span class="line"></span><br><span class="line">properties.put(ProducerConfig.INTERCEPTOR_CLASSES_CONFIG, interceptors);</span><br></pre></td></tr></table></figure><h2 id="4-序列化器"><a href="#4-序列化器" class="headerlink" title="4. 序列化器"></a>4. 序列化器</h2><p>创建Kafka 生产者时必须指定序列化器。Kafka 除提供默认的字符串序列化器 org.apache.kafka.common.serialization.StringSerializer，还提供了整形和字节数组序列化器等。</p><h2 id="5-分区器"><a href="#5-分区器" class="headerlink" title="5.分区器"></a>5.分区器</h2><h3 id="5-1-分区原因"><a href="#5-1-分区原因" class="headerlink" title="5.1.分区原因"></a>5.1.分区原因</h3><ul><li>方便在集群中扩展，每个 Partition 可以通过调整以适应它所在的机器，而一个 topic 又可以存在多个Partition</li><li>提高并发。</li></ul><h3 id="5-2-分区原则"><a href="#5-2-分区原则" class="headerlink" title="5.2.分区原则"></a>5.2.分区原则</h3><p>kafka 中默认分区器为 <strong>org.apache.kafka.clients.producer.internals.DefaultPartitioner，</strong>其实现了 org.apache.kafka.clients.producer.Partitioner 接口。默认分区原则为:</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">* The <span class="keyword">default</span> partitioning strategy:</span><br><span class="line">* &lt;ul&gt;</span><br><span class="line">* &lt;li&gt;If a partition is specified in the record, use it</span><br><span class="line">* &lt;li&gt;If no partition is specified but a key is present choose a partition based on a hash of the key</span><br><span class="line">* &lt;li&gt;If no partition or key is present choose a partition in a round-robin fashion</span><br></pre></td></tr></table></figure><ol><li><p><strong>指明 partition 的情况下</strong></p><p>直接将指明的值直接作为 partition 的值</p></li><li><p><strong>没有指明 partition 值但是有 key</strong> </p><p>将 key 的 hash 值 与 topic 的 partition 数 进行取余 得到 partition 值</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// hash the keyBytes to choose a partition</span></span><br><span class="line"><span class="keyword">return</span> Utils.toPositive(Utils.murmur2(keyBytes)) % numPartitions;</span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">// 将 number 转换为正数。 </span></span><br><span class="line"><span class="comment">// 当 number 为正时，返回原始值。 </span></span><br><span class="line"><span class="comment">// 当 number 为负数时，返回原始值位与0x7fffffff的绝对值之和。 0x7FFFFFFF 的二进制表示就是除了首位是 0，其余都是1, </span></span><br><span class="line"><span class="comment">// 即最大的整型数 int</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">int</span> <span class="title">toPositive</span><span class="params">(<span class="keyword">int</span> number)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> number &amp; <span class="number">0x7fffffff</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p><strong>没有指定 partition 值又没有 key 值</strong></p><p><strong>round-robin</strong> 既第一次调用时随即生成一个整数(后面每次调用在这个整数上进行自增)，将这个值与 topic 可用的 partition 总数取余，得到 partition 值。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"> <span class="keyword">int</span> nextValue = nextValue(topic);</span><br><span class="line"><span class="comment">// broker 集群中 topic 主题可以利用的分区数</span></span><br><span class="line">List&lt;PartitionInfo&gt; availablePartitions = cluster.availablePartitionsForTopic(topic);</span><br><span class="line"><span class="keyword">if</span> (availablePartitions.size() &gt; <span class="number">0</span>) &#123;</span><br><span class="line">    <span class="keyword">int</span> part = Utils.toPositive(nextValue) % availablePartitions.size();</span><br><span class="line">    <span class="keyword">return</span> availablePartitions.get(part).partition();</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="comment">// no partitions are available, give a non-available partition</span></span><br><span class="line">    <span class="keyword">return</span> Utils.toPositive(nextValue) % numPartitions;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> ConcurrentMap&lt;String, AtomicInteger&gt; topicCounterMap = <span class="keyword">new</span> ConcurrentHashMap&lt;&gt;();</span><br><span class="line">  </span><br><span class="line"> <span class="function"><span class="keyword">private</span> <span class="keyword">int</span> <span class="title">nextValue</span><span class="params">(String topic)</span> </span>&#123;</span><br><span class="line">     AtomicInteger counter = topicCounterMap.get(topic);</span><br><span class="line">     <span class="comment">// 如果是第一次分区，随机生成一个数</span></span><br><span class="line">     <span class="keyword">if</span> (<span class="keyword">null</span> == counter) &#123;</span><br><span class="line">         counter = <span class="keyword">new</span> AtomicInteger(ThreadLocalRandom.current().nextInt());</span><br><span class="line">         AtomicInteger currentCounter = topicCounterMap.putIfAbsent(topic, counter);</span><br><span class="line">         <span class="keyword">if</span> (currentCounter != <span class="keyword">null</span>) &#123;</span><br><span class="line">             counter = currentCounter;</span><br><span class="line">         &#125;</span><br><span class="line">     &#125;</span><br><span class="line">     <span class="comment">// 不是第一次分区， 则++</span></span><br><span class="line">     <span class="keyword">return</span> counter.getAndIncrement();</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure></li></ol><h1 id="6-原理"><a href="#6-原理" class="headerlink" title="6.原理"></a>6.原理</h1><h2 id="6-1-整体架构"><a href="#6-1-整体架构" class="headerlink" title="6.1.整体架构"></a>6.1.整体架构</h2><p>在生产者将消息发往 Kafka之前，有可能需要经历拦截器、序列化器和分区器等一系列的作用，随后才真正进入消息发送流程。</p><img src="../images/kafka1.png" alt="" style="zoom:50%;" /><h3 id="6-1-1-消息累加器-RecordAccumulator"><a href="#6-1-1-消息累加器-RecordAccumulator" class="headerlink" title="6.1.1.消息累加器 RecordAccumulator"></a>6.1.1.消息累加器 RecordAccumulator</h3><p>整个生产者客户端由两个线程协调运行，这两个线程分别为 <strong>主线程</strong> 和 <strong>Sender线程</strong>[发送线程]。</p><p>在主线程中由 KafkaProducer 创建消息，然后通过可能的拦截器、序列化器和分区器的作用之后缓存到消息累加器 [RecordAccumulator，也称为消息收集器]。</p><p>Sender 线程负责从 RecordAccumulator 中获取消息并将其发送到 Kafka 中。</p><ol><li><p>RecordAccumulator 主要用来缓存消息，以便 Sender 线程可以批量发送，进而减少网络传输的资源消耗提升性能。</p></li><li><p>RecordAccumulator 缓存的大小可以通过生产者客户端参数 buffer.memory 的配置，默认值为32MB。如果生产者发送消息的速度超过发送到服务器的速度，则会导致生产者空间不足，这个时候KafkaProducer的send()方法调用要么被阻塞，要么抛出异常，这个取决于参数max.block.ms的配置，此参数的默认值为60000，即60秒。</p></li></ol><h3 id="6-1-2-ProducerBatch"><a href="#6-1-2-ProducerBatch" class="headerlink" title="6.1.2.ProducerBatch"></a>6.1.2.ProducerBatch</h3><p><strong>在 RecordAccumulator 的内部为每个分区都维护了一个双端队列</strong></p><p>主线程中发送过来的消息都会被追回到 RecordAccumulator 的某个双端队列 [Deque]中，</p><p>队列中的内容就是 ProducerBatch,即 Deque<ProducerBatch>。</p><p>消息写入缓存时，追回到双端队列的尾部；Sender 读取消息时，从双端队列的头部读取。</p><ul><li><p><strong><font color='red'>注意</font></strong></p><ol><li><p><strong>ProducerBatch 不是 ProducerRecord</strong></p><blockquote><p>ProducerBatch 是指一个消息批次，ProducerRecord 会被包含在 ProducerBatch 中，这样可以使字节的使用更加紧凑。与此同时，将较小的 ProducerRecord 拼凑成一个较大的ProducerBatch 可以减少网络请求的次数以提升整体的吞量。如果生产者客户端需要向很多分区发送消息，则可以将 buffer.memory 参数适当调大以增加整体的吞吐量。</p></blockquote></li><li><p><strong>ProducerBatch 的大小和 batch.size 参数有着密切的关系。</strong></p><blockquote><p>当一条消息 [ProducerRecord] 流入 RecordAccumulator 时，会先寻找与消息分区所对应的双端队列,如果没有则创建，再从这个双端队列的尾部获取一个 ProducerBatch[如果没有则创建]，查看 ProducerBatch 中是否还可以写入这个 ProdcucerRecord，如果可以则写入，如果不可以则需要创建一个新的 ProducerBatch。</p></blockquote><blockquote><p>在新建 ProducerBatch 时评估这条消息的大小是否超过 batch.size 参数的大小，如果不超过，那么就以 batch.size 参数的大小来创建 ProducerBatch，这样在使用完这段内存区域后，可以通过 BufferPool 的管理来进行复用；如果超过，那么就以评估的大小创建ProducerBatch，这段内存区域不会被复用。</p></blockquote></li></ol></li></ul><h3 id="6-1-3-BufferPool"><a href="#6-1-3-BufferPool" class="headerlink" title="6.1.3. BufferPool"></a>6.1.3. BufferPool</h3><p>消息在网络上都是以字节[Byte]的形式传输的，在发送之前需要创建一块内存区域来保存对应的消息。在Kafka生产者客户端中，通过 java.io.ByteBuffer 实现消息内存的创建和释放。不过频繁的创建和释放是比较耗费资源的，在RecordAccumulator的内部还有一个BufferPool，它主要是用来实现ByteBuffer的复用，以实现缓存的高效利用。</p><p>不过 BufferPool 只针对特定大小的 ByteBuffer 进行管理，而其它大小的 ByteBuffer 不会缓存进BufferPool 中，这个特定的大小由 batch.size 参数指定，默认值为64KB，可以适当地调大batch.size 参数以便多缓存一些消息。</p><p>Sender 从 RecordAccumulator 中获取缓存的消息之后，会进一步将原本<code>&lt;分区，Deque&lt;ProducerBatch&gt;&gt;</code>的保存形式转变成&lt;Node, List<ProducerBatch>&gt;的形式，其中Node表示Kafka集群的broker节点。对于网络连接来说，生产者客户端是与具体的broker节点建立的连接，就是向具体的broker节点发送消息，而并不关心消息属于哪一个分区；而对于KafkaProducer的应用逻辑而言，我们只关注向哪个分区中发送哪些消息，所以这里需要做一个应用逻辑层面到网络I/O层面的转换。</p><p>在转换成&lt;Node, List<ProducerBatch>&gt;的形式之后，Sender还会进一步封装成&lt;Node, Request&gt;的形式，这样就可以将Request请求发往各个Node了，这里的Request是指Kafka的各种协议请求，对于消息发送而言就是指具体的ProducerRequest。</p><h3 id="6-1-4-InFlightRequest"><a href="#6-1-4-InFlightRequest" class="headerlink" title="6.1.4. InFlightRequest"></a>6.1.4. InFlightRequest</h3><p>请求在从 Sender 线程发往 Kafka 之前还会保存到 InFlightRequests 中，InFlightRequest 保存对象的具体形式为Map&lt;NodeId, Deque<Request>&gt;</p><p>它的主要作用是缓存已经发出去但还没有收到响应的请求[NodeId是一个 String 类型，表示节点的id编号]。</p><h1 id="7-总结"><a href="#7-总结" class="headerlink" title="7.总结"></a>7.总结</h1><h5 id="Kafka-生产者如何保证不丢失，不重复？"><a href="#Kafka-生产者如何保证不丢失，不重复？" class="headerlink" title="Kafka 生产者如何保证不丢失，不重复？"></a><font color='blue'>Kafka 生产者如何保证不丢失，不重复？</font></h5><p>生产者丢数据，即发送的数据根本没有保存到 Broker 端。出现这个情况的原因可能是，网络抖动，导致消息压根就没有发送到 Broker 端；也可能是消息本身不合格导致 Broker 拒绝接收（比如消息太大了，超过了 Broker 的承受能力）等等。</p><p>上面所说比如网络原因导致消息没有成功发送到 broker 端，常见，也并不可怕。可怕的不是没发送成功，而是发送失败了你不做任何处理。</p><p>很简单的一个<strong>重试配置</strong>，基本就可以解决这种网络瞬时抖动问题。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">props.put(<span class="string">&quot;retries&quot;</span>, <span class="number">10</span>);</span><br></pre></td></tr></table></figure><p>当然还有很多其他原因导致的，不能只依靠 kafka 的配置来做处理，我们看一下 kafka 发送端的源码，其实人家是提供了两个方法的，通常会出问题的方法是那个简单的 send，没有 callback（回调）。简单的 send发送后不会去管它的结果是否成功，而 callback 能准确地告诉你消息是否真的提交成功了。一旦出现消息提交失败的情况，你就可以有针对性地进行处理。</p><p><font color='red'><strong>因此，一定要使用带有回调通知的 send 方法。</strong></font></p><p>我们知道，broker 一般不会有一个，我们就是要通过多 Broker 达到高可用的效果，所以对于生产者程序来说，也不能简单的认为发送到一台就算成功，如果只满足于一台，那台机器如果损坏了，那消息必然会丢失。设置 <strong>acks = all</strong>，表明所有副本 Broker 都要接收到消息，该消息才算是“已提交”，这样可以达到高可用的效果。</p><p>考虑到 producer,broker,consumer 之间都有可能造成消息重复，所以我们要求接收端需要支持消息去重的功能，最好借助业务消息本身的幂等性来做。其中有些大数据组件，如 hbase，elasticsearch 天然就支持幂等操作。<br>举例：<br>在华泰证券中Kafka的幂等性是如何保证的？在接收端，启动专门的消费者拉取 kafka 数据存入 hbase。hbase 的 rowkey 的设计主要包括 SecurityId（股票id）和 timestamp（行情数据时间）。消费线程从 kafka 拉取数据后反序列化，然后批量插入 hbase，只有插入成功后才往 kafka 中持久化 offset。这样的好处是，如果在中间任意一个阶段发生报错，程序恢复后都会从上一次持久化 offset 的位置开始消费数据，而不会造成数据丢失。如果中途有重复消费的数据，则插入 hbase 的 rowkey 是相同的，数据只会覆盖不会重复，最终达到数据一致。</p><p><strong>在0.11之前主要是通过下游系统具有幂等性来保证 Exactly Once。但是这样有几个缺陷：</strong></p><blockquote><p>要求下游系统支持幂等操作，限制了Kafka的适用场景</p></blockquote><blockquote><p>实现门槛相对较高，需要用户对Kafka的工作机制非常了解</p></blockquote><blockquote><p>对于Kafka Stream而言，Kafka Producer本身就是“下游”系统，能让Producer具有幂等处理特性，那就可以让Kafka Stream在一定程度上支持Exactly once语义。</p></blockquote><p><strong>0.11之后的版本，引入了 <code>Producer ID（PID）</code>和 <code>Sequence Number</code> 实现 <code>Producer </code>的幂等语义。</strong></p><blockquote><p>Producer ID：每个新的Producer在初始化的时候会被分配一个唯一的PID</p></blockquote><blockquote><p>Sequence Number：对于每个PID，该Producer发送数据的每个&lt;Topic, Partition&gt;都对应一个从0开始单调递增的Sequence Number。</p></blockquote><p>Broker端也会为每个&lt;PID, Topic, Partition&gt;维护一个序号，并且每次Commit一条消息时将其对应序号递增。对于接收的每条消息，如果其序号比Broker维护的序号（即最后一次Commit的消息的序号）大一，则Broker会接受它，否则将其丢弃：</p><blockquote><p>如果消息序号比Broker维护的序号大一以上，说明中间有数据尚未写入，也即乱序，此时Broker拒绝该消息，Producer 抛出InvalidSequenceNumber</p></blockquote><blockquote><p>如果消息序号小于等于Broker维护的序号，说明该消息已被保存，即为重复消息，Broker直接丢弃该消息，Producer 抛出 DuplicateSequenceNumber</p></blockquote><p>这种机制很好的解决了数据重复和数据乱序的问题。</p>]]></content>
      
      
      <categories>
          
          <category> Kafka </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kafka </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
