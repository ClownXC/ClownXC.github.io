<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title></title>
      <link href="2021/01/20/Flink%E7%AE%97%E5%AD%90(Operator)%E7%AE%80%E4%BB%8B/"/>
      <url>2021/01/20/Flink%E7%AE%97%E5%AD%90(Operator)%E7%AE%80%E4%BB%8B/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><h2 id="数据转换算子简介"><a href="#数据转换算子简介" class="headerlink" title="数据转换算子简介"></a>数据转换算子简介</h2><p>在 Flink 应用程序中，无论你的应用程序是批程序，还是流程序，都是上图这种模型，有数据源（source），有数据下游（sink）</p><ol><li><p><strong>Source</strong>:</p><p>数据源，Flink 在流处理和批处理上的 source 大概有 4 类</p><ul><li><p>基于本地集合的 source</p></li><li><p>基于文件的 source</p></li><li><p>基于网络套接字的 source</p></li><li><p>自定义的 source</p></li></ul></li><li><p><strong>Transformation</strong></p><p>数据转换的各种操作，有 Map / FlatMap / Filter / KeyBy / Reduce / Fold / Aggregations / Window / WindowAll / Union / Window join / Split / Select / Project 等，操作很多。</p></li><li><p><strong>Sink</strong>: 接收器，Sink 是指 Flink 将转换计算后的数据发送的地点。Flink 常见的 Sink 大概有如下几类：写入文件、打印出来、写入 Socket 、自定义的 Sink 。</p><p>自定义的 sink 常见的有 Apache kafka、RabbitMQ、MySQL、ElasticSearch、Hadoop FileSystem 等，同理也可以定义自己的 Sink。</p></li></ol><h1 id="常用算子"><a href="#常用算子" class="headerlink" title="常用算子"></a>常用算子</h1><h4 id="Map"><a href="#Map" class="headerlink" title="Map"></a>Map</h4><p>Map 算子的输入流是 DataStream，经过 Map 算子后返回的数据格式是 SingleOutputStreamOperator 类型，获取一个元素并生成一个元素</p><h4 id="FlatMap"><a href="#FlatMap" class="headerlink" title="FlatMap"></a>FlatMap</h4><p>FlatMap 算子的输入流是 DataStream，经过 FlatMap 算子后返回的数据格式是 SingleOutputStreamOperator 类型，获取一个元素并生成零个、一个或多个元素</p><h4 id="Filter"><a href="#Filter" class="headerlink" title="Filter"></a>Filter</h4><p>对每个元素都进行判断，返回为 true 的元素，如果为 false 则丢弃数据</p><h4 id="KeyBy"><a href="#KeyBy" class="headerlink" title="KeyBy"></a>KeyBy</h4><p>KeyBy 在逻辑上是基于 key 对流进行分区，相同的 Key 会被分到一个分区（这里分区指的就是下游算子多个并行节点的其中一个）。在内部，它使用 hash 函数对流进行分区。它返回 KeyedDataStream 数据流</p><img src="/Users/joker/Documents/chen_blog/images/截屏2021-01-20 上午9.17.14.png" alt="截屏2021-01-20 上午9.17.14" style="zoom:50%;" /><h4 id="Reduce"><a href="#Reduce" class="headerlink" title="Reduce"></a>Reduce</h4><p>Reduce 返回单个的结果值，并且 reduce 操作每处理一个元素总是创建一个新值。常用的方法有 average、sum、min、max、count</p><h4 id="Aggregations"><a href="#Aggregations" class="headerlink" title="Aggregations"></a>Aggregations</h4><p>DataStream API 支持各种聚合，例如 min、max、sum 等。 这些函数可以应用于 KeyedStream 以获得 Aggregations 聚合</p><h4 id="Union"><a href="#Union" class="headerlink" title="Union"></a>Union</h4><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-06-30-061732.jpg" alt="img"></p><p>Union 函数将两个或多个数据流结合在一起。 这样后面在使用的时候就只需使用一个数据流就行了。 如果我们将一个流与自身组合，那么组合后的数据流会有两份同样的数据。</p><h4 id="Select"><a href="#Select" class="headerlink" title="Select"></a>Select</h4><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-06-30-062535.jpg" alt="img"></p><p>上面用 Split 算子将数据流拆分成两个数据流（奇数、偶数），接下来你可能想从拆分流中选择特定流，那么就得搭配使用 Select 算子（一般这两者都是搭配在一起使用的）</p><h4 id="Split"><a href="#Split" class="headerlink" title="Split"></a>Split</h4><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-06-30-062027.jpg" alt="img"></p><p>此功能根据条件将流拆分为两个或多个流。 当你获得混合流然后你可能希望单独处理每个数据流时，可以使用此方法</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="2021/01/12/%E6%97%A5%E5%BF%97%E6%AE%B5%EF%BC%9A%E4%BF%9D%E5%AD%98%E6%B6%88%E6%81%AF%E5%AF%B9%E8%B1%A1%E5%AE%9E%E7%8E%B0/"/>
      <url>2021/01/12/%E6%97%A5%E5%BF%97%E6%AE%B5%EF%BC%9A%E4%BF%9D%E5%AD%98%E6%B6%88%E6%81%AF%E5%AF%B9%E8%B1%A1%E5%AE%9E%E7%8E%B0/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="2021/01/06/%E5%BA%94%E7%94%A8%E5%B1%82/"/>
      <url>2021/01/06/%E5%BA%94%E7%94%A8%E5%B1%82/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p>应用层的任务是通过应用进程间的交互来完成特定网络应用。</p><p>应用层协议定义的是应用进程间的通信和交互的规则。对于不同的网络应用需要不同的应用层协议。把应用层交互的数据单元称为报文。</p><h2 id="2-域名系统-DNS"><a href="#2-域名系统-DNS" class="headerlink" title="2. 域名系统 DNS"></a>2. 域名系统 DNS</h2><p>用户在与互联网中的主机通信时，必须知道对方的 IP 地址，但是32位 IP 地址太长不便于，即使使用点分十进制的方式也不容易记忆，所以实际中都是使用域名来与对方通信，如访问百度可以直接输入 <a href="https://links.jianshu.com/go?to=http://www.baidu.com">www.baidu.com</a> 而不用使用百度服务器的IP地址。域名系统DNS就是把域名转换为 IP 地址。</p><h3 id="2-1-域名"><a href="#2-1-域名" class="headerlink" title="2.1.域名"></a>2.1.域名</h3><p>任何一个连接在互联网上的主机或路由器都有一个<strong>唯一的层次结构</strong>的名字，即<strong>域名</strong>。每个域名都由<strong>标号</strong>序列组成，而各标号之间<strong>用点隔开</strong>。</p><p>![屏幕快照 2020-03-30 上午11.10.15](/Users/zxc/Documents/big_data/计算机网络/应用层.assets/屏幕快照 2020-03-30 上午11.10.15.png)</p><p>它由三个标号组成，其中 com 是顶级域名，baidu 是二级域名，www 是三级域名。域名其实还有一个<strong>根</strong>，不过根没有名字，根下面才是顶级域名。</p><p>DNS 规定，标号都是由英文字母和数字组成，<strong>每一个标号不超过63个字符</strong>，也不区分大小写。标号中除了字符**(-)<strong>外不能使用其他标点符号。级别最低的域名写在最左边，而级别最高的顶级域名写在最右边。</strong>由多个标号组成的完整域名总共不超过255个字符**。</p><h4 id="2-2-域名服务器"><a href="#2-2-域名服务器" class="headerlink" title="2.2.域名服务器"></a>2.2.域名服务器</h4><p>一个服务器所负责的管辖的范围叫做<strong>区</strong>。各单位根据具体情况来划分自己管辖范围的<strong>区</strong>。但在一个区中的所有节点必须是能够连通的。每个区设置相应的<strong>权限域名服务器</strong>，用来保存该区中的所有主机域名到IP地址的映射。</p><img src="/Users/zxc/Documents/big_data/计算机网络/应用层.assets/屏幕快照 2020-03-30 上午11.13.38.png" alt="屏幕快照 2020-03-30 上午11.13.38" style="zoom:80%;" /><p><strong>本地域名服务器</strong>（local name server）</p><p>又称<strong>默认域名服务器</strong>，本地域名服务器不属于上图的域名服务器层次结构，但它对域名系统非常重要。<strong>当一台主机发出 DNS 查询请求时，这个查询报文就发给本地域名服务器</strong>。</p><h4 id="2-4-域名解析过程"><a href="#2-4-域名解析过程" class="headerlink" title="2.4. 域名解析过程"></a>2.4. 域名解析过程</h4><ol><li><p><strong>主机向本地域名服务器</strong>的查询一般都是采用<strong>递归查询</strong>（recursive query）。所谓的递归查询就是：如果主机所询问的本地域名服务器不知道被查询域名的 IP 地址，那么本地域名服务器就以 DNS 客户的身份，向其他根域名服务器继续发出查询请求报文（即替该主机继续查询），而不是让该主机自己进行下一步查询。因此，递归查询返回的查询结果或者所要查询的 IP 地址，或者是报错，表示无法查询到所需的 IP 地址。</p></li><li><p><strong>本地域名服务器向根域名服务器</strong>的查询通常都是<strong>迭代查询</strong>（interative query）。迭代查询的特点是：当根域名服务器收到本地域名服务器发出的迭代查询请求报文时，要么给出所要查询的 IP 地址，要么告诉本地域名服务器下一步需要查询的域名服务器，然后让本地服务器进行后续的查询而不是替本地域名服务器进行后续的查询。</p><p>根域名服务器通常是把自己知道的顶级域名服务器 IP 地址告诉本地域名服务器，让本地域名服务器再向对应的顶级域名服务器查询。顶级域名服务器在收到本地域名服务器的查询请求后，要么给出所要查询的 IP 地址，要么告诉本地域名服务器下一步应当向哪个权限域名服务器进行查询，本地域名服务器就这样进行迭代查询。最后，知道了所要解析的域名的 IP 地址吗，然后把这个结果返回给发起查询的主机。当然，本地域名服务器也可以采用递归查询，这取决于最初的查询请求报文的设置是要求使用哪一种查询方式。</p></li></ol>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="2021/01/05/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95%E4%B9%8B%E5%BF%AB%E9%80%9F%E6%8E%92%E5%BA%8F/"/>
      <url>2021/01/05/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95%E4%B9%8B%E5%BF%AB%E9%80%9F%E6%8E%92%E5%BA%8F/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><h1 id="排序算法-快速排序"><a href="#排序算法-快速排序" class="headerlink" title="排序算法: 快速排序"></a>排序算法: 快速排序</h1><p>从序列中选择一个轴点元素</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="2020/10/09/HDFS%E6%93%A6%E9%99%A4%E7%BC%96%E7%A0%81/"/>
      <url>2020/10/09/HDFS%E6%93%A6%E9%99%A4%E7%BC%96%E7%A0%81/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><h1 id="1-目的"><a href="#1-目的" class="headerlink" title="1.目的"></a>1.目的</h1><p>复制很昂贵 HDFS 中的默认3x复制方案在存储空间和其他资源（例如，网络带宽）中具有200％的开销。但是，对于I/O活动相对较低的暖和冷数据集，在正常操作期间很少访问其他块副本，但仍会消耗与第一个副本相同的资源量。</p><p>因此，自然的改进是使用擦除编码（EC）代替复制，其提供相同级别的容错并且具有更少的存储空间。在典型的擦除编码（EC）设置中，存储开销不超过50％。EC文件的复制因子没有意义。它始终为1，无法通过-setrep命令进行更改。</p><p>在存储系统中，EC最值得注意的用途是廉价磁盘冗余阵列（RAID）。RAID通过条带化实现EC，条带化将逻辑顺序数据（例如文件）划分为更小的单元（例如位，字节或块），并将连续单元存储在不同的磁盘上。在本指南的其余部分中，这个条带分布单元称为条带化单元（或单元）。对于每个原始数据单元条带，计算并存储一定数量的奇偶校验单元 - 其过程称为编码。可以通过基于幸存数据和奇偶校验单元的解码计算来恢复任何条带化单元上的错误。</p><p>将EC与HDFS集成可以提高存储效率，同时仍然提供与传统的基于复制的HDFS部署类似的数据持久性。例如，具有6个块的3x复制文件将消耗6 * 3 = 18个磁盘空间块。但是使用EC（6数据，3奇偶校验）部署，它将只消耗9块磁盘空间。</p><h1 id="2-架构"><a href="#2-架构" class="headerlink" title="2.架构"></a>2.架构</h1><p>在 EC 的背景下，条带化具有几个关键优势。首先，它启用在线EC（以EC格式立即写入数据），避免转换阶段并立即节省存储空间。在线EC还通过并行利用多个磁盘轴来增强顺序I / O性能; 这在具有高端网络的集群中尤其可取。其次，它自然地将一个小文件分发到多个DataNode，并且无需将多个文件捆绑到一个编码组中。这极大地简化了文件操作，例如删除，配额报告和联合命名空间之间的迁移。</p><p>在典型的HDFS群集中，小文件占总存储消耗的3/4以上。为了更好地支持小文件，在第一阶段的工作中，HDFS支持使用条带化的EC。在未来，HDFS还将支持连续的EC布局。有关详细信息，请参阅设计文档和有关 <a href="https://issues.apache.org/jira/browse/HDFS-7285">HDFS-7285 的</a>讨论。</p><h2 id="部署"><a href="#部署" class="headerlink" title="部署"></a>部署</h2><h3 id="群集和硬件配置"><a href="#群集和硬件配置" class="headerlink" title="群集和硬件配置"></a><strong>群集和硬件配置</strong></h3><p>擦除编码在 CPU 和网络方面对集群提出了额外的要求。</p><p>编码和解码工作在 HDFS 客户端和 DataNode 上消耗额外的 CPU。</p><p>擦除编码要求群集中的数据节点数与配置的 EC 条带宽度相同。对于 EC 策略 RS（6,3），这意味着至少9个DataNode。</p><p><strong><em>*</em>*擦除编码文件也分布在机架上，用于机架容错。这意味着在读取和写入条带文件时，大多数操作都是在机架外。因此，网络二分带宽非常重要。*\</strong>***</p><p><strong><em>*</em>*对于机架容错，拥有足够数量的机架也很重要，因此平均而言，每个机架保持的块数不超过EC奇偶校验块的数量。计算它的公式将是（数据块+奇偶校验块）/奇偶校验块，向上舍入。对于EC政策RS（6,3），这意味着最少3个机架（由（6 + 3）/ 3 = 3计算），理想情况下9个或更多来处理计划内和计划外中断。对于机架数量少于奇偶校验单元数量的群集，HDFS无法保持机架容错，但仍会尝试跨多个节点分布条带文件以保留节点级别的容错。因此，建议设置具有相似数量的DataNode的机架。*\</strong>***</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="2020/10/09/Hadoop2.x%E4%B8%8EHadoop3.x/"/>
      <url>2020/10/09/Hadoop2.x%E4%B8%8EHadoop3.x/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><h1 id="1-存储-Scheme"><a href="#1-存储-Scheme" class="headerlink" title="1.存储 Scheme"></a>1.<strong>存储 Scheme</strong></h1>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="2020/09/14/%E6%95%B0%E6%8D%AE%E5%B9%B3%E5%8F%B0%E6%9E%B6%E6%9E%84-Kappa/"/>
      <url>2020/09/14/%E6%95%B0%E6%8D%AE%E5%B9%B3%E5%8F%B0%E6%9E%B6%E6%9E%84-Kappa/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p>虽然 Lambda 架构使用起来十分灵活，并且可以适用于很多的应用场景，但在实际应用的时候，Lambda 架构也存在着一些不足，主要表现在它的维护很复杂。使用 Lambda 架构时，架构师需要维护两个复杂的分布式系统，并且保证他们逻辑上产生相同的结果输出到服务层中。</p><h1 id="1-Kappa-架构"><a href="#1-Kappa-架构" class="headerlink" title="1. Kappa 架构"></a>1. Kappa 架构</h1><p>Kappa 架构是由 LinkedIn 的前首席工程师杰伊·克雷普斯（Jay Kreps）提出的一种架构思想。</p><p>像 Apache Kafka 这样的流处理平台是具有永久保存数据日志的功能的，通过平台的这一特性，我们可以重新处理部署于速度层架构中的历史数据。</p><p>与 Lambda 架构不同的是，Kappa 架构去掉了批处理层这一体系结构，而只保留了速度层。你只需要在业务逻辑改变又或者是代码更改的时候进行数据的重新处理。</p><p>Kappa 架构的核心思想包括以下三点：</p><ol><li>用 Kafka 或者类似的分布式队列系统保存数据，你需要几天的数据量就保存几天。</li><li>当需要全量重新计算时，重新起一个流计算实例，从头开始读取数据进行处理，并输出到一个新的结果存储中。</li><li>当新的实例做完后，停止老的流计算实例，并把老的一些结果删除。</li></ol><p>和 Lambda 架构相比，在 Kappa 架构下，只有在有必要的时候才会对历史数据进行重复计算，并且实时计算和批处理过程使用的是同一份代码。或许有些人会质疑流式处理对于历史数据的高吞吐量会力不从心，但是这可以通过控制新实例的并发数进行改善。</p><p>上面架构图中，新老实例使用了各自的结果存储，这便于随时进行回滚，更进一步，假如我们产出的是一些算法模型之类的数据，用户还可以同时对新老两份数据进行效果验证，做一些A/B test或者使用bandit算法来最大限度的使用这些数据。</p><h1 id="2-Kappa-选型"><a href="#2-Kappa-选型" class="headerlink" title="2. Kappa 选型"></a>2. Kappa 选型</h1><h1 id="3-Kappa-实现"><a href="#3-Kappa-实现" class="headerlink" title="3. Kappa 实现"></a>3. Kappa 实现</h1><p>Apache Kafka 这样的流处理平台是具有永久保存数据日志的功能的。通过平台这一特性，我们可以重新处理部署速度层架构的历史数据了。</p><img src="/Users/joker/Library/Application Support/typora-user-images/截屏2020-09-15 上午9.31.26.png" alt="截屏2020-09-15 上午9.31.26" style="zoom:50%;" /><ol><li><p>第一步。部署 Apache Kafka，并设置数据日志的保留期。这里的保留期指的是希望能够重新处理的历史数据的时间区间。</p><p>例如，你希望重新处理最多一年的历史数据，那就可以把 Apache Kafka 中的保留期设置为 365 天。如果你希望能够处理所有的历史数据，那就可以把 Apache Kafka 中的保留期设置成“永久”。</p></li><li><p>第二步。如果我们需要改进现有的逻辑算法，那就表示需要对历史数据进行重新处理。</p><p>重新启动一个 Apache Kafka 作业实例。这个作业实例将重头开始，重新计算保留好的历史数据，并将结果输出到一个新的数据视图中，Apache Kafka 的底层是使用 Log Offset 来判断现在已经处理到哪个数据块了，所以只需要把 Log Offset 参数设置成0，新的作业实例就会重头开始处理历史数据。</p></li><li><p>第三步。当这个新的数据视图处理过的数据进度赶上了旧的数据视图时，我们的应用便可以切换到新的数据视图中读取。</p></li><li><p>第四步。停止旧版本的作业实例，并删除旧的数据视图。</p></li></ol><h1 id="4-Kappa-总结"><a href="#4-Kappa-总结" class="headerlink" title="4. Kappa 总结"></a>4. Kappa 总结</h1><p>因为 Kappa 架构只保留了速度层而缺少批处理层，在速度层上处理大规模数据可能会有数据更新出错的情况发生，这就需要花费更多的时间在处理这些错误异常上面。</p><p>还有一点，Kappa 架构的批处理和流处理都放在了速度层上，这导致了这种架构是使用同一套代码来处理算法逻辑的。所以 Kappa 架构并不适用于批处理和流处理代码逻辑不一致的场景。</p><p>1、消息中间件缓存的数据量和回溯数据有性能瓶颈。通常算法需要过去180天的数据，如果都存在消息中间件，无疑有非常大的压力。同时，一次性回溯订正 180 天级别的数据，对实时计算的资源消耗也非常大。</p><p>2、在实时数据处理时，遇到大量不同的实时流进行关联时，非常依赖实时计算系统的能力，很可能因为数据流先后顺序问题，导致数据丢失。</p><p>如果你所面对的业务逻辑是设计一种稳健的机器学习模型来预测即将发生的事情，那么你应该优先考虑使用 Lambda 架构，因为它拥有批处理层和速度层来确保更少的错误。</p><p>如果你所面对的业务逻辑是希望实时性比较高，而且客户端又是根据运行时发生的实时事件来做出回应的，那么你就应该优先考虑使用 Kappa 架构。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="2020/09/14/%E6%95%B0%E6%8D%AE%E5%B9%B3%E5%8F%B0%E6%9E%B6%E6%9E%84-Lambda/"/>
      <url>2020/09/14/%E6%95%B0%E6%8D%AE%E5%B9%B3%E5%8F%B0%E6%9E%B6%E6%9E%84-Lambda/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><h1 id="1-Lambda-架构"><a href="#1-Lambda-架构" class="headerlink" title="1. Lambda 架构"></a>1. Lambda 架构</h1><h2 id="1-1-简介"><a href="#1-1-简介" class="headerlink" title="1.1. 简介"></a>1.1. 简介</h2><p>Lambda 架构是由 Storm 的作者 Nathan Marz 提出的一个实时大数据处理框架。Lambda 架构是其根据多年进行分布式大数据系统的经验总结提炼而成。Lambda 架构的目标是设计出一个能满足实时大数据系统关键特性的架构，包括有：高容错、低延时和可扩展等。Lambda 架构整合离线计算和实时计算，融合不可变性（Immunability），读写分离和复杂性隔离等一系列架构原则，可集成 Hadoop，Kafka，Storm，Spark，Hbase 等各类大数据组件。</p><p>数据从底层的数据源开始，经过各种各样的格式进入大数据平台，在大数据平台中经过Kafka、Flume等数据组件进行收集，然后分成两条线进行计算。一条线是进入流式计算平台（例如 Storm、Flink或者Spark Streaming），去计算实时的一些指标；另一条线进入批量数据处理离线计算平台（例如Mapreduce、Hive，Spark SQL），去计算T+1的相关业务指标，这些指标需要隔日才能看见。</p><h2 id="1-2-三层架构"><a href="#1-2-三层架构" class="headerlink" title="1.2. 三层架构"></a>1.2. 三层架构</h2><img src="/Users/joker/Library/Application Support/typora-user-images/截屏2020-09-15 上午9.36.06.png" alt="截屏2020-09-15 上午9.36.06" style="zoom:40%;" /><h3 id="1-2-1-Batch-Layer-批处理层"><a href="#1-2-1-Batch-Layer-批处理层" class="headerlink" title="1.2.1. Batch Layer(批处理层)"></a>1.2.1. Batch Layer(批处理层)</h3><p>在全体数据集上在线运行查询函数得到结果的代价太大，同时处理查询时间过长，导致用户体验不好。如果预先在数据集上计算并保存预计算的结果，查询的时候直接返回预计算的结果，而无需重新进行复制耗时的计算。</p><p>batchview 是一个批处理过程，如采用 Hadoop 或 spark 支持的 map－reduce 方式。采用这种方式计算得到的每个view 都支持再次计算，且每次计算的结果都相同。</p><p>批处理层使用可处理大量数据的分布式处理系统预先计算结果。它通过处理所有的已有历史数据来实现数据的准确性。这意味着它是基于完整的数据集来重新计算的，能够修复任何错误，然后更新现有的数据视图。输出通常存储在只读数据库中，更新则完全取代现有的预先计算好的视图。</p><p>Batch Layer 的功能主要有两点：</p><ol><li>存储数据集</li><li>在数据集上预先计算查询函数，构建查询所对应的 View</li></ol><h3 id="1-2-2-Speed-Layer-速度处理层"><a href="#1-2-2-Speed-Layer-速度处理层" class="headerlink" title="1.2.2. Speed Layer(速度处理层)"></a>1.2.2. Speed Layer(速度处理层)</h3><p>Batch Layer可以很好的处理离线数据，但有很多场景数据不断实时生成，且需要实时查询处理。Speed Layer是用来处理增量的实时数据。</p><p>SpeedLayer 和 BatchLayer 比较类似，对数据进行计算并生成 RealtimeView，其主要的区别在于</p><ol><li>SpeedLayer 处理的数据是最近的增量数据流，BatchLayer 处理的是全体数据集</li><li>Speed<strong>Layer **为了效率，接收到新数据及时更新 Realtime</strong>View<strong>，而 Batch</strong>Layer** 根据全体离线数据直接得到Batch<strong>View</strong>。Speed**Layer **是一种增量计算，而非重新计算（recomputation）。</li><li>Speed<strong>Layer</strong>因为采用增量计算，所以延迟小，而 Batch<strong>Layer</strong> 是全数据集的计算，耗时比较长。</li></ol><p>速度层通过提供最新数据的实时视图来最小化延迟。速度层所生成的数据视图可能不如批处理层最终生成的视图那样准确或完整，但它们几乎在收到数据后立即可用。而当同样的数据在批处理层处理完成后，在速度层的数据就可以被替代掉了。</p><h3 id="1-2-3-Serving-Layer"><a href="#1-2-3-Serving-Layer" class="headerlink" title="1.2.3. Serving Layer"></a>1.2.3. Serving Layer</h3><p>Batch<strong>Layer</strong> 通过对 Master<strong>Dataset **执行查询获得 Batch</strong>View<strong>，</strong>Speed Layer <strong>通过增量计算提供 Realtime</strong>View<strong>。Lambda 架构的 Serving</strong>Layer <strong>用于响应用户的查询请求，合并 BatchView 和 Realtime View 中的结果数据集到最终的数据集。因此，Serving</strong>Layer **的职责包含：</p><ol><li>对 batch<strong>View **和 RealTime</strong>View **的随机访问</li><li>更新 Batch<strong>Veiw **和 RealTime</strong>View**，并负责结合两者的数据，对用户提供统一的接口</li></ol><h2 id="1-3-流程"><a href="#1-3-流程" class="headerlink" title="1.3. 流程"></a>1.3. 流程</h2><p>数据流进入系统后，同时发往 Batch Layer 和 Speed Layer 处理。Batch Layer 以不可变模型离线存储所有数据集，通过在全体数据集上不断重新计算构建查询所对应的 Batch Views。Speed Layer 处理增量的实时数据流，不断更新查询所对应的 Realtime Views。Serving Layer 响应用户的查询请求，合并 Batch View 和 Realtime View 中的结果数据集到最终的数据集。</p><h2 id="1-4-资料"><a href="#1-4-资料" class="headerlink" title="1.4. 资料"></a>1.4. 资料</h2><p>例如广告投放预测这种推荐系统一般都会用到 Lambda 架构。一般能做精准广告投放的公司都会拥有海量用户特征、用户历史浏览记录和网页类型分类这些历史数据的。业界比较流行的做法有在批处理层用 Alternating Least Squares (ALS) 算法，也就是 Collaborative Filtering 协同过滤算法，可以得出与用户特性一致其他用户感兴趣的广告类型，也可以得出和用户感兴趣类型的广告相似的广告，而用 k-means 也可以对客户感兴趣的广告类型进行分类。这里的结果是批处理层的结果。</p><p>在速度层中根据用户的实时浏览网页类型在之前分好类的广告中寻找一些top K的广告出来。最终服务层可以结合速度层的top K广告和批处理层中分类好的点击率高的相似广告，做出选择投放给用户。</p><h2 id="1-5-组件选型"><a href="#1-5-组件选型" class="headerlink" title="1.5. 组件选型"></a>1.5. 组件选型</h2><p>Lambda 架构中各组件在大数据生态系统中和阿里集团的常用组件。</p><p>数据流存储选用不可变日志的分布式系统 Kafka；BatchLayer 数据集的存储选用 Hadoop 的 HDFS；BatchView 的加工采用 MapReduce，Spark；BatchView 数据的存储采用 MySQL（查询少量的最近结果数据）、Hbase（查询大量的历史结果数据）。SpeedLayer 采用增量数据处理 Flink；RealtimeView 增量结果数据集采用内存数据库 Redis。</p><img src="/Users/joker/Library/Application Support/typora-user-images/截屏2020-09-14 下午4.40.45.png" alt="截屏2020-09-14 下午4.40.45" style="zoom:30%;" /><h1 id="2-实时处理系统架构"><a href="#2-实时处理系统架构" class="headerlink" title="2.实时处理系统架构"></a>2.实时处理系统架构</h1><img src="/Users/joker/Library/Application Support/typora-user-images/截屏2020-09-14 下午4.35.49.png" alt="截屏2020-09-14 下午4.35.49" style="zoom:50%;" /><p>即从上面的架构中我们可以看出，其由下面的几部分构成：</p><ol><li>Flume 集群</li><li>Kafka 集群</li><li>Flink 集群</li></ol><h3 id="2-1-1-Flume-集群"><a href="#2-1-1-Flume-集群" class="headerlink" title="2.1.1. Flume 集群"></a>2.1.1. Flume 集群</h3><p>Flume 的基本架构是 Agent。它是一个完整的数据收集工具，含有三个核心组件，分别是 Source、Channel、Sink。数据以Event为基本单位经过Source、Channel、Sink，从外部数据源来，向外部的目的地去。</p><h3 id="2-1-2-Kafka-集群"><a href="#2-1-2-Kafka-集群" class="headerlink" title="2.1.2. Kafka 集群"></a>2.1.2. Kafka 集群</h3><p>Kafka 是一个分布式的、可分区的、可复制的消息系统，维护消息队列。</p><p>Kafka 的整体架构非常简单，是显式分布式架构，Producer、Broker 和 Consumer 都可以有多个。Producer，consumer 实现 Kafka 注册的接口，数据从 Producer 发送到 Broker，Broker 承担一个中间缓存和分发的作用。Broker 分发注册到系统中的 Consumer。Broker 的作用类似于缓存，即活跃的数据和离线处理系统之间的缓存。客户端和服务器端的通信，是基于简单、高性能、且与编程语言无关的TCP协议。</p><h3 id="2-1-3-Flink-集群"><a href="#2-1-3-Flink-集群" class="headerlink" title="2.1.3. Flink 集群"></a>2.1.3. Flink 集群</h3><p>Flink 核心是一个流式的数据流执行引擎，其针对数据流的分布式计算提供了数据分布、数据通信以及容错机制等功能。基于流执行引擎，Flink 提供了诸多更高抽象层的 API 以便用户编写分布式任</p><h2 id="3-2-离线处理系统架构"><a href="#3-2-离线处理系统架构" class="headerlink" title="3.2. 离线处理系统架构"></a>3.2. 离线处理系统架构</h2><img src="/Users/joker/Library/Application Support/typora-user-images/截屏2020-09-15 下午3.35.45.png" alt="截屏2020-09-15 下午3.35.45" style="zoom:50%;" /><h1 id="4-Lambda-架构不足"><a href="#4-Lambda-架构不足" class="headerlink" title="4.Lambda 架构不足"></a>4.Lambda 架构不足</h1><p>虽然 Lambda 架构使用起来灵活，并且可以适用于很多的应用场景，但在实际应用的时候，Lambda 架构也存在着一些不足，主要表现在它的维护很复杂。</p><p>维护 Lambda 架构的复杂性在于我们要同时维护两套系统架构：批处理层和速度层，在架构中加入批处理层是因为从批处理层得到的结果具有高准确性，而加入速度层是因为它在处理大规模数据时具有低延时性。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Cloudera平台搭建</title>
      <link href="2020/05/25/Cloudera%E5%B9%B3%E5%8F%B0%E6%90%AD%E5%BB%BA/"/>
      <url>2020/05/25/Cloudera%E5%B9%B3%E5%8F%B0%E6%90%AD%E5%BB%BA/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><h1 id="1-服务器环境准备"><a href="#1-服务器环境准备" class="headerlink" title="1.服务器环境准备"></a>1.服务器环境准备</h1><h2 id="1-1-服务器环境概述"><a href="#1-1-服务器环境概述" class="headerlink" title="1.1 服务器环境概述"></a>1.1 服务器环境概述</h2><p>数据集群包含两台专业服务器，通过XenServer服务器虚拟化软件把两台专业服务器虚拟化为五台虚拟服务器(1个master和4个slave)；五台虚拟服务器都安装了<code>CentOS7(Linux)</code>操作系统，在此基础上安装了 <code>Java</code>、<code>C/C++</code>、<code>Scala</code>等基本开发工具，以及<code>Hadoop(HDFS,YARN)</code>、<code>MySQL</code>、<code>ZooKeeper</code>、<code>Kafka</code> 、<code>Spark2</code></p><a id="more"></a><p><code>Hbase</code>、<code>Spark</code>等数据集群必须的大数据存储及处理软件。数据集群需要安装的软件及其层次关系如表1.1所示。数据集群除了安装Hadoop、Spark、Hbase等组件外，在Master节点和data1节点安装了MySQL数据库。</p><table><thead><tr><th>主机</th><th>所在服务器</th><th>密码</th></tr></thead><tbody><tr><td>192.168.10.96</td><td>192.168.10.90</td><td>123456</td></tr><tr><td>192.168.10.98</td><td>192.168.10.90</td><td>123456</td></tr><tr><td>192.168.10.100</td><td>192.168.10.90</td><td>123456</td></tr><tr><td>192.168.10.102</td><td>192.168.10.120</td><td>123456</td></tr><tr><td>192.168.10.104</td><td>192.168.10.120</td><td>123456</td></tr></tbody></table><h2 id="1-2-关闭防火墙"><a href="#1-2-关闭防火墙" class="headerlink" title="1.2 关闭防火墙"></a>1.2 关闭防火墙</h2><p>关闭五台虚拟主机防火墙，分别在主机上执行以下命令</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">systemctl stop firewalld</span><br><span class="line">systemctl <span class="built_in">disable</span> firewalld</span><br></pre></td></tr></table></figure><h2 id="1-3-关闭-selinux"><a href="#1-3-关闭-selinux" class="headerlink" title="1.3 关闭 selinux"></a>1.3 关闭 selinux</h2><p>三台机器在root用户下执行以下命令关闭selinux</p><p>三台机器执行以下命令，关闭</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">selinuxvim /etc/selinux/config </span><br><span class="line"></span><br><span class="line">SELINUX=disabled</span><br></pre></td></tr></table></figure><h2 id="1-4-时间服务器"><a href="#1-4-时间服务器" class="headerlink" title="1.4 时间服务器"></a>1.4 时间服务器</h2><p>网络时间协议 <code>NTP(Network Time Protocol)</code>，可以用来同步网络中各个计算机的时间。<code>CentOS7</code>自带了<code>ntp</code>服务，这个服务不仅可以设置让本机和其他计算机做时间同步，还可以让本机扮演一个<code>time server</code>的角色，让局域网其他计算机和本机同步时间。修改配置文件<code>/etc/ntp.conf</code>可以把一台计算机设置为时间服务器，或与其他服务器同步时间。</p><p>以下设置<code>master</code> 为时间服务器，其他计算机 <code>(data1~data4)</code> 和 <code>master</code> 实现时间同步。以下以 <code>data1</code> (IP地址<code>192.168.10.98</code>)为例，介绍 <code>slave</code>  与 <code>master</code> 做时间同步。</p><h3 id="1-4-1-设置-master-基准时间"><a href="#1-4-1-设置-master-基准时间" class="headerlink" title="1.4.1 设置 master 基准时间"></a>1.4.1 设置 <code>master</code> 基准时间</h3><p>配置 <code>master</code> 做 <code>time server</code> ,<code>master</code> 本身不和其他机器时间同步，而是取本地硬件时间。所以，需要先把<code>master</code> 机器的时间调整准确。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@master~] date -s 09/13/2017         //设定日期 </span><br><span class="line">[root@master~] date -s 11:12:00          //设定时间</span><br><span class="line">[root@master~] clock -w</span><br><span class="line">[root@master~] hwclock -w</span><br></pre></td></tr></table></figure><h3 id="1-4-2-设置-master-为时间服务器"><a href="#1-4-2-设置-master-为时间服务器" class="headerlink" title="1.4.2 设置 master 为时间服务器"></a>1.4.2 <strong>设置</strong> <code>master</code> 为时间服务器</h3><p>将master配置成一个time server，需要修改/etc/ntp.conf。</p><p>如果master连接Internet，则master可以与以下“上级时间服务器”进行同步。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">server 0.centos.pool.ntp.org iburst     </span><br><span class="line">server 1.centos.pool.ntp.org iburst</span><br><span class="line">server 2.centos.pool.ntp.org iburst</span><br><span class="line">server 3.centos.pool.ntp.org </span><br></pre></td></tr></table></figure><h3 id="1-4-3-允许本机-ntpd-和本地硬件时间同步"><a href="#1-4-3-允许本机-ntpd-和本地硬件时间同步" class="headerlink" title="1.4.3. 允许本机 ntpd 和本地硬件时间同步"></a>1.4.3. 允许本机 <code>ntpd</code> 和本地硬件时间同步</h3><p>如果master与上级服务器同步失败，或master没有连接Internet，则和本地硬件时间同步。把以下带下划线内容添加到“上级服务器”的后面。如果master与上级服务器同步失败，和本地硬件时间同步。如图7-1所示。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">server 127.127.1.0</span><br><span class="line">fudge 127.127.1.0 stratum 10</span><br></pre></td></tr></table></figure><h2 id="7-5-关闭chronyd"><a href="#7-5-关闭chronyd" class="headerlink" title="7.5 关闭chronyd"></a>7.5 关闭chronyd</h2><p><code>chronyd</code>也是与时间相关的服务，设置为开机自启动，这个服务会导致<code>ntp</code>无法开启开机自启动，所以需要关闭该进程。时间服务器和客户机都要关闭。</p><ul><li>查看<code>chronyd</code>状态</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@master ~] systemctl status chronyd </span><br></pre></td></tr></table></figure><ul><li>关闭<code>chronyd</code>服务</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@master ~] systemctl <span class="built_in">disable</span> chronyd.service  </span><br></pre></td></tr></table></figure><h2 id="7-6-CentOS7-设置开机自启动-ntp-服务"><a href="#7-6-CentOS7-设置开机自启动-ntp-服务" class="headerlink" title="7.6  CentOS7 设置开机自启动 ntp 服务"></a>7.6  <code>CentOS7</code> 设置开机自启动 <code>ntp</code> 服务</h2><p>局域网时间服务器和客户机都应启动开机自启动<code>ntp</code>服务。</p><ul><li>设置<code>master</code>开机自启动<code>ntp</code>服务</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@master ~] systemctl enable ntpd.service </span><br></pre></td></tr></table></figure><ul><li>查看<code>ntpd</code>状态</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@master ~] systemctl status ntpd  </span><br></pre></td></tr></table></figure><ul><li>设置<code>data1</code>开机自启动<code>ntp</code>服务</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@data1 ~] systemctl enable ntpd.service </span><br></pre></td></tr></table></figure><ul><li>查看<code>ntpd</code>状态</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@data1 ~] systemctl status ntpd</span><br></pre></td></tr></table></figure><h2 id="1-4-更改主机名"><a href="#1-4-更改主机名" class="headerlink" title="1.4 更改主机名"></a>1.4 更改主机名</h2><p>五台机器分别更改主机名</p><table><thead><tr><th>主机</th><th>主机名</th></tr></thead><tbody><tr><td>192.168.10.96</td><td>master</td></tr><tr><td>192.168.10.98</td><td>data1</td></tr><tr><td>192.168.10.100</td><td>data2</td></tr><tr><td>192.168.10.102</td><td>data3</td></tr><tr><td>192.168.10.104</td><td>data4</td></tr></tbody></table><h2 id="1-6-安装-JDK"><a href="#1-6-安装-JDK" class="headerlink" title="1.6 安装 JDK"></a>1.6 安装 JDK</h2><h3 id="1-6-1-卸载-JDK1-8"><a href="#1-6-1-卸载-JDK1-8" class="headerlink" title="1.6.1 卸载 JDK1.8"></a>1.6.1 卸载 JDK1.8</h3><blockquote><p>Centos 默认安装了  JDK1.8 ,但 JDK1.8 的可执行文件和众多库文件分布在不同目录下。给hadoop环境变量设置造成很多不便。因此，建议卸载系统默认安装的 JDK1.8 </p></blockquote><ul><li>查看 jdk 版本信息</li><li>查看已安装 jdk 组件</li><li>卸载 jdk 及其组件</li></ul><h3 id="1-6-2-重新安装-JDK1-8"><a href="#1-6-2-重新安装-JDK1-8" class="headerlink" title="1.6.2 重新安装 JDK1.8"></a>1.6.2 重新安装 JDK1.8</h3><ul><li><p>解压 jdk1.8</p></li><li><p>安装 jdk1.8</p></li><li><p>修改配置文件</p><ul><li><p>配置环境变量</p></li><li><p>使环境变量生效</p></li></ul></li></ul><h1 id="2-准备cloudera安装包"><a href="#2-准备cloudera安装包" class="headerlink" title="2.准备cloudera安装包"></a>2.准备cloudera安装包</h1><h2 id="2-1-Cloudera-Manager-5"><a href="#2-1-Cloudera-Manager-5" class="headerlink" title="2.1 Cloudera Manager 5"></a>2.1 Cloudera Manager 5</h2><blockquote><p>文件名: cloudera-manager-centos7-cm5.14.0_x86_64.tar.gz</p><p>下载地址: <a href="https://archive.cloudera.com/cm5/cm/5/">https://archive.cloudera.com/cm5/cm/5/</a></p></blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">由于是离线部署，因此需要预先下载好需要的文件。</span><br><span class="line">需要准备的文件有:</span><br><span class="line"></span><br><span class="line">Cloudera Manager 5</span><br><span class="line">文件名: cloudera-manager-centos7-cm5.14.0_x86_64.tar.gz</span><br><span class="line">下载地址: https://archive.cloudera.com/cm5/cm/5/</span><br><span class="line">CDH安装包（Parecls包）</span><br><span class="line">版本号必须与Cloudera Manager相对应</span><br><span class="line">下载地址: https://archive.cloudera.com/cdh5/parcels/5.14.0/</span><br><span class="line">需要下载下面3个文件：</span><br><span class="line">CDH-5.14.0-1.cdh5.14.0.p0.23-el7.parcel</span><br><span class="line">CDH-5.14.0-1.cdh5.14.0.p0.23-el7.parcel.sha1</span><br><span class="line">manifest.json</span><br><span class="line">MySQL jdbc驱动</span><br><span class="line">文件名: mysql-connector-java-.tar.gz</span><br><span class="line">下载地址: https://dev.mysql.com/downloads/connector/j/</span><br><span class="line">解压出: mysql-connector-java-bin.jar</span><br></pre></td></tr></table></figure><h1 id="4-所有机器安装依赖包"><a href="#4-所有机器安装依赖包" class="headerlink" title="4.所有机器安装依赖包"></a>4.所有机器安装依赖包</h1><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum -y install chkconfig python bind-utils psmisc libxslt zlib sqlite cyrus-sasl-plain cyrus-sasl-gssapi fuse portmap fuse-libs redhat-lsb</span><br></pre></td></tr></table></figure><h1 id="5-安装mysql数据库"><a href="#5-安装mysql数据库" class="headerlink" title="5.安装mysql数据库"></a>5.安装mysql数据库</h1><p>在第二台机器上(随机选择的机器，计划在第一台机器上安装cloudera管理服务比较耗费资源,所以在第二台机器上安装mysql数据库)安装mysql数据库.</p><p>参考【MySQL安装之yum安装教程】</p><h1 id="6-安装cloudera服务端"><a href="#6-安装cloudera服务端" class="headerlink" title="6.安装cloudera服务端"></a>6.安装cloudera服务端</h1><h2 id="6-1-解压服务端管理安装包"><a href="#6-1-解压服务端管理安装包" class="headerlink" title="6.1 解压服务端管理安装包"></a>6.1 解压服务端管理安装包</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">所有节点上传cloudera-manager-centos7-cm5.14.0_x86_64.tar.gz文件并解压</span></span><br><span class="line">[root@node01 ~]# tar -zxvf cloudera-manager-centos7-cm5.14.2_x86_64.tar.gz -C /opt</span><br><span class="line">[root@node02 ~]# tar -zxvf cloudera-manager-centos7-cm5.14.2_x86_64.tar.gz -C /opt</span><br><span class="line">[root@node03 ~]# tar -zxvf cloudera-manager-centos7-cm5.14.2_x86_64.tar.gz -C /opt</span><br></pre></td></tr></table></figure><p>解压完可以在/opt目录下看到文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[root@node01 ~]# cd /opt/</span><br><span class="line">[root@node01 opt]# ll</span><br><span class="line">total 0</span><br><span class="line">drwxr-xr-x. 4 1106 4001 36 Apr  3  2018 cloudera</span><br><span class="line">drwxr-xr-x. 9 1106 4001 88 Apr  3  2018 cm-5.14.2</span><br><span class="line">[root@node01 opt]# cd cloudera/</span><br><span class="line">[root@node01 cloudera]# ll</span><br><span class="line">total 0</span><br><span class="line">drwxr-xr-x. 2 1106 4001 6 Apr  3  2018 csd</span><br><span class="line">drwxr-xr-x. 2 1106 4001 6 Apr  3  2018 parcel-repo</span><br><span class="line">[root@node01 cloudera]# </span><br></pre></td></tr></table></figure><h2 id="6-2-创建客户端运行目录"><a href="#6-2-创建客户端运行目录" class="headerlink" title="6.2 创建客户端运行目录"></a>6.2 创建客户端运行目录</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">所有节点手动创建文件夹</span></span><br><span class="line">[root@node01 ~]# mkdir /opt/cm-5.14.2/run/cloudera-scm-agent</span><br><span class="line">[root@node02 ~]# mkdir /opt/cm-5.14.2/run/cloudera-scm-agent</span><br><span class="line">[root@node03 ~]# mkdir /opt/cm-5.14.2/run/cloudera-scm-agent</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="6-3-创建cloudera-scm用户"><a href="#6-3-创建cloudera-scm用户" class="headerlink" title="6.3 创建cloudera-scm用户"></a>6.3 创建cloudera-scm用户</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">所有节点创建cloudera-scm用户</span></span><br><span class="line">useradd --system --home=/opt/cm-5.14.0/run/cloudera-scm-server --no-create-home --shell=/bin/false --comment &quot;Cloudera SCM User&quot; cloudera-scm</span><br></pre></td></tr></table></figure><h2 id="6-4-初始化数据库"><a href="#6-4-初始化数据库" class="headerlink" title="6.4 初始化数据库"></a>6.4 初始化数据库</h2><p>初始化数据库（只需要在Cloudera Manager Server节点执行）</p><p>将提供的msyql驱动包上传到第一台机器的root home目录下，然后将mysql jdbc驱动放入相应位置:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[root@node01 ~]# cp mysql-connector-java.jar /opt/cm-5.14.2/share/cmf/lib/</span><br><span class="line">[root@node01 ~]#  /opt/cm-5.14.2/share/cmf/schema/scm_prepare_database.sh mysql -h node02 -uroot -p&#x27;!Qaz123456&#x27; --scm-host node01 scm scm &#x27;!Qaz123456&#x27;    </span><br><span class="line">JAVA_HOME=/usr/java/jdk1.8.0_211-amd64</span><br><span class="line">Verifying that we can write to /opt/cm-5.14.2/etc/cloudera-scm-server</span><br><span class="line">Creating SCM configuration file in /opt/cm-5.14.2/etc/cloudera-scm-server</span><br><span class="line">Executing:  /usr/java/jdk1.8.0_211-amd64/bin/java -cp /usr/share/java/mysql-connector-java.jar:/usr/share/java/oracle-connector-java.jar:/opt/cm-5.14.2/share/cmf/schema/../lib/* com.cloudera.enterprise.dbutil.DbCommandExecutor /opt/cm-5.14.2/etc/cloudera-scm-server/db.properties com.cloudera.cmf.db.</span><br><span class="line">[                          main] DbCommandExecutor              INFO  Successfully connected to database.</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">显示初始化成功</span></span><br><span class="line">All done, your SCM database is configured correctly!</span><br><span class="line">[root@node01 ~]# </span><br></pre></td></tr></table></figure><p>脚本参数说明:<br>${数据库类型} -h ${数据库所在节点ip/hostname} -u${数据库用户名} -p${数据库密码} –scm-host ${Cloudera Manager Server节点ip/hostname} scm(数据库)  scm(用户名) scm(密码)</p><h2 id="6-5-修改所有节点客户端配置"><a href="#6-5-修改所有节点客户端配置" class="headerlink" title="6.5 修改所有节点客户端配置"></a>6.5 修改<strong>所有节点</strong>客户端配置</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">将其中的server_host参数修改为Cloudera Manager Server节点的主机名</span></span><br><span class="line">[root@node01 ~]# vi /opt/cm-5.14.2/etc/cloudera-scm-agent/config.ini</span><br><span class="line">[root@node01 ~]# vi /opt/cm-5.14.2/etc/cloudera-scm-agent/config.ini </span><br><span class="line">[General]</span><br><span class="line"><span class="meta">#</span><span class="bash"> 将默认的server_host=localhost 修改成node01</span></span><br><span class="line">server_host=node01</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="6-6-上传CDH安装包"><a href="#6-6-上传CDH安装包" class="headerlink" title="6.6 上传CDH安装包"></a>6.6 上传CDH安装包</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">将如下文件放到Server节点的/opt/cloudera/parcel-repo/目录中:</span></span><br><span class="line"><span class="meta">#</span><span class="bash">CDH-5.14.2-1.cdh5.14.2.p0.3-el7.parcel</span></span><br><span class="line"><span class="meta">#</span><span class="bash">CDH-5.14.2-1.cdh5.14.2.p0.3-el7.parcel.sha1</span></span><br><span class="line"><span class="meta">#</span><span class="bash">manifest.json</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 重命名sha1文件</span></span><br><span class="line">[root@node01 parcel-repo]# mv CDH-5.14.2-1.cdh5.14.2.p0.3-el7.parcel.sha1 CDH-5.14.2-1.cdh5.14.2.p0.3-el7.parcel.sha</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="6-7-更改安装目录用户组权限"><a href="#6-7-更改安装目录用户组权限" class="headerlink" title="6.7 更改安装目录用户组权限"></a>6.7 更改安装目录用户组权限</h2><p><strong>所有节点</strong>更改cm相关文件夹的用户及用户组</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@node01 ~]# chown -R cloudera-scm:cloudera-scm /opt/cloudera</span><br><span class="line">[root@node01 ~]# chown -R cloudera-scm:cloudera-scm /opt/cm-5.14.2</span><br><span class="line">[root@node01 ~]# </span><br></pre></td></tr></table></figure><h2 id="6-8-启动Cloudera-Manager和agent"><a href="#6-8-启动Cloudera-Manager和agent" class="headerlink" title="6.8 启动Cloudera Manager和agent"></a>6.8 启动Cloudera Manager和agent</h2><p>Server(node01)节点</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@node01 ~]# /opt/cm-5.14.2/etc/init.d/cloudera-scm-server start</span><br><span class="line">Starting cloudera-scm-server:                              [  OK  ]</span><br><span class="line">[root@node01 ~]# /opt/cm-5.14.2/etc/init.d/cloudera-scm-agent start </span><br><span class="line">Starting cloudera-scm-agent:                               [  OK  ]</span><br><span class="line">[root@node01 ~]# </span><br><span class="line"></span><br></pre></td></tr></table></figure><h1 id="7-服务安装"><a href="#7-服务安装" class="headerlink" title="7.服务安装"></a>7.服务安装</h1><p>使用浏览器登录cloudera-manager的web界面,用户名和密码都是admin</p><p><img src="assets/1570789521954.png" alt="1570789521954"></p><p>登陆之后，在协议页面勾选接受协议,点击继续</p><p><img src="assets/1570790224972.png" alt="1570790224972"></p><p>选择免费版本，免费版本已经能够满足我们日常业务需求,选择免费版即可.点击继续</p><p><img src="assets/1570790460382.png" alt="1570790460382"></p><p>如下图，点击继续</p><p><img src="assets/1570790639728.png" alt="1570790639728"></p><p>如下图，点击当前管理的机器，然后选择机器，点击继续</p><p><img src="assets/1570790871033.png" alt="1570790871033"></p><p>如下图，然后选择你的parcel对应版本的包</p><p><img src="assets/1570790984431.png" alt="1570790984431"></p><p>点击后，进入安装页面，稍等片刻</p><p>如下图，集群安装中 </p><p><img src="assets/1570791090600.png" alt="1570791090600"></p><p>如下图，安装包分配成功，点击继续</p><p><img src="assets/1570793156892.png" alt="1570793156892"></p><p><img src="assets/1570793245342.png" alt="1570793245342"></p><p>针对这样的警告，需要在每一台机器输入如下命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">echo never &gt; &#x2F;sys&#x2F;kernel&#x2F;mm&#x2F;transparent_hugepage&#x2F;defrag</span><br><span class="line">echo never &gt; &#x2F;sys&#x2F;kernel&#x2F;mm&#x2F;transparent_hugepage&#x2F;enabled</span><br><span class="line">echo &#39;vm.swappiness&#x3D;10&#39;&gt;&gt; &#x2F;etc&#x2F;sysctl.conf</span><br><span class="line">sysctl vm.swappiness&#x3D;10</span><br></pre></td></tr></table></figure><p>如下图，然后点击重新运行，不出以为，就不会在出现警告了，点击完成,进入hadoop生态圈服务组件的安装</p><p><img src="assets/1570793435494.png" alt="1570793435494"></p><p>如下图，选择自定义服务，我们先安装好最基础的服务组合。那么在安装之前，如果涉及到hive和oozie的安装，那么先去mysql中，自己创建数据库，并赋予权限；</p><p>因此：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">database</span> hive;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">database</span> oozie;</span><br><span class="line"></span><br><span class="line"><span class="keyword">grant</span> <span class="keyword">all</span> <span class="keyword">on</span> *.* <span class="keyword">to</span> hive <span class="keyword">identified</span> <span class="keyword">by</span> <span class="string">&#x27;!Qaz123456&#x27;</span>;</span><br><span class="line"><span class="keyword">grant</span> <span class="keyword">all</span> <span class="keyword">on</span> *.* <span class="keyword">to</span> oozie <span class="keyword">identified</span> <span class="keyword">by</span> <span class="string">&#x27;!Qaz123456&#x27;</span>;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>如果出现如下错误:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">mysql&gt;</span><span class="bash"> grant all on *.* to oozie identified by <span class="string">&#x27;!Qaz123456&#x27;</span>;</span></span><br><span class="line">ERROR 1045 (28000): Access denied for user &#x27;root&#x27;@&#x27;localhost&#x27; (using password: YES)</span><br><span class="line"><span class="meta">mysql&gt;</span><span class="bash"> update mysql.user <span class="built_in">set</span> Grant_priv=<span class="string">&#x27;Y&#x27;</span>,Super_priv=<span class="string">&#x27;Y&#x27;</span> <span class="built_in">where</span> user = <span class="string">&#x27;root&#x27;</span> and host = <span class="string">&#x27;localhost&#x27;</span>;</span></span><br><span class="line">Query OK, 1 row affected (0.00 sec)</span><br><span class="line">Rows matched: 1  Changed: 1  Warnings: 0</span><br><span class="line"></span><br><span class="line"><span class="meta">mysql&gt;</span><span class="bash"> flush privileges;</span></span><br><span class="line"><span class="meta">mysql&gt;</span><span class="bash"> quit</span></span><br><span class="line">Bye</span><br><span class="line">You have new mail in /var/spool/mail/root</span><br><span class="line">[root@node02 ~]# systemctl restart mysqld.service</span><br><span class="line">[root@node02 ~]# mysql -u root -p                </span><br><span class="line">Enter password: </span><br><span class="line">Welcome to the MySQL monitor.  Commands end with ; or \g.</span><br><span class="line">Your MySQL connection id is 3</span><br><span class="line">Server version: 5.7.27 MySQL Community Server (GPL)</span><br><span class="line"></span><br><span class="line">Copyright (c) 2000, 2019, Oracle and/or its affiliates. All rights reserved.</span><br><span class="line"></span><br><span class="line">Oracle is a registered trademark of Oracle Corporation and/or its</span><br><span class="line">affiliates. Other names may be trademarks of their respective</span><br><span class="line">owners.</span><br><span class="line"></span><br><span class="line">Type &#x27;help;&#x27; or &#x27;\h&#x27; for help. Type &#x27;\c&#x27; to clear the current input statement.</span><br><span class="line"></span><br><span class="line"><span class="meta">mysql&gt;</span><span class="bash"> grant all on *.* to hive identified by <span class="string">&#x27;!Qaz123456&#x27;</span>;</span></span><br><span class="line">Query OK, 0 rows affected, 1 warning (0.00 sec)</span><br><span class="line"></span><br><span class="line"><span class="meta">mysql&gt;</span><span class="bash"> grant all on *.* to oozie identified by <span class="string">&#x27;!Qaz123456&#x27;</span>;</span></span><br><span class="line">Query OK, 0 rows affected, 1 warning (0.00 sec)</span><br><span class="line"></span><br><span class="line"><span class="meta">mysql&gt;</span><span class="bash"> </span></span><br></pre></td></tr></table></figure><p>这样再安装软件！</p><p>那么，选择自定义服务,如果我们后续需要其他服务时我们在进行添加</p><p><img src="assets/1570795590845.png" alt="1570795590845"></p><p>然后点击继续，进入选择服务添加分配页面，分配即可</p><p><img src="assets/1570795837600.png" alt="1570795837600"></p><p>选择完成后服务，如下图,可以点击按照主机查看服务分部情况</p><p><img src="assets/1570795867075.png" alt="1570795867075"></p><p><img src="assets/1570795808683.png" alt="1570795808683"></p><p>点击继续后，如下图，输入mysql数据库中数数据库scm，用户名scm，密码!Qaz123456,点击测试连接，大概等30s，显示成功，点击继续</p><p><img src="assets/1570796013065.png" alt="1570796013065"></p><p>一路点击继续,剩下的就是等待</p><p><img src="assets/1570796339035.png" alt="1570796339035"></p><p>如上图，如果等待时间过长，我们可以将manager所在机器(也就是node01)停止后把内存调整的大一些建议如果是笔记本4g以上，如果是云环境8g以上，我们这里先调整为4g以上，重新启node01机器后重新启动cloudera的server和agent</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@node01 ~]# cd /opt/cm-5.14.2/etc/init.d</span><br><span class="line"><span class="meta">#</span><span class="bash">启动server</span></span><br><span class="line">[root@node01 init.d]# ./cloudera-scm-server start</span><br><span class="line"><span class="meta">#</span><span class="bash">启动agent</span></span><br><span class="line">[root@node01 init.d]# ./cloudera-scm-agent start</span><br></pre></td></tr></table></figure><h1 id="8-重新登录cloudera-manager"><a href="#8-重新登录cloudera-manager" class="headerlink" title="8.重新登录cloudera manager"></a>8.重新登录cloudera manager</h1><p>登录成功后，如下图，重新启动集群,接下来就是等待.</p><p><img src="assets/1570799640007.png" alt="1570799640007"></p><h1 id="9-集群测试"><a href="#9-集群测试" class="headerlink" title="9.集群测试"></a>9.集群测试</h1><h2 id="9-1-文件系统测试"><a href="#9-1-文件系统测试" class="headerlink" title="9.1 文件系统测试"></a>9.1 文件系统测试</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">切换hdfs用户对hdfs文件系统进行测试是否能够进行正常读写</span></span><br><span class="line">[root@node01 ~]# su hdfs</span><br><span class="line">[hdfs@node01 ~]# hadoop dfs -ls /</span><br><span class="line">DEPRECATED: Use of this script to execute hdfs command is deprecated.</span><br><span class="line">Instead use the hdfs command for it.</span><br><span class="line"></span><br><span class="line">Found 1 items</span><br><span class="line">d-wx------   - hdfs supergroup          0 2019-10-11 08:21 /tmp</span><br><span class="line">[hdfs@node01 ~]# touch test</span><br><span class="line">[hdfs@node01 ~]# vi test </span><br><span class="line">hello world</span><br><span class="line"></span><br><span class="line">[hdfs@node01 ~]$ hadoop dfs -put words /test</span><br><span class="line">[hdfs@node01 ~]$ hadoop dfs -ls /</span><br><span class="line">DEPRECATED: Use of this script to execute hdfs command is deprecated.</span><br><span class="line">Instead use the hdfs command for it.</span><br><span class="line"></span><br><span class="line">Found 2 items</span><br><span class="line">drwxr-xr-x   - hdfs supergroup          0 2019-10-11 09:09 /test</span><br><span class="line">d-wx------   - hdfs supergroup          0 2019-10-11 08:21 /tmp</span><br><span class="line">[hdfs@node01 ~]$ hadoop dfs -ls /test</span><br><span class="line">DEPRECATED: Use of this script to execute hdfs command is deprecated.</span><br><span class="line">Instead use the hdfs command for it.</span><br><span class="line"></span><br><span class="line">Found 1 items</span><br><span class="line">-rw-r--r--   3 hdfs supergroup         12 2019-10-11 09:09 /test/words</span><br><span class="line">[hdfs@node01 ~]$ hadoop dfs -text /test/words</span><br><span class="line">DEPRECATED: Use of this script to execute hdfs command is deprecated.</span><br><span class="line">Instead use the hdfs command for it.</span><br><span class="line"></span><br><span class="line">hello world</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="9-2-yarn集群测试"><a href="#9-2-yarn集群测试" class="headerlink" title="9.2 yarn集群测试"></a>9.2 yarn集群测试</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br></pre></td><td class="code"><pre><span class="line">[hdfs@node01 ~]$ hadoop jar /opt/cloudera/parcels/CDH-5.14.2-1.cdh5.14.2.p0.3/jars/hadoop-mapreduce-examples-2.6.0-cdh5.14.2.jar wordcount /test/words /test/output</span><br><span class="line">19/10/11 22:47:59 INFO client.RMProxy: Connecting to ResourceManager at node03.kaikeba.com/192.168.52.120:8032</span><br><span class="line">19/10/11 22:47:59 INFO mapreduce.JobSubmissionFiles: Permissions on staging directory /user/hdfs/.staging are incorrect: rwx---rwx. Fixing permissions to correct value rwx------</span><br><span class="line">19/10/11 22:48:00 INFO input.FileInputFormat: Total input paths to process : 1</span><br><span class="line">19/10/11 22:48:00 INFO mapreduce.JobSubmitter: number of splits:1</span><br><span class="line">19/10/11 22:48:00 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1570847238197_0001</span><br><span class="line">19/10/11 22:48:01 INFO impl.YarnClientImpl: Submitted application application_1570847238197_0001</span><br><span class="line">19/10/11 22:48:01 INFO mapreduce.Job: The url to track the job: http://node03.kaikeba.com:8088/proxy/application_1570847238197_0001/</span><br><span class="line">19/10/11 22:48:01 INFO mapreduce.Job: Running job: job_1570847238197_0001</span><br><span class="line">19/10/11 22:48:28 INFO mapreduce.Job: Job job_1570847238197_0001 running in uber mode : false</span><br><span class="line">19/10/11 22:48:28 INFO mapreduce.Job:  map 0% reduce 0%</span><br><span class="line">19/10/11 22:50:10 INFO mapreduce.Job:  map 100% reduce 0%</span><br><span class="line">19/10/11 22:50:17 INFO mapreduce.Job:  map 100% reduce 17%</span><br><span class="line">19/10/11 22:50:19 INFO mapreduce.Job:  map 100% reduce 33%</span><br><span class="line">19/10/11 22:50:21 INFO mapreduce.Job:  map 100% reduce 50%</span><br><span class="line">19/10/11 22:50:24 INFO mapreduce.Job:  map 100% reduce 67%</span><br><span class="line">19/10/11 22:50:25 INFO mapreduce.Job:  map 100% reduce 83%</span><br><span class="line">19/10/11 22:50:29 INFO mapreduce.Job:  map 100% reduce 100%</span><br><span class="line">19/10/11 22:50:29 INFO mapreduce.Job: Job job_1570847238197_0001 completed successfully</span><br><span class="line">19/10/11 22:50:30 INFO mapreduce.Job: Counters: 49</span><br><span class="line">        File System Counters</span><br><span class="line">                FILE: Number of bytes read=144</span><br><span class="line">                FILE: Number of bytes written=1044048</span><br><span class="line">                FILE: Number of read operations=0</span><br><span class="line">                FILE: Number of large read operations=0</span><br><span class="line">                FILE: Number of write operations=0</span><br><span class="line">                HDFS: Number of bytes read=118</span><br><span class="line">                HDFS: Number of bytes written=16</span><br><span class="line">                HDFS: Number of read operations=21</span><br><span class="line">                HDFS: Number of large read operations=0</span><br><span class="line">                HDFS: Number of write operations=12</span><br><span class="line">        Job Counters </span><br><span class="line">                Launched map tasks=1</span><br><span class="line">                Launched reduce tasks=6</span><br><span class="line">                Data-local map tasks=1</span><br><span class="line">                Total time spent by all maps in occupied slots (ms)=100007</span><br><span class="line">                Total time spent by all reduces in occupied slots (ms)=24269</span><br><span class="line">                Total time spent by all map tasks (ms)=100007</span><br><span class="line">                Total time spent by all reduce tasks (ms)=24269</span><br><span class="line">                Total vcore-milliseconds taken by all map tasks=100007</span><br><span class="line">                Total vcore-milliseconds taken by all reduce tasks=24269</span><br><span class="line">                Total megabyte-milliseconds taken by all map tasks=102407168</span><br><span class="line">                Total megabyte-milliseconds taken by all reduce tasks=24851456</span><br><span class="line">        Map-Reduce Framework</span><br><span class="line">                Map input records=1</span><br><span class="line">                Map output records=2</span><br><span class="line">                Map output bytes=20</span><br><span class="line">                Map output materialized bytes=120</span><br><span class="line">                Input split bytes=106</span><br><span class="line">                Combine input records=2</span><br><span class="line">                Combine output records=2</span><br><span class="line">                Reduce input groups=2</span><br><span class="line">                Reduce shuffle bytes=120</span><br><span class="line">                Reduce input records=2</span><br><span class="line">                Reduce output records=2</span><br><span class="line">                Spilled Records=4</span><br><span class="line">                Shuffled Maps =6</span><br><span class="line">                Failed Shuffles=0</span><br><span class="line">                Merged Map outputs=6</span><br><span class="line">                GC time elapsed (ms)=581</span><br><span class="line">                CPU time spent (ms)=11830</span><br><span class="line">                Physical memory (bytes) snapshot=1466945536</span><br><span class="line">                Virtual memory (bytes) snapshot=19622957056</span><br><span class="line">                Total committed heap usage (bytes)=1150287872</span><br><span class="line">        Shuffle Errors</span><br><span class="line">                BAD_ID=0</span><br><span class="line">                CONNECTION=0</span><br><span class="line">                IO_ERROR=0</span><br><span class="line">                WRONG_LENGTH=0</span><br><span class="line">                WRONG_MAP=0</span><br><span class="line">                WRONG_REDUCE=0</span><br><span class="line">        File Input Format Counters </span><br><span class="line">                Bytes Read=12</span><br><span class="line">        File Output Format Counters </span><br><span class="line">                Bytes Written=16</span><br><span class="line">You have new mail in /var/spool/mail/root</span><br><span class="line">[hdfs@node01 ~]$ hdfs dfs -ls /test/output</span><br><span class="line">Found 7 items</span><br><span class="line">-rw-r--r--   3 hdfs supergroup          0 2019-10-11 22:50 /test/output/_SUCCESS</span><br><span class="line">-rw-r--r--   3 hdfs supergroup          0 2019-10-11 22:50 /test/output/part-r-00000</span><br><span class="line">-rw-r--r--   3 hdfs supergroup          8 2019-10-11 22:50 /test/output/part-r-00001</span><br><span class="line">-rw-r--r--   3 hdfs supergroup          0 2019-10-11 22:50 /test/output/part-r-00002</span><br><span class="line">-rw-r--r--   3 hdfs supergroup          0 2019-10-11 22:50 /test/output/part-r-00003</span><br><span class="line">-rw-r--r--   3 hdfs supergroup          0 2019-10-11 22:50 /test/output/part-r-00004</span><br><span class="line">-rw-r--r--   3 hdfs supergroup          8 2019-10-11 22:50 /test/output/part-r-00005</span><br><span class="line">[hdfs@node01 ~]$  hdfs dfs -text /test/output/part-r-00001</span><br><span class="line">world   1</span><br><span class="line">[hdfs@node01 ~]$  hdfs dfs -text /test/output/part-r-00005</span><br><span class="line">hello   1</span><br><span class="line">You have new mail in /var/spool/mail/root</span><br><span class="line">[hdfs@node01 ~]$ </span><br></pre></td></tr></table></figure><h1 id="10-手动添加Kafka服务"><a href="#10-手动添加Kafka服务" class="headerlink" title="10.手动添加Kafka服务"></a>10.手动添加Kafka服务</h1><p>我们以安装kafka为例进行演示</p><h2 id="10-1-检查kafka安装包"><a href="#10-1-检查kafka安装包" class="headerlink" title="10.1 检查kafka安装包"></a>10.1 检查kafka安装包</h2><p>首先检查是否已经存在Kafka的parcel安装包，如下图提示远程提供，说明我们下载的parcel安装包中不包含Kafka的parcel安装包，这时需要我们手动到官网上下载</p><p><img src="assets/1570850847758.png" alt="1570850847758"></p><h2 id="10-2-检查Kafka安装包版本"><a href="#10-2-检查Kafka安装包版本" class="headerlink" title="10.2 检查Kafka安装包版本"></a>10.2 检查Kafka安装包版本</h2><p>首先查看搭建cdh版本 和kafka版本，是否是支持的：</p><p>登录如下网址：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">https:&#x2F;&#x2F;www.cloudera.com&#x2F;documentation&#x2F;enterprise&#x2F;release-notes&#x2F;topics&#x2F;rn_consolidated_pcm.html#pcm_kafka</span><br></pre></td></tr></table></figure><p>我的CDH版本是cdh5.14.0 ，我想要的kafka版本是1.0.1</p><p>因此选择：</p><p><img src="assets/1633376-20190508130456986-1658024926.png" alt="img"></p><h2 id="10-3-下载Kafka-parcel安装包"><a href="#10-3-下载Kafka-parcel安装包" class="headerlink" title="10.3 下载Kafka parcel安装包"></a>10.3 下载Kafka parcel安装包</h2><p>然后下载：<a href="http://archive.cloudera.com/kafka/parcels/3.1.0/">http://archive.cloudera.com/kafka/parcels/3.1.0/</a></p><p><img src="assets/1633376-20190508130549214-56463812.png" alt="img"></p><p>需要将下载的KAFKA-3.1.0-1.3.1.0.p0.35-el7.parcel.sha1 改成 KAFKA-3.1.0-1.3.1.0.p0.35-el7.parcel.sha</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@node01 ~]# mv KAFKA-3.1.0-1.3.1.0.p0.35-el7.parcel.sha1 KAFKA-3.1.0-1.3.1.0.p0.35-el7.parcel.sha</span><br><span class="line">You have new mail in /var/spool/mail/root</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>然后将这三个文件，拷贝到parcel-repo目录下。如果有相同的文件，即manifest.json，只需将之前的重命名备份即可。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[root@node01 ~] cd /opt/cloudera/parcel-repo/</span><br><span class="line">[root@node01 parcel-repo]# mv manifest.json bak_manifest.json </span><br><span class="line"><span class="meta">#</span><span class="bash">拷贝到parcel-repo目录下</span></span><br><span class="line">[root@node01 ~]# mv KAFKA-3.1.0-1.3.1.0.p0.35-el7.parcel* manifest.json /opt/cloudera/parcel-repo/</span><br><span class="line">[root@node01 ~]# ll</span><br><span class="line">total 989036</span><br><span class="line">-rw-------. 1 root root      1260 Apr 16 01:35 anaconda-ks.cfg</span><br><span class="line">-rw-r--r--. 1 root root 832469335 Oct 11 13:23 cloudera-manager-centos7-cm5.14.2_x86_64.tar.gz</span><br><span class="line">-rw-r--r--. 1 root root 179439263 Oct 10 20:14 jdk-8u211-linux-x64.rpm</span><br><span class="line">-rw-r--r--. 1 root root    848399 Oct 11 17:02 mysql-connector-java.jar</span><br><span class="line">-rw-r--r--  1 root root        12 Oct 11 21:01 words</span><br><span class="line">You have new mail in /var/spool/mail/root</span><br><span class="line">[root@node01 ~]# ll</span><br></pre></td></tr></table></figure><h2 id="10-4-分配激活Kafka"><a href="#10-4-分配激活Kafka" class="headerlink" title="10.4 分配激活Kafka"></a>10.4 分配激活Kafka</h2><p>如下图，在管理首页选择parcel</p><p><img src="assets/1570859081770.png" alt="1570859081770"></p><p>如下图，检查更新多点击几次，就会出现分配按钮</p><p><img src="assets/1570859361922.png" alt="1570859361922"></p><p>点击分配，等待分配按钮激活</p><p><img src="assets/1570859491074.png" alt="1570859491074"></p><p>如下图，正在分配中…</p><p><img src="assets/1570859524848.png" alt="1570859524848"></p><p>如下图按钮已经激活</p><p><img src="assets/1570859672818.png" alt="1570859672818"></p><p><img src="assets/1570859816399.png" alt="1570859816399"></p><p>如上两张图图，点击激活和确定，然后等待激活    </p><p>正在激活…</p><p><img src="assets/1570859868869.png" alt="1570859868869"></p><p>如下图，分配并激活成功</p><p><img src="assets/1570859891912.png" alt="1570859891912"></p><h2 id="10-5-添加Kafka服务"><a href="#10-5-添加Kafka服务" class="headerlink" title="10.5 添加Kafka服务"></a>10.5 添加Kafka服务</h2><p>点击cloudera manager回到主页</p><p><img src="assets/1570860017451.png" alt="1570860017451"></p><p>页面中点击下拉操作按钮，点击添加服务</p><p><img src="assets/1570849216563.png" alt="1570849216563"></p><p>如下图，点击选择kafka，点击继续</p><p><img src="assets/1570849280747.png" alt="1570849280747"></p><p>如下图，选择Kakka Broker在三个节点上安装，Kafka MirrorMaker安装在node03上，Gateway安装在node02上（服务选择安装，需要自己根据每台机器上健康状态而定,这里只是作为参考）</p><p><img src="assets/1570849433668.png" alt="1570849433668"></p><p>如下图，填写Destination Broker List和Source Broker List后点击继续</p><p><strong>注意:这里和上一步中选择的角色分配有关联,Kafka Broker选择的是三台机器Destination Broker List中就填写三台机器的主机名，中间使用逗号分开，如果选择的是一台机器那么久选择一台，一次类推.Source Broker List和Destination Broker List填写一样.</strong></p><p><img src="assets/1570861737283.png" alt="1570861737283"></p><p>如下图，添加服务，最终状态为已完成，启动过程中会出现错误不用管，这时因为CDH给默认将kafka的内存设置为50M,太小了， 后续需要我们手动调整,点击继续</p><p><img src="assets/1570862070794.png" alt="1570862070794"></p><p>如下图,点击完成.</p><p><img src="assets/1570862239690.png" alt="1570862239690"></p><p>如下图，添加成功的Kafka服务</p><p><img src="assets/1570862272925.png" alt="1570862272925"></p><h2 id="10-6-配置Kafka的内存"><a href="#10-6-配置Kafka的内存" class="headerlink" title="10.6 配置Kafka的内存"></a>10.6 配置Kafka的内存</h2><p>如下图，点击Kafka服务</p><p><img src="assets/1570862705339.png" alt="1570862705339"></p><p>如下图，点击实例，点击Kafka Broker（<strong>我们先配置node01节点的内存大小,node02和node03内存配置方式相同，需要按照此方式进行修改</strong>）</p><p><img src="assets/1570862765551.png" alt="1570862765551"></p><p>如上图，点击Kafka Broker之后，如下图所示，点击配置</p><p><img src="assets/1570862896070.png" alt="1570862896070"></p><p>右侧浏览器垂直滚动条往下找到broker_max_heap_size，修改值为256,点击保存更改</p><p><img src="assets/1570863035798.png" alt="1570863035798"></p><p><strong>node02和node03按照上述步骤进行同样修改.</strong></p><h2 id="10-7-重新启动kafka集群"><a href="#10-7-重新启动kafka集群" class="headerlink" title="10.7 重新启动kafka集群"></a>10.7 重新启动kafka集群</h2><p>点击启动</p><p><img src="assets/1570863234060.png" alt="1570863234060"></p><p>点击启动</p><p><img src="assets/1570863253382.png" alt="1570863253382"></p><p>启动成功</p><p><img src="assets/1570863458643.png" alt="1570863458643"></p><h1 id="11-手动添加服务"><a href="#11-手动添加服务" class="headerlink" title="11.手动添加服务"></a>11.手动添加服务</h1><p>请参考【10.手动添加Kafka服务】操作步骤.</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Cloudera </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="2020/04/22/Flink-%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C/Kafka%20%E6%B6%88%E6%81%AF%E4%B8%8D%E4%B8%A2%E5%A4%B1/"/>
      <url>2020/04/22/Flink-%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C/Kafka%20%E6%B6%88%E6%81%AF%E4%B8%8D%E4%B8%A2%E5%A4%B1/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p><strong>生产者，Broker，消费者都是有可能丢数据的</strong></p><h3 id="生产端"><a href="#生产端" class="headerlink" title="生产端"></a><strong>生产端</strong></h3><p>生产者丢数据，即发送的数据根本没有保存到 Broker 端。出现这个情况的原因可能是，网络抖动，导致消息压根就没有发送到 Broker 端；也可能是消息本身不合格导致 Broker 拒绝接收（比如消息太大了，超过了 Broker 的承受能力）等等。</p><p>上面所说比如网络原因导致消息没有成功发送到 broker 端，常见，也并不可怕。可怕的不是没发送成功，而是发送失败了你不做任何处理。</p><p>很简单的一个<strong>重试配置</strong>，基本就可以解决这种网络瞬时抖动问题。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">props.put(<span class="string">&quot;retries&quot;</span>, <span class="number">10</span>);</span><br></pre></td></tr></table></figure><p>当然还有很多其他原因导致的，不能只依靠 kafka 的配置来做处理，我们看一下 kafka 发送端的源码，其实人家是提供了两个方法的，通常会出问题的方法是那个简单的 send，没有 callback（回调）。简单的 send发送后不会去管它的结果是否成功，而 callback 能准确地告诉你消息是否真的提交成功了。一旦出现消息提交失败的情况，你就可以有针对性地进行处理。</p><p><font color='red'><strong>因此，一定要使用带有回调通知的 send 方法。</strong></font></p><p>我们知道，broker 一般不会有一个，我们就是要通过多 Broker 达到高可用的效果，所以对于生产者程序来说，也不能简单的认为发送到一台就算成功，如果只满足于一台，那台机器如果损坏了，那消息必然会丢失。设置 <strong>acks = all</strong>，表明所有副本 Broker 都要接收到消息，该消息才算是“已提交”，这样可以达到高可用的效果。</p><h3 id="Broker-端"><a href="#Broker-端" class="headerlink" title="Broker 端"></a><strong>Broker 端</strong></h3><p>数据已经保存在 broker 端，但是数据却丢失了。出现这个的原因可能是，Broker 机器 down 了，当然broker 是高可用的，假如你的消息保存在 N 个 Kafka Broker 上，那么至少有 1 个存活就不会丢。</p><h4 id="解决"><a href="#解决" class="headerlink" title="解决"></a>解决</h4><ol><li><h5 id="消息冗余"><a href="#消息冗余" class="headerlink" title="消息冗余"></a><font color='blue'>消息冗余</font></h5><p>前面我们说到，kafka 是有限度的保证消息不丢失，这里的限度，指至少要有一台 broker 可以正常提供服务。至少一台，这种说法可并不准确，应该说至少一台存储了你消息的的 broker。我们知道分区可以设置副本数，假如你只设置副本为1，只要挂的刚好是你副本的那台，即使你有1000台broker，也无济于事。</p><p>因此，副本的设置尤为重要，一般设置 <strong><code>replication.factor &gt;= 3</code>**，毕竟目前</strong>防止消息丢失的主要机制就是冗余**。</p><p>但仅仅设置副本数就有用吗？并不能保证 broker 端一定存储了三个副本呀。假如共有三个broker，发送一条消息的时候，某个 broker 刚好宕机了，即使你配置了<code>replication.factor = 3</code>，也最多只会有2台副本。因此，我们还要确认，至少要被写入到多少个副本才算是“已提交”。</p><p><strong><code>min.insync.replicas &gt; 1</code></strong> <strong>,</strong> 控制的是消息至少要被写入到多少个副本才算是“已提交”。设置成大于 1 可以提升消息持久性。</p><p>说到这，可能会有疑问，上面生产端不是已经配置 <code>acks=all</code>了，和这个参数不是冲突了吗？？注意 <code>acks = all</code> 是针对所有副本 Broker 都要接收到消息，假如 ISR中只有1个副本了，<code>acks=all</code> 也就相当于 <code>acks=1</code> 了，引入 <code>min.insync.replicas</code> 的目的就是为了做一个下限的限制，不能只满足于 <code>ISR</code> 全部写入，还要保证ISR 中的写入个数不少于 <code>min.insync.replicas</code>。</p><p>对了，请确保 <strong>replication.factor &gt; min.insync.replicas</strong>。一般设置为<strong>replication.factor = min.insync.replicas + 1</strong>。如果两者相等，有一个副本挂机，整个分区就无法正常工作了。我们不仅要考虑消息的可靠性，防止消息丢失，更应该考虑可用性问题。</p></li><li><h5 id="leader-选举"><a href="#leader-选举" class="headerlink" title="leader 选举"></a><strong><font color='blue'>leader 选举</font></strong></h5><p>我们知道kafka中有领导者副本（Leader Replica）和追随者副本（Follower Replica），而follower replica存在的唯一目的就是防止消息丢失，并不参与具体的业务逻辑的交互。只有leader 才参与服务，follower的作用就是充当leader的候补，平时的操作也只有信息同步。ISR也就是这组与leader保持同步的replica集合，我们要保证不丢消息，首先要保证ISR的存活（至少有一个备份存活），那存活的概念是什么呢，不仅需要机器正常，还需要跟上leader的消息进度，当达到一定程度的时候就会认为“非存活”状态。</p><p>假设这么一种场景，有Leader,Follow1,Follow2；其中Follow2落后于Leader太多，因此不在leader副本和follower1副本所在的ISR集合之中。此时Leader,Follow1都宕机了，只剩下Follow2了，Follow2还在，就会进行新的选举，不过在选举之前首先要判断<strong>unclean.leader.election.enable</strong>参数的值。如果<strong>unclean.leader.election.enable参数的值为false，那么就意味着非ISR中的副本不能够参与选举</strong>，此时无法进行新的选举，此时整个分区处于不可用状态。如果unclean.leader.election.enable参数的值为true，那么可以从非ISR集合中选举follower副本称为新的leader。如果让非ISR中的Follow2成为Leader会有什么后果呢？</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/W801CaTfHvWaFrR74ZUdpjXa1bibaWGJVvpGEwX6kTcN6iciaB3UxYQKpMIpjBN3zyt68JGdxvapgicBSUevcbkvQg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="img"></p><p>我们说Follow2已经落后之前的Leader很多，他成为新的Leader后从客户端继续收取消息，此时，原来的leader副本恢复，成为了新的follower副本，准备向新的leader副本同步消息，但是它发现自身的LEO（LEO是Log End Offset的缩写，它表示了当前日志文件中下一条待写入消息的offset）比leader副本的LEO还要大。Kafka中有一个准则，follower副本的LEO是不能够大于leader副本的，所以新的follower副本就需要截断日志至leader副本的LEO处，截断日志，不就丢失了之前的消息吗？即图中所示，丢失了3和4两条数据，并且新的Follow和新Leader之间的消息也不一致。</p></li></ol><p>   因此，如果要保证消息不丢失，需设置：</p><p>   <strong>unclean.leader.election.enable=false</strong>，但是Kafka的可用性就会降低，具体怎么选择需要读者根据实际的业务逻辑进行权衡，可靠性优先还是可用性优先。从Kafka 0.11.0.0版本开始将此参数从true设置为false，可以看出Kafka的设计者偏向于可靠性。</p><h3 id="消费端丢数据"><a href="#消费端丢数据" class="headerlink" title="消费端丢数据"></a><strong>消费端丢数据</strong></h3><p>Consumer 程序有个“位移”的概念，表示的是这个 Consumer 当前消费到的 Topic 分区的位置。Kafka默认是自动提交位移的，这样可能会有个问题，假如你在pull(拉取)30条数据，处理到第20条时自动提交了offset，但是在处理21条的时候出现了异常，当你再次pull数据时，由于之前是自动提交的offset，所以是从30条之后开始拉取数据，这也就意味着21-30条的数据发生了丢失。</p><p>消费端保证不丢数据，最重要就是保证offset的准确性。我们能做的，就是确保消息消费完成再提交。Consumer 端有个参数 ，设置 <strong>enable.auto.commit=</strong> <strong>false</strong>， 并且采用手动提交位移的方式。如果在处理数据时发生了异常，那就把当前处理失败的offset进行提交(放在finally代码块中)注意一定要确保offset的正确性，当下次再次消费的时候就可以从提交的offset处进行再次消费。consumer在处理数据的时候失败了，其实可以把这条数据给缓存起来，可以是redis、DB、file等，也可以把这条消息存入专门用于存储失败消息的topic中，让其它的consumer专门处理失败的消息。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="2020/04/04/Flink-%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C/Kafka%20%E6%B6%88%E6%81%AF%E6%9C%89%E5%BA%8F/"/>
      <url>2020/04/04/Flink-%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C/Kafka%20%E6%B6%88%E6%81%AF%E6%9C%89%E5%BA%8F/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="2020/04/01/MySQL-REDO/"/>
      <url>2020/04/01/MySQL-REDO/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p>innodb 事务日志包括 redo log 和 undo log。redo log 是重做日志，提供前滚操作，undo log 是回滚日志，提供回滚操作。</p><p>undo log 不是 redo log 的逆向过程，其实它们都算是用来恢复的日志</p><ol><li><strong>redo log 通常是物理日志，记录的是数据页的物理修改，而不是某一行或某几行修改成怎样怎样，它用来恢复提交后的物理数据页(恢复数据页，且只能恢复到最后一次提交的位置)。</strong></li><li>undo 用来回滚行记录到某个版本。undo log 一般是逻辑日志，根据每行记录进行记录。</li></ol>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Hadoop源码学习[5]-Edits 双缓冲机制</title>
      <link href="2020/02/23/Hadoop%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0%5B3%5D-Edits%E5%8F%8C%E7%BC%93%E5%86%B2%E6%9C%BA%E5%88%B6/"/>
      <url>2020/02/23/Hadoop%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0%5B3%5D-Edits%E5%8F%8C%E7%BC%93%E5%86%B2%E6%9C%BA%E5%88%B6/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><h3 id="单缓冲机制"><a href="#单缓冲机制" class="headerlink" title="单缓冲机制"></a>单缓冲机制</h3><p>在存储系统写数据的过程中，出于性能上的考虑，新写的数据并不是每次都 flush 到目标存储中的，而是先放入到一个 buffer 空间里，等到 buffer 空间满了，再做一次 flush 出去的动作。这种情况和人们等车的例子极为类似，一辆车等人都上满了再开，才能保证更高的效率。但是这种缓冲设计模式还是存有一个主要弊端的，当缓冲数据满后将会阻塞住后面的数据操作直到缓冲数据完全flush出去。 </p><a id="more"></a><h3 id="双缓冲机制"><a href="#双缓冲机制" class="headerlink" title="双缓冲机制"></a>双缓冲机制</h3><p>如果说增大缓冲区长度属于纵向扩展的话，那么这里所说的双缓冲则是横向上的扩展。</p><p>它的运作原理如下:<br>‌将缓冲区分为 2 份，1份为当前缓冲区 buf current，另外 1 份为预写入分区 buf ready，两个缓冲区空间大小一致。current 区负责当前的写操作存放，当我们达到缓冲处罚条件时，执行一次双缓冲的调换操作。然后由另外的程序执行 ready 区的 flush 操作。被交换变为空缓冲区的 current 区重新用于这的数据写入。<br>‌<br>‌以上的执行模式有以下2大优势:</p><ol><li>‌程序无需反复进行创建新缓冲的操作</li><li>程序的写请求不会被阻塞住，除非current缓冲区已经满了同时ready缓冲区数据还没有全部flush出去</li></ol><p>如果说增大缓冲区长度属于纵向扩展的话，那么这里所说的双缓冲则是横向上的扩展。</p><p>它的运作原理如下:<br>‌将缓冲区分为2份，1份为当前缓冲区buf current，另外1份为预写入分区buf<br>ready，两个缓冲区空间大小一致。current区负责当前的写操作存放，当我们达到缓冲处罚条件时，执行一次双缓冲的调换操作。然后由另外的程序执行ready区的flush操作。被交换变为空缓冲区的current区重新用于这的数据写入。<br>‌<br>‌以上的执行模式有以下2大优势:<br>‌1）程序无需反复进行创建新缓冲的操作<br>‌2）程序的写请求不会被阻塞住，除非current缓冲区已经满了同时ready缓冲区数据还没有全部flush出去</p><p>在使用双缓冲区模式时，因为缓冲区是可能存在 concurrent 使用的情况，所以这里需要有 thread<br>safe 的处理，以此保证每个操作是原子的更新。比如双缓冲在swap的时候就不应该发生 add 缓冲的动作，再比如还应该有一个线程可先性的变量来告诉程序，ready 缓冲区是否已完全 flush 出去。</p><h2 id="手写双缓冲"><a href="#手写双缓冲" class="headerlink" title="手写双缓冲"></a>手写双缓冲</h2><h2 id="HDFS-双缓冲机制"><a href="#HDFS-双缓冲机制" class="headerlink" title="HDFS 双缓冲机制"></a>HDFS 双缓冲机制</h2>]]></content>
      
      
      <categories>
          
          <category> Hadoop 源码阅读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
            <tag> Hadoop </tag>
            
            <tag> Hadoop 源码阅读 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>使用 Flink ProcessFunction 处理宕机告警</title>
      <link href="2020/01/28/%E4%BD%BF%E7%94%A8-Flink-ProcessFunction-%E5%A4%84%E7%90%86%E5%AE%95%E6%9C%BA%E5%91%8A%E8%AD%A6/"/>
      <url>2020/01/28/%E4%BD%BF%E7%94%A8-Flink-ProcessFunction-%E5%A4%84%E7%90%86%E5%AE%95%E6%9C%BA%E5%91%8A%E8%AD%A6/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p>Flink 的底层 API 就是 ProcessFunction，它是一个低阶的流处理操作，它可以访问流处理程序的基础构建模块：Event、State、Timer。ProcessFunction 可以被认为是一种提供了对 KeyedState 和定时器访问的 FlatMapFunction。每当数据源中接收到一个事件，就会调用来此函数来处理。对于容错的状态，ProcessFunction 可以通过 RuntimeContext 访问 KeyedState。</p><a id="more"></a><h3 id="ProcessFunction-介绍"><a href="#ProcessFunction-介绍" class="headerlink" title="ProcessFunction 介绍"></a>ProcessFunction 介绍</h3><p>Flink 的底层 API 就是 ProcessFunction，它是一个低阶的流处理操作，它可以访问流处理程序的基础构建模块：Event、State、Timer。ProcessFunction 可以被认为是一种提供了对 KeyedState 和定时器访问的 FlatMapFunction。每当数据源中接收到一个事件，就会调用来此函数来处理。对于容错的状态，ProcessFunction 可以通过 RuntimeContext 访问 KeyedState。</p><p>定时器可以对处理时间和事件时间的变化做一些处理。每次调用 processElement() 都可以获得一个 Context 对象，通过该对象可以访问元素的事件时间戳以及 TimerService。TimerService 可以为尚未发生的事件时间/处理时间实例注册回调。当定时器到达某个时刻时，会调用 onTimer() 方法。在调用期间，所有状态再次限定为定时器创建的 key，允许定时器操作 KeyedState。如果要访问 KeyedState 和定时器，那必须在 KeyedStream 上使用 KeyedProcessFunction，比如在 keyBy 算子之后使用：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">dataStream.keyBy(...).process(<span class="keyword">new</span> KeyedProcessFunction&lt;&gt;()&#123;</span><br><span class="line"></span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure><p>KeyedProcessFunction 是 ProcessFunction 函数的一个扩展，它可以在 onTimer 和 processElement 方法中获取到分区的 Key 值，这对于数据传递是很有帮助的，因为经常有这样的需求，经过 keyBy 算子之后可能还需要这个 key 字段，那么在这里直接构建成一个新的对象（新增一个 key 字段），然后下游的算子直接使用这个新对象中的 key 就好了，而不在需要重复的拼一个唯一的 key。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">processElement</span><span class="params">(String value, Context ctx, Collector&lt;String&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    System.out.println(ctx.getCurrentKey());</span><br><span class="line">    out.collect(value);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onTimer</span><span class="params">(<span class="keyword">long</span> timestamp, OnTimerContext ctx, Collector&lt;String&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    System.out.println(ctx.getCurrentKey());</span><br><span class="line">    <span class="keyword">super</span>.onTimer(timestamp, ctx, out);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="CoProcessFunction-介绍"><a href="#CoProcessFunction-介绍" class="headerlink" title="CoProcessFunction 介绍"></a>CoProcessFunction 介绍</h3><p>如果要在两个输入流上进行操作，可以使用 CoProcessFunction，这个函数可以传入两个不同的数据流输入，并为来自两个不同数据源的事件分别调用 processElement1() 和 processElement2() 方法。可以按照下面的步骤来实现一个典型的 Join 操作：</p><ul><li>为一个数据源的数据建立一个状态对象</li><li>从数据源处有新数据流过来的时候更新这个状态对象</li><li>在另一个数据源接收到元素时，关联状态对象并对其产生出连接的结果</li></ul><p>比如，将监控的 metric 数据和告警规则数据进行一个连接，在流数据的状态中存储了告警规则数据，当有监控数据过来时，根据监控数据的 metric 名称和一些 tag 去找对应告警规则计算表达式，然后通过规则的表达式对数据进行加工处理，判断是否要告警，如果是要告警则会关联构造成一个新的对象，新对象中不仅有初始的监控 metric 数据，还有含有对应的告警规则数据以及通知策略数据，组装成这样一条数据后，下游就可以根据这个数据进行通知，通知还会在状态中存储这个告警状态，表示它在什么时间告过警了，下次有新数据过来的时候，判断新数据是否是恢复的，如果属于恢复则把该状态清除。</p><h3 id="Timer-介绍"><a href="#Timer-介绍" class="headerlink" title="Timer 介绍"></a>Timer 介绍</h3><p>Timer 提供了一种定时触发器的功能，通过 TimerService 接口注册 timer。TimerService 在内部维护两种类型的定时器（处理时间和事件时间定时器）并排队执行。处理时间定时器的触发依赖于 ProcessingTimeService，它负责管理所有基于处理时间的触发器，内部使用 ScheduledThreadPoolExecutor 调度定时任务；事件时间定时器的触发依赖于系统当前的 Watermark。需要注意的一点就是：<strong>Timer 只能在 KeyedStream 中使用</strong>。</p><p>TimerService 会删除每个 Key 和时间戳重复的定时器，即每个 Key 在同一个时间戳上最多有一个定时器。如果为同一时间戳注册了多个定时器，则只会调用一次 onTimer（） 方法。Flink 会同步调用 onTimer() 和 processElement() 方法，因此不必担心状态的并发修改问题。TimerService 不仅提供了注册和删除 Timer 的功能，还可以通过它来获取当前的系统时间和 Watermark 的值。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-14-160008.png" alt="img"></p><h4 id="容错"><a href="#容错" class="headerlink" title="容错"></a>容错</h4><p>定时器具有容错能力，并且会与应用程序的状态一起进行 Checkpoint，如果发生故障重启会从 Checkpoint／Savepoint 中恢复定时器的状态。如果有处理时间定时器原本是要在恢复起来的那个时间之前触发的，那么在恢复的那一刻会立即触发该定时器。定时器始终是异步的进行 Checkpoint（除 RocksDB 状态后端存储、增量的 Checkpoint、基于堆的定时器外）。因为定时器实际上也是一种特殊状态的状态，在 Checkpoint 时会写入快照中，所以如果有大量的定时器，则无非会增加一次 Checkpoint 所需要的时间，必要的话得根据实际情况合并定时器。</p><h4 id="合并定时器"><a href="#合并定时器" class="headerlink" title="合并定时器"></a>合并定时器</h4><p>由于 Flink 仅为每个 Key 和时间戳维护一个定时器，因此可以通过降低定时器的频率来进行合并以减少定时器的数量。对于频率为 1 秒的定时器（基于事件时间或处理时间），可以将目标时间向下舍入为整秒数，则定时器最多提前 1 秒触发，但不会迟于我们的要求，精确到毫秒。因此，每个键每秒最多有一个定时器。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">long</span> coalescedTime = ((ctx.timestamp() + timeout) / <span class="number">1000</span>) * <span class="number">1000</span>;</span><br><span class="line">ctx.timerService().registerProcessingTimeTimer(coalescedTime);</span><br></pre></td></tr></table></figure><p>由于事件时间计时器仅在 Watermark 到达时才触发，因此可以将当前 Watermark 与下一个 Watermark 的定时器一起调度和合并：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">long</span> coalescedTime = ctx.timerService().currentWatermark() + <span class="number">1</span>;</span><br><span class="line">ctx.timerService().registerEventTimeTimer(coalescedTime);</span><br></pre></td></tr></table></figure><p>定时器也可以类似下面这样移除：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//删除处理时间定时器</span></span><br><span class="line"><span class="keyword">long</span> timestampOfTimerToStop = ...</span><br><span class="line">ctx.timerService().deleteProcessingTimeTimer(timestampOfTimerToStop);</span><br><span class="line"></span><br><span class="line"><span class="comment">//删除事件时间定时器</span></span><br><span class="line"><span class="keyword">long</span> timestampOfTimerToStop = ...</span><br><span class="line">ctx.timerService().deleteEventTimeTimer(timestampOfTimerToStop);</span><br></pre></td></tr></table></figure><p>如果没有该时间戳的定时器，则删除定时器无效。</p><h3 id="如果利用-ProcessFunction-处理宕机告警？"><a href="#如果利用-ProcessFunction-处理宕机告警？" class="headerlink" title="如果利用 ProcessFunction 处理宕机告警？"></a>如果利用 ProcessFunction 处理宕机告警？</h3><p>前面介绍了 ProcessFunction 和 Timer，那么这里讲下笔者公司生产环境的一个案例 —— 利用 ProcessFunction 处理宕机告警？</p><h4 id="宕机告警需求分析"><a href="#宕机告警需求分析" class="headerlink" title="宕机告警需求分析"></a>宕机告警需求分析</h4><p>首先大家应该知道生产环境的服务器一般都是有部署各种各种的服务或者中间件的，那么如果一台机器突然发生了一些突发情况，比如断电、自然灾害、人为因素、服务把机器跑宕机等，那么机器一宕机，原先跑在该机器的服务都会掉线，导致服务出现短暂不可用（可能应用会调度到其他机器）或者直接不可用（没有调度策略并且是运行的单实例），这对于生产环境来说，就麻烦比较大，可能会出现很大的损失，所以这种紧急情况就特别需要实时性非常高的告警。</p><p>在面对这个需求时首先得想一想怎么去判定一台机器是否处于宕机，因为会在机器上部署采集机器信息的 Agent，如果机器是正常的，每隔一定时间（假设时间间隔为 10 秒） Agent 会将数据进行上传，所有的监控数据上传至消息队列后，接下来就需要对这些监控数据处理。那么当机器处于宕机的状态，则运行在机器的 Agent 就已经停止工作了，则它就不会继续上传监控信息来了，所以这里就可以根据判定是否有这台机器的监控数据上来，如果持续有，那么说明机器在线，如果持续一段时间没有收到该机器的数据，则意味着该机器宕机了，那么可能就有人想问了，这个持续时间设置多少合适呢？这个得根据实际情况去做大量的测试和调优了，如果设置的过短，假设数据在消息队列中堆积了一会，那么也会出现误判的宕机告警；如果设置的过长，那么可能机器中途宕机过然后重启了，但是时间还是在设置的预定时间之内，这种情况就出现了宕机告警漏报，也是不允许的（告警延迟性增大并且可能告警漏报），所以就得根据实际情况两者之间做一个权衡。</p><p>在分析完需求后，接下来就得看如何去实现这种需求，怎么去判断机器是否一直有数据上来？那么这里就利用了 Timer 机制。</p><h4 id="宕机告警实现"><a href="#宕机告警实现" class="headerlink" title="宕机告警实现"></a>宕机告警实现</h4><p>机器监控数据有很多的指标，这里列几种比较常见的比如 Mem、CPU、Load、Swap 等，那么这几种数据采集上来的结构都是 MetricEvent 类型。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MetricEvent</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//指标名</span></span><br><span class="line">    <span class="keyword">private</span> String name;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//数据时间</span></span><br><span class="line">    <span class="keyword">private</span> Long timestamp;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//指标具体字段</span></span><br><span class="line">    <span class="keyword">private</span> Map&lt;String, Object&gt; fields;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//指标的标识</span></span><br><span class="line">    <span class="keyword">private</span> Map&lt;String, String&gt; tags;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>就拿 CPU 来举个例子，它发上来的数据是下面这种的：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="attr">&quot;name&quot;</span>: <span class="string">&quot;cpu&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;timestamp&quot;</span>: <span class="number">1571108814142</span>,</span><br><span class="line">    <span class="attr">&quot;fields&quot;</span>: &#123;</span><br><span class="line">        <span class="attr">&quot;usedPercent&quot;</span>: <span class="number">93.896484375</span>,</span><br><span class="line">        <span class="attr">&quot;max&quot;</span>: <span class="number">2048</span>,</span><br><span class="line">        <span class="attr">&quot;used&quot;</span>: <span class="number">1923</span></span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="attr">&quot;tags&quot;</span>: &#123;</span><br><span class="line">        <span class="attr">&quot;cluster_name&quot;</span>: <span class="string">&quot;zhisheng&quot;</span>,</span><br><span class="line">        <span class="attr">&quot;host_ip&quot;</span>: <span class="string">&quot;121.12.17.11&quot;</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这里笔者写了个模拟 Mem、CPU、Load、Swap 监控数据的工具类：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">BuildMachineMetricDataUtil</span> </span>&#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> String BROKER_LIST = <span class="string">&quot;localhost:9092&quot;</span>;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> String METRICS_TOPIC = <span class="string">&quot;zhisheng_metrics&quot;</span>;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> Random random = <span class="keyword">new</span> Random();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> List&lt;String&gt; hostIps = Arrays.asList(<span class="string">&quot;121.12.17.10&quot;</span>, <span class="string">&quot;121.12.17.11&quot;</span>, <span class="string">&quot;121.12.17.12&quot;</span>, <span class="string">&quot;121.12.17.13&quot;</span>);</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">writeDataToKafka</span><span class="params">()</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">        Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">        props.put(<span class="string">&quot;bootstrap.servers&quot;</span>, BROKER_LIST);</span><br><span class="line">        props.put(<span class="string">&quot;key.serializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span>);</span><br><span class="line">        props.put(<span class="string">&quot;value.serializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span>);</span><br><span class="line">        KafkaProducer producer = <span class="keyword">new</span> KafkaProducer&lt;String, String&gt;(props);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">            <span class="keyword">long</span> timestamp = System.currentTimeMillis();</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; hostIps.size(); i++) &#123;</span><br><span class="line">                MetricEvent cpuData = buildCpuData(hostIps.get(i), timestamp);</span><br><span class="line">                MetricEvent loadData = buildLoadData(hostIps.get(i), timestamp);</span><br><span class="line">                MetricEvent memData = buildMemData(hostIps.get(i), timestamp);</span><br><span class="line">                MetricEvent swapData = buildSwapData(hostIps.get(i), timestamp);</span><br><span class="line">                ProducerRecord cpuRecord = <span class="keyword">new</span> ProducerRecord&lt;String, String&gt;(METRICS_TOPIC, <span class="keyword">null</span>, <span class="keyword">null</span>, GsonUtil.toJson(cpuData));</span><br><span class="line">                ProducerRecord loadRecord = <span class="keyword">new</span> ProducerRecord&lt;String, String&gt;(METRICS_TOPIC, <span class="keyword">null</span>, <span class="keyword">null</span>, GsonUtil.toJson(loadData));</span><br><span class="line">                ProducerRecord memRecord = <span class="keyword">new</span> ProducerRecord&lt;String, String&gt;(METRICS_TOPIC, <span class="keyword">null</span>, <span class="keyword">null</span>, GsonUtil.toJson(memData));</span><br><span class="line">                ProducerRecord swapRecord = <span class="keyword">new</span> ProducerRecord&lt;String, String&gt;(METRICS_TOPIC, <span class="keyword">null</span>, <span class="keyword">null</span>, GsonUtil.toJson(swapData));</span><br><span class="line">                producer.send(cpuRecord);</span><br><span class="line">                producer.send(loadRecord);</span><br><span class="line">                producer.send(memRecord);</span><br><span class="line">                producer.send(swapRecord);</span><br><span class="line">            &#125;</span><br><span class="line">            producer.flush();</span><br><span class="line">            Thread.sleep(<span class="number">10000</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">        writeDataToKafka();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> MetricEvent <span class="title">buildCpuData</span><span class="params">(String hostIp, Long timestamp)</span> </span>&#123;</span><br><span class="line">        MetricEvent metricEvent = <span class="keyword">new</span> MetricEvent();</span><br><span class="line">        Map&lt;String, String&gt; tags = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line">        Map&lt;String, Object&gt; fields = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line">        <span class="keyword">int</span> used = random.nextInt(<span class="number">2048</span>);</span><br><span class="line">        <span class="keyword">int</span> max = <span class="number">2048</span>;</span><br><span class="line">        metricEvent.setName(<span class="string">&quot;cpu&quot;</span>);</span><br><span class="line">        metricEvent.setTimestamp(timestamp);</span><br><span class="line">        tags.put(<span class="string">&quot;cluster_name&quot;</span>, <span class="string">&quot;zhisheng&quot;</span>);</span><br><span class="line">        tags.put(<span class="string">&quot;host_ip&quot;</span>, hostIp);</span><br><span class="line">        fields.put(<span class="string">&quot;usedPercent&quot;</span>, (<span class="keyword">double</span>) used / max * <span class="number">100</span>);</span><br><span class="line">        fields.put(<span class="string">&quot;used&quot;</span>, used);</span><br><span class="line">        fields.put(<span class="string">&quot;max&quot;</span>, max);</span><br><span class="line">        metricEvent.setFields(fields);</span><br><span class="line">        metricEvent.setTags(tags);</span><br><span class="line">        <span class="keyword">return</span> metricEvent;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> MetricEvent <span class="title">buildLoadData</span><span class="params">(String hostIp, Long timestamp)</span> </span>&#123;</span><br><span class="line">        <span class="comment">//构建 load 数据，和构建 CPU 数据类似</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> MetricEvent <span class="title">buildSwapData</span><span class="params">(String hostIp, Long timestamp)</span> </span>&#123;</span><br><span class="line">        <span class="comment">//构建swap数据，和构建 CPU 数据类似</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> MetricEvent <span class="title">buildMemData</span><span class="params">(String hostIp, Long timestamp)</span> </span>&#123;</span><br><span class="line">        <span class="comment">//构建内存的数据，和构建 CPU 数据类似</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>然后 Flink 应用程序实时的去消费 Kafka 中的机器监控数据，先判断数据能够正常消费到。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">final</span> ParameterTool parameterTool = ExecutionEnvUtil.createParameterTool(args);</span><br><span class="line">StreamExecutionEnvironment env = ExecutionEnvUtil.prepare(parameterTool);</span><br><span class="line"></span><br><span class="line">Properties properties = KafkaConfigUtil.buildKafkaProps(parameterTool);</span><br><span class="line">FlinkKafkaConsumer011&lt;MetricEvent&gt; consumer = <span class="keyword">new</span> FlinkKafkaConsumer011&lt;&gt;(</span><br><span class="line">        parameterTool.get(<span class="string">&quot;metrics.topic&quot;</span>),</span><br><span class="line">        <span class="keyword">new</span> MetricSchema(),</span><br><span class="line">        properties);</span><br><span class="line">env.addSource(consumer)</span><br><span class="line">        .assignTimestampsAndWatermarks(<span class="keyword">new</span> MetricWatermark())</span><br><span class="line">        .print();</span><br></pre></td></tr></table></figure><p>再确定能够消费到机器监控数据之后，接下来需要对数据进行构造成 OutageMetricEvent 对象：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">OutageMetricEvent</span> </span>&#123;</span><br><span class="line">    <span class="comment">//机器集群名</span></span><br><span class="line">    <span class="keyword">private</span> String clusterName;</span><br><span class="line">    <span class="comment">//机器 host ip</span></span><br><span class="line">    <span class="keyword">private</span> String hostIp;</span><br><span class="line">    <span class="comment">//事件时间</span></span><br><span class="line">    <span class="keyword">private</span> Long timestamp;</span><br><span class="line">    <span class="comment">//机器告警是否恢复</span></span><br><span class="line">    <span class="keyword">private</span> Boolean recover;</span><br><span class="line">    <span class="comment">//机器告警恢复时间</span></span><br><span class="line">    <span class="keyword">private</span> Long recoverTime;</span><br><span class="line">    <span class="comment">//系统时间</span></span><br><span class="line">    <span class="keyword">private</span> Long systemTimestamp;</span><br><span class="line">    <span class="comment">//机器 CPU 使用率</span></span><br><span class="line">    <span class="keyword">private</span> Double cpuUsePercent;</span><br><span class="line">    <span class="comment">//机器内存使用率</span></span><br><span class="line">    <span class="keyword">private</span> Double memUsedPercent;</span><br><span class="line">    <span class="comment">//机器 SWAP 使用率</span></span><br><span class="line">    <span class="keyword">private</span> Double swapUsedPercent;</span><br><span class="line">    <span class="comment">//机器 load5</span></span><br><span class="line">    <span class="keyword">private</span> Double load5;</span><br><span class="line">    <span class="comment">//告警数量</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> counter = <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>通过 FlatMap 算子转换：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">new</span> FlatMapFunction&lt;MetricEvent, OutageMetricEvent&gt;() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">flatMap</span><span class="params">(MetricEvent metricEvent, Collector&lt;OutageMetricEvent&gt; collector)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        Map&lt;String, String&gt; tags = metricEvent.getTags();</span><br><span class="line">        <span class="keyword">if</span> (tags.containsKey(CLUSTER_NAME) &amp;&amp; tags.containsKey(HOST_IP)) &#123;</span><br><span class="line">            OutageMetricEvent outageMetricEvent = OutageMetricEvent.buildFromEvent(metricEvent);</span><br><span class="line">            <span class="keyword">if</span> (outageMetricEvent != <span class="keyword">null</span>) &#123;</span><br><span class="line">                collector.collect(outageMetricEvent);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>将数据转换后，需要将监控数据按照机器的 IP 进行 KeyBy，因为每台机器可能都会出现错误，所以都要将不同机器的状态都保存着，然后使用 process 算子，在该算子中，使用 ValueState 保存 OutageMetricEvent 和机器告警状态信息，另外还有一个 delay 字段定义的是持续多久没有收到监控数据的时间，alertCountLimit 表示的是告警的次数，如果超多一定的告警次数则会静默。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">OutageProcessFunction</span> <span class="keyword">extends</span> <span class="title">KeyedProcessFunction</span>&lt;<span class="title">String</span>, <span class="title">OutageMetricEvent</span>, <span class="title">OutageMetricEvent</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> ValueState&lt;OutageMetricEvent&gt; outageMetricState;</span><br><span class="line">    <span class="keyword">private</span> ValueState&lt;Boolean&gt; recover;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> delay;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> alertCountLimit;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">OutageProcessFunction</span><span class="params">(<span class="keyword">int</span> delay, <span class="keyword">int</span> alertCountLimit)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.delay = delay;</span><br><span class="line">        <span class="keyword">this</span>.alertCountLimit = alertCountLimit;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(Configuration parameters)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        TypeInformation&lt;OutageMetricEvent&gt; outageInfo = TypeInformation.of(<span class="keyword">new</span> TypeHint&lt;OutageMetricEvent&gt;() &#123;</span><br><span class="line">        &#125;);</span><br><span class="line">        TypeInformation&lt;Boolean&gt; recoverInfo = TypeInformation.of(<span class="keyword">new</span> TypeHint&lt;Boolean&gt;() &#123;</span><br><span class="line">        &#125;);</span><br><span class="line">        outageMetricState = getRuntimeContext().getState(<span class="keyword">new</span> ValueStateDescriptor&lt;&gt;(<span class="string">&quot;outage_zhisheng&quot;</span>, outageInfo));</span><br><span class="line">        recover = getRuntimeContext().getState(<span class="keyword">new</span> ValueStateDescriptor&lt;&gt;(<span class="string">&quot;recover_zhisheng&quot;</span>, recoverInfo));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">processElement</span><span class="params">(OutageMetricEvent outageMetricEvent, Context ctx, Collector&lt;OutageMetricEvent&gt; collector)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        OutageMetricEvent current = outageMetricState.value();</span><br><span class="line">        <span class="keyword">if</span> (current == <span class="keyword">null</span>) &#123;</span><br><span class="line">            current = <span class="keyword">new</span> OutageMetricEvent(outageMetricEvent.getClusterName(), outageMetricEvent.getHostIp(),</span><br><span class="line">                    outageMetricEvent.getTimestamp(), outageMetricEvent.getRecover(), System.currentTimeMillis());</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">if</span> (outageMetricEvent.getLoad5() != <span class="keyword">null</span>) &#123;</span><br><span class="line">                current.setLoad5(outageMetricEvent.getLoad5());</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">if</span> (outageMetricEvent.getCpuUsePercent() != <span class="keyword">null</span>) &#123;</span><br><span class="line">                current.setCpuUsePercent(outageMetricEvent.getCpuUsePercent());</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">if</span> (outageMetricEvent.getMemUsedPercent() != <span class="keyword">null</span>) &#123;</span><br><span class="line">                current.setMemUsedPercent(outageMetricEvent.getMemUsedPercent());</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">if</span> (outageMetricEvent.getSwapUsedPercent() != <span class="keyword">null</span>) &#123;</span><br><span class="line">                current.setSwapUsedPercent(outageMetricEvent.getSwapUsedPercent());</span><br><span class="line">            &#125;</span><br><span class="line">            current.setSystemTimestamp(System.currentTimeMillis());</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (recover.value() != <span class="keyword">null</span> &amp;&amp; !recover.value() &amp;&amp; outageMetricEvent.getTimestamp() &gt; current.getTimestamp()) &#123;</span><br><span class="line">            OutageMetricEvent recoverEvent = <span class="keyword">new</span> OutageMetricEvent(outageMetricEvent.getClusterName(), outageMetricEvent.getHostIp(),</span><br><span class="line">                    current.getTimestamp(), <span class="keyword">true</span>, System.currentTimeMillis());</span><br><span class="line">            recoverEvent.setRecoverTime(ctx.timestamp());</span><br><span class="line">            log.info(<span class="string">&quot;触发宕机恢复事件:&#123;&#125;&quot;</span>, recoverEvent);</span><br><span class="line">            collector.collect(recoverEvent);</span><br><span class="line">            current.setCounter(<span class="number">0</span>);</span><br><span class="line">            outageMetricState.update(current);</span><br><span class="line">            recover.update(<span class="keyword">true</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        current.setTimestamp(outageMetricEvent.getTimestamp());</span><br><span class="line">        outageMetricState.update(current);</span><br><span class="line">        ctx.timerService().registerEventTimeTimer(current.getSystemTimestamp() + delay);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onTimer</span><span class="params">(<span class="keyword">long</span> timestamp, OnTimerContext ctx, Collector&lt;OutageMetricEvent&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        OutageMetricEvent result = outageMetricState.value();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (result != <span class="keyword">null</span> &amp;&amp; timestamp &gt;= result.getSystemTimestamp() + delay &amp;&amp; System.currentTimeMillis() - result.getTimestamp() &gt;= delay) &#123;</span><br><span class="line">            <span class="keyword">if</span> (result.getCounter() &gt; alertCountLimit) &#123;</span><br><span class="line">                log.info(<span class="string">&quot;宕机告警次数大于:&#123;&#125; :&#123;&#125;&quot;</span>, alertCountLimit, result);</span><br><span class="line">                <span class="keyword">return</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            log.info(<span class="string">&quot;触发宕机告警事件:timestamp = &#123;&#125;, result = &#123;&#125;&quot;</span>, System.currentTimeMillis(), result);</span><br><span class="line">            result.setRecover(<span class="keyword">false</span>);</span><br><span class="line">            out.collect(result);</span><br><span class="line">            ctx.timerService().registerEventTimeTimer(timestamp + delay);</span><br><span class="line">            result.setCounter(result.getCounter() + <span class="number">1</span>);</span><br><span class="line">            result.setSystemTimestamp(timestamp);</span><br><span class="line">            outageMetricState.update(result);</span><br><span class="line">            recover.update(<span class="keyword">false</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在 processElement 方法中不断的处理数据，在处理的时候会从状态中获取看之前状态是否存在数据，在该方法内部最后通过 <code>ctx.timerService().registerEventTimeTimer(current.getSystemTimestamp() + delay);</code> 去注册一个事件时间的定时器，时间戳是当前的系统时间加上 delay 的时间。</p><p>在 onTimer 方法中就是具体的定时器，在定时器中获取到状态值，然后将状态值中的时间与 delay 的时间差是否满足，如果满足则表示一直没有数据过来，接着对比目前告警的数量与定义的限制数量，如果大于则不告警了，如果小于则表示触发了宕机告警并且打印相关的日志，然后更新状态中的值。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onTimer</span><span class="params">(<span class="keyword">long</span> timestamp, OnTimerContext ctx, Collector&lt;OutageMetricEvent&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    OutageMetricEvent result = outageMetricState.value();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (result != <span class="keyword">null</span> &amp;&amp; timestamp &gt;= result.getSystemTimestamp() + delay &amp;&amp; System.currentTimeMillis() - result.getTimestamp() &gt;= delay) &#123;</span><br><span class="line">        <span class="keyword">if</span> (result.getCounter() &gt; alertCountLimit) &#123;</span><br><span class="line">            log.info(<span class="string">&quot;宕机告警次数大于:&#123;&#125; :&#123;&#125;&quot;</span>, alertCountLimit, result);</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        log.info(<span class="string">&quot;触发宕机告警事件:timestamp = &#123;&#125;, result = &#123;&#125;&quot;</span>, System.currentTimeMillis(), result);</span><br><span class="line">        result.setRecover(<span class="keyword">false</span>);</span><br><span class="line">        out.collect(result);</span><br><span class="line">        ctx.timerService().registerEventTimeTimer(timestamp + delay);</span><br><span class="line">        result.setCounter(result.getCounter() + <span class="number">1</span>);</span><br><span class="line">        result.setSystemTimestamp(timestamp);</span><br><span class="line">        outageMetricState.update(result);</span><br><span class="line">        recover.update(<span class="keyword">false</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这样就完成了告警事件的判断了，接下来的算子就可以将告警事件转换成告警消息，然后将告警消息发送到下游去通知。那么就这样可以完成一个机器宕机告警的需求。</p>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
            <tag> Flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink Job 并行度设置</title>
      <link href="2019/12/30/Flink-Job-%E5%B9%B6%E8%A1%8C%E5%BA%A6%E8%AE%BE%E7%BD%AE/"/>
      <url>2019/12/30/Flink-Job-%E5%B9%B6%E8%A1%8C%E5%BA%A6%E8%AE%BE%E7%BD%AE/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p>生产环境如果 Job 突然消费不及时了，或者 Job 就根本不在消费数据了，那么该怎么办？首先得看下相关的监控查看 Job 是否在正常运行，是否出现反压的情况，是否这会生产数据量过大然而并行度却是根据之前数据量设置的，种种原因都需要一个个排查一下，然后找到根因才能够对应的去解决。这节来讲解下遇到这种问题后如何合理配置并行度呢？</p><a id="more"></a><h3 id="Source-端并行度的配置"><a href="#Source-端并行度的配置" class="headerlink" title="Source 端并行度的配置"></a>Source 端并行度的配置</h3><p>假设数据源端是 Kafka，在出现作业消费不及时的时候，首先看下 Kafka 的监控是不是现在生产者生产的数据上涨速度较快，从而导致作业目前的消费速度就是跟不上 Kafka 生产者的生产速度，如果是这样的话，那么就得查看作业的并行度和 Kafka 的分区数是否一致，如果小于 Kafka 的分区数，那么可以增大并行度至 Kafka 的分区数，然后再观察作业消费速度是否可以跟上数据生产速度；如果已经等于 Kafka 的分区数了，那得考虑下是否 Kafka 要扩大分区，但是这样可能会带来 Kafka 其他的问题，这个操作需要谨慎。</p><p>Kafka 中数据出现堆积的话，还可以分析下数据的类型，如果数据不重要，但是又要保证数据的及时性，可以修改作业让作业始终从最新的数据开始消费，丢弃之前堆积的数据，这样就可以保证数据的及时性。举个例子，假如一个实时告警作业它突然消费不及时，Kafka 中堆积了几亿条数据（数据延迟几个小时），那么如果作业调高并行度重启后，它还是从上一次提交的 offset 处开始消费的话，这样告警作业即使现在消费速度跟的上了，但是它要处理掉之前堆积的几亿条数据也是要一段时间的，那么就意味着这个作业仍将有段时间处于 ‘不可用’。因为即使判断出来要告警，可能这个告警信息的原数据已经是几个小时前的了，没准这个告警此时已经恢复了，但是还发出来告警这就意味着延迟性比较大，还会对告警消息接收者造成一定的干扰，所以这种场景下建议重启作业就直接开始从最新的数据开始消费。当然不同的场景可能不一样，如果金融行业的交易数据，那么是肯定不能允许这样丢数据的，即使堆积了，也要慢慢的去消费堆积的数据，直到后面追平至最新的数据。</p><p>在 Source 端设置并行度的话，如果数据源是 Kafka 的话，建议并行度不要超过 Kafka 的分区数，因为一个并行度会去处理一至多个分区的数据，如果设置过多的话，会出现部分并行度空闲。如果是其他的数据源，可以根据实际情况合理增大并行度来提高作业的处理数据能力。</p><h3 id="中间-Operator-并行度的配置"><a href="#中间-Operator-并行度的配置" class="headerlink" title="中间 Operator 并行度的配置"></a>中间 Operator 并行度的配置</h3><p>数据从 Source 端流入后，通常会进行一定的数据转换、聚合才能够满足需求，在数据转换中可能会和第三方系统进行交互，在交互的过程中可能会因为网络原因或者第三方服务原因导致有一定的延迟，从而导致这个数据交互的算子处理数据的吞吐量会降低，可能会造成反压，从而会影响上游的算子的消费。那么在这种情况下这些与第三方系统有交互的算子得稍微提高并行度，防止出现这种反压问题（当然反压问题不一定就这样可以解决，具体如何处理参见 9.1 节）。</p><p>除了这种与第三方服务做交互的外，另外可能的性能瓶颈也会出现在这类算子中，比如你 Kafka 过来的数据是 JSON 串的 String，然后需要转换成对象，在大数据量的情况下这个转换也是比较耗费性能的。</p><p>所以数据转换中间过程的算子也是非常重要的，如果哪一步算子的并行度设置的不合理，可能就会造成各种各样的问题出现。</p><h3 id="Sink-端并行度的配置"><a href="#Sink-端并行度的配置" class="headerlink" title="Sink 端并行度的配置"></a>Sink 端并行度的配置</h3><p>Sink 端是数据流向下游的地方，可以根据 Sink 端的数据量进行评估，可能有的作业是 Source 端的数据量最大，然后数据量不断的变少，最后到 Sink 端的数据就一点点了，比较常见的就是监控告警的场景。Source 端的数据是海量的，但是通过逐层的过滤和转换，到最后判断要告警的数据其实已经减少很多很多了，那么在最后的这个地方就可以将并行度设置的小一些。</p><p>当然也可能会有这样的情况，在 Source 端的数据量是最小的，拿到 Source 端流过来的数据后做了细粒度的拆分，那么数据量就不断的增加了，到 Sink 端的数据量就非常非常的大了。那么在 Sink 到下游的存储中间件的时候就需要提高并行度。</p><p>另外 Sink 端也是要与下游的服务进行交互，并行度还得根据下游的服务抗压能力来设置，如果在 Flink Sink 这端的数据量过大的话，然后在 Sink 处并行度也设置的很大，但是下游的服务完全撑不住这么大的并发写入，也是可能会造成下游服务直接被写挂的，下游服务可能还要对外提供一些其他的服务，如果稳定性不能保证的话，会造成很大的影响，所以最终还是要在 Sink 处的并行度做一定的权衡。</p><h3 id="Operator-Chain"><a href="#Operator-Chain" class="headerlink" title="Operator Chain"></a>Operator Chain</h3><p>对于一般的作业（无特殊耗性能处），可以尽量让算子的并行度从 Source 端到 Sink 端都保持一致，这样可以尽可能的让 Job 中的算子进行 chain 在一起，形成链，数据在链中可以直接传输，而不需要再次进行序列化与反序列化，这样带来的性能消耗就会得到降低。在 9.2 节中具体讲解了算子 chain 在一起的条件，忘记的话可以去回顾一下。</p>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Flink Side Output 分流</title>
      <link href="2019/12/28/Flink-Side-Output-%E5%88%86%E6%B5%81/"/>
      <url>2019/12/28/Flink-Side-Output-%E5%88%86%E6%B5%81/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p>通常，在 Kafka 的 topic 中会有很多数据，这些数据虽然结构是一致的，但是类型可能不一致，举个例子：Kafka 中的监控数据有很多种：机器、容器、应用、中间件等，如果要对这些数据分别处理，就需要对这些数据流进行一个拆分。</p><a id="more"></a><h3 id="使用-Filter-分流"><a href="#使用-Filter-分流" class="headerlink" title="使用 Filter 分流"></a>使用 Filter 分流</h3><p>使用 filter 算子根据数据的字段进行过滤分成机器、容器、应用、中间件等。伪代码如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">DataStreamSource&lt;MetricEvent&gt; data = KafkaConfigUtil.buildSource(env);  <span class="comment">//从 Kafka 获取到所有的数据流</span></span><br><span class="line">SingleOutputStreamOperator&lt;MetricEvent&gt; machineData = data.filter(m -&gt; <span class="string">&quot;machine&quot;</span>.equals(m.getTags().get(<span class="string">&quot;type&quot;</span>)));  <span class="comment">//过滤出机器的数据</span></span><br><span class="line">SingleOutputStreamOperator&lt;MetricEvent&gt; dockerData = data.filter(m -&gt; <span class="string">&quot;docker&quot;</span>.equals(m.getTags().get(<span class="string">&quot;type&quot;</span>)));    <span class="comment">//过滤出容器的数据</span></span><br><span class="line">SingleOutputStreamOperator&lt;MetricEvent&gt; applicationData = data.filter(m -&gt; <span class="string">&quot;application&quot;</span>.equals(m.getTags().get(<span class="string">&quot;type&quot;</span>)));  <span class="comment">//过滤出应用的数据</span></span><br><span class="line">SingleOutputStreamOperator&lt;MetricEvent&gt; middlewareData = data.filter(m -&gt; <span class="string">&quot;middleware&quot;</span>.equals(m.getTags().get(<span class="string">&quot;type&quot;</span>)));    <span class="comment">//过滤出中间件的数据</span></span><br></pre></td></tr></table></figure><h3 id="使用-Split-分流"><a href="#使用-Split-分流" class="headerlink" title="使用 Split 分流"></a>使用 Split 分流</h3><p>先在 split 算子里面定义 OutputSelector 的匿名内部构造类，然后重写 select 方法，根据数据的类型将不同的数据放到不同的 tag 里面，这样返回后的数据格式是 SplitStream，然后要使用这些数据的时候，可以通过 select 去选择对应的数据类型，伪代码如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">DataStreamSource&lt;MetricEvent&gt; data = KafkaConfigUtil.buildSource(env);  <span class="comment">//从 Kafka 获取到所有的数据流</span></span><br><span class="line">SplitStream&lt;MetricEvent&gt; splitData = data.split(<span class="keyword">new</span> OutputSelector&lt;MetricEvent&gt;() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Iterable&lt;String&gt; <span class="title">select</span><span class="params">(MetricEvent metricEvent)</span> </span>&#123;</span><br><span class="line">        List&lt;String&gt; tags = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">        String type = metricEvent.getTags().get(<span class="string">&quot;type&quot;</span>);</span><br><span class="line">        <span class="keyword">switch</span> (type) &#123;</span><br><span class="line">            <span class="keyword">case</span> <span class="string">&quot;machine&quot;</span>:</span><br><span class="line">                tags.add(<span class="string">&quot;machine&quot;</span>);</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            <span class="keyword">case</span> <span class="string">&quot;docker&quot;</span>:</span><br><span class="line">                tags.add(<span class="string">&quot;docker&quot;</span>);</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            <span class="keyword">case</span> <span class="string">&quot;application&quot;</span>:</span><br><span class="line">                tags.add(<span class="string">&quot;application&quot;</span>);</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            <span class="keyword">case</span> <span class="string">&quot;middleware&quot;</span>:</span><br><span class="line">                tags.add(<span class="string">&quot;middleware&quot;</span>);</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            <span class="keyword">default</span>:</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> tags;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">DataStream&lt;MetricEvent&gt; machine = splitData.select(<span class="string">&quot;machine&quot;</span>);</span><br><span class="line">DataStream&lt;MetricEvent&gt; docker = splitData.select(<span class="string">&quot;docker&quot;</span>);</span><br><span class="line">DataStream&lt;MetricEvent&gt; application = splitData.select(<span class="string">&quot;application&quot;</span>);</span><br><span class="line">DataStream&lt;MetricEvent&gt; middleware = splitData.select(<span class="string">&quot;middleware&quot;</span>);</span><br></pre></td></tr></table></figure><p>上面这种只分流一次是没有问题的，注意如果要使用它来做连续的分流，那是有问题的，笔者曾经就遇到过这个问题，当时记录了博客 —— <a href="http://www.54tianzhisheng.cn/2019/06/12/flink-split/">Flink 从0到1学习—— Flink 不可以连续 Split(分流)？</a> ，当时排查这个问题还查到两个相关的 Flink Issue。</p><ul><li><a href="https://issues.apache.org/jira/browse/FLINK-5031">https://issues.apache.org/jira/browse/FLINK-5031</a></li><li><a href="https://issues.apache.org/jira/browse/FLINK-11084">https://issues.apache.org/jira/browse/FLINK-11084</a></li></ul><p>这两个 Issue 反映的就是连续 split 不起作用，在第二个 Issue 下面的评论就有回复说 Side Output 的功能比 split 更强大， split 会在后面的版本移除（其实在 1.7.x 版本就已经设置为过期），那么下面就来学习一下 Side Output。</p><h3 id="使用-Side-Output-分流"><a href="#使用-Side-Output-分流" class="headerlink" title="使用 Side Output 分流"></a>使用 Side Output 分流</h3><p>要使用 Side Output 的话，你首先需要做的是定义一个 OutputTag 来标识 Side Output，代表这个 Tag 是要收集哪种类型的数据，如果是要收集多种不一样类型的数据，那么你就需要定义多种 OutputTag。要完成本节前面的需求，需要定义 4 个 OutputTag，如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//创建 output tag</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> OutputTag&lt;MetricEvent&gt; machineTag = <span class="keyword">new</span> OutputTag&lt;MetricEvent&gt;(<span class="string">&quot;machine&quot;</span>) &#123;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> OutputTag&lt;MetricEvent&gt; dockerTag = <span class="keyword">new</span> OutputTag&lt;MetricEvent&gt;(<span class="string">&quot;docker&quot;</span>) &#123;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> OutputTag&lt;MetricEvent&gt; applicationTag = <span class="keyword">new</span> OutputTag&lt;MetricEvent&gt;(<span class="string">&quot;application&quot;</span>) &#123;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> OutputTag&lt;MetricEvent&gt; middlewareTag = <span class="keyword">new</span> OutputTag&lt;MetricEvent&gt;(<span class="string">&quot;middleware&quot;</span>) &#123;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>定义好 OutputTag 后，可以使用下面几种函数来处理数据：</p><ul><li>ProcessFunction</li><li>KeyedProcessFunction</li><li>CoProcessFunction</li><li>ProcessWindowFunction</li><li>ProcessAllWindowFunction</li></ul><p>在利用上面的函数处理数据的过程中，需要对数据进行判断，将不同种类型的数据存到不同的 OutputTag 中去，如下代码所示：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">DataStreamSource&lt;MetricEvent&gt; data = KafkaConfigUtil.buildSource(env);  <span class="comment">//从 Kafka 获取到所有的数据流</span></span><br><span class="line">SingleOutputStreamOperator&lt;MetricEvent&gt; sideOutputData = data.process(<span class="keyword">new</span> ProcessFunction&lt;MetricEvent, MetricEvent&gt;() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">processElement</span><span class="params">(MetricEvent metricEvent, Context context, Collector&lt;MetricEvent&gt; collector)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        String type = metricEvent.getTags().get(<span class="string">&quot;type&quot;</span>);</span><br><span class="line">        <span class="keyword">switch</span> (type) &#123;</span><br><span class="line">            <span class="keyword">case</span> <span class="string">&quot;machine&quot;</span>:</span><br><span class="line">                context.output(machineTag, metricEvent);</span><br><span class="line">            <span class="keyword">case</span> <span class="string">&quot;docker&quot;</span>:</span><br><span class="line">                context.output(dockerTag, metricEvent);</span><br><span class="line">            <span class="keyword">case</span> <span class="string">&quot;application&quot;</span>:</span><br><span class="line">                context.output(applicationTag, metricEvent);</span><br><span class="line">            <span class="keyword">case</span> <span class="string">&quot;middleware&quot;</span>:</span><br><span class="line">                context.output(middlewareTag, metricEvent);</span><br><span class="line">            <span class="keyword">default</span>:</span><br><span class="line">                collector.collect(metricEvent);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure><p>好了，既然上面已经将不同类型的数据放到不同的 OutputTag 里面了，那么该如何去获取呢？可以使用 getSideOutput 方法来获取不同 OutputTag 的数据，比如：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">DataStream&lt;MetricEvent&gt; machine = sideOutputData.getSideOutput(machineTag);</span><br><span class="line">DataStream&lt;MetricEvent&gt; docker = sideOutputData.getSideOutput(dockerTag);</span><br><span class="line">DataStream&lt;MetricEvent&gt; application = sideOutputData.getSideOutput(applicationTag);</span><br><span class="line">DataStream&lt;MetricEvent&gt; middleware = sideOutputData.getSideOutput(middlewareTag);</span><br></pre></td></tr></table></figure><p>这样你就可以获取到 Side Output 数据了，其实在 3.4 和 3.5 节就讲了 Side Output 在 Flink 中的应用（处理窗口的延迟数据），大家如果没有印象了可以再返回去复习一下。</p>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Flink Checkpoint 和 Savepoint 区别</title>
      <link href="2019/12/28/Flink-Checkpoint-%E5%92%8C-Savepoint-%E5%8C%BA%E5%88%AB/"/>
      <url>2019/12/28/Flink-Checkpoint-%E5%92%8C-Savepoint-%E5%8C%BA%E5%88%AB/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p>Checkpoint 在 Flink 中是一个非常重要的 Feature，Checkpoint 使 Flink 的状态具有良好的容错性，通过 Checkpoint 机制，Flink 可以对作业的状态和计算位置进行恢复。</p><a id="more"></a><h3 id="Checkpoint-简介"><a href="#Checkpoint-简介" class="headerlink" title="Checkpoint 简介"></a>Checkpoint 简介</h3><p>为了保障的容错，Flink 需要对状态进行快照。Flink 可以从 Checkpoint 中恢复流的状态和位置，从而使得应用程序发生故障后能够得到与无故障执行相同的语义。</p><p>Flink 的 Checkpoint 有以下先决条件：</p><ul><li><p><strong>需要具有持久性且支持重放一定时间范围内数据的数据源。</strong></p><blockquote><p>例如：Kafka、RabbitMQ 等。这里为什么要求支持重放一定时间范围内的数据呢？因为 Flink 的容错机制决定了，当 Flink 任务失败后会自动从最近一次成功的 Checkpoint 处恢复任务，此时可能需要把任务失败前消费的部分数据再消费一遍，所以必须要求数据源支持重放。假如一个Flink 任务消费 Kafka 并将数据写入到 MySQL 中，任务从 Kafka 读取到数据，还未将数据输出到 MySQL 时任务突然失败了，此时如果 Kafka 不支持重放，就会造成这部分数据永远丢失了。支持重放数据的数据源可以保障任务消费失败后，能够重新消费来保障任务不丢数据。</p></blockquote></li><li><p><strong>需要一个能保存状态的持久化存储介质</strong></p><blockquote><p>例如：HDFS、S3 等。当 Flink 任务失败后，自动从 Checkpoint 处恢复，但是如果 Checkpoint 时保存的状态信息快照全丢了，那就会影响 Flink 任务的正常恢复。就好比我们看书时经常使用书签来记录当前看到的页码，当下次看书时找到书签的位置继续阅读即可，但是如果书签三天两头经常丢，那我们就无法通过书签来恢复阅读。</p></blockquote></li></ul><p>Flink 中 Checkpoint 是默认关闭的，对于需要保障 At Least Once 和 Exactly Once 语义的任务，强烈建议开启 Checkpoint，对于丢一小部分数据不敏感的任务，可以不开启 Checkpoint，例如：一些推荐相关的任务丢一小部分数据并不会影响推荐效果。</p><hr><h2 id="Checkpoint-使用"><a href="#Checkpoint-使用" class="headerlink" title="Checkpoint 使用"></a><strong>Checkpoint 使用</strong></h2><p>首先调用 StreamExecutionEnvironment 的方法 enableCheckpointing(n) 来开启 Checkpoint，参数 n 以毫秒为单位表示 Checkpoint 的时间间隔。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"><span class="comment">// 开启 Checkpoint，每 1000毫秒进行一次 Checkpoint</span></span><br><span class="line">env.enableCheckpointing(<span class="number">1000</span>);</span><br><span class="line"><span class="comment">// Checkpoint 语义设置为 EXACTLY_ONCE</span></span><br><span class="line">env.getCheckpointConfig().setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE);</span><br><span class="line"><span class="comment">// CheckPoint 的超时时间</span></span><br><span class="line">env.getCheckpointConfig().setCheckpointTimeout(<span class="number">60000</span>);</span><br><span class="line"><span class="comment">// 同一时间，只允许 有 1 个 Checkpoint 在发生</span></span><br><span class="line">env.getCheckpointConfig().setMaxConcurrentCheckpoints(<span class="number">1</span>);</span><br><span class="line"><span class="comment">// 两次 Checkpoint 之间的最小时间间隔为 500 毫秒</span></span><br><span class="line">env.getCheckpointConfig().setMinPauseBetweenCheckpoints(<span class="number">500</span>);</span><br><span class="line"><span class="comment">// 当 Flink 任务取消时，保留外部保存的 CheckPoint 信息</span></span><br><span class="line">env.getCheckpointConfig().enableExternalizedCheckpoints(ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// 当有较新的 Savepoint 时，作业也会从 Checkpoint 处恢复</span></span><br><span class="line">env.getCheckpointConfig().setPreferCheckpointForRecovery(<span class="keyword">true</span>);</span><br><span class="line"><span class="comment">// 作业最多允许 Checkpoint 失败 1 次（flink 1.9 开始支持）</span></span><br><span class="line">env.getCheckpointConfig().setTolerableCheckpointFailureNumber(<span class="number">1</span>);</span><br><span class="line"><span class="comment">// Checkpoint 失败后，整个 Flink 任务也会失败（flink 1.9 之前）</span></span><br><span class="line">env.getCheckpointConfig.setFailTasksOnCheckpointingErrors(<span class="keyword">true</span>)</span><br></pre></td></tr></table></figure><p>以上 Checkpoint 相关的参数描述如下所示：</p><ul><li>Checkpoint 语义：EXACTLY<em>ONCE 或 AT</em>LEAST<em>ONCE，EXACTLY</em>ONCE 表示所有要消费的数据被恰好处理一次，即所有数据既不丢数据也不重复消费；AT<em>LEAST</em>ONCE 表示要消费的数据至少处理一次，可能会重复消费。</li><li>Checkpoint 超时时间：如果 Checkpoint 时间超过了设定的超时时间，则 Checkpoint 将会被终止。</li><li>同时进行的 Checkpoint 数量：默认情况下，当一个 Checkpoint 在进行时，JobManager 将不会触发下一个 Checkpoint，但 Flink 允许多个 Checkpoint 同时在发生。</li><li>两次 Checkpoint 之间的最小时间间隔：从上一次 Checkpoint 结束到下一次 Checkpoint 开始，中间的间隔时间。例如，env.enableCheckpointing(60000) 表示 1 分钟触发一次 Checkpoint，同时再设置两次 Checkpoint 之间的最小时间间隔为 30 秒，假如任务运行过程中一次 Checkpoint 就用了50s，那么等 Checkpoint 结束后，理论来讲再过 10s 就要开始下一次 Checkpoint 了，但是由于设置了最小时间间隔为30s，所以需要再过 30s 后，下次 Checkpoint 才开始。注：如果配置了该参数就决定了同时进行的 Checkpoint 数量只能为 1。</li><li>当任务被取消时，外部 Checkpoint 信息是否被清理：Checkpoint 在默认的情况下仅用于恢复运行失败的 Flink 任务，当任务手动取消时 Checkpoint 产生的状态信息并不保留。当然可以通过该配置来保留外部的 Checkpoint 状态信息，这些被保留的状态信息在作业手动取消时不会被清除，这样就可以使用该状态信息来恢复 Flink 任务，对于需要从状态恢复的任务强烈建议配置为外部 Checkpoint 状态信息不清理。可选择的配置项为：</li><li>ExternalizedCheckpointCleanup.RETAIN<em>ON</em>CANCELLATION：当作业手动取消时，保留作业的 Checkpoint 状态信息。注意，这种情况下，需要手动清除该作业保留的 Checkpoint 状态信息，否则这些状态信息将永远保留在外部的持久化存储中。</li><li>ExternalizedCheckpointCleanup.DELETE<em>ON</em>CANCELLATION：当作业取消时，Checkpoint 状态信息会被删除。仅当作业失败时，作业的 Checkpoint 才会被保留用于任务恢复。</li><li>任务失败，当有较新的 Savepoint 时，作业是否回退到 Checkpoint 进行恢复：默认情况下，当 Savepoint 比 Checkpoint 较新时，任务会从 Savepoint 处恢复。</li><li>作业可以容忍 Checkpoint 失败的次数：默认值为 0，表示不能接受 Checkpoint 失败。</li></ul><p>关于 Checkpoint 时，状态后端相关的配置请参阅本课 4.2 节。</p><h3 id="Savepoint-介绍、Savepoint-与-Checkpoint-的区别及使用"><a href="#Savepoint-介绍、Savepoint-与-Checkpoint-的区别及使用" class="headerlink" title="Savepoint 介绍、Savepoint 与 Checkpoint 的区别及使用"></a>Savepoint 介绍、Savepoint 与 Checkpoint 的区别及使用</h3><p>Savepoint 与 Checkpoint 类似，同样需要把状态信息存储到外部介质，当作业失败时，可以从外部存储中恢复。Savepoint 与 Checkpoint 的区别很多：</p><table><thead><tr><th align="center">Checkpoint</th><th align="center">Savepoint</th></tr></thead><tbody><tr><td align="center">由 Flink 的 JobManager 定时自动触发并管理</td><td align="center">由用户手动触发并管理</td></tr><tr><td align="center">主要用于任务发生故障时，为任务提供给自动恢复机制</td><td align="center">主要用于升级 Flink 版本、修改任务的逻辑代码、调整算子的并行度，且必须手动恢复</td></tr><tr><td align="center">当使用 RocksDBStateBackend 时，支持增量方式对状态信息进行快照</td><td align="center">仅支持全量快照</td></tr><tr><td align="center">Flink 任务停止后，Checkpoint 的状态快照信息默认被清除</td><td align="center">一旦触发 Savepoint，状态信息就被持久化到外部存储，除非用户手动删除</td></tr><tr><td align="center">Checkpoint 设计目标：轻量级且尽可能快地恢复任务</td><td align="center">Savepoint 的生成和恢复成本会更高一些，Savepoint 更多地关注代码的可移植性和兼容任务的更改操作</td></tr></tbody></table><p>除了上述描述外，Checkpoint 和 Savepoint 在当前的实现上基本相同。</p><p>强烈建议在程序中给算子分配 Operator ID，以便来升级程序。主要通过 <code>uid(String)</code> 方法手动指定算子的 ID ，这些 ID 将用于恢复每个算子的状态。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">DataStream&lt;String&gt; stream = env.</span><br><span class="line">  <span class="comment">// Stateful source (e.g. Kafka) with ID</span></span><br><span class="line">  .addSource(<span class="keyword">new</span> StatefulSource())</span><br><span class="line">  .uid(<span class="string">&quot;source-id&quot;</span>) <span class="comment">// ID for the source operator</span></span><br><span class="line">  .shuffle()</span><br><span class="line">  <span class="comment">// Stateful mapper with ID</span></span><br><span class="line">  .map(<span class="keyword">new</span> StatefulMapper())</span><br><span class="line">  .uid(<span class="string">&quot;mapper-id&quot;</span>) <span class="comment">// ID for the mapper</span></span><br><span class="line">  <span class="comment">// Stateless printing sink</span></span><br><span class="line">  .print(); <span class="comment">// Auto-generated ID</span></span><br></pre></td></tr></table></figure><p>如果不为算子手动指定 ID，Flink 会为算子自动生成 ID。当 Flink 任务从 Savepoint 中恢复时，是按照 Operator ID 将快照信息与算子进行匹配的，只要这些 ID 不变，Flink 任务就可以从 Savepoint 中恢复。自动生成的 ID 取决于代码的结构，并且对代码更改比较敏感，因此强烈建议给程序中所有有状态的算子手动分配 Operator ID。如下左图所示，一个 Flink 任务包含了 算子 A 和 算子 B，代码中都未指定 Operator ID，所以 Flink 为 Task A 自动生成了 Operator ID 为 aaa，为 Task B 自动生成了 Operator ID 为 bbb，且 Savepoint 成功完成。但是在代码改动后，任务并不能从 Savepoint 中正常恢复，因为 Flink 为算子生成的 Operator ID 取决于代码结构，代码改动后可能会把算子 B 的 Operator ID 改变成 ccc，导致任务从 Savepoint 恢复时，SavePoint 中只有 Operator ID 为 aaa 和 bbb 的状态信息，算子 B 找不到 Operator ID 为 ccc 的状态信息，所以算子 B 不能正常恢复。这里如果在写代码时通过 <code>uid(String)</code> 手动指定了 Operator ID，就不会存在 上述问题了。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-19-020528.jpg" alt="img"></p><p>Savepoint 需要用户手动去触发，触发 Savepoint 的方式如下所示：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/flink savepoint :jobId [:targetDirectory]</span><br></pre></td></tr></table></figure><p>这将触发 ID 为 <code>:jobId</code> 的作业进行 Savepoint，并返回创建的 Savepoint 路径，用户需要此路径来还原和删除 Savepoint 。</p><p>使用 YARN 触发 Savepoint 的方式如下所示：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin&#x2F;flink savepoint :jobId [:targetDirectory] -yid :yarnAppId</span><br></pre></td></tr></table></figure><p>这将触发 ID 为 <code>:jobId</code> 和 YARN 应用程序 ID <code>:yarnAppId</code> 的作业进行 Savepoint，并返回创建的 Savepoint 路径。</p><p>使用 Savepoint 取消 Flink 任务：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin&#x2F;flink cancel -s [:targetDirectory] :jobId</span><br></pre></td></tr></table></figure><p>这将自动触发 ID 为 <code>:jobid</code> 的作业进行 Savepoint，并在 Checkpoint 结束后取消该任务。此外，可以指定一个目标文件系统目录来存储 Savepoint 的状态信息，也可以在 flink 的 conf 目录下 flink-conf.yaml 中配置 state.savepoints.dir 参数来指定 Savepoint 的默认目录，触发 Savepoint 时，如果不指定目录则使用该默认目录。无论使用哪种方式配置，都需要保障配置的目录能被所有的 JobManager 和 TaskManager 访问。</p><h3 id="Checkpoint-流程"><a href="#Checkpoint-流程" class="headerlink" title="Checkpoint 流程"></a>Checkpoint 流程</h3><p>Flink 任务 Checkpoint 的详细流程如下所示：</p><ol><li>JobManager 端的 CheckPointCoordinator 会定期向所有 SourceTask 发送 CheckPointTrigger，Source Task 会在数据流中安插 Checkpoint barrier</li></ol><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-11-06-021819.png" alt="img"></p><ol start="2"><li>当 task 收到上游所有实例的 barrier 后，向自己的下游继续传递 barrier，然后自身同步进行快照，并将自己的状态异步写入到持久化存储中</li></ol><ul><li>如果是增量 Checkpoint，则只是把最新的一部分更新写入到外部持久化存储中</li><li>为了下游尽快进行 Checkpoint，所以 task 会先发送 barrier 到下游，自身再同步进行快照</li></ul><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-11-06-021846.png" alt="img"></p><blockquote><p>注：Task B 必须接收到上游 Task A 所有实例发送的 barrier 时，Task B 才能开始进行快照，这里有一个 barrier 对齐的概念，关于 barrier 对齐的详细介绍请参阅 9.5.1 节 Flink 内部如何保证 Exactly Once 中的 barrier 对齐部分</p></blockquote><ol start="3"><li><p>当 task 将状态信息完成备份后，会将备份数据的地址（state handle）通知给 JobManager 的CheckPointCoordinator，如果 Checkpoint 的持续时长超过了 Checkpoint 设定的超时时间CheckPointCoordinator 还没有收集完所有的 State Handle，CheckPointCoordinator 就会认为本次 Checkpoint 失败，会把这次 Checkpoint 产生的所有状态数据全部删除</p></li><li><p>如果 CheckPointCoordinator 收集完所有算子的 State Handle，CheckPointCoordinator 会把整个 StateHandle 封装成 completed Checkpoint Meta，写入到外部存储中，Checkpoint 结束</p></li></ol><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-11-06-021900.png" alt="img"></p><h3 id="基于-RocksDB-的增量-Checkpoint-实现原理"><a href="#基于-RocksDB-的增量-Checkpoint-实现原理" class="headerlink" title="基于 RocksDB 的增量 Checkpoint 实现原理"></a>基于 RocksDB 的增量 Checkpoint 实现原理</h3><p>当使用 RocksDBStateBackend 时，增量 Checkpoint 是如何实现的呢？</p><p>RocksDB 是一个基于 LSM 实现的 KV 数据库。LSM 全称 Log Structured Merge Trees，LSM 树本质是将大量的磁盘随机写操作转换成磁盘的批量写操作来极大地提升磁盘数据写入效率。一般 LSM Tree 实现上都会有一个基于内存的 MemTable 介质，所有的增删改操作都是写入到 MemTable 中，当 MemTable 足够大以后，将 MemTable 中的数据 flush 到磁盘中生成不可变且内部有序的 ssTable（Sorted String Table）文件，全量数据保存在磁盘的多个 ssTable 文件中。HBase 也是基于 LSM Tree 实现的，HBase 磁盘上的 HFile 就相当于这里的 ssTable 文件，每次生成的 HFile 都是不可变的而且内部有序的文件。基于 ssTable 不可变的特性，才实现了增量 Checkpoint，具体流程如下所示：</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-11-06-021910.png" alt="img"></p><p>第一次 Checkpoint 时生成的状态快照信息包含了两个 sstable 文件：sstable1 和 sstable2 及 Checkpoint1 的元数据文件 MANIFEST-chk1，所以第一次 Checkpoint 时需要将 sstable1、sstable2 和 MANIFEST-chk1 上传到外部持久化存储中。第二次 Checkpoint 时生成的快照信息为 sstable1、sstable2、sstable3 及元数据文件 MANIFEST-chk2，由于 sstable 文件的不可变特性，所以状态快照信息的 sstable1、sstable2 这两个文件并没有发生变化，sstable1、sstable2 这两个文件不需要重复上传到外部持久化存储中，因此第二次 Checkpoint 时，只需要将 sstable3 和 MANIFEST-chk2 文件上传到外部持久化存储中即可。这里只将新增的文件上传到外部持久化存储，也就是所谓的增量 Checkpoint。</p><p>基于 LSM Tree 实现的数据库为了提高查询效率，都需要定期对磁盘上多个 sstable 文件进行合并操作，合并时会将删除的、过期的以及旧版本的数据进行清理，从而降低 sstable 文件的总大小。图中可以看到第三次 Checkpoint 时生成的快照信息为sstable3、sstable4、sstable5 及元数据文件 MANIFEST-chk3， 其中新增了 sstable4 文件且 sstable1 和 sstable2 文件合并成 sstable5 文件，因此第三次 Checkpoint 时只需要向外部持久化存储上传 sstable4、sstable5 及元数据文件 MANIFEST-chk3。</p><p>基于 RocksDB 的增量 Checkpoint 从本质上来讲每次 Checkpoint 时只将本次 Checkpoint 新增的快照信息上传到外部的持久化存储中，依靠的是 LSM Tree 中 sstable 文件不可变的特性。对 LSM Tree 感兴趣的同学可以深入研究 RocksDB 或 HBase 相关原理及实现。</p><h3 id="状态如何从-Checkpoint-恢复"><a href="#状态如何从-Checkpoint-恢复" class="headerlink" title="状态如何从 Checkpoint 恢复"></a>状态如何从 Checkpoint 恢复</h3><p>在 Checkpoint 和 Savepoint 的比较过程中，知道了相比 Savepoint 而言，Checkpoint 的成本更低一些，但有些场景 Checkpoint 并不能完全满足我们的需求。所以在使用过程中，如果我们的需求能使用 Checkpoint 来解决优先使用 Checkpoint。当 Flink 任务中的一些依赖组件需要升级重启时，例如 hdfs、Kafka、yarn 升级或者 Flink 任务的 Sink 端对应的 MySQL、Redis 由于某些原因需要重启时，Flink 任务在这段时间也需要重启。但是由于 Flink 任务的代码并没有修改，所以 Flink 任务启动时可以从 Checkpoint 处恢复任务，此时必须配置取消 Flink 任务时保留外部存储的 Checkpoint 状态信息。从 Checkpoint 处恢复任务的命令如下所示，checkpointMetaDataPath 表示 Checkpoint 的目录。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/flink run -s :checkpointMetaDataPath xxx.jar [:runArgs]</span><br></pre></td></tr></table></figure><p>如果 flink on yarn 模式，启动命令如下所示：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/flink run -s :checkpointMetaDataPath -yid :yarnAppId xxx.jar [:runArgs]</span><br></pre></td></tr></table></figure><p>问题来了，Flink 自动维护 Checkpoint，所以用户在这里并拿不到任务取消之前最后一次 Checkpoint 的目录。那怎么办呢？如下图所示，在任务取消之前，Flink 任务的 WebUI 中可以看到 Checkpoint 的目录，可以在取消任务之前将此目录保存起来，恢复时就可以从该目录恢复任务。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-19-020530.jpg" alt="img"></p><p>上述方法最大缺陷就是用户的人力成本太高了，假如需要重启 100 个任务，难道需要用户手动维护 100 个任务的 Checkpoint 目录吗？可以做一个简单后台项目，用于管理和发布 Flink 任务，这里讲述一种通过 rest api 来获取 Checkpoint 目录的方式。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-19-20531.jpg" alt="img"></p><p>如上图所示是 Flink JobManager 的 overview 页面，只需要将端口号后面的路径和参数按照以下替换即可：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http:&#x2F;&#x2F;node107.bigdata.dmp.local.com:35524&#x2F;jobs&#x2F;a1c70b36d19b3a9fc2713ba98cfc4a4f&#x2F;metrics?get&#x3D;lastCheckpointExternalPath</span><br></pre></td></tr></table></figure><p>调用以上接口，即可返回 a1c70b36d19b3a9fc2713ba98cfc4a4f 对应的 job 最后一次 Checkpoint 的目录，返回格式如下所示。</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[</span><br><span class="line">  &#123;</span><br><span class="line">    <span class="attr">&quot;id&quot;</span>: <span class="string">&quot;lastCheckpointExternalPath&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;value&quot;</span>: <span class="string">&quot;hdfs:/user/flink/checkpoints/a1c70b36d19b3a9fc2713ba98cfc4a4f/chk-18&quot;</span></span><br><span class="line">  &#125;</span><br><span class="line">]</span><br></pre></td></tr></table></figure><p>通过这种方式可以方便地维护所有 Flink 任务的 Checkpoint 目录，当然也可以通过 Metrics 的 Reporter 将 Checkpoint 目录保存到外部存储介质中，当任务需要从 Checkpoint 处恢复时，则从外部存储中读取到相应的 Checkpoint 目录。</p><p>当设置取消 Flink 任务保留外部的 Checkpoint 状态信息时，可能会带来的负面影响是：长期运行下去，hdfs 上将会保留很多废弃的且不再会使用的 Checkpoint 目录，所以如果开启了此配置，需要制定策略，定期清理那些不再会使用到的 Checkpoint 目录。</p><h3 id="状态如何从-Savepoint-恢复"><a href="#状态如何从-Savepoint-恢复" class="headerlink" title="状态如何从 Savepoint 恢复"></a>状态如何从 Savepoint 恢复</h3><p>如下所示，从 Savepoint 恢复任务的命令与 Checkpoint 恢复命令类似，savepointPath 表示 Savepoint 保存的目录，Savepoint 的各种触发方式都会返回 Savepoint 目录。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin&#x2F;flink run -s :savepointPath xxx.jar [:runArgs]</span><br></pre></td></tr></table></figure><p>如果 flink on yarn 模式，启动命令如下所示：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/flink run -s :savepointPath -yid :yarnAppId xxx.jar [:runArgs]</span><br></pre></td></tr></table></figure><p>默认情况下，恢复操作将尝试将 Savepoint 的所有状态映射到要还原的程序。如果删除了算子，则可以通过 <code>--allowNonRestoredState</code>（short：<code>-n</code>）选项跳过那些无法映射到新程序的状态：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin&#x2F;flink run -s :savepointPath -n xxx.jar [:runArgs]</span><br></pre></td></tr></table></figure><p>如果从 Savepoint 恢复时，在任务中添加一个需要状态的新算子，会发生什么？向任务添加新算子时，它将在没有任何状态的情况下进行初始化，Savepoint 中包含每个有状态算子的状态，无状态算子根本不是 Savepoint 的一部分，新算子的行为类似于无状态算子。</p><p>如果在任务中对算子进行重新排序，会发生什么？如果给这些算子分配了 ID，它们将像往常一样恢复。如果没有分配 ID ，则有状态算子自动生成的 ID 很可能在重新排序后发生更改，这将导致无法从之前的 Savepoint 中恢复。</p><p>Savepoint 目录里的状态快照信息，目前不支持移动位置，由于技术原因元数据文件中使用绝对路径来保存数据。如果因为某种原因必须要移动 Savepoint 文件，那么有两种方案来实现：</p><ul><li>使用编辑器修改 Savepoint 的元数据文件信息，将旧路径改为新路径</li><li>可以使用 <code>SavepointV2Serializer</code> 类以编程方式读取、操作和重写元数据文件的新路径</li></ul><p>长期使用 Savepoint 同样要注意清理那些废弃 Savepoint 目录的问题。</p>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Flink Parallelism 和 Slot 深度理解</title>
      <link href="2019/12/28/Flink-Parallelism-%E5%92%8C-Slot-%E6%B7%B1%E5%BA%A6%E7%90%86%E8%A7%A3/"/>
      <url>2019/12/28/Flink-Parallelism-%E5%92%8C-Slot-%E6%B7%B1%E5%BA%A6%E7%90%86%E8%A7%A3/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p>parallelism 是并行的意思，在 Flink 里面代表每个算子的并行度，适当的提高并行度可以大大提高 Job 的执行效率，比如你的 Job 消费 Kafka 数据过慢，适当调大可能就消费正常了。</p><a id="more"></a><p>相信使用过 Flink 的你或多或少遇到过下面这个问题（笔者自己的项目曾经也出现过这样的问题），错误信息如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Caused by: akka.pattern.AskTimeoutException: </span><br><span class="line">Ask timed out on [Actor[akka:&#x2F;&#x2F;flink&#x2F;user&#x2F;taskmanager_0#15608456]] after [10000 ms]. </span><br><span class="line">Sender[null] sent message of type &quot;org.apache.flink.runtime.rpc.messages.LocalRpcInvocation&quot;.</span><br></pre></td></tr></table></figure><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/FkaM6A.jpg" alt="img"></p><p>跟着这问题在 Flink 的 Issue 列表里看到了一个类似的问题：<a href="https://gitbook.cn/gitchat/column/undefined/topic/5db6bf5cf6a6211cb961664b">https://issues.apache.org/jira/browse/FLINK-9056</a><a href="https://issues.apache.org/jira/browse/FLINK-9056">https://issues.apache.org/jira/browse/FLINK-9056</a> ，看下面的评论意思大概就是 TaskManager 的 Slot 数量不足导致的 Job 提交失败，在 Flink 1.63 中已经修复了，变成抛出异常了。</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/p4Tr9Z.jpg" alt="img"></p><p>竟然知道了是因为 Slot 不足的原因了，那么我们就要先了解下 Slot 是什么呢？不过再了解 Slot 之前这里先介绍下 parallelism。</p><h3 id="什么是-Parallelism？"><a href="#什么是-Parallelism？" class="headerlink" title="什么是 Parallelism？"></a>什么是 Parallelism？</h3><p>parallelism 是并行的意思，在 Flink 里面代表每个算子的并行度，适当的提高并行度可以大大提高 Job 的执行效率，比如你的 Job 消费 Kafka 数据过慢，适当调大可能就消费正常了。</p><p>那么在 Flink 中怎么设置并行度呢？</p><h3 id="如何设置-Parallelism？"><a href="#如何设置-Parallelism？" class="headerlink" title="如何设置 Parallelism？"></a>如何设置 Parallelism？</h3><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-06-055925.png" alt="img"></p><p>如上图，在 Flink 配置文件中可以看到默认并行度是 1。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cat flink-conf.yaml | grep parallelism</span><br><span class="line"></span><br><span class="line"># The parallelism used for programs that did not specify and other parallelism.</span><br><span class="line">parallelism.default: 1</span><br></pre></td></tr></table></figure><p>所以如果在你的 Flink Job 里面不设置任何 parallelism 的话，那么它也会有一个默认的 parallelism（默认为 1），那也意味着可以修改这个配置文件的默认并行度来提高 Job 的执行效率。如果是使用命令行启动你的 Flink Job，那么你也可以这样设置并行度(使用 -p n 参数)：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">.&#x2F;bin&#x2F;flink run -p 10 &#x2F;Users&#x2F;zhisheng&#x2F;word-count.jar</span><br></pre></td></tr></table></figure><p>你也可以通过 <code>env.setParallelism(n)</code> 来设置整个程序的并行度：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">StreamExecutionEnvironment env &#x3D; StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">env.setParallelism(10);</span><br></pre></td></tr></table></figure><p>注意：这样设置的并行度是整个程序的并行度，那么后面如果每个算子不单独设置并行度覆盖的话，那么后面每个算子的并行度就都是以这里设置的并行度为准了。如何给每个算子单独设置并行度呢？</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">data.keyBy(<span class="keyword">new</span> xxxKey())</span><br><span class="line">    .flatMap(<span class="keyword">new</span> XxxFlatMapFunction()).setParallelism(<span class="number">5</span>)</span><br><span class="line">    .map(<span class="keyword">new</span> XxxMapFunction).setParallelism(<span class="number">5</span>)</span><br><span class="line">    .addSink(<span class="keyword">new</span> XxxSink()).setParallelism(<span class="number">1</span>)</span><br></pre></td></tr></table></figure><p>如上就是给每个算子单独设置并行度，这样的话，就算程序设置了 <code>env.setParallelism(10)</code> 也是会被覆盖的。这也说明优先级是：算子设置并行度 &gt; env 设置并行度 &gt; 配置文件默认并行度。</p><p>并行度讲到这里应该都懂了，下面就继续讲什么是 Slot？</p><h3 id="什么是-Slot？"><a href="#什么是-Slot？" class="headerlink" title="什么是 Slot？"></a>什么是 Slot？</h3><p>其实 Slot 的概念在 1.2 节中已经提及到，这里再细讲一点。</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/r19yJh.jpg" alt="img"></p><p>图中 TaskManager 是从 JobManager 处接收需要部署的 Task，任务能配置的最大并行度由 TaskManager 上可用的 Slot 决定。每个任务代表分配给任务槽的一组资源，Slot 在 Flink 里面可以认为是资源组，Flink 将每个任务分成子任务并且将这些子任务分配到 Slot 中，这样就可以并行的执行程序。</p><p>例如，如果 TaskManager 有四个 Slot，那么它将为每个 Slot 分配 25％ 的内存。 可以在一个 Slot 中运行一个或多个线程。 同一 Slot 中的线程共享相同的 JVM。 同一 JVM 中的任务共享 TCP 连接和心跳消息。TaskManager 的一个 Slot 代表一个可用线程，该线程具有固定的内存，注意 Slot 只对内存隔离，没有对 CPU 隔离。默认情况下，Flink 允许子任务共享 Slot，即使它们是不同 Task 的 subtask，只要它们来自相同的 Job，这种共享模式可以大大的提高资源利用率。拿下面的图片来讲解会更好些。</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/ECv5y2.jpg" alt="img"></p><p>上面图片中有两个 TaskManager，每个 TaskManager 有三个 Slot，这样我们的算子最大并行度那么就可以达到 6 个，在同一个 Slot 里面可以执行 1 至多个子任务。那么再看上面的图片，source/map/keyby/window/apply 算子最大可以设置 6 个并行度，sink 只设置了 1 个并行度。</p><p>每个 Flink TaskManager 在集群中提供 Slot，Slot 的数量通常与每个 TaskManager 的可用 CPU 内核数成比例（一般情况下 Slot 个数是每个 TaskManager 的 CPU 核数）。Flink 配置文件中设置的一个 TaskManager 默认的 Slot 是 1。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-06-062913.png" alt="img"></p><p><code>taskmanager.numberOfTaskSlots: 1</code> 该参数可以根据实际情况做一定的修改。</p><h3 id="Slot-和-Parallelism-的关系"><a href="#Slot-和-Parallelism-的关系" class="headerlink" title="Slot 和 Parallelism 的关系"></a>Slot 和 Parallelism 的关系</h3><p>下面用几张图片来更加深刻的理解下 Slot 和 Parallelism，并清楚它们之间的关系。</p><p>1、Slot 是指 TaskManager 最大能并发执行的能力</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/zpX2sh.jpg" alt="img"></p><p>如上图，如果设置的单个 TaskManager 的 Slot 个数为 3，启动 3 个 TaskManager 后，那么就一共有 9 个 Slot。</p><p>2、parallelism 是指 TaskManager 实际使用的并发能力</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/npq4kW.jpg" alt="img"></p><p>运行程序默认的并行度为 1，9 个 Slot 只用了 1 个，有 8 个处于空闲，设置合适的并行度才能提高 Job 计算效率。</p><p>3、parallelism 是可配置、可指定的</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/xAuHJn.jpg" alt="img"></p><p>上图中 example2 每个算子设置的并行度是 2， example3 每个算子设置的并行度是 9。</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/syrCLs.jpg" alt="img"></p><p>example4 除了 sink 是设置的并行度为 1，其他算子设置的并行度都是 9。</p><h3 id="可能会遇到-Slot-和-Parallelism-的问题"><a href="#可能会遇到-Slot-和-Parallelism-的问题" class="headerlink" title="可能会遇到 Slot 和 Parallelism 的问题"></a>可能会遇到 Slot 和 Parallelism 的问题</h3><p>好了，既然 Slot 和 Parallelism 大家都了解了，现在再来看前面提到的问题（Slot 资源不够），这时问题的答案就已经很明显了，就是程序设置的并行度超过了 TaskManager 可用的 Slot 数量，所以程序一直在等待资源调度并超过了一定的时间（该时间可配置），所以才会抛出该异常错误。</p><p>还原代码查找根因，当时笔者的程序设置的并行度是 30（设置 30 是因为 Kafka 分区数有 30 个，想着一个并行度去消费一个分区的数据），没曾想到 Flink 的 Slot 不够，后面了解到该情况后就降低并行度到 10，这样就意味着一个并行度要去消费 3 个 Kafka 分区的数据，调整并行度后速度还是跟的上并且再也没有抛出该异常了。注意如果调小并行度后消费速度过慢，那可以再试试调大些试试，如果还是这样，那么只能增加 TaskManager 的个数从而间接性的增加 Slot 个数来解决该问题了。</p><p>该问题对于刚接触 Flink 的来说是比较容易遇见的，如果你对 Slot 和 Parallelism 不了解的话，那么就会感觉很苦恼，相信你看完这篇文章后就能够豁然开朗了。另外可能还会有各种各样的并行度设置的问题，比如：</p><ul><li>程序某个算子执行了比较复杂的操作，延迟很久，导致该算子处理数据特别慢，那么可以考虑给该算子处增加并行度</li><li>Flink Source 处的并行度超过 Kafka 分区数，因为 Flink 的一个并行度可以处理一至多个分区的数据，如果并行度多于 Kafka 的分区数，那么就会造成有的并行度空闲，浪费资源，建议最多 Flink Source 端的并行度不要超过 Kafka 分区数</li></ul><p>总之，要做到既让 Job 能够及时消费数据，又能够节省资源，需要理解并合理设置并行度和 Slot。</p>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>使用 Flink ParameterTool 读取配置</title>
      <link href="2019/12/28/%E4%BD%BF%E7%94%A8-Flink-ParameterTool-%E8%AF%BB%E5%8F%96%E9%85%8D%E7%BD%AE/"/>
      <url>2019/12/28/%E4%BD%BF%E7%94%A8-Flink-ParameterTool-%E8%AF%BB%E5%8F%96%E9%85%8D%E7%BD%AE/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p>Flink 配置的管理很不方便，比如像算子的并行度配置、Kafka 数据源的配置（broker 地址、topic 名、group.id）、Checkpoint 是否开启、状态后端存储路径、数据库地址、用户名和密码等，Flink 作为流计算引擎，处理源源不断的数据是其本意，但是在处理数据的过程中，往往可能需要一些参数的传递</p><a id="more"></a><h3 id="Flink-Job-配置"><a href="#Flink-Job-配置" class="headerlink" title="Flink Job 配置"></a>Flink Job 配置</h3><p>在 Flink 中其实是有几种方法来管理配置。</p><h4 id="使用-Configuration"><a href="#使用-Configuration" class="headerlink" title="使用 Configuration"></a>使用 Configuration</h4><p>Flink 提供了 withParameters 方法，它可以传递 Configuration 中的参数给，要使用它，需要实现那些 Rich 函数，比如实现 RichMapFunction，而不是 MapFunction，因为 Rich 函数中有 open 方法，然后可以重写 open 方法通过 Configuration 获取到传入的参数值。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"><span class="comment">// Configuration 类来存储参数</span></span><br><span class="line">Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">configuration.setString(<span class="string">&quot;name&quot;</span>, <span class="string">&quot;zhisheng&quot;</span>);</span><br><span class="line"></span><br><span class="line">env.fromElements(WORDS)</span><br><span class="line">        .flatMap(<span class="keyword">new</span> RichFlatMapFunction&lt;String, Tuple2&lt;String, Integer&gt;&gt;() &#123;</span><br><span class="line"></span><br><span class="line">            String name;</span><br><span class="line"></span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(Configuration parameters)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="comment">//读取配置</span></span><br><span class="line">                name = parameters.getString(<span class="string">&quot;name&quot;</span>, <span class="string">&quot;&quot;</span>);</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">flatMap</span><span class="params">(String value, Collector&lt;Tuple2&lt;String, Integer&gt;&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                String[] splits = value.toLowerCase().split(<span class="string">&quot;\\W+&quot;</span>);</span><br><span class="line"></span><br><span class="line">                <span class="keyword">for</span> (String split : splits) &#123;</span><br><span class="line">                    <span class="keyword">if</span> (split.length() &gt; <span class="number">0</span>) &#123;</span><br><span class="line">                        out.collect(<span class="keyword">new</span> Tuple2&lt;&gt;(split + name, <span class="number">1</span>));</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;).withParameters(configuration)    <span class="comment">//将参数传递给函数</span></span><br><span class="line">        .print();</span><br></pre></td></tr></table></figure><p>但是要注意这个 withParameters 只在批程序中支持，流程序中是没有该方法的，并且这个 withParameters 要在每个算子后面使用才行，并不是一次使用就所有都可以获取到，如果所有算子都要该配置，那么就重复设置多次就会比较繁琐。</p><h3 id="ParameterTool-管理配置"><a href="#ParameterTool-管理配置" class="headerlink" title="ParameterTool 管理配置"></a>ParameterTool 管理配置</h3><p>上面通过 Configuration 的局限性很大，其实在 Flink 中还可以通过使用 ParameterTool 类读取配置，它可以读取环境变量、运行参数、配置文件，下面分别讲下每种如何使用。</p><h4 id="读取运行参数"><a href="#读取运行参数" class="headerlink" title="读取运行参数"></a>读取运行参数</h4><p>我们知道 Flink UI 上是支持为每个 Job 单独传入 arguments（参数）的，它的格式要求是如下这种。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">--brokers 127.0.0.1:9200</span><br><span class="line">--username admin</span><br><span class="line">--password 123456</span><br></pre></td></tr></table></figure><p>或者这种</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">-brokers 127.0.0.1:9200</span><br><span class="line">-username admin</span><br><span class="line">-password 123456</span><br></pre></td></tr></table></figure><p>然后在 Flink 程序中你可以直接使用 <code>ParameterTool.fromArgs(args)</code> 获取到所有的参数，然后如果你要获取某个参数对应的值的话，可以通过 <code>parameterTool.get(&quot;username&quot;)</code> 方法。那么在这个地方其实你就可以将配置放在一个第三方的接口，然后这个参数值中传入一个接口，拿到该接口后就能够通过请求去获取更多你想要的配置。</p><h4 id="读取系统属性"><a href="#读取系统属性" class="headerlink" title="读取系统属性"></a>读取系统属性</h4><p>ParameterTool 还支持通过 <code>ParameterTool.fromSystemProperties()</code> 方法读取系统属性。</p><h4 id="读取配置文件"><a href="#读取配置文件" class="headerlink" title="读取配置文件"></a>读取配置文件</h4><p>除了上面两种外，ParameterTool 还支持 <code>ParameterTool.fromPropertiesFile(&quot;/application.properties&quot;)</code> 读取 properties 配置文件。你可以将所有要配置的地方（比如并行度和一些 Kafka、MySQL 等配置）都写成可配置的，然后其对应的 key 和 value 值都写在配置文件中，最后通过 ParameterTool 去读取配置文件获取对应的值。</p><h4 id="ParameterTool-获取值"><a href="#ParameterTool-获取值" class="headerlink" title="ParameterTool 获取值"></a>ParameterTool 获取值</h4><p>ParameterTool 类提供了很多便捷方法去获取值。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-09-134119.png" alt="img"></p><p>你可以在应用程序的 main() 方法中直接使用这些方法返回的值，例如：你可以按如下方法来设置一个算子的并行度：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ParameterTool parameters = ParameterTool.fromArgs(args);</span><br><span class="line"><span class="keyword">int</span> parallelism = parameters.get(<span class="string">&quot;mapParallelism&quot;</span>, <span class="number">2</span>);</span><br><span class="line">DataStream&lt;Tuple2&lt;String, Integer&gt;&gt; counts = data.flatMap(<span class="keyword">new</span> Tokenizer()).setParallelism(parallelism);</span><br></pre></td></tr></table></figure><p>因为 ParameterTool 是可序列化的，所以你可以将它当作参数进行传递给自定义的函数。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ParameterTool parameters = ParameterTool.fromArgs(args);</span><br><span class="line">DataStream&lt;Tuple2&lt;String, Integer&gt;&gt; counts = dara.flatMap(<span class="keyword">new</span> Tokenizer(parameters));</span><br></pre></td></tr></table></figure><p>然后在函数内部使用 ParameterTool 来获取命令行参数，这样就意味着你在作业任何地方都可以获取到参数，而不是像 withParameters 一样需要每次都设置。</p><h4 id="注册全局参数"><a href="#注册全局参数" class="headerlink" title="注册全局参数"></a>注册全局参数</h4><p>在 ExecutionConfig 中可以将 ParameterTool 注册为全作业参数的参数，这样就可以被 JobManager 的 web 端以及用户自定义函数中以配置值的形式访问。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">env.getConfig().setGlobalJobParameters(ParameterTool.fromArgs(args));</span><br></pre></td></tr></table></figure><p>然后就可以在用户自定义的 Rich 函数中像如下这样获取到参数值了。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">env.addSource(<span class="keyword">new</span> RichSourceFunction&lt;String&gt;() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">(SourceContext&lt;String&gt; sourceContext)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">            ParameterTool parameterTool = (ParameterTool) getRuntimeContext().getExecutionConfig().getGlobalJobParameters();</span><br><span class="line">            sourceContext.collect(System.currentTimeMillis() + parameterTool.get(<span class="string">&quot;os.name&quot;</span>) + parameterTool.get(<span class="string">&quot;user.home&quot;</span>));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">cancel</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure><p>在笔者公司内通常是以 Job 运行的环境变量为准，比如我们是运行在 K8s 上面，那么我们会为我们的这个 Flink Job 设置很多环境变量，设置的环境变量的值就得通过 ParameterTool 类去获取，我们是会优先根据环境变量的值为准，如果环境变量的值没有就会去读取应用运行参数，如果应用运行参数也没有才会去读取之前已经写好在配置文件中的配置。大概代码如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> ParameterTool <span class="title">createParameterTool</span><span class="params">(<span class="keyword">final</span> String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> ParameterTool</span><br><span class="line">            .fromPropertiesFile(ExecutionEnv.class.getResourceAsStream(<span class="string">&quot;/application.properties&quot;</span>))</span><br><span class="line">            .mergeWith(ParameterTool.fromArgs(args))</span><br><span class="line">            .mergeWith(ParameterTool.fromSystemProperties())</span><br><span class="line">            .mergeWith(ParameterTool.fromMap(getenv()));<span class="comment">// mergeWith 会使用最新的配置</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//获取 Job 设置的环境变量</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">static</span> Map&lt;String, String&gt; <span class="title">getenv</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    Map&lt;String, String&gt; map = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line">    <span class="keyword">for</span> (Map.Entry&lt;String, String&gt; entry : System.getenv().entrySet()) &#123;</span><br><span class="line">        map.put(entry.getKey().toLowerCase().replace(<span class="string">&#x27;_&#x27;</span>, <span class="string">&#x27;.&#x27;</span>), entry.getValue());</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> map;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这样如果 Job 要更改一些配置，直接在 Job 在 K8s 上面的环境变量进行配置就好了，修改配置后然后重启 Job 就可以运行起来了，整个过程都不需要再次将作业重新编译打包的。但是这样其实也有一定的坏处，重启一个作业的代价很大，因为在重启后你又要去保证状态要恢复到之前未重启时的状态，尽管 Flink 中的 Checkpoint 和 Savepoint 已经很强大了，但是对于复杂的它来说我们多一事不如少一事，所以其实更希望能够直接动态的获取配置，如果配置做了更改，作业能够感知到。在 Flink 中有的配置是不能够动态设置的，但是比如应用业务配置却是可以做到动态的配置，这时就需要使用比较强大的广播变量，广播变量在之前 3.4 节已经介绍过了，如果忘记可以再回去查看，另外在 11.4 节中会通过一个实际案例来教你如何使用广播变量去动态的更新配置数据。</p><h3 id="ParameterTool-源码剖析"><a href="#ParameterTool-源码剖析" class="headerlink" title="ParameterTool 源码剖析"></a>ParameterTool 源码剖析</h3><p>ParameterTool 这个类还是比较简单的，它继承自 ExecutionConfig.GlobalJobParameters 类，然后提供了上面讲的哪几种方法去获取配置数据：</p><ul><li>fromArgs(String[] args)</li><li>fromPropertiesFile(String path)</li><li>fromPropertiesFile(File file)</li><li>fromPropertiesFile(InputStream inputStream)</li><li>fromSystemProperties()</li></ul><p>还可以传入的一个 <code>Map</code> 配置进去，这样最后也是返回一个 ParameterTool 对象。另外就是提供了好些个 get() 方法去获取不同类型的参数值，也支持通过 mergeWith 方法来将两个不同的 ParameterTool 类进行合并，优先以新传入的参数为准，因为内部是使用的 Map 来存储的，mergeWith 操作会将新的 ParameterTool 数据全部 putAll 进一个 Map 集合中，所以会覆盖前一个相同 key 的值。</p>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
            <tag> Flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink CEP API 学习</title>
      <link href="2019/12/26/Flink-CEP-API-%E5%AD%A6%E4%B9%A0/"/>
      <url>2019/12/26/Flink-CEP-API-%E5%AD%A6%E4%B9%A0/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><h3 id="准备依赖"><a href="#准备依赖" class="headerlink" title="准备依赖"></a>准备依赖</h3><p>要开发 Flink CEP 应用程序，首先你得在项目的 <code>pom.xml</code> 中添加依赖。</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-cep_$&#123;scala.binary.version&#125;<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><p>这个依赖有两种，一个是 Java 版本的，一个是 Scala 版本，根据项目的开发语言自行选择。</p><a id="more"></a><h3 id="Flink-CEP-应用入门"><a href="#Flink-CEP-应用入门" class="headerlink" title="Flink CEP 应用入门"></a>Flink CEP 应用入门</h3><p>准备好依赖后，我们开始第一个 Flink CEP 应用程序，这里我们只做一个简单的数据流匹配，当匹配成功后将匹配的两条数据打印出来。首先定义实体类 Event 如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Event</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> Integer id;</span><br><span class="line">    <span class="keyword">private</span> String name;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>然后构造读取 Socket 数据流将数据进行转换成 Event，代码如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">SingleOutputStreamOperator&lt;Event&gt; eventDataStream = env.socketTextStream(<span class="string">&quot;127.0.0.1&quot;</span>, <span class="number">9200</span>)</span><br><span class="line">    .flatMap(<span class="keyword">new</span> FlatMapFunction&lt;String, Event&gt;() &#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">flatMap</span><span class="params">(String s, Collector&lt;Event&gt; collector)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">            <span class="keyword">if</span> (StringUtil.isNotEmpty(s)) &#123;</span><br><span class="line">                String[] split = s.split(<span class="string">&quot;,&quot;</span>);</span><br><span class="line">                <span class="keyword">if</span> (split.length == <span class="number">2</span>) &#123;</span><br><span class="line">                    collector.collect(<span class="keyword">new</span> Event(Integer.valueOf(split[<span class="number">0</span>]), split[<span class="number">1</span>]));</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;);</span><br></pre></td></tr></table></figure><p>接着就是定义 CEP 中的匹配规则了，下面的规则表示第一个事件的 id 为 42，紧接着的第二个事件 id 要大于 10，满足这样的连续两个事件才会将这两条数据进行打印出来。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">Pattern&lt;Event, ?&gt; pattern = Pattern.&lt;Event&gt;begin(<span class="string">&quot;start&quot;</span>).where(</span><br><span class="line">        <span class="keyword">new</span> SimpleCondition&lt;Event&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">filter</span><span class="params">(Event event)</span> </span>&#123;</span><br><span class="line">                log.info(<span class="string">&quot;start &#123;&#125;&quot;</span>, event.getId());</span><br><span class="line">                <span class="keyword">return</span> event.getId() == <span class="number">42</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">).next(<span class="string">&quot;middle&quot;</span>).where(</span><br><span class="line">        <span class="keyword">new</span> SimpleCondition&lt;Event&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">filter</span><span class="params">(Event event)</span> </span>&#123;</span><br><span class="line">                log.info(<span class="string">&quot;middle &#123;&#125;&quot;</span>, event.getId());</span><br><span class="line">                <span class="keyword">return</span> event.getId() &gt;= <span class="number">10</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">);</span><br><span class="line"></span><br><span class="line">CEP.pattern(eventDataStream, pattern).select(<span class="keyword">new</span> PatternSelectFunction&lt;Event, String&gt;() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">select</span><span class="params">(Map&lt;String, List&lt;Event&gt;&gt; p)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        StringBuilder builder = <span class="keyword">new</span> StringBuilder();</span><br><span class="line">        log.info(<span class="string">&quot;p = &#123;&#125;&quot;</span>, p);</span><br><span class="line">        builder.append(p.get(<span class="string">&quot;start&quot;</span>).get(<span class="number">0</span>).getId()).append(<span class="string">&quot;,&quot;</span>).append(p.get(<span class="string">&quot;start&quot;</span>).get(<span class="number">0</span>).getName()).append(<span class="string">&quot;\n&quot;</span>)</span><br><span class="line">                .append(p.get(<span class="string">&quot;middle&quot;</span>).get(<span class="number">0</span>).getId()).append(<span class="string">&quot;,&quot;</span>).append(p.get(<span class="string">&quot;middle&quot;</span>).get(<span class="number">0</span>).getName());</span><br><span class="line">        <span class="keyword">return</span> builder.toString();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;).print();<span class="comment">//打印结果</span></span><br></pre></td></tr></table></figure><p>然后笔者在终端开启 Socket，输入的两条数据如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">42,zhisheng</span><br><span class="line">20,zhisheng</span><br></pre></td></tr></table></figure><p>作业打印出来的日志如下图：</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-29-072247.png" alt="img"></p><p>整个作业 print 出来的结果如下图：</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-29-072320.png" alt="img"></p><p>好了，一个完整的 Flink CEP 应用程序如上，相信你也能大概理解上面的代码，接着来详细的讲解一下 Flink CEP 中的 Pattern API。</p><h3 id="Pattern-API"><a href="#Pattern-API" class="headerlink" title="Pattern API"></a>Pattern API</h3><p>你可以通过 Pattern API 去定义从流数据中匹配事件的 Pattern，每个复杂 Pattern 都是由多个简单的 Pattern 组成的，拿前面入门的应用来讲，它就是由 <code>start</code> 和 <code>middle</code> 两个简单的 Pattern 组成的，在其每个 Pattern 中都只是简单的处理了流数据。在处理的过程中需要标示该 Pattern 的名称，以便后续可以使用该名称来获取匹配到的数据，如 <code>p.get(&quot;start&quot;).get(0)</code> 它就可以获取到 Pattern 中匹配的第一个事件。接下来我们先来看下简单的 Pattern 。</p><h4 id="单个-Pattern"><a href="#单个-Pattern" class="headerlink" title="单个 Pattern"></a>单个 Pattern</h4><h5 id="数量"><a href="#数量" class="headerlink" title="数量"></a>数量</h5><p>单个 Pattern 后追加的 Pattern 如果都是相同的，那如果要都重新再写一遍，换做任何人都会比较痛苦，所以就提供了 times(n) 来表示期望出现的次数，该 times() 方法还有很多写法，如下所示：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"> <span class="comment">//期望符合的事件出现 4 次</span></span><br><span class="line"> start.times(<span class="number">4</span>);</span><br><span class="line"></span><br><span class="line"> <span class="comment">//期望符合的事件不出现或者出现 4 次</span></span><br><span class="line"> start.times(<span class="number">4</span>).optional();</span><br><span class="line"></span><br><span class="line">  <span class="comment">//期望符合的事件出现 2 次或者 3 次或者 4 次</span></span><br><span class="line"> start.times(<span class="number">2</span>, <span class="number">4</span>);</span><br><span class="line"></span><br><span class="line"> <span class="comment">//期望出现 2 次、3 次或 4 次，并尽可能多地重复</span></span><br><span class="line"> start.times(<span class="number">2</span>, <span class="number">4</span>).greedy();</span><br><span class="line"></span><br><span class="line"><span class="comment">//期望出现 2 次、3 次、4 次或者不出现</span></span><br><span class="line"> start.times(<span class="number">2</span>, <span class="number">4</span>).optional();</span><br><span class="line"></span><br><span class="line"> <span class="comment">//期望出现 0、2、3 或 4 次并尽可能多地重复</span></span><br><span class="line"> start.times(<span class="number">2</span>, <span class="number">4</span>).optional().greedy();</span><br><span class="line"></span><br><span class="line"> <span class="comment">//期望出现一个或多个事件</span></span><br><span class="line"> start.oneOrMore();</span><br><span class="line"></span><br><span class="line"> <span class="comment">//期望出现一个或多个事件，并尽可能多地重复这些事件</span></span><br><span class="line"> start.oneOrMore().greedy();</span><br><span class="line"></span><br><span class="line"> <span class="comment">//期望出现一个或多个事件或者不出现</span></span><br><span class="line"> start.oneOrMore().optional();</span><br><span class="line"></span><br><span class="line"> <span class="comment">//期望出现更多次，并尽可能多地重复或者不出现</span></span><br><span class="line"> start.oneOrMore().optional().greedy();</span><br><span class="line"></span><br><span class="line"> <span class="comment">//期望出现两个或多个事件</span></span><br><span class="line"> start.timesOrMore(<span class="number">2</span>);</span><br><span class="line"></span><br><span class="line"> <span class="comment">//期望出现 2 次或 2 次以上，并尽可能多地重复</span></span><br><span class="line"> start.timesOrMore(<span class="number">2</span>).greedy();</span><br><span class="line"></span><br><span class="line"> <span class="comment">//期望出现 2 次或更多的事件，并尽可能多地重复或者不出现</span></span><br><span class="line"> start.timesOrMore(<span class="number">2</span>).optional().greedy();</span><br></pre></td></tr></table></figure><h5 id="条件"><a href="#条件" class="headerlink" title="条件"></a>条件</h5><p>可以通过 <code>pattern.where()</code>、<code>pattern.or()</code> 或 <code>pattern.until()</code> 方法指定事件属性的条件。条件可以是 <code>IterativeConditions</code> 或<code>SimpleConditions</code>。比如 SimpleCondition 可以像下面这样使用：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">start.where(<span class="keyword">new</span> SimpleCondition&lt;Event&gt;() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">filter</span><span class="params">(Event value)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&quot;zhisheng&quot;</span>.equals(value.getName());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure><h4 id="组合-Pattern"><a href="#组合-Pattern" class="headerlink" title="组合 Pattern"></a>组合 Pattern</h4><p>前面已经对单个 Pattern 做了详细对讲解，接下来讲解如何将多个 Pattern 进行组合来完成一些需求。在完成组合 Pattern 之前需要定义第一个 Pattern，然后在第一个的基础上继续添加新的 Pattern。比如定义了第一个 Pattern 如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Pattern&lt;Event, ?&gt; start = Pattern.&lt;Event&gt;begin(<span class="string">&quot;start&quot;</span>);</span><br></pre></td></tr></table></figure><p>接下来，可以为此指定更多的 Pattern，通过指定的不同的连接条件。比如：</p><ul><li>next()：要求比较严格，该事件一定要紧跟着前一个事件。</li><li>followedBy()：该事件在前一个事件后面就行，两个事件之间可能会有其他的事件。</li><li>followedByAny()：该事件在前一个事件后面的就满足条件，两个事件之间可能会有其他的事件，返回值比上一个多。</li><li>notNext()：不希望前一个事件后面紧跟着该事件出现。</li><li>notFollowedBy()：不希望后面出现该事件。</li></ul><p>具体怎么写呢，可以看下样例：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Pattern&lt;Event, ?&gt; strict = start.next(<span class="string">&quot;middle&quot;</span>).where(...);</span><br><span class="line"></span><br><span class="line">Pattern&lt;Event, ?&gt; relaxed = start.followedBy(<span class="string">&quot;middle&quot;</span>).where(...);</span><br><span class="line"></span><br><span class="line">Pattern&lt;Event, ?&gt; nonDetermin = start.followedByAny(<span class="string">&quot;middle&quot;</span>).where(...);</span><br><span class="line"></span><br><span class="line">Pattern&lt;Event, ?&gt; strictNot = start.notNext(<span class="string">&quot;not&quot;</span>).where(...);</span><br><span class="line"></span><br><span class="line">Pattern&lt;Event, ?&gt; relaxedNot = start.notFollowedBy(<span class="string">&quot;not&quot;</span>).where(...);</span><br></pre></td></tr></table></figure><p>可能概念讲了很多，但是还是不太清楚，这里举个例子说明一下，假设有个 Pattern 是 <code>a b</code>，给定的数据输入顺序是 <code>a c b b</code>，对于上面那种不同的连接条件可能最后返回的值不一样。</p><ol><li>a 和 b 之间使用 next() 连接，那么则返回 {}，即没有匹配到数据</li><li>a 和 b 之间使用 followedBy() 连接，那么则返回 {a, b}</li><li>a 和 b 之间使用 followedByAny() 连接，那么则返回 {a, b}, {a, b}</li></ol><p>相信通过上面的这个例子讲解你就知道了它们的区别，尤其是 followedBy() 和 followedByAny()，笔者一开始也是毕竟懵，后面也是通过代码测试才搞明白它们之间的区别的。除此之外，还可以为 Pattern 定义时间约束。例如，可以通过 <code>pattern.within(Time.seconds(10))</code> 方法定义此 Pattern 应该 10 秒内完成匹配。 该时间不仅支持处理时间还支持事件时间。另外还可以与 consecutive()、allowCombinations() 等组合，更多的请看下图中 Pattern 类的方法。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-29-164118.png" alt="img"></p><h4 id="Group-Pattern"><a href="#Group-Pattern" class="headerlink" title="Group Pattern"></a>Group Pattern</h4><p>业务需求比较复杂的场景，如果要使用 Pattern 来定义的话，可能这个 Pattern 会很长并且还会嵌套，比如由 begin、followedBy、followedByAny、next 组成和嵌套，另外还可以再和 oneOrMore()、times(#ofTimes)、times(#fromTimes, #toTimes)、optional()、consecutive()、allowCombinations() 等结合使用。效果如下面这种：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">Pattern&lt;Event, ?&gt; start = Pattern.begin(</span><br><span class="line">    Pattern.&lt;Event&gt;begin(<span class="string">&quot;start&quot;</span>).where(...).followedBy(<span class="string">&quot;start_middle&quot;</span>).where(...)</span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="comment">//next 表示连续</span></span><br><span class="line">Pattern&lt;Event, ?&gt; strict = start.next(</span><br><span class="line">    Pattern.&lt;Event&gt;begin(<span class="string">&quot;next_start&quot;</span>).where(...).followedBy(<span class="string">&quot;next_middle&quot;</span>).where(...)</span><br><span class="line">).times(<span class="number">3</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">//followedBy 代表在后面就行</span></span><br><span class="line">Pattern&lt;Event, ?&gt; relaxed = start.followedBy(</span><br><span class="line">    Pattern.&lt;Event&gt;begin(<span class="string">&quot;followedby_start&quot;</span>).where(...).followedBy(<span class="string">&quot;followedby_middle&quot;</span>).where(...)</span><br><span class="line">).oneOrMore();</span><br><span class="line"></span><br><span class="line"><span class="comment">//followedByAny</span></span><br><span class="line">Pattern&lt;Event, ?&gt; nonDetermin = start.followedByAny(</span><br><span class="line">    Pattern.&lt;Event&gt;begin(<span class="string">&quot;followedbyany_start&quot;</span>).where(...).followedBy(<span class="string">&quot;followedbyany_middle&quot;</span>).where(...)</span><br><span class="line">).optional();</span><br></pre></td></tr></table></figure><p>关于上面这些 Pattern 操作的更详细的解释可以查看<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.9/dev/libs/cep.html#groups-of-patterns">官网</a>。</p><h4 id="事件匹配跳过策略"><a href="#事件匹配跳过策略" class="headerlink" title="事件匹配跳过策略"></a>事件匹配跳过策略</h4><p>对于给定组合的复杂 Pattern，有的事件可能会匹配到多个 Pattern，如果要控制将事件的匹配数，需要指定跳过策略。在 Flink CEP 中跳过策略有四种类型，如下所示：</p><ul><li>NO_SKIP：不跳过，将发出所有可能的匹配事件。</li><li>SKIP_TO_FIRST：丢弃包含 PatternName 第一个之前匹配事件的每个部分匹配。</li><li>SKIP_TO_LAST：丢弃包含 PatternName 最后一个匹配事件之前的每个部分匹配。</li><li>SKIP_PAST_LAST_EVENT：丢弃包含匹配事件的每个部分匹配。</li><li>SKIP_TO_NEXT：丢弃以同一事件开始的所有部分匹配。</li></ul><p>这几种策略都是根据 AfterMatchSkipStrategy 来实现的，可以看下它们的类结构图，如下所示：</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-29-133737.png" alt="img"></p><p>关于这几种跳过策略的具体区别可以查看<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.9/dev/libs/cep.html#after-match-skip-strategy">官网</a>，至于如何使用跳过策略，其实 AfterMatchSkipStrategy 抽象类中已经提供了 5 种静态方法可以直接使用，方法如下：</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-29-135526.png" alt="img"></p><p>使用方法如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">AfterMatchSkipStrategy skipStrategy = ...; <span class="comment">// 使用 AfterMatchSkipStrategy 调用不同的静态方法</span></span><br><span class="line">Pattern.begin(<span class="string">&quot;start&quot;</span>, skipStrategy);</span><br></pre></td></tr></table></figure><h3 id="检测-Pattern"><a href="#检测-Pattern" class="headerlink" title="检测 Pattern"></a>检测 Pattern</h3><p>编写好了 Pattern 之后，你需要的是将其应用在流数据中去做匹配。这时要做的就是构造一个 PatternStream，它可以通过 <code>CEP.pattern(eventDataStream, pattern)</code> 来获取一个 PatternStream 对象，在 <code>CEP.pattern()</code> 方法中，你可以选择传入两个参数（DataStream 和 Pattern），也可以选择传入三个参数 （DataStream、Pattern 和 EventComparator），因为 CEP 类中它有两个不同参数数量的 pattern 方法。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CEP</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> &lt;T&gt; <span class="function">PatternStream&lt;T&gt; <span class="title">pattern</span><span class="params">(DataStream&lt;T&gt; input, Pattern&lt;T, ?&gt; pattern)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> PatternStream(input, pattern);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> &lt;T&gt; <span class="function">PatternStream&lt;T&gt; <span class="title">pattern</span><span class="params">(DataStream&lt;T&gt; input, Pattern&lt;T, ?&gt; pattern, EventComparator&lt;T&gt; comparator)</span> </span>&#123;</span><br><span class="line">        PatternStream&lt;T&gt; stream = <span class="keyword">new</span> PatternStream(input, pattern);</span><br><span class="line">        <span class="keyword">return</span> stream.withComparator(comparator);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="选择-Pattern"><a href="#选择-Pattern" class="headerlink" title="选择 Pattern"></a>选择 Pattern</h4><p>在获取到 PatternStream 后，你可以通过 select 或 flatSelect 方法从匹配到的事件流中查询。如果使用的是 select 方法，则需要实现传入一个 PatternSelectFunction 的实现作为参数，PatternSelectFunction 具有为每个匹配事件调用的 select 方法，该方法的参数是 <code>Map&gt;</code>，这个 Map 的 key 是 Pattern 的名字，在前面入门案例中设置的 <code>start</code> 和 <code>middle</code> 在这时就起作用了，你可以通过类似 <code>get(&quot;start&quot;)</code> 方法的形式来获取匹配到 <code>start</code>的所有事件。如果使用的是 flatSelect 方法，则需要实现传入一个 PatternFlatSelectFunction 的实现作为参数，这个和 PatternSelectFunction 不一致地方在于它可以返回多个结果，因为这个接口中的 flatSelect 方法含有一个 Collector，它可以返回多个数据到下游去。两者的样例如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">CEP.pattern(eventDataStream, pattern).select(<span class="keyword">new</span> PatternSelectFunction&lt;Event, String&gt;() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">select</span><span class="params">(Map&lt;String, List&lt;Event&gt;&gt; p)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        StringBuilder builder = <span class="keyword">new</span> StringBuilder();</span><br><span class="line">        builder.append(p.get(<span class="string">&quot;start&quot;</span>).get(<span class="number">0</span>).getId()).append(<span class="string">&quot;,&quot;</span>).append(p.get(<span class="string">&quot;start&quot;</span>).get(<span class="number">0</span>).getName()).append(<span class="string">&quot;\n&quot;</span>)</span><br><span class="line">                .append(p.get(<span class="string">&quot;middle&quot;</span>).get(<span class="number">0</span>).getId()).append(<span class="string">&quot;,&quot;</span>).append(p.get(<span class="string">&quot;middle&quot;</span>).get(<span class="number">0</span>).getName());</span><br><span class="line">        <span class="keyword">return</span> builder.toString();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;).print();</span><br><span class="line"></span><br><span class="line">CEP.pattern(eventDataStream, pattern).flatSelect(<span class="keyword">new</span> PatternFlatSelectFunction&lt;Event, String&gt;() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">flatSelect</span><span class="params">(Map&lt;String, List&lt;Event&gt;&gt; map, Collector&lt;String&gt; collector)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">for</span> (Map.Entry&lt;String, List&lt;Event&gt;&gt; entry : map.entrySet()) &#123;</span><br><span class="line">            collector.collect(entry.getKey() + <span class="string">&quot; &quot;</span> + entry.getValue().get(<span class="number">0</span>).getId() + <span class="string">&quot;,&quot;</span> + entry.getValue().get(<span class="number">0</span>).getName());</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;).print();</span><br></pre></td></tr></table></figure><p>关于 PatternStream 中的 select 或 flatSelect 方法其实可以传入不同的参数，比如传入 OutputTag 和 PatternTimeoutFunction 去处理延迟的数据，具体查看下图。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-29-125416.png" alt="img"></p><p>如果使用的 Flink CEP 版本是大于等于 1.8 的话，还可以使用 process 方法，在上图中也可以看到在 PatternStream 类中包含了该方法。要使用 process 的话，得传入一个 PatternProcessFunction 的实现作为参数，在该实现中需要重写 processMatch 方法。使用 PatternProcessFunction 比使用 PatternSelectFunction 和 PatternFlatSelectFunction 更好的是，它支持获取应用的的上下文，那么也就意味着它可以访问时间（因为 Context 接口继承自 TimeContext 接口）。另外如果要处理延迟的数据可以与 TimedOutPartialMatchHandler 接口的实现类一起使用。</p><h3 id="CEP-时间属性"><a href="#CEP-时间属性" class="headerlink" title="CEP 时间属性"></a>CEP 时间属性</h3><h4 id="根据事件时间处理延迟数据"><a href="#根据事件时间处理延迟数据" class="headerlink" title="根据事件时间处理延迟数据"></a>根据事件时间处理延迟数据</h4><p>在 CEP 中，元素处理的顺序很重要，当时间策略设置为事件时间时，为了确保能够按照事件时间的顺序来处理元素，先来的事件会暂存在缓冲区域中，然后对缓冲区域中的这些事件按照事件时间进行排序，当水印到达时，比水印时间小的事件会按照顺序依次处理的。这意味着水印之间的元素是按照事件时间顺序处理的。</p><p>注意：当作业设置的时间属性是事件时间是，CEP 中会认为收到的水印时间是正确的，会严格按照水印的时间来处理元素，从而保证能顺序的处理元素。另外对于这种延迟的数据（和 3.5 节中的延迟数据类似），CEP 中也是支持通过 side output 设置 OutputTag 标签来将其收集。使用方式如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">PatternStream&lt;Event&gt; patternStream = CEP.pattern(inputDataStream, pattern);</span><br><span class="line"></span><br><span class="line">OutputTag&lt;String&gt; lateDataOutputTag = <span class="keyword">new</span> OutputTag&lt;String&gt;(<span class="string">&quot;late-data&quot;</span>)&#123;&#125;;</span><br><span class="line"></span><br><span class="line">SingleOutputStreamOperator&lt;ComplexEvent&gt; result = patternStream</span><br><span class="line">    .sideOutputLateData(lateDataOutputTag)</span><br><span class="line">    .select(</span><br><span class="line">        <span class="keyword">new</span> PatternSelectFunction&lt;Event, ComplexEvent&gt;() &#123;...&#125;</span><br><span class="line">    );</span><br><span class="line"></span><br><span class="line">DataStream&lt;String&gt; lateData = result.getSideOutput(lateDataOutputTag);</span><br></pre></td></tr></table></figure><h4 id="时间上下文"><a href="#时间上下文" class="headerlink" title="时间上下文"></a>时间上下文</h4><p>在 PatternProcessFunction 和 IterativeCondition 中可以通过 TimeContext 访问当前正在处理的事件的时间（Event Time）和此时机器上的时间（Processing Time）。你可以查看到这两个类中都包含了 Context，而这个 Context 继承自 TimeContext，在 TimeContext 接口中定义了获取事件时间和处理时间的方法。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">TimeContext</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">long</span> <span class="title">timestamp</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">long</span> <span class="title">currentProcessingTime</span><span class="params">()</span></span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Flink CEP深入理解</title>
      <link href="2019/12/26/Flink-CEP%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3/"/>
      <url>2019/12/26/Flink-CEP%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p>CEP 的英文全称是 Complex Event Processing，翻译成中文为复杂事件处理。它可以用于处理实时数据并在事件流到达时从事件流中提取信息，并根据定义的规则来判断事件是否匹配，如果匹配则会触发新的事件做出响应。除了支持单个事件的简单无状态的模式匹配（例如基于事件中的某个字段进行筛选过滤），也可以支持基于关联／聚合／时间窗口等多个事件的复杂有状态模式的匹配（例如判断用户下单事件后 30 分钟内是否有支付事件）。</p><a id="more"></a><h3 id="CEP-是什么？"><a href="#CEP-是什么？" class="headerlink" title="CEP 是什么？"></a>CEP 是什么？</h3><p>CEP 的英文全称是 Complex Event Processing，翻译成中文为复杂事件处理。</p><p><font color='blue'>CEP 可以用于处理实时数据并在事件流到达时从事件流中提取信息，根据定义的规则来判断事件是否匹配，如果匹配则会触发新的事件做出响应。除了支持单个事件的简单无状态的模式匹配（例如基于事件中的某个字段进行筛选过滤），也可以支持基于关联／聚合／时间窗口等多个事件的复杂有状态模式的匹配（例如判断用户下单事件后 30 分钟内是否有支付事件）。</font></p><p>因为这种事件匹配通常是根据提前制定好的规则去匹配的，而这些规则一般来说不仅多，而且复杂，所以就会引入一些规则引擎来处理这种复杂事件匹配。市面上常用的规则引擎有如下这些。</p><h3 id="规则引擎对比"><a href="#规则引擎对比" class="headerlink" title="规则引擎对比"></a>规则引擎对比</h3><h4 id="Drools"><a href="#Drools" class="headerlink" title="Drools"></a>Drools</h4><p>Drools 是一款使用 Java 编写的开源规则引擎，通常用来解决业务代码与业务规则的分离，它内置的 Drools Fusion 模块也提供 CEP 的功能。</p><p>优势：</p><ul><li>功能较为完善，具有如系统监控、操作平台等功能。</li><li>规则支持动态更新。</li></ul><p>劣势：</p><ul><li>以内存实现时间窗功能，无法支持较长跨度的时间窗。</li><li>无法有效支持定时触达（如用户在浏览发生一段时间后触达条件判断）。</li></ul><h4 id="Aviator"><a href="#Aviator" class="headerlink" title="Aviator"></a>Aviator</h4><p>Aviator 是一个高性能、轻量级的 Java 语言实现的表达式求值引擎，主要用于各种表达式的动态求值。</p><p>优势：</p><ul><li>支持大部分运算操作符。</li><li>支持函数调用和自定义函数。</li><li>支持正则表达式匹配。</li><li>支持传入变量并且性能优秀。</li></ul><p>劣势：</p><ul><li>没有 if else、do while 等语句，没有赋值语句，没有位运算符。</li></ul><h4 id="EasyRules"><a href="#EasyRules" class="headerlink" title="EasyRules"></a>EasyRules</h4><p>EasyRules 集成了 MVEL 和 SpEL 表达式的一款轻量级规则引擎。</p><p>优势：</p><ul><li>轻量级框架，学习成本低。</li><li>基于 POJO。</li><li>为定义业务引擎提供有用的抽象和简便的应用</li><li>支持从简单的规则组建成复杂规则</li></ul><h4 id="Esper"><a href="#Esper" class="headerlink" title="Esper"></a>Esper</h4><p>Esper 设计目标为 CEP 的轻量级解决方案，可以方便的嵌入服务中，提供 CEP 功能。</p><p>优势：</p><ul><li>轻量级可嵌入开发，常用的 CEP 功能简单好用。</li><li>EPL 语法与 SQL 类似，学习成本较低。</li></ul><p>劣势：</p><ul><li>单机全内存方案，需要整合其他分布式和存储。</li><li>以内存实现时间窗功能，无法支持较长跨度的时间窗。</li><li>无法有效支持定时触达（如用户在浏览发生一段时间后触达条件判断）。</li></ul><h4 id="Flink-CEP"><a href="#Flink-CEP" class="headerlink" title="Flink CEP"></a>Flink CEP</h4><p>Flink 是一个流式系统，具有高吞吐低延迟的特点，Flink CEP 是一套极具通用性、易于使用的实时流式事件处理方案。</p><p>优势：</p><ul><li>继承了 Flink 高吞吐的特点</li><li>事件支持存储到外部，可以支持较长跨度的时间窗。</li><li>可以支持定时触达（用 followedBy ＋ PartternTimeoutFunction 实现）</li></ul><p>劣势：</p><ul><li>无法动态更新规则（痛点）</li></ul><h3 id="Flink-CEP-介绍"><a href="#Flink-CEP-介绍" class="headerlink" title="Flink CEP 介绍"></a>Flink CEP 介绍</h3><p>Flink CEP是在 Flink 之上实现的复杂事件处理(CEP)库，它允许我们在事件流中检测事件的模式。</p><p>因为搭配了 Flink 实时处理的能力，所以 Flink CEP 能够在流处理的场景去做一些实时的复杂事件匹配，它与传统的数据库查询是不一致的，比如，传统的数据库的数据是静态的，但是查询却是动态的，所以传统的数据库查询做不到实时的反馈查询结果，而 Flink CEP 则是查询规则是静态的，数据是动态实时的，如果它作用于一个无限的数据流上，这就意味着它可以将某种规则的数据匹配一直保持下去（除非作业停止）；另外 Flink CEP 不需要去存储那些与匹配不相关联的数据，遇到这种数据它会立即丢弃。</p><p>虽然 Flink CEP 拥有 Flink 的本身优点和支持复杂场景的规则处理，但是它本身其实也有非常严重的缺点，那就是不能够动态的更新规则。通常引入规则引擎比较友好的一点是可以将一些业务规则抽象出来成为配置，然后更改这些配置后其实是能够自动生效的，但是在 Flink 中却无法做到这点，甚至规则通常还是要写死在代码里面。</p><p><font color='grey'>举个例子，你在一个 Flink CEP 的作业中定义了一条规则：机器的 CPU 使用率连续 30 秒超过 90% 则发出告警，然后将这个作业上线，上线后发现告警很频繁，你可能会觉得可能规则之前定义的不合适，那么接下来你要做的就是将作业取消，然后重新修改代码并进行编译打包成一个 fat jar，接着上传该 jar 并运行。整个流程下来，你有没有想过会消耗多长的时间？五分钟？或者更长？但是你的目的就是要修改一个配置，如果你在作业中将上面的 30 秒和 90% 做成了配置，可能这样所需要的时间会减少，你只需要重启作业，然后通过传入新的参数将作业重新启动，但是重启作业这步是不是不能少，然而对于流作业来说，重启作业带来的代价很大。</font></p><h3 id="Flink-CEP-如何动态更新规则"><a href="#Flink-CEP-如何动态更新规则" class="headerlink" title="Flink CEP 如何动态更新规则"></a>Flink CEP 如何动态更新规则</h3><p>国内的 Flink 技术分享会却看到有几家公司对这块做了优化，让 Flink CEP 支持动态的更新规则，下面分享一下他们几家公司的思路。</p><ul><li>A 公司：用户更新规则后，新规则会被翻译成 Java 代码，并编译打包成可执行 jar，停止作业并使用 Savepoint 将状态保存下来，启动新的作业并读取之前保存的状态，会根据规则文件中的数量和复杂度对作业的数量做一个规划，防止单作业负载过高，架构如下图所示。</li></ul><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-28-142601.png" alt="img"></p><ul><li>B 公司：规则中心存储规则，规则里面直接存储了 Java 代码，加载这些规则后然后再用 Groovy 做动态编译解析，其架构如下图所示。</li></ul><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-28-143822.png" alt="img"></p><ul><li>C 公司：增加函数，在函数方法中监听规则的变化，如果需要更新则通过 Groovy 加载 Pattern 类进行动态注入，采用 Zookeeper 和 MySQL 管理规则，如果规则发生变化，则从数据库中获取到新的规则，然后更新 Flink CEP 中的 NFA 逻辑，注意状态要根据业务需要选择是否重置，其架构设计如下图所示。</li></ul><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-28-150827.png" alt="img"></p><p>第一种方法，笔者不推荐，因为它这样的做法还是要将作业重启，无非就是做了一个自动化的操作，不是人为的手动重启，从 B 公司和 C 公司两种方法可以发现要实现 Flink CEP 动态的更新规则无非要做的就是：</p><ul><li>监听规则的变化</li><li>将规则变成 Java 代码</li><li>通过 Groovy 动态编译解析</li><li>更改 NFA 的内部逻辑</li><li>状态是否保留</li></ul><h3 id="Flink-CEP-使用场景分析"><a href="#Flink-CEP-使用场景分析" class="headerlink" title="Flink CEP 使用场景分析"></a>Flink CEP 使用场景分析</h3><p>上面虽然提到了一个 Flink CEP 的痛点，但是并不能就此把它的优势给抹去，它可以运用的场景其实还有很多，这里笔者拿某些场景来做个分析。</p><h4 id="实时反作弊和风控"><a href="#实时反作弊和风控" class="headerlink" title="实时反作弊和风控"></a>实时反作弊和风控</h4><p>对于电商来说，羊毛党是必不可少的，国内拼多多曾爆出 100 元的无门槛券随便领，当晚被人褥几百亿，对于这种情况肯定是没有做好及时的风控。另外还有就是商家上架商品时通过频繁修改商品的名称和滥用标题来提高搜索关键字的排名、批量注册一批机器账号快速刷单来提高商品的销售量等作弊行为，各种各样的作弊手法也是需要不断的去制定规则去匹配这种行为。</p><h4 id="实时营销"><a href="#实时营销" class="headerlink" title="实时营销"></a>实时营销</h4><p>分析用户在手机 APP 的实时行为，统计用户的活动周期，通过为用户画像来给用户进行推荐。比如用户在登录 APP 后 1 分钟内只浏览了商品没有下单；用户在浏览一个商品后，3 分钟内又去查看其他同类的商品，进行比价行为；用户商品下单后 1 分钟内是否支付了该订单。如果这些数据都可以很好的利用起来，那么就可以给用户推荐浏览过的类似商品，这样可以大大提高购买率。</p><h4 id="实时网络攻击检测"><a href="#实时网络攻击检测" class="headerlink" title="实时网络攻击检测"></a>实时网络攻击检测</h4><p>当下互联网安全形势仍然严峻，网络攻击屡见不鲜且花样众多，这里我们以 DDOS（分布式拒绝服务攻击）产生的流入流量来作为遭受攻击的判断依据。对网络遭受的潜在攻击进行实时检测并给出预警，云服务厂商的多个数据中心会定时向监控中心上报其瞬时流量，如果流量在预设的正常范围内则认为是正常现象，不做任何操作；如果某数据中心在 10 秒内连续 5 次上报的流量超过正常范围的阈值，则触发一条警告的事件；如果某数据中心 30 秒内连续出现 30 次上报的流量超过正常范围的阈值，则触发严重的告警。</p>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Flink 状态后端存储</title>
      <link href="2019/12/19/Flink-%E7%8A%B6%E6%80%81%E5%90%8E%E7%AB%AF%E5%AD%98%E5%82%A8/"/>
      <url>2019/12/19/Flink-%E7%8A%B6%E6%80%81%E5%90%8E%E7%AB%AF%E5%AD%98%E5%82%A8/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p>Flink 提供了以下三种开箱即用的<strong>状态后端</strong>(用于存储状态数据)，可以为所有 Flink 作业配置相同的状态后端，也可以为每个 Flink 作业配置指定的状态后端。当需要对具体的某一种 State 做 Checkpoint 时，此时就需要具体的状态后端存储，刚好 Flink 内置提供了不同的状态后端存储，用于指定状态的存储方式和位置。状态可以存储在 Java 堆内存中或者堆外</p><a id="more"></a><h3 id="State-Backends"><a href="#State-Backends" class="headerlink" title="State Backends"></a>State Backends</h3><p>当需要对具体的某一种 State 做 Checkpoint 时，此时就需要具体的状态后端存储，Flink 内置提供了不同的状态后端存储，用于指定状态的存储方式和位置。状态可以存储在 Java 堆内存中或者堆外，在 Flink 安装路径下 conf 目录中的 flink-conf.yaml 配置文件中也有状态后端存储相关的配置，Flink 还特有一个 CheckpointingOptions 类来控制 state 存储的相关配置，该类中有如下配置：</p><ol><li>state.backend: 用于存储和进行状态 checkpoint 的状态后端存储方式，无默认值</li><li>state.checkpoints.num-retained: 要保留的已完成 checkpoint 的最大数量，默认值为 1</li><li>state.backend.async: 状态后端是否使用异步快照方法，默认值为 true</li><li>state.backend.incremental: 状态后端是否创建增量检查点，默认值为 false</li><li>state.backend.local-recovery: 状态后端配置本地恢复，默认情况下，本地恢复被禁用</li><li>taskmanager.state.local.root-dirs: 定义存储本地恢复的基于文件的状态的目录</li><li>state.savepoints.dir: 存储 savepoints 的目录</li><li>state.checkpoints.dir: 存储 checkpoint 的数据文件和元数据</li><li>state.backend.fs.memory-threshold: 状态数据文件的最小大小，默认值是 1024</li></ol><p>虽然配置这么多，但是，Flink 还支持基于每个 Job 单独设置状态后端存储，方法如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">env.setStateBackend(<span class="keyword">new</span> MemoryStateBackend());  <span class="comment">//设置堆内存存储</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//env.setStateBackend(new FsStateBackend(checkpointDir, asyncCheckpoints));   //设置文件存储</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//env.setStateBackend(new RocksDBStateBackend(checkpointDir, incrementalCheckpoints));  //设置 RocksDB 存储</span></span><br></pre></td></tr></table></figure><p>Flink提供了以下三种开箱即用的状态后端(用于存储状态数据)，可以为所有 Flink 作业配置相同的状态后端(flink-conf.yaml )，也可以为每个 Flink 作业配置指定的状态后端。</p><ol><li>MemoryStateBackend</li><li>FsStateBackend</li><li>RocksDBStateBackend</li></ol><p><img src="../images/flink/2.png"></p><h3 id="MemoryStateBackend-使用及剖析"><a href="#MemoryStateBackend-使用及剖析" class="headerlink" title="MemoryStateBackend 使用及剖析"></a>MemoryStateBackend 使用及剖析</h3><p><font color='blue'>如果 Job 没有配置指定状态后端存储的话，就会默认采取 MemoryStateBackend 策略。</font></p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">2019-04-28 00:16:41.892 [Sink: zhisheng (1/4)] INFO  org.apache.flink.streaming.runtime.tasks.StreamTask  - No state backend has been configured, using default (Memory / Job Manager) MemoryStateBackend (data in heap memory / checkpoints to Job Manager) (checkpoints: &#x27;null&#x27;, savepoints: &#x27;null&#x27;, asynchronous: TRUE, maxStateSize: 5242880)</span><br></pre></td></tr></table></figure><p><font color='blue'>如果没有配置任何状态存储，使用默认的 MemoryStateBackend 策略，这种状态后端存储把数据以内部对象的形式保存在 Task Managers 的内存（JVM 堆）中，当应用程序触发 checkpoint 时，会将此时的状态进行快照然后存储在 Job Manager 的内存中。因为状态是存储在内存中的，所以这种情况会有点限制</font></font></p><ol><li>不适合在生产环境中使用，仅用于本地测试的情况较多，主要适用于状态很小的 Job，因为它会将状态最终存储在 Job Manager 中，如果状态较大的话，那么会使得 Job Manager 的内存比较紧张，从而导致 Job Manager 会出现 OOM 等问题，然后造成连锁反应使所有的 Job 都挂掉，所以 Job 的状态与之前的 Checkpoint 的数据所占的内存要小于 JobManager 的内存。</li><li>每个单独的状态大小不能超过最大的 DEFAULT<em>MAX</em>STATE_SIZE(5MB)，可以通过构造 MemoryStateBackend 参数传入不同大小的 maxStateSize。</li><li>Job 的操作符状态和 keyed 状态加起来都不要超过 RPC 系统的默认配置 10 MB，虽然可以修改该配置，但是不建议去修改。</li></ol><p>另外就是 MemoryStateBackend 支持配置是否是异步快照还是同步快照，它有一个字段 asynchronousSnapshots 来表示，可选值有：</p><ul><li>TRUE（true 代表使用异步的快照，这样可以避免因快照而导致数据流处理出现阻塞等问题）</li><li>FALSE（同步）</li><li>UNDEFINED（默认值）</li></ul><h3 id="FsStateBackend-使用及剖析"><a href="#FsStateBackend-使用及剖析" class="headerlink" title="FsStateBackend 使用及剖析"></a>FsStateBackend 使用及剖析</h3><p><font color='blue'>这种状态后端存储也是将工作状态存储在 Task Manager 中的内存（JVM 堆）中，但是 checkpoint 的时候，它和 MemoryStateBackend 不一样，它是将状态存储在文件（可以是本地文件，也可以是 HDFS）中，这个文件具体是哪种需要配置</font></p><p>比如：”hdfs://namenode:40010/flink/checkpoints” 或 “file://flink/checkpoints” (通常使用 HDFS 比较多，如果是使用本地文件，可能会造成 Job 恢复的时候找不到之前的 checkkpoint，因为 Job 重启后如果由调度器重新分配在不同的机器的 Task Manager 执行时就会导致这个问题，所以还是建议使用 HDFS 或者其他的分布式文件系统)。</p><p>同样 FsStateBackend 也是支持通过 asynchronousSnapshots 字段来控制是使用异步还是同步来进行 checkpoint 的，异步可以避免在状态 checkpoint 时阻塞数据流的处理，然后还有一点的就是在 FsStateBackend 有个参数 fileStateThreshold，如果状态大小比 MAX<em>FILE</em>STATE_THRESHOLD（1MB） 小的话，那么会将状态数据直接存储在 meta data 文件中，而不是存储在配置的文件中（避免出现很小的状态文件），如果该值为 “-1” 表示尚未配置，在这种情况下会使用默认值（1024，该默认值可以通过 <code>state.backend.fs.memory-threshold</code>来配置）。</p><p>那么我们该什么时候使用 FsStateBackend 呢？</p><ul><li>如果你要处理大状态，长窗口等有状态的任务，那么 FsStateBackend 就比较适合</li><li>使用分布式文件系统，如 HDFS 等，这样 failover 时 Job 的状态可以恢复</li></ul><p>使用 FsStateBackend 需要注意的地方有什么呢？</p><ul><li>工作状态仍然是存储在 Task Manager 中的内存中，虽然在 Checkpoint 的时候会存在文件中，所以还是得注意这个状态要保证不超过 Task Manager 的内存</li></ul><h3 id="如何使用-RocksDBStateBackend-及剖析"><a href="#如何使用-RocksDBStateBackend-及剖析" class="headerlink" title="如何使用 RocksDBStateBackend 及剖析"></a>如何使用 RocksDBStateBackend 及剖析</h3><p><font color='blue'>RocksDBStateBackend 和上面两种都有点不一样，RocksDB 是一种嵌入式的本地数据库，它会在本地文件系统中维护状态，KeyedStateBackend 等会直接写入本地 RocksDB 中，它还需要配置一个文件系统（一般是 HDFS），比如 <code>hdfs://namenode:40010/flink/checkpoints</code>，当触发 checkpoint 的时候，会把整个 RocksDB 数据库复制到配置的文件系统中去，当 failover 时从文件系统中将数据恢复到本地。</font></p><p>官方推荐使用 RocksDB 来作为状态的后端存储</p><ol><li>state 直接存放在 RocksDB 中，不需要存在内存中，这样就可以减少 Task Manager 的内存压力，如果是存内存的话大状态的情况下会导致 GC 次数比较多，同时还能在 checkpoint 时将状态持久化到远端的文件系统，那么就比较适合在生产环境中使用</li><li>RocksDB 本身支持 checkpoint 功能</li><li>RocksDBStateBackend 支持增量的 checkpoint，在 RocksDBStateBackend 中有一个字段 enableIncrementalCheckpointing 来确认是否开启增量的 checkpoint，默认是不开启的，在 CheckpointingOptions 类中有个 state.backend.incremental 参数来表示，增量 checkpoint 非常使用于超大状态的场景。</li></ol><p><strong>RocksDBStateBackend 这个类的相关属性以及构造函数。</strong></p><p><strong>属性</strong>：</p><ul><li>checkpointStreamBackend：用于创建 checkpoint 流的状态后端</li><li>localRocksDbDirectories：RocksDB 目录的基本路径，默认是 Task Manager 的临时目录</li><li>enableIncrementalCheckpointing：是否增量 checkpoint</li><li>numberOfTransferingThreads：用于传输(下载和上传)状态的线程数量，默认为 1</li><li>enableTtlCompactionFilter：是否启用压缩过滤器来清除带有 TTL 的状态</li></ul><p><strong>构造函数</strong>：</p><ul><li>RocksDBStateBackend(String checkpointDataUri)：单参数，只传入一个路径</li><li>RocksDBStateBackend(String checkpointDataUri, boolean enableIncrementalCheckpointing)：两个参数，传入 checkpoint 数据目录路径和是否开启增量 checkpoint</li><li>RocksDBStateBackend(StateBackend checkpointStreamBackend)：传入一种 StateBackend</li><li>RocksDBStateBackend(StateBackend checkpointStreamBackend, TernaryBoolean enableIncrementalCheckpointing)：传入一种 StateBackend 和是否开启增量 checkpoint</li><li>RocksDBStateBackend(RocksDBStateBackend original, Configuration config, ClassLoader classLoader)：私有的构造方法，用于重新配置状态后端</li></ul><p>既然知道这么多构造函数了，那么使用就很简单了，根据你的场景考虑使用哪种构造函数创建 RocksDBStateBackend 对象就行了，然后通过 <code>env.setStateBackend()</code> 传入对象实例就行，如下所示：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//env.setStateBackend(new RocksDBStateBackend(checkpointDir, incrementalCheckpoints));  //设置 RocksDB 存储</span></span><br></pre></td></tr></table></figure><p>那么在使用 RocksDBStateBackend 时该注意什么呢：</p><ul><li>当使用 RocksDB 时，状态大小将受限于磁盘可用空间的大小</li><li>状态存储在 RocksDB 中，整个更新和获取状态的操作都是要通过序列化和反序列化才能完成的，跟状态直接存储在内存中，性能可能会略低些</li><li>如果你应用程序的状态很大，那么使用 RocksDB 无非是最佳的选择</li></ul><p>另外在 Flink 源码中有一个专门的 RocksDBOptions 来表示 RocksDB 相关的配置：</p><ul><li>state.backend.rocksdb.localdir：本地目录(在 Task Manager 上)，RocksDB 将其文件放在其中</li><li>state.backend.rocksdb.timer-service.factory：定时器服务实现，默认值是 HEAP</li><li>state.backend.rocksdb.checkpoint.transfer.thread.num：用于在后端传输(下载和上载)文件的线程数，默认是 1</li><li>state.backend.rocksdb.ttl.compaction.filter.enabled：是否启用压缩过滤器来清除带有 TTL 的状态，默认值是 false</li></ul><h3 id="如何选择状态后端存储？"><a href="#如何选择状态后端存储？" class="headerlink" title="如何选择状态后端存储？"></a>如何选择状态后端存储？</h3><p>通过上面三种 State Backends 的介绍，让大家了解了状态存储有哪些种类，然后对每种状态存储是该如何使用的、它们内部的实现、使用场景、需要注意什么都细讲了一遍，三种存储方式各有特点，可以满足不同场景的需求，通常来说，在开发程序之前，我们要先分析自己 Job 的场景和状态大小的预测，然后根据预测来进行选择何种状态存储，如果拿捏不定的话，建议先在测试环境进行测试，只有选择了正确的状态存储后端，这样才能够保证后面自己的 Job 在生产环境能够稳定的运行。</p>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Flink 常用的 Source 和 Sink Connectors</title>
      <link href="2019/12/18/Flink-%E5%B8%B8%E7%94%A8%E7%9A%84-Source-%E5%92%8C-Sink-Connectors/"/>
      <url>2019/12/18/Flink-%E5%B8%B8%E7%94%A8%E7%9A%84-Source-%E5%92%8C-Sink-Connectors/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p>Flink Job 的大致结构就是 <code>Source ——&gt; Transformation ——&gt; Sink</code>。</p><a id="more"></a><h3 id="Data-Source-介绍"><a href="#Data-Source-介绍" class="headerlink" title="Data Source 介绍"></a>Data Source 介绍</h3><p>Data Source 是什么呢？就字面意思其实就可以知道：数据来源。</p><p>Flink 做为一款流式计算框架，它可用来做批处理，即处理静态的数据集、历史的数据集；也可以用来做流处理，即处理实时的数据流（做计算操作），然后将处理后的数据实时下发，只要数据源源不断过来，Flink 就能够一直计算下去。</p><p>Flink 中你可以使用 <code>StreamExecutionEnvironment.addSource(sourceFunction)</code> 来为你的程序添加数据来源。</p><p>Flink 已经提供了若干实现好了的 source function，当然你也可以通过实现 SourceFunction 来自定义非并行的 source 或者实现 ParallelSourceFunction 接口或者扩展 RichParallelSourceFunction 来自定义并行的 source。</p><p>那么常用的 Data Source 有哪些呢？</p><h3 id="常用的-Data-Source"><a href="#常用的-Data-Source" class="headerlink" title="常用的 Data Source"></a>常用的 Data Source</h3><p>StreamExecutionEnvironment 中可以使用以下这些已实现的 stream source。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-23-083744.png" alt="img"></p><p>总的来说可以分为下面几大类：</p><h4 id="基于集合"><a href="#基于集合" class="headerlink" title="基于集合"></a>基于集合</h4><ol><li>fromCollection(Collection) - 从 Java 的 Java.util.Collection 创建数据流。集合中的所有元素类型必须相同。</li><li>fromCollection(Iterator, Class) - 从一个迭代器中创建数据流。Class 指定了该迭代器返回元素的类型。</li><li>fromElements(T …) - 从给定的对象序列中创建数据流。所有对象类型必须相同。</li></ol><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">DataStream&lt;Event&gt; input = env.fromElements(</span><br><span class="line">    <span class="keyword">new</span> Event(<span class="number">1</span>, <span class="string">&quot;barfoo&quot;</span>, <span class="number">1.0</span>),</span><br><span class="line">    <span class="keyword">new</span> Event(<span class="number">2</span>, <span class="string">&quot;start&quot;</span>, <span class="number">2.0</span>),</span><br><span class="line">    <span class="keyword">new</span> Event(<span class="number">3</span>, <span class="string">&quot;foobar&quot;</span>, <span class="number">3.0</span>),</span><br><span class="line">    ...</span><br><span class="line">);</span><br></pre></td></tr></table></figure><ol><li>fromParallelCollection(SplittableIterator, Class) - 从一个迭代器中创建并行数据流。Class 指定了该迭代器返回元素的类型。</li><li>generateSequence(from, to) - 创建一个生成指定区间范围内的数字序列的并行数据流。</li></ol><h4 id="基于文件"><a href="#基于文件" class="headerlink" title="基于文件"></a>基于文件</h4><p>1、readTextFile(path) - 读取文本文件，即符合 TextInputFormat 规范的文件，并将其作为字符串返回。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">final</span> StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">DataStream&lt;String&gt; text = env.readTextFile(<span class="string">&quot;file:///path/to/file&quot;</span>);</span><br></pre></td></tr></table></figure><p>2、readFile(fileInputFormat, path) - 根据指定的文件输入格式读取文件（一次）。</p><p>3、readFile(fileInputFormat, path, watchType, interval, pathFilter, typeInfo) - 这是上面两个方法内部调用的方法。它根据给定的 fileInputFormat 和读取路径读取文件。根据提供的 watchType，这个 source 可以定期（每隔 interval 毫秒）监测给定路径的新数据（FileProcessingMode.PROCESS<em>CONTINUOUSLY），或者处理一次路径对应文件的数据并退出（FileProcessingMode.PROCESS</em>ONCE）。你可以通过 pathFilter 进一步排除掉需要处理的文件。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">final</span> StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">DataStream&lt;MyEvent&gt; stream = env.readFile(</span><br><span class="line">        myFormat, myFilePath, FileProcessingMode.PROCESS_CONTINUOUSLY, <span class="number">100</span>,</span><br><span class="line">        FilePathFilter.createDefaultFilter(), typeInfo);</span><br></pre></td></tr></table></figure><p><strong>实现:</strong></p><p>在具体实现上，Flink 把文件读取过程分为两个子任务，即目录监控和数据读取。每个子任务都由单独的实体实现。目录监控由单个非并行（并行度为1）的任务执行，而数据读取由并行运行的多个任务执行。后者的并行性等于作业的并行性。单个目录监控任务的作用是扫描目录（根据 watchType 定期扫描或仅扫描一次），查找要处理的文件并把文件分割成切分片（splits），然后将这些切分片分配给下游 reader。reader 负责读取数据。每个切分片只能由一个 reader 读取，但一个 reader 可以逐个读取多个切分片。</p><p><strong>重要注意：</strong></p><p>如果 watchType 设置为 FileProcessingMode.PROCESS_CONTINUOUSLY，则当文件被修改时，其内容将被重新处理。这会打破“exactly-once”语义，因为在文件末尾附加数据将导致其所有内容被重新处理。</p><p>如果 watchType 设置为 FileProcessingMode.PROCESS_ONCE，则 source 仅扫描路径一次然后退出，而不等待 reader 完成文件内容的读取。当然 reader 会继续阅读，直到读取所有的文件内容。关闭 source 后就不会再有检查点。这可能导致节点故障后的恢复速度较慢，因为该作业将从最后一个检查点恢复读取。</p><h4 id="基于-Socket"><a href="#基于-Socket" class="headerlink" title="基于 Socket"></a>基于 Socket</h4><p>socketTextStream(String hostname, int port) - 从 socket 读取。元素可以用分隔符切分。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">DataStream&lt;Tuple2&lt;String, Integer&gt;&gt; dataStream = env</span><br><span class="line">        .socketTextStream(<span class="string">&quot;localhost&quot;</span>, <span class="number">9999</span>) <span class="comment">// 监听 localhost 的 9999 端口过来的数据</span></span><br><span class="line">        .flatMap(<span class="keyword">new</span> Splitter())</span><br><span class="line">        .keyBy(<span class="number">0</span>)</span><br><span class="line">        .timeWindow(Time.seconds(<span class="number">5</span>))</span><br><span class="line">        .sum(<span class="number">1</span>);</span><br></pre></td></tr></table></figure><h4 id="自定义"><a href="#自定义" class="headerlink" title="自定义"></a>自定义</h4><p>addSource - 添加一个新的 source function。例如，你可以用 addSource(new FlinkKafkaConsumer011&lt;&gt;(…)) 从 Apache Kafka 读取数据。</p><p><strong>说说上面几种的特点</strong></p><ol><li>基于集合：有界数据集，更偏向于本地测试用</li><li>基于文件：适合监听文件修改并读取其内容</li><li>基于 Socket：监听主机的 host port，从 Socket 中获取数据</li><li>自定义 addSource：大多数的场景数据都是无界的，会源源不断过来。比如去消费 Kafka 某个 topic 上的数据，这时候就需要用到这个 addSource，可能因为用的比较多的原因吧，Flink 直接提供了 FlinkKafkaConsumer011 等类可供你直接使用。你可以去看看 FlinkKafkaConsumerBase 这个基础类，它是 Flink Kafka 消费的最根本的类。</li></ol><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">DataStream&lt;KafkaEvent&gt; input = env</span><br><span class="line">        .addSource(</span><br><span class="line">            <span class="keyword">new</span> FlinkKafkaConsumer011&lt;&gt;(</span><br><span class="line">                parameterTool.getRequired(<span class="string">&quot;input-topic&quot;</span>), <span class="comment">//从参数中获取传进来的 topic </span></span><br><span class="line">                <span class="keyword">new</span> KafkaEventSchema(),</span><br><span class="line">                parameterTool.getProperties())</span><br><span class="line">            .assignTimestampsAndWatermarks(<span class="keyword">new</span> CustomWatermarkExtractor()));</span><br></pre></td></tr></table></figure><p>Flink 目前支持如下面常见的 Source：</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/UTfWCZ.jpg" alt="img"></p><p>如果你想自定义自己的 Source 呢？在后面 3.8 节会讲解。</p><h3 id="Data-Sink-介绍"><a href="#Data-Sink-介绍" class="headerlink" title="Data Sink 介绍"></a>Data Sink 介绍</h3><p>Data sink 有点把数据存储下来（落库）的意思。Flink 在拿到数据后做一系列的计算后，最后要将计算的结果往下游发送。比如将数据存储到 MySQL、ElasticSearch、Cassandra，或者继续发往 Kafka、 RabbitMQ 等消息队列，更或者直接调用其他的第三方应用服务（比如告警）。</p><h3 id="常用的-Data-Sink"><a href="#常用的-Data-Sink" class="headerlink" title="常用的 Data Sink"></a>常用的 Data Sink</h3><p>上面介绍了 Flink Data Source 有哪些，这里也看看 Flink Data Sink 支持的有哪些。</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/siWsAK.jpg" alt="img"></p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-23-084839.png" alt="img"></p><p>可以看到有 Kafka、ElasticSearch、Socket、RabbitMQ、JDBC、Cassandra POJO、File、Print 等 Sink 的方式。</p>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Flink 多种时间语义对比</title>
      <link href="2019/12/18/Flink-%E5%A4%9A%E7%A7%8D%E6%97%B6%E9%97%B4%E8%AF%AD%E4%B9%89%E5%AF%B9%E6%AF%94/"/>
      <url>2019/12/18/Flink-%E5%A4%9A%E7%A7%8D%E6%97%B6%E9%97%B4%E8%AF%AD%E4%B9%89%E5%AF%B9%E6%AF%94/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p>Flink 在流应用程序中支持不同的 <strong>Time</strong> 概念，有 Processing Time、Event Time 和 Ingestion Time。下面我们一起来看看这三个 Time。</p><a id="more"></a><h3 id="Processing-Time"><a href="#Processing-Time" class="headerlink" title="Processing Time"></a>Processing Time</h3><p><strong>Processing Time 是指事件被处理时机器的系统时间。</strong></p><p><font color='grey'>如果 Flink Job 设置的时间策略是 Processing Time 的话，那么后面所有基于时间的操作（如时间窗口）都将会使用当时机器的系统时间。每小时 Processing Time 窗口将包括在系统时钟指示整个小时之间到达特定操作的所有事件。</font></p><p><font color='grey'>例如，如果应用程序在上午 9:15 开始运行，则第一个每小时 Processing Time 窗口将包括在上午 9:15 到上午 10:00 之间处理的事件，下一个窗口将包括在上午 10:00 到 11:00 之间处理的事件。</font></p><p>Processing Time 是最简单的 “Time” 概念，不需要流和机器之间的协调，它提供了最好的性能和最低的延迟。但是，在分布式和异步的环境下，Processing Time 不能提供确定性，因为它容易受到事件到达系统的速度（例如从消息队列）、事件在系统内操作流动的速度以及中断的影响。</p><h3 id="Event-Time"><a href="#Event-Time" class="headerlink" title="Event Time"></a>Event Time</h3><p><strong>Event Time 是指事件发生的时间，一般就是数据本身携带的时间。这个时间通常是在事件到达 Flink 之前就确定的，并且可以从每个事件中获取到事件时间戳。</strong></p><p><strong>在 Event Time 中，时间取决于数据，而跟其他没什么关系。Event Time 程序必须指定如何生成 Event Time 水印，这是表示 Event Time 进度的机制。</strong></p><p><font color='grey'>完美的说，无论事件什么时候到达或者其怎么排序，最后处理 Event Time 将产生完全一致和确定的结果。但是，除非事件按照已知顺序（事件产生的时间顺序）到达，否则处理 Event Time 时将会因为要等待一些无序事件而产生一些延迟。由于只能等待一段有限的时间，因此就难以保证处理 Event Time 将产生完全一致和确定的结果。</font></p><p><font color='grey'>假设所有数据都已到达，Event Time 操作将按照预期运行，即使在处理无序事件、延迟事件、重新处理历史数据时也会产生正确且一致的结果。 例如，每小时事件时间窗口将包含带有落入该小时的事件时间戳的所有记录，不管它们到达的顺序如何（是否按照事件产生的时间）。</font></p><h3 id="Ingestion-Time"><a href="#Ingestion-Time" class="headerlink" title="Ingestion Time"></a>Ingestion Time</h3><p><strong>Ingestion Time 是事件进入 Flink 的时间。 在数据源操作处（进入 Flink source 时），每个事件将进入 Flink 时当时的时间作为时间戳，并且基于时间的操作（如时间窗口）会利用这个时间戳。</strong></p><p>Ingestion Time 在概念上位于 Event Time 和 Processing Time 之间。 与 Processing Time 相比，成本可能会高一点，但结果更可预测。因为 Ingestion Time 使用稳定的时间戳（只在进入 Flink 的时候分配一次），所以对事件的不同窗口操作将使用相同的时间戳（第一次分配的时间戳），而在 Processing Time 中，每个窗口操作符可以将事件分配给不同的窗口（基于机器系统时间和到达延迟）。</p><p>与 Event Time 相比，Ingestion Time 程序无法处理任何无序事件或延迟数据，但程序中不必指定如何生成水印。</p><p>在 Flink 中，Ingestion Time 与 Event Time 非常相似，唯一区别就是 Ingestion Time 具有自动分配时间戳和自动生成水印功能。</p><h3 id="三种-Time-对比结果"><a href="#三种-Time-对比结果" class="headerlink" title="三种 Time 对比结果"></a>三种 Time 对比结果</h3><p>一张图概括上面说的三种 Time：</p><img src="/Users/joker/Library/Application Support/typora-user-images/截屏2021-01-18 上午10.55.42.png" alt="截屏2021-01-18 上午10.55.42" style="zoom:50%;" /><h3 id="使用场景分析"><a href="#使用场景分析" class="headerlink" title="使用场景分析"></a>使用场景分析</h3><p>通过上面两个图相信大家已经对 Flink 中的这三个 Time 有所了解了，那么我们实际生产环境中通常该如何选择哪种 Time 呢？</p><p>一般来说在生产环境中将 Event Time 与 Processing Time 对比的比较多，这两个也是我们常用的策略，Ingestion Time 一般用的较少。</p><p>用 Processing Time 的场景大多是用户不关心事件时间，它只需要关心这个时间窗口要有数据进来，只要有数据进来，就可以对进来窗口中的数据进行一系列的计算操作，然后再将计算后的数据发往下游。</p><p>而用 Event Time 的场景一般是业务需求需要时间这个字段（比如购物时是要先有下单事件、再有支付事件；借贷事件的风控是需要依赖时间来做判断的；机器异常检测触发的告警也是要具体的异常事件的时间展示出来；商品广告及时精准推荐给用户依赖的就是用户在浏览商品的时间段/频率/时长等信息），只能根据事件时间来处理数据，而且还要从事件中获取到事件的时间。</p><p>但是使用事件时间的话，就可能有这样的情况：数据源采集的数据往消息队列中发送时可能因为网络抖动、服务可用性、消息队列的分区数据堆积的影响而导致数据到达的不一定及时，可能会出现数据出现一定的乱序、延迟几分钟等，庆幸的是 Flink 支持通过 WaterMark 机制来处理这种延迟的数据。关于 WaterMark 的机制我会在后面的文章讲解。</p><h3 id="设置-Time-策略"><a href="#设置-Time-策略" class="headerlink" title="设置 Time 策略"></a>设置 Time 策略</h3><p>在创建完流运行环境的时候，然后就可以通过 <code>env.setStreamTimeCharacteristic</code> 设置时间策略：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">final</span> StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 其他两种:</span></span><br><span class="line"><span class="comment">// env.setStreamTimeCharacteristic(TimeCharacteristic.IngestionTime);</span></span><br><span class="line"><span class="comment">// env.setStreamTimeCharacteristic(TimeCharacteristic.ProcessingTime);</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Flink Window 机制深入理解</title>
      <link href="2019/12/17/Flink-Window-%E6%9C%BA%E5%88%B6%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3/"/>
      <url>2019/12/17/Flink-Window-%E6%9C%BA%E5%88%B6%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p>目前有许多数据分析的场景从批处理到流处理的演变， 虽然可以将批处理作为流处理的特殊情况来处理，但是分析无穷集的流数据通常需要思维方式的转变并且具有其自己的术语，例如，“windowing（窗口化）”、“at-least-once（至少一次）”、“exactly-once（只有一次）” 。</p><a id="more"></a><h2 id="Flink-Window-基础概念与实现原理"><a href="#Flink-Window-基础概念与实现原理" class="headerlink" title="Flink Window 基础概念与实现原理"></a>Flink Window 基础概念与实现原理</h2><p>Apache Flink 是一个为生产环境而生的流处理器，具有易于使用的 API，可以用于定义高级流分析程序。Flink 的 API 在数据流上具有非常灵活的窗口定义，使其在其他开源流处理框架中脱颖而出。</p><h3 id="Window-有什么作用？"><a href="#Window-有什么作用？" class="headerlink" title="Window 有什么作用？"></a>Window 有什么作用？</h3><p>通常来讲，Window 就是用来对一个无限的流设置一个有限的集合，在有界的数据集上进行操作的一种机制。Window 又可以分为基于时间（Time-based）的 Window 以及基于数量（Count-based）的 window。</p><img src="/Users/joker/Library/Application Support/typora-user-images/截屏2021-01-19 上午8.47.04.png" alt="截屏2021-01-19 上午8.47.04" style="zoom:30%;" /><h3 id="Flink-自带的-Window"><a href="#Flink-自带的-Window" class="headerlink" title="Flink 自带的 Window"></a>Flink 自带的 Window</h3><p>Flink 认为 Batch 是 Streaming 的一个特例，所以 Flink 底层引擎是一个流式引擎，在上面实现了流处理和批处理。而窗口 [window] 就是从 Streaming 到 Batch 的一个桥梁。</p><p>在流处理应用中，数据是连续不断的，因此我们不可能等到所有数据都到了才开始处理。当然我们可以每来一个消息就处理一次，但是有时我们需要做一些聚合类的处理，例如：在过去的1分钟内有多少用户点击了我们的网页。在这种情况下，我们必须定义一个窗口，用来收集最近一分钟内的数据，并对这个窗口内的数据进行计算。</p><p>Flink 在 KeyedStream 中提供了下面几种 Window：</p><ul><li>以时间驱动的 Time Window</li><li>以事件数量驱动的 Count Window</li><li>以会话间隔驱动的 Session Window</li></ul><p>提供上面三种 Window 机制后，由于某些特殊的需要，DataStream API 也提供了定制化的 Window 操作，供用户自定义 Window。</p><p>下面将先围绕上面说的三种 Window 来进行分析并教大家如何使用，然后对其原理分析，最后在解析其源码实现。</p><h3 id="Time-Window-使用"><a href="#Time-Window-使用" class="headerlink" title="Time Window 使用"></a>Time Window 使用</h3><p>正如命名那样，Time Window 根据时间来聚合流数据。例如：一分钟的时间窗口就只会收集一分钟的元素，并在一分钟过后对窗口中的所有元素应用于下一个算子。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">dataStream.keyBy(<span class="number">1</span>)</span><br><span class="line">    .timeWindow(Time.minutes(<span class="number">1</span>)) <span class="comment">//time Window 每分钟统计一次数量和</span></span><br><span class="line">    .sum(<span class="number">1</span>);</span><br></pre></td></tr></table></figure><p>在第一个窗口中（1 ～ 2 分钟）和为 7、第二个窗口中（2 ～ 3 分钟）和为 12、第三个窗口中（3 ～ 4 分钟）和为 7、第四个窗口中（4 ～ 5 分钟）和为 19。</p><p>另外在 Time Window 中还支持滑动的时间窗口，比如定义了一个每 30s 滑动一次的 1 分钟时间窗口，它会每隔 30s 去统计过去一分钟窗口内的数据，同样使用也很简单，输入两个时间参数，如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">dataStream.keyBy(<span class="number">1</span>)</span><br><span class="line">    .timeWindow(Time.minutes(<span class="number">1</span>), Time.seconds(<span class="number">30</span>)) <span class="comment">//sliding time Window 每隔 30s 统计过去一分钟的数量和</span></span><br><span class="line">    .sum(<span class="number">1</span>);</span><br></pre></td></tr></table></figure><p>滑动时间窗口的数据聚合流程如下图所示：</p><h3 id="Count-Window-使用"><a href="#Count-Window-使用" class="headerlink" title="Count Window 使用"></a>Count Window 使用</h3><p>Apache Flink 还提供计数窗口功能，如果计数窗口的值设置的为 3 ，那么将会在窗口中收集 3 个事件，并在添加第 3 个元素时才会计算窗口中所有事件的值。</p><ul><li><p><strong><code>Tumbling Count Window</code></strong></p><blockquote><p>当我们想要每100个用户购买行为事件统计购买总数，那么每当窗口中填满100个元素了，就会对窗口进行计算，这种窗口我们称之为滚动计数窗口</p></blockquote></li><li><p><strong><code>Sliding Count Window</code></strong></p><blockquote><p>但是对于某些应用，它们需要的窗口是不间断的，需要平滑地进行窗口聚合。比如，我们可以每30秒计算一次最近一分钟用户购买的商品总数。这种窗口我们称为滑动时间窗口（Sliding Time Window）。在滑窗中，一个元素可以对应多个窗口。通过使用 DataStream API，我们可以这样实现：</p></blockquote></li></ul><p>在 Flink 中使用 Count Window 非常简单，输入一个 long 类型的参数，这个参数代表窗口中事件的数量，使用如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">dataStream.keyBy(<span class="number">1</span>)</span><br><span class="line">    .countWindow(<span class="number">3</span>) <span class="comment">//统计每 3 个元素的数量之和</span></span><br><span class="line">    .sum(<span class="number">1</span>);</span><br></pre></td></tr></table></figure><p>计数窗口的数据窗口聚合流程如下图所示：</p><p>另外在 Count Window 中还支持滑动的计数窗口，比如定义了一个每 3 个事件滑动一次的 4 个事件的计数窗口，它会每隔 3 个事件去统计过去 4 个事件计数窗口内的数据，使用也很简单，输入两个 long 类型的参数，如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">dataStream.keyBy(<span class="number">1</span>) </span><br><span class="line">    .countWindow(<span class="number">4</span>, <span class="number">3</span>) <span class="comment">//每隔 3 个元素统计过去 4 个元素的数量之和</span></span><br><span class="line">    .sum(<span class="number">1</span>);</span><br></pre></td></tr></table></figure><h3 id="Session-Window-使用"><a href="#Session-Window-使用" class="headerlink" title="Session Window 使用"></a>Session Window 使用</h3><p>Apache Flink 还提供了会话窗口，使用该窗口的时候可以传入一个时间参数（表示某种数据维持的会话持续时长），如果超过这个时间，就代表着超出会话时长。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">dataStream.keyBy(<span class="number">1</span>)</span><br><span class="line">    .window(ProcessingTimeSessionWindows.withGap(Time.seconds(<span class="number">5</span>)))<span class="comment">//表示如果 5s 内没出现数据则认为超出会话时长，然后计算这个窗口的和</span></span><br><span class="line">    .sum(<span class="number">1</span>);</span><br></pre></td></tr></table></figure><h3 id="如何自定义-Window？"><a href="#如何自定义-Window？" class="headerlink" title="如何自定义 Window？"></a>如何自定义 Window？</h3><p>Apache Flink 还提供了用户可自定义的 Window</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-23-073301.png" alt="img"></p><h3 id="3-2-8-Window-源码定义"><a href="#3-2-8-Window-源码定义" class="headerlink" title="3.2.8 Window 源码定义"></a>3.2.8 Window 源码定义</h3><p>Flink 中自带的 Window 主要利用了 KeyedStream 的 API 来实现</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">Window</span> </span>&#123;</span><br><span class="line">    <span class="comment">//获取属于此窗口的最大时间戳</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="keyword">long</span> <span class="title">maxTimestamp</span><span class="params">()</span></span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>查看源码可以看见 Window 这个抽象类有如下实现类：</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-17-163050.png" alt="img"></p><p><strong>TimeWindow</strong> 源码定义如下:</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TimeWindow</span> <span class="keyword">extends</span> <span class="title">Window</span> </span>&#123;</span><br><span class="line">    <span class="comment">//窗口开始时间</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">long</span> start;</span><br><span class="line">    <span class="comment">//窗口结束时间</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">long</span> end;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>GlobalWindow</strong> 源码定义如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">GlobalWindow</span> <span class="keyword">extends</span> <span class="title">Window</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> GlobalWindow INSTANCE = <span class="keyword">new</span> GlobalWindow();</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="title">GlobalWindow</span><span class="params">()</span> </span>&#123; &#125;</span><br><span class="line">    <span class="comment">//对外提供 get() 方法返回 GlobalWindow 实例，并且是个全局单例</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> GlobalWindow <span class="title">get</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> INSTANCE;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="Window-组件之-WindowAssigner-使用及源码分析"><a href="#Window-组件之-WindowAssigner-使用及源码分析" class="headerlink" title="Window 组件之 WindowAssigner 使用及源码分析"></a>Window 组件之 WindowAssigner 使用及源码分析</h3><p>到达窗口操作符的元素被传递给 WindowAssigner。WindowAssigner 将元素分配给一个或多个窗口，可能会创建新的窗口。</p><p>窗口本身只是元素列表的标识符，它可能提供一些可选的元信息，例如 TimeWindow 中的开始和结束时间。注意，元素可以被添加到多个窗口，这也意味着一个元素可以同时在多个窗口存在。我们来看下 WindowAssigner 的代码的定义吧：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">WindowAssigner</span>&lt;<span class="title">T</span>, <span class="title">W</span> <span class="keyword">extends</span> <span class="title">Window</span>&gt; <span class="keyword">implements</span> <span class="title">Serializable</span> </span>&#123;</span><br><span class="line">    <span class="comment">//分配数据到窗口并返回窗口集合</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> Collection&lt;W&gt; <span class="title">assignWindows</span><span class="params">(T element, <span class="keyword">long</span> timestamp, WindowAssignerContext context)</span></span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>查看源码可以看见 WindowAssigner 这个抽象类有如下实现类：</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-17-163413.png" alt="img"></p><p>这些 WindowAssigner 实现类的作用介绍：</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-05-16-155715.jpg" alt="img"></p><p>TumblingEventTimeWindows 的源码分析，如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TumblingEventTimeWindows</span> <span class="keyword">extends</span> <span class="title">WindowAssigner</span>&lt;<span class="title">Object</span>, <span class="title">TimeWindow</span>&gt; </span>&#123;</span><br><span class="line">    <span class="comment">//定义属性</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">long</span> size;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">long</span> offset;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//构造方法</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="title">TumblingEventTimeWindows</span><span class="params">(<span class="keyword">long</span> size, <span class="keyword">long</span> offset)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (Math.abs(offset) &gt;= size) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> IllegalArgumentException(<span class="string">&quot;TumblingEventTimeWindows parameters must satisfy abs(offset) &lt; size&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">this</span>.size = size;</span><br><span class="line">        <span class="keyword">this</span>.offset = offset;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//重写 WindowAssigner 抽象类中的抽象方法 assignWindows</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Collection&lt;TimeWindow&gt; <span class="title">assignWindows</span><span class="params">(Object element, <span class="keyword">long</span> timestamp, WindowAssignerContext context)</span> </span>&#123;</span><br><span class="line">        <span class="comment">//实现该 TumblingEventTimeWindows 中的具体逻辑</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//其他方法，对外提供静态方法，供其他类调用</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>1、定义好实现类的属性</p><p>2、根据定义的属性添加构造方法</p><p>3、重写 WindowAssigner 中的 assignWindows 等方法</p><p>4、定义其他的方法供外部调用</p><h3 id="Window-组件之-Trigger-使用及源码分析"><a href="#Window-组件之-Trigger-使用及源码分析" class="headerlink" title="Window 组件之 Trigger 使用及源码分析"></a>Window 组件之 Trigger 使用及源码分析</h3><p>Trigger 表示触发器，每个窗口都拥有一个 Trigger（触发器），该 Trigger 决定何时计算和清除窗口。当先前注册的计时器超时时，将为插入窗口的每个元素调用触发器。在每个事件上，触发器都可以决定触发，即清除（删除窗口并丢弃其内容），或者启动并清除窗口。一个窗口可以被求值多次，并且在被清除之前一直存在。注意，在清除窗口之前，窗口将一直消耗内存。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">Trigger</span>&lt;<span class="title">T</span>, <span class="title">W</span> <span class="keyword">extends</span> <span class="title">Window</span>&gt; <span class="keyword">implements</span> <span class="title">Serializable</span> </span>&#123;</span><br><span class="line">    <span class="comment">//当有数据进入到 Window 运算符就会触发该方法</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> TriggerResult <span class="title">onElement</span><span class="params">(T element, <span class="keyword">long</span> timestamp, W window, TriggerContext ctx)</span> <span class="keyword">throws</span> Exception</span>;</span><br><span class="line">    <span class="comment">//当使用触发器上下文设置的处理时间计时器触发时调用</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> TriggerResult <span class="title">onProcessingTime</span><span class="params">(<span class="keyword">long</span> time, W window, TriggerContext ctx)</span> <span class="keyword">throws</span> Exception</span>;</span><br><span class="line">    <span class="comment">//当使用触发器上下文设置的事件时间计时器触发时调用该方法</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> TriggerResult <span class="title">onEventTime</span><span class="params">(<span class="keyword">long</span> time, W window, TriggerContext ctx)</span> <span class="keyword">throws</span> Exception</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>当有数据流入 Window 运算符时就会触发 onElement 方法、当处理时间和事件时间生效时会触发 onProcessingTime 和 onEventTime 方法。每个触发动作的返回结果用 TriggerResult 定义。继续来看下 TriggerResult 的源码定义：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">enum</span> <span class="title">TriggerResult</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//不做任何操作</span></span><br><span class="line">    CONTINUE(<span class="keyword">false</span>, <span class="keyword">false</span>),</span><br><span class="line"></span><br><span class="line">    <span class="comment">//处理并移除窗口中的数据</span></span><br><span class="line">    FIRE_AND_PURGE(<span class="keyword">true</span>, <span class="keyword">true</span>),</span><br><span class="line"></span><br><span class="line">    <span class="comment">//处理窗口数据，窗口计算后不做清理</span></span><br><span class="line">    FIRE(<span class="keyword">true</span>, <span class="keyword">false</span>),</span><br><span class="line"></span><br><span class="line">    <span class="comment">//清除窗口中的所有元素，并且在不计算窗口函数或不发出任何元素的情况下丢弃窗口</span></span><br><span class="line">    PURGE(<span class="keyword">false</span>, <span class="keyword">true</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>查看源码可以看见 Trigger 这个抽象类有如下实现类：</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-17-163751.png" alt="img"></p><p>这些 Trigger 实现类的作用介绍：</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-05-17-145735.jpg" alt="img"></p><p>如果你细看了上面图中某个类的具体实现的话，你会发现一个规律，拿 CountTrigger 的源码来分析，如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CountTrigger</span>&lt;<span class="title">W</span> <span class="keyword">extends</span> <span class="title">Window</span>&gt; <span class="keyword">extends</span> <span class="title">Trigger</span>&lt;<span class="title">Object</span>, <span class="title">W</span>&gt; </span>&#123;</span><br><span class="line">    <span class="comment">//定义属性</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">long</span> maxCount;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> ReducingStateDescriptor&lt;Long&gt; stateDesc = <span class="keyword">new</span> ReducingStateDescriptor&lt;&gt;(<span class="string">&quot;count&quot;</span>, <span class="keyword">new</span> Sum(), LongSerializer.INSTANCE);</span><br><span class="line">    <span class="comment">//构造方法</span></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="title">CountTrigger</span><span class="params">(<span class="keyword">long</span> maxCount)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.maxCount = maxCount;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//重写抽象类 Trigger 中的抽象方法 </span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> TriggerResult <span class="title">onElement</span><span class="params">(Object element, <span class="keyword">long</span> timestamp, W window, TriggerContext ctx)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="comment">//实现 CountTrigger 中的具体逻辑</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> TriggerResult <span class="title">onEventTime</span><span class="params">(<span class="keyword">long</span> time, W window, TriggerContext ctx)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> TriggerResult.CONTINUE;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> TriggerResult <span class="title">onProcessingTime</span><span class="params">(<span class="keyword">long</span> time, W window, TriggerContext ctx)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> TriggerResult.CONTINUE;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>套路</strong>：</p><ol><li>定义好实现类的属性</li><li>根据定义的属性添加构造方法</li><li>重写 Trigger 中的 onElement、onEventTime、onProcessingTime 等方法</li><li>定义其他的方法供外部调用</li></ol><h3 id="Window-组件之-Evictor-使用及源码分析"><a href="#Window-组件之-Evictor-使用及源码分析" class="headerlink" title="Window 组件之 Evictor 使用及源码分析"></a>Window 组件之 Evictor 使用及源码分析</h3><p>Evictor 表示驱逐者，它可以遍历窗口元素列表，并可以决定从列表的开头删除首先进入窗口的一些元素，然后其余的元素被赋给一个计算函数，如果没有定义 Evictor，触发器直接将所有窗口元素交给计算函数。</p><p>我们来看看 Evictor 的源码定义如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">Evictor</span>&lt;<span class="title">T</span>, <span class="title">W</span> <span class="keyword">extends</span> <span class="title">Window</span>&gt; <span class="keyword">extends</span> <span class="title">Serializable</span> </span>&#123;</span><br><span class="line">    <span class="comment">//在窗口函数之前调用该方法选择性地清除元素</span></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">evictBefore</span><span class="params">(Iterable&lt;TimestampedValue&lt;T&gt;&gt; elements, <span class="keyword">int</span> size, W window, EvictorContext evictorContext)</span></span>;</span><br><span class="line">    <span class="comment">//在窗口函数之后调用该方法选择性地清除元素</span></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">evictAfter</span><span class="params">(Iterable&lt;TimestampedValue&lt;T&gt;&gt; elements, <span class="keyword">int</span> size, W window, EvictorContext evictorContext)</span></span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>查看源码可以看见 Evictor 这个接口有如下实现类：</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-17-163942.png" alt="img"></p><p>这些 Evictor 实现类的作用介绍：</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-05-17-153505.jpg" alt="img"></p><p>如果你细看了上面三种中某个类的实现的话，你会发现一个规律，比如我就拿 CountEvictor 的源码来分析，如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CountEvictor</span>&lt;<span class="title">W</span> <span class="keyword">extends</span> <span class="title">Window</span>&gt; <span class="keyword">implements</span> <span class="title">Evictor</span>&lt;<span class="title">Object</span>, <span class="title">W</span>&gt; </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">1L</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//定义属性</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">long</span> maxCount;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">boolean</span> doEvictAfter;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//构造方法</span></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="title">CountEvictor</span><span class="params">(<span class="keyword">long</span> count, <span class="keyword">boolean</span> doEvictAfter)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.maxCount = count;</span><br><span class="line">        <span class="keyword">this</span>.doEvictAfter = doEvictAfter;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//构造方法</span></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="title">CountEvictor</span><span class="params">(<span class="keyword">long</span> count)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.maxCount = count;</span><br><span class="line">        <span class="keyword">this</span>.doEvictAfter = <span class="keyword">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//重写 Evictor 中的 evictBefore 方法</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">evictBefore</span><span class="params">(Iterable&lt;TimestampedValue&lt;Object&gt;&gt; elements, <span class="keyword">int</span> size, W window, EvictorContext ctx)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (!doEvictAfter) &#123;</span><br><span class="line">            <span class="comment">//调用内部的关键实现方法 evict</span></span><br><span class="line">            evict(elements, size, ctx);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//重写 Evictor 中的 evictAfter 方法</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">evictAfter</span><span class="params">(Iterable&lt;TimestampedValue&lt;Object&gt;&gt; elements, <span class="keyword">int</span> size, W window, EvictorContext ctx)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (doEvictAfter) &#123;</span><br><span class="line">            <span class="comment">//调用内部的关键实现方法 evict</span></span><br><span class="line">            evict(elements, size, ctx);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">evict</span><span class="params">(Iterable&lt;TimestampedValue&lt;Object&gt;&gt; elements, <span class="keyword">int</span> size, EvictorContext ctx)</span> </span>&#123;</span><br><span class="line">        <span class="comment">//内部的关键实现方法</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//其他的方法</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>套路</strong>：</p><ol><li>定义好实现类的属性</li><li>根据定义的属性添加构造方法</li><li>重写 Evictor 中的 evictBefore 和 evictAfter 方法</li><li>定义关键的内部实现方法 evict，处理具体的逻辑</li><li>定义其他的方法供外部调用</li></ol><p>上面我们详细讲解了 Window 中的组件 WindowAssigner、Trigger、Evictor，然后继续回到问题：如何自定义 Window？</p><p>上文讲解了 Flink 自带的 Window（Time Window、Count Window、Session Window），然后还分析了他们的源码实现，通过这几个源码，我们可以发现，它最后调用的都有一个方法，那就是 Window 方法，如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//提供自定义 Window</span></span><br><span class="line"><span class="keyword">public</span> &lt;W extends Window&gt; <span class="function">WindowedStream&lt;T, KEY, W&gt; <span class="title">window</span><span class="params">(WindowAssigner&lt;? <span class="keyword">super</span> T, W&gt; assigner)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> WindowedStream&lt;&gt;(<span class="keyword">this</span>, assigner);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//构造一个 WindowedStream 实例</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">WindowedStream</span><span class="params">(KeyedStream&lt;T, K&gt; input,</span></span></span><br><span class="line"><span class="function"><span class="params">        WindowAssigner&lt;? <span class="keyword">super</span> T, W&gt; windowAssigner)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">this</span>.input = input;</span><br><span class="line">    <span class="keyword">this</span>.windowAssigner = windowAssigner;</span><br><span class="line">    <span class="comment">//获取一个默认的 Trigger</span></span><br><span class="line">    <span class="keyword">this</span>.trigger = windowAssigner.getDefaultTrigger(input.getExecutionEnvironment());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>可以看到这个 Window 方法传入的参数是一个 WindowAssigner 对象（你可以利用 Flink 现有的 WindowAssigner，也可以根据上面的方法来自定义自己的 WindowAssigner），然后再通过构造一个 WindowedStream 实例（在构造实例的会传入 WindowAssigner 和获取默认的 Trigger）来创建一个 Window。</p><p>另外你可以看到滑动计数窗口，在调用 window 方法之后，还调用了 WindowedStream 的 evictor 和 trigger 方法，trigger 方法会覆盖掉你之前调用 Window 方法中默认的 trigger，如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//滑动计数窗口</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> WindowedStream&lt;T, KEY, GlobalWindow&gt; <span class="title">countWindow</span><span class="params">(<span class="keyword">long</span> size, <span class="keyword">long</span> slide)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> window(GlobalWindows.create()).evictor(CountEvictor.of(size)).trigger(CountTrigger.of(slide));</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//trigger 方法</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> WindowedStream&lt;T, K, W&gt; <span class="title">trigger</span><span class="params">(Trigger&lt;? <span class="keyword">super</span> T, ? <span class="keyword">super</span> W&gt; trigger)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (windowAssigner <span class="keyword">instanceof</span> MergingWindowAssigner &amp;&amp; !trigger.canMerge()) &#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> UnsupportedOperationException(<span class="string">&quot;A merging window assigner cannot be used with a trigger that does not support merging.&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (windowAssigner <span class="keyword">instanceof</span> BaseAlignedWindowAssigner) &#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> UnsupportedOperationException(<span class="string">&quot;Cannot use a &quot;</span> + windowAssigner.getClass().getSimpleName() + <span class="string">&quot; with a custom trigger.&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//覆盖之前的 trigger</span></span><br><span class="line">    <span class="keyword">this</span>.trigger = trigger;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">this</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>从上面的各种窗口实现，你就会发现了：Evictor 是可选的，但是 WindowAssigner 和 Trigger 是必须会有的，这种创建 Window 的方法充分利用了 KeyedStream 和 WindowedStream 的 API，再加上现有的 WindowAssigner、Trigger、Evictor，你就可以创建 Window 了，另外你还可以自定义这三个窗口组件的实现类来满足你公司项目的需求。</p>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>状态一致性</title>
      <link href="2019/12/16/Flink%20%E7%8A%B6%E6%80%81%E4%B8%80%E8%87%B4%E6%80%A7/"/>
      <url>2019/12/16/Flink%20%E7%8A%B6%E6%80%81%E4%B8%80%E8%87%B4%E6%80%A7/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><h1 id="1-概述"><a href="#1-概述" class="headerlink" title="1. 概述"></a>1. 概述</h1><h1 id="2-状态一致性分类"><a href="#2-状态一致性分类" class="headerlink" title="2. 状态一致性分类"></a>2. 状态一致性分类</h1><h2 id="2-1-AT-MOST-ONCE"><a href="#2-1-AT-MOST-ONCE" class="headerlink" title="2.1. AT-MOST-ONCE"></a>2.1. <code>AT-MOST-ONCE</code></h2><h2 id="2-2-AT-LEAST-ONCE"><a href="#2-2-AT-LEAST-ONCE" class="headerlink" title="2.2. AT-LEAST-ONCE"></a>2.2. <code>AT-LEAST-ONCE</code></h2><h2 id="2-3-EXACTLY-ONCE"><a href="#2-3-EXACTLY-ONCE" class="headerlink" title="2.3. EXACTLY-ONCE"></a>2.3. <code>EXACTLY-ONCE</code></h2><blockquote><p> <code>Flink</code> 使用 <code>checkpoint</code>保证 <code>EXACTLY-ONCE</code></p></blockquote><h1 id="3-端到端状态一致性"><a href="#3-端到端状态一致性" class="headerlink" title="3. 端到端状态一致性"></a>3. 端到端状态一致性</h1><blockquote><p><strong><code>Flink</code> 通过快照机制和 <code>Barrier</code> 来实现一致性的保证，当任务中途 <code>crash</code> 或者<code>cancel</code> 之后，可以通过<code>checkpoing</code> 或者 <code>savepoint</code> 来进行恢复，实现数据流的重放。从而让任务达到一致性的效果，这种一致性需要开启 <code>exactly_once</code>模式之后才行。</strong></p><p>需要记住的是这边的 <code>Flink</code>  <code>exactly_once</code> 只是说在 <code>Flink</code> 内部是 <code>exactly_once</code> 的，并不能保证与外部存储交互时的 <code>exactly_once</code>，如果要实现外部存储连接后的 <code>exactly_once</code>，需要进行做一些特殊的处理。</p></blockquote><h2 id="3-1-预写日志"><a href="#3-1-预写日志" class="headerlink" title="3.1. 预写日志"></a>3.1. 预写日志</h2><h2 id="3-2-两阶段提交"><a href="#3-2-两阶段提交" class="headerlink" title="3.2. 两阶段提交"></a>3.2. 两阶段提交</h2><h1 id="4-Flink-Kafka-端到端状态一致性"><a href="#4-Flink-Kafka-端到端状态一致性" class="headerlink" title="4. Flink + Kafka 端到端状态一致性"></a>4. <code>Flink</code> + <code>Kafka</code> 端到端状态一致性</h1>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Spark Streaming 和 Kafka 整合开发</title>
      <link href="2019/12/15/Spark%20Streaming%20%E5%92%8C%20Kafka%20%E6%95%B4%E5%90%88%E5%BC%80%E5%8F%91/"/>
      <url>2019/12/15/Spark%20Streaming%20%E5%92%8C%20Kafka%20%E6%95%B4%E5%90%88%E5%BC%80%E5%8F%91/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p>Apache Kafka 是一个分布式的消息发布-订阅系统。可以说，任何实时大数据处理工具缺少与 Kafka 整合都是不完整的。本文将介绍如何使用 Spark Streaming 从 Kafka 中接收数据，</p><p>这里介绍两种方法</p><ol><li>使用 Receivers 和 Kafka 高层次的 API</li><li>使用 Direct API，这是使用低层次的 KafkaAPI，并没有使用到 Receivers，是 Spark 1.3.0中开始引入的。这两种方法有不同的编程模型，性能特点和语义担保</li></ol><a id="more"></a><h2 id="基于-Receivers-的方法"><a href="#基于-Receivers-的方法" class="headerlink" title="基于 Receivers 的方法"></a>基于 Receivers 的方法</h2><p>使用了 Receivers 来接收数据。Receivers 的实现使用到 Kafka 高层次的消费者API。对于所有的Receivers，接收到的数据将会保存在 Spark executors 中，然后由 Spark Streaming 启动的 Job 来处理这些数据。</p><p>然而，在默认的配置下，这种方法在失败的情况下会丢失数据，为了保证零数据丢失，你可以在 Spark Streaming 中使用 WAL 日志，这是在 Spark 1.2.0 才引入的功能，这使得我们可以将接收到的数据保存到WAL 中，所以在失败的时候，我们可以从 WAL 中恢复，而不至于丢失数据。</p><h3 id="Direct-API"><a href="#Direct-API" class="headerlink" title="Direct API"></a>Direct API</h3><p>和基于 Receiver 接收数据不一样，这种方式定期地从 Kafka 的 <code>topic+partition</code> 中查询最新的偏移量，再根据定义的偏移量范围在每个 batch 里面处理数据。当作业需要处理的数据来临时，Spark 通过调用 Kafka 的简单消费者 API 读取一定范围的数据。</p><p>和基于 Receiver 方式相比，这种方式主要有一些几个优点：</p><ol><li><p><strong>简化并行</strong></p><p>我们不需要创建多个Kafka 输入流，然后union他们。而使用directStream，Spark Streaming将会创建和 Kafka 分区一样的 RDD 分区个数，而且会从 Kafka 并行地读取数据，也就是说Spark分区将会和Kafka分区有一一对应的关系，这对我们来说很容易理解和使用；</p></li><li><p><strong>高效</strong></p><p>第一种实现零数据丢失是通过将数据预先保存在 WAL 中，这将会复制一遍数据，这种方式实际上很不高效，因为这导致了数据被拷贝两次：一次是被 Kafka 复制；另一次是写到WAL中。但是本文介绍的方法因为没有Receiver，从而消除了这个问题，所以不需要WAL日志；</p></li><li><p><strong>恰好一次语义（Exactly-once semantics）</strong></p><p>文章中通过使用Kafka高层次的API把偏移量写入Zookeeper中，这是读取Kafka中数据的传统方法。虽然这种方法可以保证零数据丢失，但是还是存在一些情况导致数据会丢失，因为在失败情况下通过Spark Streaming读取偏移量和Zookeeper中存储的偏移量可能不一致。而本文提到的方法是通过Kafka低层次的API，并没有使用到Zookeeper，偏移量仅仅被Spark Streaming保存在Checkpoint中。这就消除了Spark Streaming和Zookeeper中偏移量的不一致，而且可以保证每个记录仅仅被Spark Streaming读取一次，即使是出现故障。</p></li></ol><p>但是本方法唯一的坏处就是没有更新 Zookeeper 中的偏移量，所以基于 Zookeeper 的 Kafka 监控工具将会无法显示消费的状况。然而你可以通过 Spark 提供的 API 手动地将偏移量写入到 Zookeeper 中。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><h4 id="Spark-Streaming-和-Kafka-整合是如何保证数据零丢失"><a href="#Spark-Streaming-和-Kafka-整合是如何保证数据零丢失" class="headerlink" title="Spark Streaming 和 Kafka 整合是如何保证数据零丢失 ? "></a><font color='blue'>Spark Streaming 和 Kafka 整合是如何保证数据零丢失 ? </font></h4><p>当我们正确地部署好 Spark Streaming，我们就可以使用 Spark Streaming 提供的零数据丢失机制。为了体验这个关键的特性，你需要满足以下几个先决条件：</p><ol><li>输入的数据来自可靠的数据源和可靠的接收器</li><li>应用程序的 metadata 被 application 的 driver 持久化了(checkpointed)</li><li>启用了 WAL 特性(Write ahead log)</li></ol>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Watermark 与 Window 结合来处理延迟数据</title>
      <link href="2019/12/15/Watermark-%E4%B8%8E-Window-%E7%BB%93%E5%90%88%E6%9D%A5%E5%A4%84%E7%90%86%E5%BB%B6%E8%BF%9F%E6%95%B0%E6%8D%AE/"/>
      <url>2019/12/15/Watermark-%E4%B8%8E-Window-%E7%BB%93%E5%90%88%E6%9D%A5%E5%A4%84%E7%90%86%E5%BB%B6%E8%BF%9F%E6%95%B0%E6%8D%AE/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p>在设置 Periodic Watermark 时，是允许提供一个参数，表示数据最大的延迟时间。其实这个值要结合自己的业务以及数据的情况来设置，如果该值设置的太小会导致数据因为网络或者其他的原因从而导致乱序或者延迟的数据太多，那么最后窗口触发的时候，可能窗口里面的数据量很少，那么这样计算的结果很可能误差会很大，对于有的场景（要求正确性比较高）是不太符合需求的。但是如果该值设置的太大，那么就会导致很多窗口一直在等待延迟的数据，从而一直不触发，这样首先就会导致数据的实时性降低，另外将这么多窗口的数据存在内存中，也会增加作业的内存消耗，从而可能会导致作业发生 OOM 的问题。</p><a id="more"></a><p>综上建议：</p><ul><li>合理设置允许数据最大延迟时间</li><li>不太依赖事件时间的场景就不要设置时间策略为 EventTime</li></ul><h3 id="延迟数据该如何处理-三种方法"><a href="#延迟数据该如何处理-三种方法" class="headerlink" title="延迟数据该如何处理(三种方法)"></a>延迟数据该如何处理(三种方法)</h3><h4 id="丢弃（默认）"><a href="#丢弃（默认）" class="headerlink" title="丢弃（默认）"></a>丢弃（默认）</h4><p>在 Flink 中，对这么延迟数据的默认处理方式是丢弃。</p><h4 id="allowedLateness-再次指定允许数据延迟的时间"><a href="#allowedLateness-再次指定允许数据延迟的时间" class="headerlink" title="allowedLateness 再次指定允许数据延迟的时间"></a>allowedLateness 再次指定允许数据延迟的时间</h4><p>allowedLateness 表示允许数据延迟的时间，这个方法是在 WindowedStream 中的，用来设置允许窗口数据延迟的时间，超过这个时间的元素就会被丢弃，这个的默认值是 0，该设置仅针对于以事件时间开的窗口，它的源码如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> WindowedStream&lt;T, K, W&gt; <span class="title">allowedLateness</span><span class="params">(Time lateness)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">final</span> <span class="keyword">long</span> millis = lateness.toMilliseconds();</span><br><span class="line">    checkArgument(millis &gt;= <span class="number">0</span>, <span class="string">&quot;The allowed lateness cannot be negative.&quot;</span>);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">this</span>.allowedLateness = millis;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">this</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>之前有多个小伙伴问过我 Watermark 中允许的数据延迟和这个数据延迟的区别是啥？我的回复是该允许延迟的时间是在 Watermark 允许延迟的基础上增加的时间。那么具体该如何使用 allowedLateness 呢。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">dataStream.assignTimestampsAndWatermarks(<span class="keyword">new</span> TestWatermarkAssigner())</span><br><span class="line">    .keyBy(<span class="keyword">new</span> TestKeySelector())</span><br><span class="line">    .timeWindow(Time.milliseconds(<span class="number">1</span>), Time.milliseconds(<span class="number">1</span>))</span><br><span class="line">    .allowedLateness(Time.milliseconds(<span class="number">2</span>))  <span class="comment">//表示允许再次延迟 2 毫秒</span></span><br><span class="line">    .apply(<span class="keyword">new</span> WindowFunction&lt;Integer, String, Integer, TimeWindow&gt;() &#123;</span><br><span class="line">        <span class="comment">//计算逻辑</span></span><br><span class="line">    &#125;);</span><br></pre></td></tr></table></figure><h4 id="sideOutputLateData-收集迟到的数据"><a href="#sideOutputLateData-收集迟到的数据" class="headerlink" title="sideOutputLateData 收集迟到的数据"></a>sideOutputLateData 收集迟到的数据</h4><p>sideOutputLateData 这个方法同样是 WindowedStream 中的方法，该方法会将延迟的数据发送到给定 OutputTag 的 side output 中去，然后你可以通过 <code>SingleOutputStreamOperator.getSideOutput(OutputTag)</code> 来获取这些延迟的数据。具体的操作方法如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//定义 OutputTag</span></span><br><span class="line">OutputTag&lt;Integer&gt; lateDataTag = <span class="keyword">new</span> OutputTag&lt;Integer&gt;(<span class="string">&quot;late&quot;</span>)&#123;&#125;;</span><br><span class="line"></span><br><span class="line">SingleOutputStreamOperator&lt;String&gt; windowOperator = dataStream</span><br><span class="line">        .assignTimestampsAndWatermarks(<span class="keyword">new</span> TestWatermarkAssigner())</span><br><span class="line">        .keyBy(<span class="keyword">new</span> TestKeySelector())</span><br><span class="line">        .timeWindow(Time.milliseconds(<span class="number">1</span>), Time.milliseconds(<span class="number">1</span>))</span><br><span class="line">        .allowedLateness(Time.milliseconds(<span class="number">2</span>))</span><br><span class="line">        .sideOutputLateData(lateDataTag)    <span class="comment">//指定 OutputTag</span></span><br><span class="line">        .apply(<span class="keyword">new</span> WindowFunction&lt;Integer, String, Integer, TimeWindow&gt;() &#123;</span><br><span class="line">            <span class="comment">//计算逻辑</span></span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">windowOperator.addSink(resultSink);</span><br><span class="line"></span><br><span class="line"><span class="comment">//通过指定的 OutputTag 从 Side Output 中获取到延迟的数据之后，你可以通过 addSink() 方法存储下来，这样可以方便你后面去排查哪些数据是延迟的。</span></span><br><span class="line">windowOperator.getSideOutput(lateDataTag)</span><br><span class="line">        .addSink(lateResultSink);</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Flink 数据倾斜</title>
      <link href="2019/12/15/Flink-%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C/"/>
      <url>2019/12/15/Flink-%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p>在大数据计算场景，无论使用 MapReduce、Spark 还是 Flink 计算框架，无论是批处理还是流处理都存在数据倾斜的问题，通过本节学习产生数据倾斜的原因及如何在生产环境解决数据倾斜。</p><a id="more"></a><h3 id="数据倾斜简介"><a href="#数据倾斜简介" class="headerlink" title="数据倾斜简介"></a>数据倾斜简介</h3><p>分析一个计算各 app PV 的案例，如下图所示，圆球表示 app1 的日志，方块表示 app2 的日志，Source 端从外部系统读取用户上报的各 app 行为日志，要计算各 app 的 PV，所以按照 app 进行 keyBy，相同 app 的数据发送到同一个 Operator 实例中处理，keyBy 后对 app 的 PV 值进行累加来，最后将计算的 PV 结果输出到外部 Sink 端。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-11-12-004442.jpg" alt="img"></p><p>可以看到在任务运行过程中，计算 Count 的算子有两个并行度，其中一个并行度处理 app1 的数据，另一个并行度处理 app2 的数据。由于 app1 比较热门，所以 app1 的日志量远大于 app2 的日志量，造成计算 app1 PV 的并行度压力过大成为整个系统的瓶颈，而计算 app2 PV 的并行度数据量较少所以 CPU、内存以及网络资源的使用率整体都比较低，这就是产生数据倾斜的案例。</p><h3 id="判断是否存在数据倾斜"><a href="#判断是否存在数据倾斜" class="headerlink" title="判断是否存在数据倾斜"></a>判断是否存在数据倾斜</h3><p>这里再通过一个案例来讲述 Flink 任务如何来判断是否存在数据倾斜，如下图所示，是 Flink Web UI Job 页面展示的任务执行计划，可以看到任务经过 Operator Chain 后，总共有两个 Task，上游 Task 将数据 keyBy 后发送到下游 Task，如何判断第二个 Task 计算的数据是否存在数据呢？</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-11-12-004443.jpg" alt="img"></p><p>如下图所示，通过 Flink Web UI 中 Job 页面的第一个 Subtasks 选项卡，可以看到任务的两个 Task，点击 Task，可以看到 Task 相应的 Subtask 详情。例如 Subtask 的启动时间、结束时间、持续时长、接收数据量的字节数以及接收数据的个数。图中可以看到，相同 Task 的多个 Subtask 中，有的 Subtask 接收到 1.69 TB 的数据量，有的 Subtask 接收到 17.6 TB 的数据量，通过 Flink Web UI 可以精确地看到每个 Subtask 处理了多少数据，即可判断出 Flink 任务是否存在数据倾斜，接下来学习 Flink 中如何来解决数据倾斜。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-11-12-004431.jpg" alt="img"></p><h3 id="分析和解决数据倾斜问题"><a href="#分析和解决数据倾斜问题" class="headerlink" title="分析和解决数据倾斜问题"></a>分析和解决数据倾斜问题</h3><p>在 Flink 中，很多因素都会导致数据倾斜，例如 9.6.1 节描述的 keyBy 后的聚合操作存在数据倾斜。keyBy 之前的数据直接来自于数据源，一般不会出现数据倾斜，除非数据源中的数据发生了数据倾斜。本小节将从多个角度来解决数据倾斜。</p><h5 id="keyBy-后的聚合操作存在数据倾斜"><a href="#keyBy-后的聚合操作存在数据倾斜" class="headerlink" title="keyBy 后的聚合操作存在数据倾斜"></a>keyBy 后的聚合操作存在数据倾斜</h5><p>Flink 社区关于数据倾斜的解决方案炒得最热的也莫过于 LocalKeyBy 了。Flink 中数据倾斜一般发生于 keyBy 之后的聚合操作，LocalKeyBy 的思想是：在 keyBy 上游算子数据发送之前，首先在上游算子的本地对数据进行聚合后再发送到下游，使下游接收到的数据量大大减少，从而使得 keyBy 之后的聚合操作不再是任务的瓶颈。</p><p>如下图所示，Source 算子向下游发送数据之前，首先对数据进行预聚合，Source Subtask 0 预聚合后，圆圈 PV 值为 5、方块 PV 值为 2，Source Subtask 1 预聚合后，圆圈 PV 值为 6、方块 PV 值为 1。keyBy 后，Count 算子进行 PV 值的累加，计算圆圈 PV 的 Subtask 接收到 5 和 6，只需要将 5+6 即可计算出圆圈总 PV 值为 11，计算方块 PV 的 Subtask 接收到 2 和 1，只需要将 2 +1 即可计算出方块总 PV 值为 3，最后将圆圈和方块的 PV 结果输出到 Sink 端即可。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-11-12-004439.jpg" alt="img"></p><p>使用该方案计算 PV，带来了两个非常大的好处。</p><ul><li>在上游算子中对数据进行了预聚合，因此大大减少了上游往下游发送的数据量，从而减少了网络间的数据传输，节省了集群的带宽资源。上图案例中如果不聚合，上游需要往下游发送 14 条数据，聚合后仅仅需要发送 4 条数据即可。如果上游算子接收 1 万条数据后聚合一次，那么数据的压缩比会更大，优化效果会更加明显。</li><li>下游拿到的直接是上游聚合好的中间结果，因此下游 Count 算子计算的数据量大大减少，而且 Count 算子不再会有数据倾斜的问题。</li></ul><p>上游算子相比之前多了一个聚合的工作，所以压力必然会增加，但是只要数据源不发生数据倾斜，那么上游 Source 算子的各并行度之间的负载就会比较均衡。</p><p>这里就是 MapReduce 中 Combiner 的思想嘛，在 Map 端对数据进行预聚合之后，再将预聚合后的数据发送到 Reduce 端去处理，从而大大减少了 shuffle 的数据量。</p><p>虽然思想一样，但 Flink 流处理的预聚合相比 MapReduce 的批处理而言，带来了一个新的挑战：Flink 是天然的流式处理，即来一条数据处理一条（这里不考虑 Flink 网络传输层的 Buffer 机制），但是聚合操作要求必须是多条数据或者一批数据才能聚合，单条数据没有办法通过聚合来减少数据量。</p><p>所以从 Flink LocalKeyBy 实现原理来讲，必然会存在一个积攒批次的过程，在上游算子中必须攒够一定的数据量，对这些数据聚合后再发送到下游。既然是积攒批次，那肯定有一个积攒批次的策略，上图案例可以理解为每个批次 7 条数据，当读取到 7 条数据后，将这 7 条数据聚合后发送到下游。</p><p><strong>具体实现逻辑是：内存里维护一个计数器，每来一条数据计数器加一，并将数据聚合放到内存 Buffer 中，当计数器到达 7 时，将内存 Buffer 中的数据发送到下游、计数器清零、Buffer 清空。</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LocalKeyByFlatMap</span> <span class="keyword">extends</span> <span class="title">FlatMapFunction</span>&lt;<span class="title">String</span>, <span class="title">Tuple2</span>&lt;<span class="title">String</span>, <span class="title">Long</span>&gt;&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//本地 buffer，存放 local 端缓存的 app 的 pv 信息</span></span><br><span class="line">    <span class="keyword">private</span> HashMap&lt;String, Long&gt; localPvStat;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//缓存的数据量大小，即：缓存多少数据再向下游发送</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> batchSize;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//计数器，获取当前批次接收的数据量</span></span><br><span class="line">    <span class="keyword">private</span> AtomicInteger currentSize = <span class="keyword">new</span> AtomicInteger(<span class="number">0</span>);;</span><br><span class="line"></span><br><span class="line">    LocalKeyByFlatMap(<span class="keyword">int</span> batchSize)&#123;</span><br><span class="line">        <span class="keyword">this</span>.batchSize = batchSize;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">flatMap</span><span class="params">(String in, Collector collector)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="comment">//  将新来的数据添加到 buffer 中</span></span><br><span class="line">        Long pv = localPvStat.getOrDefault(in, <span class="number">0L</span>);</span><br><span class="line">        localPvStat.put(in, pv + <span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 如果到达设定的批次，则将 buffer 中的数据发送到下游</span></span><br><span class="line">        <span class="keyword">if</span>(currentSize.incrementAndGet() &gt;= batchSize)&#123;</span><br><span class="line">            <span class="comment">// 遍历 Buffer 中数据，发送到下游</span></span><br><span class="line">            <span class="keyword">for</span>(Map.Entry&lt;String, Long&gt; appIdPv: localPvStat.entrySet()) &#123;</span><br><span class="line">                collector.collect(Tuple2.of(appIdPv.getKey(), appIdPv.getValue()));</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">// buffer 清空，计数器清零</span></span><br><span class="line">            localPvStat.clear();</span><br><span class="line">            currentSize.set(<span class="number">0</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>代码逻辑比较简单，使用了 FlatMap 算子来做缓冲，每来一条数据都需要检索，为了提高检索效率，所以这里使用 HashMap 类型的 localPvStat 用来做 Buffer 来缓存数据，currentSize 记录当前批次已经往 localPvStat 中写入的数据量。在 LocalKeyByFlatMap 构造器中需要初始化 batchSize，即批次大小。flatMap 方法将新数据添加到 localPvStat 中，currentSize 进行加一操作，且 currentSize 加一后如果大于 batchSize 则表示当前批次的数据已经够了，需要将数据发送到下游，则遍历 localPvStat，将 Buffer 中的数据发送到下游，并将 localPvStat 清空且 currentSize 清零。</p><p>代码逻辑简单易懂，但是问题又来了，在积攒批次的过程中，如果发生故障，Flink 任务能保障 Exactly Once 吗？</p><p>直接给出答案：不能保证 Exactly Once，可能会丢数据，为什么呢？</p><p>如下图所示，batchSize 设置的 7，但是当 JobManager 触发 Checkpoint 的时候，Source Subtask 0 消费到 offset 为 13 的位置、Source Subtask 1 消费到 offset 为 12 的位置，所以 Source 0 会将 offset=13 保存到状态后端，Source 1 会将 offset=12 保存到状态后端。接着 Checkpoint barrier 跟随着数据往下游发送到 LocalKeyBy，此时 LocalKeyBy 0 的 Buffer 中只有 6 条数据、LocalKeyBy 1 的 Buffer 中只有 5 条数据，所以 LocalKeyBy 0 和 1 都不会将数据发送到下游。但是 barrier 会接着往下游传递到 Count 算子，Count 算子会对自身状态信息进行快照，Count 0 会将圆圈 PV=11 保存到状态后端、Count 1 会将圆圈 PV=3 保存到状态后端，各 task 向 JobManager 反馈，最后 Checkpoint 成功了，紧接着数据正常开始处理。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-11-12-004433.jpg" alt="img"></p><p>数据正常处理一段时间后，由于机器故障 Flink 任务突然挂了，如下图所示，Flink 任务会从状态恢复，Source Subtask 0 从 offset 为 13 的位置开始消费 Kafka，Source Subtask 1 从 offset 为 12 的位置开始消费 Kafka。Count 0 恢复后保存圆圈的 PV 为 11，Count 1 恢复后保存方块的 PV 为 3。此时任务从状态中恢复完成，正常开始处理数据，请问 Flink 任务从状态恢复后丢数据了吗？</p><p>丢了，因为 Source 0 对应的 offset 13 表示 Source 0 消费了 13 条数据，但是其中有 6 条数据缓存在 LocalKeyBy 0 的 Buffer 中没及时发送到下游，所以这 6 条数据丢了，同理 Source 1 对应的 offset 12 表示 Source 1 消费了 12 条数据，其中还有 5 条数据缓存在 LocalKeyBy 1 的 Buffer 中没及时发送到下游，所以这 5 条数据也丢了。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-11-13-%E7%A7%AF%E6%94%92%E6%89%B9%E6%AC%A1%E7%9A%84%E8%BF%87%E7%A8%8B%E4%B8%AD%20Restore.png" alt="img"></p><p>通过上述详细案例分析，知道了我们设计的 LocalKeyBy 虽然能够提高性能，但存在丢数据的风险。，<strong>Flink 虽然支持 Exactly Once，但不是说你的代码随便瞎写 Flink 也能保证 Exactly Once，做为使用 Flink 的一员，我们应该根据原理书写出能保证 Flink Exactly Once 的代码。</strong></p><p>上述方案该如何完善才能保证 Exactly Once 呢？在 Checkpoint 时上述方案会把 LocalKeyBy 算子 Buffer 中的数据丢弃，所以重点应该是如何来保证 LocalKeyBy 算子 Buffer 中的数据不丢。在 Checkpoint 时可以将 Buffer 中还未发送到下游的数据保存到 Flink 的状态中，这样当 Flink 任务从 Checkpoint 处恢复时，可以将那些在 Buffer 中的数据从状态后端恢复。如下图所示，相比上述方案，Checkpoint 时会将 LocalKeyBy 算子 Buffer 中的数据也保存到状态后端。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-11-12-4434.jpg" alt="img"></p><p>如下图所示，当 Flink 任务从 Checkpoint 处恢复时，不仅恢复 offset 信息和 PV 信息，还需要把 LocalKeyBy 算子 Buffer 中的数据恢复，这样就可以保证不丢数据了。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-11-12-004436.jpg" alt="img"></p><p>具体代码如何实现呢？Checkpoint 时 LocalKeyBy 算子可能还有缓冲的数据没发送到下游，为了保证 Exactly Once，这里需要将 Buffer 中的数据保存在状态中。</p><p>Flink 有两种 State 分别是 OperatorState 和 KeyedState，OperatorState 是一个 Operator 实例对应一个State，KeyedState 是每个 key 对应一个 State，KeyedState 只能作用于 keyby 算子之后的 KeyedStream。</p><p>上图中我们可以看出，LocalKeyBy 算子位于 keyBy 算子之前，因此 LocalKeyBy 算子内部不能使用 KeyedState，只能使用 OperatorState，且 OperatorState 只支持一种数据结构，即 ListState，所以这里 buffer 中的数据只能保存在 OperatorState 类型的 ListState 中。当 Checkpoint 时，需要将内存 buffer 中的数据添加到 ListState，状态中需要保存 KV 类型的数据，key 是 appId、value 是 app 对应的 PV 值。</p><p>这里为了在 ListState 中保存 KV 格式的数据，需要将 buffer 中 KV 类型的数据转化为 Tuple2 类型后再添加到 ListState 中。代码具体实现如下所示：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LocalKeyByFlatMap</span> <span class="keyword">extends</span> <span class="title">RichFlatMapFunction</span>&lt;<span class="title">String</span>, <span class="title">Tuple2</span>&lt;<span class="title">String</span>, <span class="title">Long</span>&gt;&gt; <span class="keyword">implements</span> <span class="title">CheckpointedFunction</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//Checkpoint 时为了保证 Exactly Once，将 buffer 中的数据保存到该 ListState 中</span></span><br><span class="line">    <span class="keyword">private</span> ListState&lt;Tuple2&lt;String, Long&gt;&gt; localPvStatListState;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//本地 buffer，存放 local 端缓存的 app 的 pv 信息</span></span><br><span class="line">    <span class="keyword">private</span> HashMap&lt;String, Long&gt; localPvStat;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//缓存的数据量大小，即：缓存多少数据再向下游发送</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> batchSize;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//计数器，获取当前批次接收的数据量</span></span><br><span class="line">    <span class="keyword">private</span> AtomicInteger currentSize;</span><br><span class="line"></span><br><span class="line">    LocalKeyByFlatMap(<span class="keyword">int</span> batchSize)&#123;</span><br><span class="line">        <span class="keyword">this</span>.batchSize = batchSize;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">flatMap</span><span class="params">(String in, Collector collector)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="comment">//  将新来的数据添加到 buffer 中</span></span><br><span class="line">        Long pv = localPvStat.getOrDefault(in, <span class="number">0L</span>);</span><br><span class="line">        localPvStat.put(in, pv + <span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 如果到达设定的批次，则将 buffer 中的数据发送到下游</span></span><br><span class="line">        <span class="keyword">if</span>(currentSize.incrementAndGet() &gt;= batchSize)&#123;</span><br><span class="line">            <span class="comment">// 遍历 Buffer 中数据，发送到下游</span></span><br><span class="line">            <span class="keyword">for</span>(Map.Entry&lt;String, Long&gt; appIdPv: localPvStat.entrySet()) &#123;</span><br><span class="line">                collector.collect(Tuple2.of(appIdPv.getKey(), appIdPv.getValue()));</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">// Buffer 清空，计数器清零</span></span><br><span class="line">            localPvStat.clear();</span><br><span class="line">            currentSize.set(<span class="number">0</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">snapshotState</span><span class="params">(FunctionSnapshotContext functionSnapshotContext)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 将 buffer 中的数据保存到状态中，来保证 Exactly Once</span></span><br><span class="line">        localPvStatListState.clear();</span><br><span class="line">        <span class="keyword">for</span>(Map.Entry&lt;String, Long&gt; appIdPv: localPvStat.entrySet()) &#123;</span><br><span class="line">            localPvStatListState.add(Tuple2.of(appIdPv.getKey(), appIdPv.getValue()));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">initializeState</span><span class="params">(FunctionInitializationContext context)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 从状态中恢复 buffer 中的数据</span></span><br><span class="line">        localPvStatListState = context.getOperatorStateStore().getListState(</span><br><span class="line">                <span class="keyword">new</span> ListStateDescriptor&lt;&gt;(<span class="string">&quot;localPvStat&quot;</span>,</span><br><span class="line">                        TypeInformation.of(<span class="keyword">new</span> TypeHint&lt;Tuple2&lt;String, Long&gt;&gt;() &#123;</span><br><span class="line">                        &#125;)));</span><br><span class="line">        localPvStat = <span class="keyword">new</span> HashMap();</span><br><span class="line">        <span class="keyword">if</span>(context.isRestored()) &#123;</span><br><span class="line">            <span class="comment">// 从状态中恢复数据到 localPvStat 中</span></span><br><span class="line">            <span class="keyword">for</span>(Tuple2&lt;String, Long&gt; appIdPv: localPvStatListState.get())&#123;</span><br><span class="line">                localPvStat.put(appIdPv.f0, appIdPv.f1);</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">//  从状态恢复时，默认认为 buffer 中数据量达到了 batchSize，需要向下游发送数据了</span></span><br><span class="line">            currentSize = <span class="keyword">new</span> AtomicInteger(batchSize);</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            currentSize = <span class="keyword">new</span> AtomicInteger(<span class="number">0</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>上述改进方案后的 LocalKeyByFlatMap 相比之前方案仅仅增加了一个属性，即：<code>ListState&gt;</code> 类型的 localPvStatListState 用来存放 Checkpoint 时 buffer 中那些可能丢失的数据。在 snapshotState 方法中将 buffer 中的数据保存到状态中，在 initializeState 方法中将状态中恢复的数据 put 到 buffer 中并初始化计数器 currentSize。代码相对比较简答，容易看懂。</p><p>请问上述代码能保障 buffer 中的数据不丢吗？如果不修改 Source Task 和 LocalKeyByFlatMap 算子的并行度，理论来讲可以保证 Exactly Once，但是一旦修改并行度，还能保证 Exactly Once 吗？当并行度降低后，getOperatorStateStore().getListState() 恢复 ListState 时，会把 ListState 中的状态信息均匀分布到各个 Operator 实例中。当上述案例中 LocalKeyBy 的并行度从 2 调节为 1 时，数据恢复如下图所示：</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-11-12-004441.jpg" alt="img"></p><p>首先 Source 端 partition 0 和 partition 1 的 offset 信息恢复没有问题，Count 算子圆圈和方块的 PV 信息恢复也没有问题。关键在于 LocalKeyBy 算子中 PV 信息恢复时会丢数据吗？状态恢复时，从状态中将 PV 信息恢复到 buffer 中的核心代码如下所示：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 从状态中恢复数据到 localPvStat 中</span></span><br><span class="line"><span class="keyword">for</span>(Tuple2&lt;String, Long&gt; appIdPv: localPvStatListState.get())&#123;</span><br><span class="line">    localPvStat.put(appIdPv.f0, appIdPv.f1);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>从状态中会恢复 4 个 Tuple2，分别是 &lt;圆圈,4&gt;、&lt;方块,2&gt;、&lt;圆圈,4&gt;、&lt;方块,1&gt;，这里有两个圆圈、两个方块，恢复到 HashMap 类型的 localPvStat，HashMap 中相同的 key 不能重复，所以 HashMap 中不可能保存两个圆圈和两个方块。恢复时 app 相同的数据，应该将其 PV 值累加，所以恢复的结果应该是 &lt;圆圈,8&gt;、&lt;方块,3&gt;。但是上述代码，仅仅是覆盖操作，假如遍历状态时返回的顺序为 &lt;圆圈,4&gt;、&lt;方块,2&gt;、&lt;圆圈,4&gt;、&lt;方块,1&gt;，那么上述恢复流程为：将上述元素依次 put 到 HashMap 中，所以 HashMap 类型的 buffer 恢复完数据后，buffer 中保存的 PV 信息为 &lt;圆圈,4&gt;、&lt;方块,1&gt;。显然恢复过程中的覆盖操作将状态数据 &lt;圆圈,4&gt;、&lt;方块,2&gt; 丢了，所以上述方案如果不修改并行度时，不会丢数据，如果修改并行度时，可能会丢数据。</p><p>在使用状态来保证 Exactly Once 时，必须考虑修改并行度后，状态如何正常恢复的情况。优化后的代码如下所示，仅仅修改 initializeState 方法中恢复状态的逻辑：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 从状态中恢复 buffer 中的数据</span></span><br><span class="line"><span class="keyword">for</span>(Tuple2&lt;String, Long&gt; appIdPv: localPvStatListState.get())&#123;</span><br><span class="line">    <span class="keyword">long</span> pv = localPvStat.getOrDefault(appIdPv.f0, <span class="number">0L</span>);</span><br><span class="line">    <span class="comment">// 如果出现 pv != 0，说明改变了并行度，</span></span><br><span class="line">    <span class="comment">// ListState 中的数据会被均匀分发到新的 subtask 中</span></span><br><span class="line">    <span class="comment">// 所以单个 subtask 恢复的状态中可能包含两个相同的 app 的数据</span></span><br><span class="line">    localPvStat.put(appIdPv.f0, pv + appIdPv.f1);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>代码中，首先从 buffer 中获取当前 app 的 PV 数据，如果 buffer 中不包含当前 app 则 PV 值返回 0，如果 buffer 中包含了当前 app 则返回相应的 PV 值，将 buffer 中的 pv 加当前的 pv，put 到 buffer 中即可保证恢复时不丢数据。</p><p>到这里 LocalKeyBy 的思路及具体代码实现都讲完了，也带着大家分析了多种可能丢数据的情况，并一一解决。上述完整的代码实现请参阅。上述代码实现有个局限性，就是需要了解业务，按照下游的聚合逻辑，在上游 keyBy 之前同样也需要实现一遍。关于通用的 LocalKeyBy 实现，Flink 源码中目前还没有此功能，对具体实现原理感兴趣的可以参阅腾讯杨华老师贡献的 <a href="https://cwiki.apache.org/confluence/display/FLINK/FLIP-44%3A+Support+Local+Aggregation+in+Flink">FLIP-44</a>。</p><h5 id="keyBy-之前发生数据倾斜"><a href="#keyBy-之前发生数据倾斜" class="headerlink" title="keyBy 之前发生数据倾斜"></a>keyBy 之前发生数据倾斜</h5><p>上一部分分析了 keyBy 后由于数据本身的特征可能会发生数据倾斜，可以在 keyBy 之前进行一次预聚合，从而使得 keyBy 后的数据量大大降低。但是如果 keyBy 之前就存在数据倾斜呢？这样上游算子的某些实例可能处理的数据较多，某些实例可能处理的数据较少，产生该情况可能是因为数据源的数据本身就不均匀，例如由于某些原因 Kafka 的 topic 中某些 partition 的数据量较大，某些 partition 的数据量较少。对于不存在 keyBy 的 Flink 任务也会出现该情况，解决思路都一样，主要在于没有 shuffle 的 Flink 任务如何来解决数据倾斜。对于这种情况，需要让 Flink 任务强制进行 shuffle。如何强制 shuffle 呢？了解一下 DataStream 的物理分区策略。</p><table><thead><tr><th align="left">分区策略</th><th align="left">描述</th></tr></thead><tbody><tr><td align="left">dataStream.partitionCustom(partitioner, “someKey”); dataStream.partitionCustom(partitioner, 0);</td><td align="left">根据指定的字段进行分区，指定字段值相同的数据发送到同一个 Operator 实例处理</td></tr><tr><td align="left">dataStream.shuffle();</td><td align="left">将数据随机地分配到下游 Operator 实例</td></tr><tr><td align="left">dataStream.rebalance();</td><td align="left">使用轮循的策略将数据发送到下游 Operator 实例</td></tr><tr><td align="left">dataStream.rescale();</td><td align="left">基于 rebalance 优化的策略，依然使用轮循策略，但仅仅是 TaskManager 内的轮循，只会在 TaskManager 本地进行 shuffle 操作，减少了网络传输</td></tr><tr><td align="left">dataStream.broadcast();</td><td align="left">将数据广播到下游所有的 Operator 实例</td></tr></tbody></table><p>在这里需要解决数据倾斜，只需要使用 shuffle、rebalance 或 rescale 即可将数据均匀分配，从而解决数据倾斜的问题。</p>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Flink WaterMark 深入理解</title>
      <link href="2019/12/13/Flink-WaterMark-%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3/"/>
      <url>2019/12/13/Flink-WaterMark-%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p>如果在进行 Window 计算操作的时候，如果使用的时间是 Processing Time，那么在 Flink 消费数据的时候，它完全不需要关心的数据本身的时间，意思也就是说不需要关心数据到底是延迟数据还是乱序数据。因为 Processing Time 只是代表数据在 Flink 被处理时的时间，这个时间是顺序的。但是如果你使用的是 Event Time 的话，那么你就不得不面临着这么个问题：事件乱序 &amp; 事件延迟。</p><a id="more"></a><p>在理想的情况下，Event Time 和 Process Time 是相等的，数据发生的时间与数据处理的时间没有延迟，但是现实流处理从事件产生，到流经 source，再到 operator，中间是有一个过程和时间的。虽然大部分情况下，流到 operator 的数据都是按照事件产生的时间顺序来的，但是也不排除由于网络延迟, 分布式等原因，导致乱序的产生，特别是使用 kafka 的话，多个分区的数据无法保证有序。</p><p><strong>然而在有些场景下，其实是特别依赖于事件时间而不是处理时间，比如：</strong></p><ul><li>错误日志的时间戳，代表着发生的错误的具体时间，开发们只有知道了这个时间戳，才能去还原那个时间点系统到底发生了什么问题，或者根据那个时间戳去关联其他的事件，找出导致问题触发的罪魁祸首</li><li>设备传感器或者监控系统实时上传对应时间点的设备周围的监控情况，通过监控大屏可以实时查看，不错漏重要或者可疑的事件</li></ul><p>这种情况下，最有意义的事件发生的顺序，而不是事件到达 Flink 后被处理的顺序。Flink 支持用户以事件时间来定义窗口（也支持以处理时间来定义窗口），那么这样就要去解决上面所说的两个问题。针对上面的问题（事件乱序 &amp; 事件延迟），Flink 引入了 Watermark 机制来解决。</p><h3 id="Watermark-是什么？"><a href="#Watermark-是什么？" class="headerlink" title="Watermark 是什么？"></a>Watermark 是什么？</h3><p>举个例子：</p><p><strong><font color='grey'>统计 8:00 ~ 9:00 这个时间段打开淘宝 App 的用户数量，Flink 这边可以开个窗口做聚合操作，但是由于网络的抖动或者应用采集数据发送延迟等问题，于是无法保证在窗口时间结束的那一刻窗口中是否已经收集好了在 8:00 ~ 9:00 中用户打开 App 的事件数据，但又不能无限期的等下去？当基于事件时间的数据流进行窗口计算时，最为困难的一点也就是如何确定对应当前窗口的事件已经全部到达。然而实际上并不能百分百的准确判断，因此业界常用的方法就是基于已经收集的消息来估算是否还有消息未到达，这就是 Watermark 的思想。</font></strong></p><p>Watermark 是一种衡量 Event Time 进展的机制，它是数据本身的一个隐藏属性，数据本身携带着对应的 Watermark。Watermark 本质来说就是一个时间戳，代表着比这时间戳早的事件已经全部到达窗口，即假设不会再有比这时间戳还小的事件到达，这个假设是触发窗口计算的基础，只有 Watermark 大于窗口对应的结束时间，窗口才会关闭和进行计算。按照这个标准去处理数据，那么如果后面还有比这时间戳更小的数据，那么就视为迟到的数据，对于这部分迟到的数据，Flink 也有相应的机制去处理。</p><p>下面通过几个图来了解一下 Watermark 是如何工作的！</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-07-08-154340.jpg" alt="img"></p><p>上图中的数据是 Flink 从消息队列中消费的，然后在 Flink 中有个 4s 的时间窗口（根据事件时间定义的窗口），消息队列中的数据是乱序过来的，数据上的数字代表着数据本身的 timestamp，<code>W(4)</code> 和 <code>W(9)</code> 是水印。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-07-08-154747.jpg" alt="img"></p><p>经过 Flink 的消费，数据 <code>1</code>、<code>3</code>、<code>2</code> 进入了第一个窗口，然后 <code>7</code> 会进入第二个窗口，接着 <code>3</code> 依旧会进入第一个窗口，然后就有水印了，此时水印过来了，就会发现水印的 timestamp 和第一个窗口结束时间是一致的，那么它就表示在后面不会有比 <code>4</code> 还小的数据过来了，接着就会触发第一个窗口的计算操作，如下图所示：</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-07-08-155309.jpg" alt="img"></p><p>那么接着后面的数据 <code>5</code> 和 <code>6</code> 会进入到第二个窗口里面，数据 <code>9</code> 会进入在第三个窗口里面。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-07-08-155558.jpg" alt="img"></p><p>那么当遇到水印 <code>9</code> 时，发现水印比第二个窗口的结束时间 <code>8</code> 还大，所以第二个窗口也会触发进行计算，然后以此继续类推下去。</p><p>相信看完上面几个图的讲解，你已经知道了 Watermark 的工作原理是啥了，那么在 Flink 中该如何去配置水印呢，下面一起来看看。</p><img src="/Users/joker/Library/Application Support/typora-user-images/截屏2021-01-19 上午10.59.07.png" alt="截屏2021-01-19 上午10.59.07" style="zoom:50%;" /><h3 id="Flink-中-Watermark-的设置"><a href="#Flink-中-Watermark-的设置" class="headerlink" title="Flink 中 Watermark 的设置"></a>Flink 中 Watermark 的设置</h3><p>在 Flink 中，数据处理中需要通过调用 DataStream 中的 assignTimestampsAndWatermarks 方法来分配时间和水印，该方法可以传入两种参数，一个是 AssignerWithPeriodicWatermarks，另一个是 AssignerWithPunctuatedWatermarks。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> SingleOutputStreamOperator&lt;T&gt; <span class="title">assignTimestampsAndWatermarks</span><span class="params">(AssignerWithPeriodicWatermarks&lt;T&gt; timestampAndWatermarkAssigner)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">final</span> <span class="keyword">int</span> inputParallelism = getTransformation().getParallelism();</span><br><span class="line">    <span class="keyword">final</span> AssignerWithPeriodicWatermarks&lt;T&gt; cleanedAssigner = clean(timestampAndWatermarkAssigner);</span><br><span class="line"></span><br><span class="line">    TimestampsAndPeriodicWatermarksOperator&lt;T&gt; operator = <span class="keyword">new</span> TimestampsAndPeriodicWatermarksOperator&lt;&gt;(cleanedAssigner);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> transform(<span class="string">&quot;Timestamps/Watermarks&quot;</span>, getTransformation().getOutputType(), operator).setParallelism(inputParallelism);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> SingleOutputStreamOperator&lt;T&gt; <span class="title">assignTimestampsAndWatermarks</span><span class="params">(AssignerWithPunctuatedWatermarks&lt;T&gt; timestampAndWatermarkAssigner)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">final</span> <span class="keyword">int</span> inputParallelism = getTransformation().getParallelism();</span><br><span class="line">    <span class="keyword">final</span> AssignerWithPunctuatedWatermarks&lt;T&gt; cleanedAssigner = clean(timestampAndWatermarkAssigner);</span><br><span class="line"></span><br><span class="line">    TimestampsAndPunctuatedWatermarksOperator&lt;T&gt; operator = <span class="keyword">new</span> TimestampsAndPunctuatedWatermarksOperator&lt;&gt;(cleanedAssigner);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> transform(<span class="string">&quot;Timestamps/Watermarks&quot;</span>, getTransformation().getOutputType(), operator).setParallelism(inputParallelism);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>所以设置 Watermark 是有如下两种方式：</p><ul><li><p>AssignerWithPunctuatedWatermarks：数据流中每一个递增的 EventTime 都会产生一个 Watermark。</p><blockquote><p>在实际的生产环境中，在 TPS 很高的情况下会产生大量的 Watermark，可能在一定程度上会对下游算子造成一定的压力，所以只有在实时性要求非常高的场景才会选择这种方式来进行水印的生成。</p></blockquote></li><li><p>AssignerWithPeriodicWatermarks：周期性的（一定时间间隔或者达到一定的记录条数）产生一个 Watermark。</p><blockquote><p>在实际的生产环境中，通常这种使用较多，它会周期性产生 Watermark 的方式，但是必须结合时间或者积累条数两个维度，否则在极端情况下会有很大的延时，所以 Watermark 的生成方式需要根据业务场景的不同进行不同的选择。</p></blockquote></li></ul><p>下面再分别详细讲下这两种的实现方式。</p><h3 id="Punctuated-Watermark"><a href="#Punctuated-Watermark" class="headerlink" title="Punctuated Watermark"></a>Punctuated Watermark</h3><p>AssignerWithPunctuatedWatermarks 接口中包含了 checkAndGetNextWatermark 方法，这个方法会在每次 extractTimestamp() 方法被调用后调用，它可以决定是否要生成一个新的水印，返回的水印只有在不为 null 并且时间戳要大于先前返回的水印时间戳的时候才会发送出去，如果返回的水印是 null 或者返回的水印时间戳比之前的小则不会生成新的水印。</p><p>那么该怎么利用这个来定义水印生成器呢？</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordPunctuatedWatermark</span> <span class="keyword">implements</span> <span class="title">AssignerWithPunctuatedWatermarks</span>&lt;<span class="title">Word</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Nullable</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Watermark <span class="title">checkAndGetNextWatermark</span><span class="params">(Word lastElement, <span class="keyword">long</span> extractedTimestamp)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> extractedTimestamp % <span class="number">3</span> == <span class="number">0</span> ? <span class="keyword">new</span> Watermark(extractedTimestamp) : <span class="keyword">null</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">extractTimestamp</span><span class="params">(Word element, <span class="keyword">long</span> previousElementTimestamp)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> element.getTimestamp();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>需要注意的是这种情况下可以为每个事件都生成一个水印，但是因为水印是要在下游参与计算的，所以过多的话会导致整体计算性能下降。</p><h3 id="3-5-4-Periodic-Watermark"><a href="#3-5-4-Periodic-Watermark" class="headerlink" title="3.5.4 Periodic Watermark"></a>3.5.4 Periodic Watermark</h3><p>通常在生产环境中使用 AssignerWithPeriodicWatermarks 来定期分配时间戳并生成水印比较多，那么先来讲下这个该如何使用。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordWatermark</span> <span class="keyword">implements</span> <span class="title">AssignerWithPeriodicWatermarks</span>&lt;<span class="title">Word</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">long</span> currentTimestamp = Long.MIN_VALUE;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">extractTimestamp</span><span class="params">(Word word, <span class="keyword">long</span> previousElementTimestamp)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (word.getTimestamp() &gt; currentTimestamp) &#123;</span><br><span class="line">            <span class="keyword">this</span>.currentTimestamp = word.getTimestamp();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> currentTimestamp;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Nullable</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Watermark <span class="title">getCurrentWatermark</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">long</span> maxTimeLag = <span class="number">5000</span>;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> Watermark(currentTimestamp == Long.MIN_VALUE ? Long.MIN_VALUE : currentTimestamp - maxTimeLag);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>上面的是我根据 Word 数据自定义的水印周期性生成器，在这个类中，有两个方法 extractTimestamp() 和 getCurrentWatermark()。extractTimestamp() 方法是从数据本身中提取 Event Time，该方法会返回当前时间戳与事件时间进行比较，如果事件的时间戳比 currentTimestamp 大的话，那么就将当前事件的时间戳赋值给 currentTimestamp。getCurrentWatermark() 方法是获取当前的水位线，这里有个 maxTimeLag 参数代表数据能够延迟的时间，上面代码中定义的 <code>long maxTimeLag = 5000;</code> 表示最大允许数据延迟时间为 5s，超过 5s 的话如果还来了之前早的数据，那么 Flink 就会丢弃了，因为 Flink 的窗口中的数据是要触发的，不可能一直在等着这些迟到的数据（由于网络的问题数据可能一直没发上来）而不让窗口触发结束进行计算操作。</p><p>通过定义这个时间，可以避免部分数据因为网络或者其他的问题导致不能够及时上传从而不把这些事件数据作为计算的，那么如果在这延迟之后还有更早的数据到来的话，那么 Flink 就会丢弃了，所以合理的设置这个允许延迟的时间也是一门细活，得观察生产环境数据的采集到消息队列再到 Flink 整个流程是否会出现延迟，统计平均延迟大概会在什么范围内波动。这也就是说明了一个事实那就是 Flink 中设计这个水印的根本目的是来解决部分数据乱序或者数据延迟的问题，而不能真正做到彻底解决这个问题，不过这一特性在相比于其他的流处理框架已经算是非常给力了。</p><p>AssignerWithPeriodicWatermarks 这个接口有四个实现类，分别如下图：</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-23-082804.png" alt="img"></p><ul><li>BoundedOutOfOrdernessTimestampExtractor：该类用来发出滞后于数据时间的水印，它的目的其实就是和我们上面定义的那个类作用是类似的，你可以传入一个时间代表着可以允许数据延迟到来的时间是多长。该类内部实现如下：</li></ul><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-23-083043.png" alt="img"></p><p>你可以像下面一样使用该类来分配时间和生成水印：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//Time.seconds(10) 代表允许延迟的时间大小</span></span><br><span class="line">dataStream.assignTimestampsAndWatermarks(<span class="keyword">new</span> BoundedOutOfOrdernessTimestampExtractor&lt;Event&gt;(Time.seconds(<span class="number">10</span>)) &#123;</span><br><span class="line">    <span class="comment">//重写 BoundedOutOfOrdernessTimestampExtractor 中的 extractTimestamp()抽象方法</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">extractTimestamp</span><span class="params">(Event event)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> event.getTimestamp();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure><ul><li>CustomWatermarkExtractor：这是一个自定义的周期性生成水印的类，在这个类里面的数据是 KafkaEvent。</li><li>AscendingTimestampExtractor：时间戳分配器和水印生成器，用于时间戳单调递增的数据流，如果数据流的时间戳不是单调递增，那么会有专门的处理方法，代码如下：</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">long</span> <span class="title">extractTimestamp</span><span class="params">(T element, <span class="keyword">long</span> elementPrevTimestamp)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">final</span> <span class="keyword">long</span> newTimestamp = extractAscendingTimestamp(element);</span><br><span class="line">    <span class="keyword">if</span> (newTimestamp &gt;= <span class="keyword">this</span>.currentTimestamp) &#123;</span><br><span class="line">        <span class="keyword">this</span>.currentTimestamp = ne∏wTimestamp;</span><br><span class="line">        <span class="keyword">return</span> newTimestamp;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        violationHandler.handleViolation(newTimestamp, <span class="keyword">this</span>.currentTimestamp);</span><br><span class="line">        <span class="keyword">return</span> newTimestamp;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>IngestionTimeExtractor：依赖于机器系统时间，它在 extractTimestamp 和 getCurrentWatermark 方法中是根据 <code>System.currentTimeMillis()</code> 来获取时间的，而不是根据事件的时间，如果这个时间分配器是在数据源进 Flink 后分配的，那么这个时间就和 Ingestion Time 一致了，所以命名也取的就是叫 IngestionTimeExtractor。</li></ul><p><strong>注意</strong>：</p><p>1、使用这种方式周期性生成水印的话，你可以通过 <code>env.getConfig().setAutoWatermarkInterval(...);</code> 来设置生成水印的间隔（每隔 n 毫秒）。</p><p>2、通常建议在数据源（source）之后就进行生成水印，或者做些简单操作比如 filter/map/flatMap 之后再生成水印，越早生成水印的效果会更好，也可以直接在数据源头就做生成水印。比如你可以在 source 源头类中的 run() 方法里面这样定义</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">(SourceContext&lt;MyType&gt; ctx)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    <span class="keyword">while</span> (<span class="comment">/* condition */</span>) &#123;</span><br><span class="line">        MyType next = getNext();</span><br><span class="line">        ctx.collectWithTimestamp(next, next.getEventTimestamp());</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (next.hasWatermarkTime()) &#123;</span><br><span class="line">            ctx.emitWatermark(<span class="keyword">new</span> Watermark(next.getWatermarkTime()));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="每个-Kafka-分区的时间戳"><a href="#每个-Kafka-分区的时间戳" class="headerlink" title="每个 Kafka 分区的时间戳"></a>每个 Kafka 分区的时间戳</h3><p>当以 Kafka 来作为数据源的时候，通常每个 Kafka 分区的数据时间戳是递增的（事件是有序的），但是当你作业设置多个并行度的时候，Flink 去消费 Kafka 数据流是并行的，那么并行的去消费 Kafka 分区的数据就会导致打乱原每个分区的数据时间戳的顺序。在这种情况下，你可以使用 Flink 中的 <code>Kafka-partition-aware</code> 特性来生成水印，使用该特性后，水印会在 Kafka 消费端生成，然后每个 Kafka 分区和每个分区上的水印最后的合并方式和水印在数据流 shuffle 过程中的合并方式一致。</p><p>如果事件时间戳严格按照每个 Kafka 分区升序，则可以使用前面提到的 AscendingTimestampExtractor 水印生成器来为每个分区生成水印。下面代码教大家如何使用 <code>per-Kafka-partition</code> 来生成水印。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">FlinkKafkaConsumer011&lt;Event&gt; kafkaSource = <span class="keyword">new</span> FlinkKafkaConsumer011&lt;&gt;(<span class="string">&quot;zhisheng&quot;</span>, schema, props);</span><br><span class="line">kafkaSource.assignTimestampsAndWatermarks(<span class="keyword">new</span> AscendingTimestampExtractor&lt;Event&gt;() &#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">extractAscendingTimestamp</span><span class="params">(Event event)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> event.eventTimestamp();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">DataStream&lt;Event&gt; stream = env.addSource(kafkaSource);</span><br></pre></td></tr></table></figure><p>下图表示水印在 Kafka 分区后如何通过流数据流传播：</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-07-09-014107.jpg" alt="img"></p>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>任务提交流程</title>
      <link href="2019/12/11/Flink%20%E4%BB%BB%E5%8A%A1%E6%8F%90%E4%BA%A4%E6%B5%81%E7%A8%8B/"/>
      <url>2019/12/11/Flink%20%E4%BB%BB%E5%8A%A1%E6%8F%90%E4%BA%A4%E6%B5%81%E7%A8%8B/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><h1 id="1-概述"><a href="#1-概述" class="headerlink" title="1. 概述"></a>1. 概述</h1><blockquote><p>用户提交的 <code>Flink Job</code> 会被转化成一个 <code>DAG</code> 任务运行，分别是：<code>StreamGraph</code>、<code>JobGraph</code>、<code>ExecutionGraph</code>，<code>Flink</code> 中 <code>JobManager</code> 与 <code>TaskManager</code>，<code>JobManager</code> 与 <code>Client</code> 的交互是基于Akka工具包的，是通过消息驱动。整个Flink Job的提交还包含着ActorSystem的创建，JobManager的启动，TaskManager的启动和注册。</p></blockquote><h1 id="2-任务提交流程"><a href="#2-任务提交流程" class="headerlink" title="2.任务提交流程"></a>2.任务提交流程</h1><h2 id="2-1-Standalone"><a href="#2-1-Standalone" class="headerlink" title="2.1.Standalone"></a>2.1.<code>Standalone</code></h2><blockquote><ol><li><code>App</code> 程序通过 <code>rest</code> 接口提交给 <code>Dispatcher</code>[ <code>rest</code>接口是跨平台，并且可以直接穿过防火墙，不需考虑拦截]。</li><li><code>Dispatcher</code> 把 <code>JobManager</code> 进程启动，把应用交给<code>JobManager</code>。</li><li>JobManager拿到应用后，向ResourceManager申请资源（slots），ResouceManager会启动对应的TaskManager进程，TaskManager空闲的slots会向ResourceManager注册。</li><li>ResourceManager会根据JobManager申请的资源数量，向TaskManager发出指令（这些slots由你提供给JobManager）。</li><li>接着，TaskManager可以直接和JobManager通信了（它们之间会有心跳包的连接），TaskManager向JobManager提供slots，JobManager向TaskManager分配在slots中执行的任务。</li><li>最后，在执行任务过程中，不同的TaskManager会有数据之间的交换。</li></ol></blockquote><h2 id="2-2-Yarn"><a href="#2-2-Yarn" class="headerlink" title="2.2. Yarn"></a>2.2. <code>Yarn</code></h2><blockquote><ol><li>提交 <code>App</code> 之前，先上传 <code>Flink</code> 的 <code>Jar</code> 包和配置到 <code>HDFS</code> ，以便 <code>JobManager</code> 和 <code>TaskManager</code> 共享<code>HDFS</code> 的数据。</li><li>客户端向ResourceManager提交Job，ResouceManager接到请求后，先分配container资源，然后通知NodeManager启动ApplicationMaster。</li><li>ApplicationMaster会加载HDFS的配置，启动对应的JobManager，然后JobManager会分析当前的作业图，将它转化成执行图（包含了所有可以并发执行的任务），从而知道当前需要的具体资源。</li><li>接着，JobManager会向ResourceManager申请资源，ResouceManager接到请求后，继续分配container资源，然后通知ApplictaionMaster启动更多的TaskManager（先分配好container资源，再启动TaskManager）。container在启动TaskManager时也会从HDFS加载数据。</li><li>最后，TaskManager启动后，会向JobManager发送心跳包。JobManager向TaskManager分配任务。</li></ol></blockquote><h1 id="3-Graph"><a href="#3-Graph" class="headerlink" title="3. Graph"></a>3. <code>Graph</code></h1><h2 id="3-1-StreamGraph"><a href="#3-1-StreamGraph" class="headerlink" title="3.1. StreamGraph"></a>3.1. <code>StreamGraph</code></h2><h2 id="3-2-JobGraph"><a href="#3-2-JobGraph" class="headerlink" title="3.2. JobGraph"></a>3.2. <code>JobGraph</code></h2><h2 id="3-3-ExecutionGraph"><a href="#3-3-ExecutionGraph" class="headerlink" title="3.3. ExecutionGraph"></a>3.3. <code>ExecutionGraph</code></h2><h2 id="3-4-物理执行图"><a href="#3-4-物理执行图" class="headerlink" title="3.4. 物理执行图"></a>3.4. <code>物理执行图</code></h2>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Hello Flink</title>
      <link href="2019/12/10/Flink%E6%A6%82%E8%BF%B0/"/>
      <url>2019/12/10/Flink%E6%A6%82%E8%BF%B0/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><h1 id="1-简介"><a href="#1-简介" class="headerlink" title="1. 简介"></a>1. 简介</h1><p>Flink 核心是一个流式的数据流执行引擎，其针对数据流的分布式计算提供了数据分布、数据通信以及容错机制等功能。基于流执行引擎，Flink 提供了诸多更高抽象层的 API 以便用户编写分布式任务</p><h2 id="1-1-无界数据流与有界数据流"><a href="#1-1-无界数据流与有界数据流" class="headerlink" title="1.1.无界数据流与有界数据流"></a>1.1.无界数据流与有界数据流</h2><h3 id="1-1-1-无界数据流"><a href="#1-1-1-无界数据流" class="headerlink" title="1.1.1.无界数据流"></a>1.1.1.无界数据流</h3><p><strong>无界数据流有一个开始但是没有结束 ，</strong> 它们不会在生成时终止并 提供数据，必须连续处理无界流，也就是说必须在获取后立即 处理 event 。对于无界 数据流我们无法等待所有数据都到达， 因为输入是无界的， 并且在任何时间点都不 会完成。处理无界数据通常要求以特定顺序（例如事件发生的顺序）获取 event ，以 便能够推断结果完整性 ，无界流的处理称为流处理 。</p><h3 id="1-1-2-有界数据流"><a href="#1-1-2-有界数据流" class="headerlink" title="1.1.2.有界数据流"></a>1.1.2.有界数据流</h3><p><strong>有界数据流有明确定义的开始和结束，</strong> 可以在执行任何计算之前 通过获取所有数据来处理有界流， 处理有界流不需要有序获取， 因为可以始终对有 界数据集进行排序， 有界流的处理也称为批处理。</p><h2 id="1-2-批处理和流处理"><a href="#1-2-批处理和流处理" class="headerlink" title="1.2.批处理和流处理"></a>1.2.批处理和流处理</h2><p>批处理的特点是有界、持久、大量， 批处理非常适合需要访问全套记录才能完成的计算工作， 一般用于离线统计。 </p><p>流处理的特点是无界、实时， 流处理方式无需针对整个数据集执行操作， 而是对通过系统传输的每个数据项执行操作 ， 一般用于实时统计 。</p><h2 id="1-3-Flink-批流一体处理"><a href="#1-3-Flink-批流一体处理" class="headerlink" title="1.3.Flink 批流一体处理"></a>1.3.Flink 批流一体处理</h2><p>在 Spark 生态体系中， 对于批处理和流处理采用了不同的技术框架，批处理由 Spark SQL 实现， 流处理由 Spark Streaming 实现， 这也是大部分框架采用的策略， 使用独立的处理器实现批处理和流处理， 而 Flink 可以同时实现批处理和流处理。</p><p>Apache Flink  是一个面向分布式数据流处理和批量数据处理的开源计算平台，能够基于同一个 Flink 运行时 (Flink Runtime) ， 提供支持流处理和批处理两种类 型应用的功能 。 现有的开源计算 方案， 会把流处理和批处理作为两种不同的应用类 型，因为它们要实现的目标是完全不相同的：流处理一般需要支持低延迟、 Exactly-once 保证 ， 而 批处理需要支持高吞吐、高效处理 ， 所以在实现的时候通常 是分别给出两套实现方法， 或者通过一个独立的开源框架来实现其中每一种处理方 案。</p><p>Flink 是完全支持流处理，作为流处理时将输入数据流视为无界数据流 ； 批处理被作为一种特殊的流处理， 只是它的输入数据流被定义为有界的 。 </p><h2 id="1-4-Flink-amp-Spark-Streaming"><a href="#1-4-Flink-amp-Spark-Streaming" class="headerlink" title="1.4.Flink &amp; Spark Streaming"></a>1.4.<strong>Flink &amp; Spark Streaming</strong></h2><p><strong>Flink 是标准的实时流处理引擎，基于事件驱动。而 Spark Streaming 是微批[Micro-Batch的模型。</strong></p><h3 id="1-4-1-任务调度"><a href="#1-4-1-任务调度" class="headerlink" title="1.4.1.任务调度"></a><strong>1.4.1.任务调度</strong></h3><p>Spark Streaming 连续不断的生成微小的数据批次，构建有向无环图 DAG，Spark Streaming 会依次创建 DStreamGraph、JobGenerator、JobScheduler。</p><p>Flink 根据用户提交的代码生成 StreamGraph，经过优化生成 JobGraph，然后提交给 JobManager进行处理，JobManager 会根据 JobGraph 生成 ExecutionGraph，ExecutionGraph 是 Flink 调度最核心的数据结构，JobManager 根据 ExecutionGraph 对 Job 进行调度。</p><h3 id="1-4-2-时间机制"><a href="#1-4-2-时间机制" class="headerlink" title="1.4.2. 时间机制"></a><strong>1.4.2. 时间机制</strong></h3><p>Spark Streaming 支持的时间机制有限，只支持<strong>处理时间</strong>。<br>Flink 支持了流处理程序在时间上的三个定义：<strong>处理时间、事件时间、注入时间</strong>。同时也支持 <strong>watermark</strong> 机制来处理滞后数据。</p><h3 id="1-4-3-容错机制"><a href="#1-4-3-容错机制" class="headerlink" title="1.4.3. 容错机制"></a><strong>1.4.3. 容错机制</strong></h3><p>对于 Spark Streaming 任务，我们可以设置 checkpoint，然后假如发生故障并重启，我们可以从上次 checkpoint 之处恢复，但是这个行为只能使得数据不丢失，可能会重复处理，不能做到恰一次处理语义。</p><h1 id="2-系统架构"><a href="#2-系统架构" class="headerlink" title="2.系统架构"></a>2.系统架构</h1><p>当 Flink 集群启动后，首先会启动一个JobManger 和一个或多个的 TaskManager。由  Client 提交任务给 JobManager，JobManager 再调度任务到各个 TaskManager 去执行， TaskManager 将心跳和统计信息汇报给 JobManager。</p><h2 id="Client"><a href="#Client" class="headerlink" title="Client"></a>Client</h2><p>Client 为提交 Job 的客户端，可以是运行在任何机器上[与 JobManager 环境连通即可]。提交 Job 后，Client 可以结束进程，也可以不结束并等待结果返回。</p><h2 id="Dispatcher"><a href="#Dispatcher" class="headerlink" title="Dispatcher"></a>Dispatcher</h2><p> 提供 REST 接口来接收 client 的 application 提交，它负责启动 TaskManager 和提交 application，同时运行 Web UI。</p><h2 id="JobManager"><a href="#JobManager" class="headerlink" title="JobManager"></a>JobManager</h2><p><strong>JobManager  负责整个 Flink  集群任务的调度以及资源的管理，从客户端中获取提交的 Flink Job，然后根据集群中 TaskManager 上 TaskSlot 的使用情况，为提交的Job 分配相应的 TaskSlot 资源并命令 TaskManager 启动从客户端中获取的应用。</strong> </p><p><strong>JobManager 相当于整个集群的 Master 节点，且整个集群有且只有一个活跃的 JobManager ，负责整个集群的任务调度和资源管理。</strong></p><p>JobManager包含了3个重要的组件</p><h3 id="Actor"><a href="#Actor" class="headerlink" title="Actor"></a>Actor</h3><p>JobManager 和 TaskManager 之间通过 Actor System 进行通信，获取任务执行的情况并通过 Actor System 将应用的任务执行情况发送给客户端。</p><h3 id="调度"><a href="#调度" class="headerlink" title="调度"></a>调度</h3><h3 id="检查点"><a href="#检查点" class="headerlink" title="检查点"></a>检查点</h3><p>Flink的检查点机制是保证其一致性容错功能的骨架。它持续的为分布式的数据流和有状态的operator生成一致性的快照。Flink的容错机制持续的构建轻量级的分布式快照，因此负载非常低。通常这些有状态的快照都被放在HDFS中存储（state backend）。程序一旦失败，Flink将停止executor并从最近的完成了的检查点开始恢复（依赖可重发的数据源+快照）</p><h2 id="TaskManager"><a href="#TaskManager" class="headerlink" title="TaskManager"></a>TaskManager</h2><p> <strong>TaskManager  相当于整个集群的 Slave 节点，负责具体的任务执行和对应任务在每个节点上的资源申请和管理。</strong></p><p> 客户端通过将编写好的 Flink 应用编译打包，提交到 JobManager，然后 JobManager 会根据已注册在 JobManager 中 TaskManager 的资源情况，将任务分配给有资源的 TaskManager节点，然后启动并运行任务。</p><p> TaskManager 从 JobManager 接收 Job然后使用 Slot 资源启动 Task，建立数据接入的网络连接，接收数据并开始数据处理。同时 TaskManager 之间的数据交互都是通过数据流的方式进行的。</p><p> 可以看出，<strong>Flink 的任务运行其实是采用多线程的方式，这和 MapReduce 多 进程的方式有很大的区别</strong>，Flink 能够极大提高 CPU 使用效率，在多个任务和 Task之间通过 TaskSlot 方式共享系统资源，每个 TaskManager 中通过管理多个 TaskSlot 资源池进行对资源进行有效管理。</p><h3 id="Slots"><a href="#Slots" class="headerlink" title="Slots"></a>Slots</h3><p><strong>Flink 中每一个 TaskManager 都是一个 JVM 进程，他可能会在独立的线程上执行一个或多个 subtask</strong></p><p><strong>为了控制一个 TaskManager 能接收多少个 task，TaskManager通过 task slot 来进行控制， 一个TaskManager至少有一个 slot</strong></p><ul><li><p><strong>Slot 共享</strong></p><p> 默认情况下，Flink 允许 subtasks共享 slot</p><p> 条件是它们都来自<strong>同一个 Job 的不同 task 的 subtask</strong>。结果可能是一个 slot 持有该 job的整个pipeline。</p><p> <strong>优点</strong></p><ul><li><p>Flink 集群需要的任务槽与作业中使用的最高并行度正好相同(前提，保持默认SlotSharingGroup)。也就是说我们不需要再去计算一个程序总共会起多少个task了。</p></li><li><p>更容易获得更充分的资源利用。如果没有slot共享，那么非密集型操作source/flatmap就会占用同密集型操作 keyAggregation/sink 一样多的资源。如果有slot共享，将task的2个并行度增加到6个，能充分利用slot资源，同时保证每个TaskManager能平均分配到重的subtasks。</p></li></ul></li><li><p><strong>SlotSharingGroup[soft]</strong></p><p> SlotSharingGroup 是Flink中用来实现 slot  共享的类，它尽可能地让 subtasks 共享一个slot。</p><p> 保证同一个 group 的并行度相同的 sub-tasks 共享同一个slots。</p><p> 算子的默认group为default [即默认一个 Job下 的 subtask都可以共享一个 slot]</p><p> 为了防止不合理的共享，用户也能通过API来强制指定 operator 的共享组</p><p> 比如：someStream.filter(…).slotSharingGroup(“group1”);就强制指定了filter的slot共享组为group1。</p><p> 怎么确定一个未做SlotSharingGroup设置算子的SlotSharingGroup什么呢(根据上游算子的group 和自身是否设置group共同确定)。适当设置可以减少每个slot运行的线程数，从而整体上减少机器的负载。</p></li></ul><h1 id="3-Flink-部署模式"><a href="#3-Flink-部署模式" class="headerlink" title="3.Flink 部署模式"></a>3.Flink 部署模式</h1><p>Flink 是支持以 Standalone、YARN、Kubernetes、Mesos 等形式部署的。</p><ul><li>Local：直接在 IDE 中运行 Flink Job 时则会在本地启动一个 mini Flink 集群</li><li>Standalone：在 Flink 目录下执行 <code>bin/start-cluster.sh</code> 脚本则会启动一个 Standalone 模式的集群</li><li>YARN：YARN 是 Hadoop 集群的资源管理系统，它可以在群集上运行各种分布式应用程序，Flink 可与其他应用并行于 YARN 中，Flink on YARN 的架构如下：</li></ul><h1 id="4-Flink-作业提交架构流程"><a href="#4-Flink-作业提交架构流程" class="headerlink" title="4.Flink 作业提交架构流程"></a>4.Flink 作业提交架构流程</h1><p>Flink 作业提交架构流程可见下图：</p><img src="/Users/joker/Documents/chen_blog/images/截屏2021-01-20 上午9.26.58.png" alt="截屏2021-01-20 上午9.26.58" style="zoom:50%;" /><ol><li><p>Program Code</p><p>用户编写的 Flink 应用程序代码。</p></li><li><p>Job Client</p><p>Job Client 不是 Flink 程序执行的内部部分，但它是任务执行的起点。Job Client 负责接受用户的程序代码，然后创建数据流，将数据流提交给 Job Manager 以便进一步执行。执行完成后，Job Client 将结果返回给用户。</p></li><li><p>Job Manager</p><p>主进程（也称为作业管理器）协调和管理程序的执行。它的主要职责包括安排任务、管理 checkpoint 、故障恢复等。机器集群中至少要有一个 master，master 负责调度 task、协调 checkpoints 和容灾，高可用设置的话可以有多个 master，但要保证一个是 leader，其他是 standby。Job Manager 包含 Actor system、Scheduler、Check pointing 三个重要的组件。</p></li><li><p>Task Manager</p><p>从 Job Manager 处接收需要部署的 Task。Task Manager 是在 JVM 中的一个或多个线程中执行任务的工作节点。任务执行的并行性由每个 Task Manager 上可用的任务槽（Slot 个数）决定。每个任务代表分配给任务槽的一组资源。例如，如果 Task Manager 有四个插槽，那么它将为每个插槽分配 25％ 的内存。可以在任务槽中运行一个或多个线程。同一插槽中的线程共享相同的 JVM。</p></li></ol>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Flink 保证Exactly Once[2]</title>
      <link href="2019/12/10/Flink-%E4%BF%9D%E8%AF%81Exactly-Once-2/"/>
      <url>2019/12/10/Flink-%E4%BF%9D%E8%AF%81Exactly-Once-2/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p>FlinkKafkaConsumer 做为 Source，从 Kafka 读取数据到 Flink 中，首先想一下设计 FlinkKafkaConsumer，需要考虑哪些？</p><a id="more"></a><h3 id="分析-FlinkKafkaConsumer-的设计思想"><a href="#分析-FlinkKafkaConsumer-的设计思想" class="headerlink" title="分析 FlinkKafkaConsumer 的设计思想"></a>分析 FlinkKafkaConsumer 的设计思想</h3><p>FlinkKafkaConsumer 做为 Source，从 Kafka 读取数据到 Flink 中，首先想一下设计 FlinkKafkaConsumer，需要考虑哪些？</p><ul><li>Flink 中 kafka 的 offset 保存在哪里，具体如何保存呢？任务重启恢复时，如何读取之前消费的 offset？</li><li>如果 Source 端并行度改变了，如何来恢复 offset？</li><li>如何保证每个 FlinkKafkaConsumer 实例消费的 partition 负载均衡？如何保证不出现有的实例消费 5 个 kafka partition，有的实例仅消费 1 个 kafka partition？</li><li>当前消费的 topic 如果动态增加了 partition，Flink 如何实现自动发现并消费？</li></ul><p>带着这些问题来看一看 FlinkKafkaConsumer 是怎么解决上述问题的。</p><h4 id="Kafka-offset-存储及如何实现-Consumer-实例消费-partition-的负载均衡"><a href="#Kafka-offset-存储及如何实现-Consumer-实例消费-partition-的负载均衡" class="headerlink" title="Kafka offset 存储及如何实现 Consumer 实例消费 partition 的负载均衡"></a>Kafka offset 存储及如何实现 Consumer 实例消费 partition 的负载均衡</h4><p>Flink 将任务恢复需要的信息都保存在状态中，当然 Kafka 的 offset 信息也保存在 Flink 的状态中，当任务从状态中恢复时会从状态中读取相应的 offset，并从 offset 位置开始消费。</p><p>在 Flink 中有两个基本的 State：Keyed State 和 Operator State。</p><ul><li>Keyed State 只能用于 KeyedStream 的 function 和 Operator 中，一个 Key 对应一个 State；</li><li>而 Operator State 可以用于所有类型的 function 和 Operator 中，一个 Operator 实例对应一个 State，假如一个算子并行度是 5 且使用 Operator State，那么这个算子的每个并行度都对应一个 State，总共 5 个 State。</li></ul><p>FlinkKafkaConsumer 做为 Source 只能使用 Operator State，Operator State 只支持一种数据结构 ListState，可以当做 List 类型的 State。所以 FlinkKafkaConsumer 中，将状态保存在 Operator State 对应的 ListState 中。具体如何保存呢？需要先了解每个 FlinkKafkaConsumer 具体怎么消费 Kafka。</p><p>对于同一个消费者组，Kafka 要求 topic 的每个 partition 只能被一个 Consumer 实例消费，相反一个 Consumer 实例可以去消费多个 partition。当 Flink 消费 Kafka 时，出现了以下三种情况：</p><table><thead><tr><th align="left">情况</th><th align="left">现象</th></tr></thead><tbody><tr><td align="left">FlinkKafkaConsumer 并行度大于 topic 的 partition 数</td><td align="left">有些 FlinkKafkaConsumer 不会消费 Kafka</td></tr><tr><td align="left">FlinkKafkaConsumer 并行度等于 topic 的 partition 数</td><td align="left">每个 FlinkKafkaConsumer 消费 1 个 partition</td></tr><tr><td align="left">FlinkKafkaConsumer 并行度小于 topic 的 partition 数</td><td align="left">每个 FlinkKafkaConsumer 至少消费 1 个 partition，可能会消费多个 partition</td></tr></tbody></table><p>Flink 是如何为每个 Consumer 实例合理地分配去消费哪些 partition 呢？源码中 KafkaTopicPartitionAssigner 类的 assign 方法，负责分配 partition 给 Consumer 实例。assign 方法的输入参数为 KafkaTopicPartition 和 Consumer 的并行度，KafkaTopicPartition 主要包含两个字段：String 类型的 topic 和 int 类型的 partition。assign 方法返回该 KafkaTopicPartition 应该分配给哪个 Consumer 实例去消费。假如 Consumer 的并行度为 5，表示包含了 5 个 subtask，assign 方法的返回值范围为 0~4，分别表示该 partition 分配给 subtask0-subtask4。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> partition Kafka 中 topic 和 partition 信息</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> numParallelSubtasks subtask 的数量</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span> 该 KafkaTopicPartition 分配给哪个 subtask 去消费</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">int</span> <span class="title">assign</span><span class="params">(KafkaTopicPartition partition, <span class="keyword">int</span> numParallelSubtasks)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> startIndex = ((partition.getTopic().hashCode() * <span class="number">31</span>) &amp; <span class="number">0x7FFFFFFF</span>) % numParallelSubtasks;</span><br><span class="line">    <span class="keyword">return</span> (startIndex + partition.getPartition()) % numParallelSubtasks;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>assign 方法是如何给 KafkaTopicPartition 分配 Consumer 实例的呢？</p><p>第一行代码根据 topic name 的 hashCode 运算后对 subtask 的数量求余生成一个 startIndex，第二行代码用 startIndex + partition 编号对 subtask 的数量求余，可以保证该 topic 的 0 号 partition 分配给 startIndex 对应的 subtask，后续的 partition 依次分配给后续的 subtask。</p><p>例如，名为 “test-topic” 的 topic 有 11 个 partition 分别为 partition0-partition10，Consumer 有 5 个并行度分别为 subtask0-subtask4。计算后的 startIndex 为 1，表示 partition0 分配给 subtask1，partition1 分配给 subtask2 以此类推，subtask 与 partition 的对应关系如下图所示。</p><p>assign 方法给 partition 分配 subtask 实际上是轮循的策略，首先计算一个起点 startIndex 分配给 partition0，后续的 partition 轮循地分配给 subtask，从而使得每个 subtask 消费的 partition 得以均衡。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-19-122937.jpg" alt="img"></p><p>每个 subtask 只负责一部分 partition，所以在维护 partition 的 offset 信息时，每个 subtask 只需要将自己消费的 partition 的 offset 信息保存到状态中即可。</p><p>保存的格式理论来讲应该是 kv 键值对，key 为 KafkaTopicPartition，value 为 Long 类型的 offset 值。但 Flink 的 Operator State 只支持 ListState 一种数据结构，不支持 kv 格式，可以将 KafkaTopicPartition 和 Long 封装为 Tuple2&lt;KafkaTopicPartition, Long&gt; 存储到 ListState 中。如下所示，Flink 源码中确实如此，使用 ListState&lt;Tuple2&lt;KafkaTopicPartition, Long&gt;&gt; 类型的 unionOffsetStates 来保存 Kafka 的 offset 信息。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/** Accessor for state in the operator state backend. */</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">transient</span> ListState&lt;Tuple2&lt;KafkaTopicPartition, Long&gt;&gt; unionOffsetStates;</span><br></pre></td></tr></table></figure><p>当 Flink 应用从 Checkpoint 恢复任务时，会从 unionOffsetStates 中读取上一次 Checkpoint 保存的 offset 信息，并从 offset 的位置开始继续消费，从而实现 Flink 任务的故障容错。例如，任务重启后，Operator State 是一个 Operator 实例对应一个 State，subtask0 依然消费 partition4 和 partition9，subtask0 从自己的 State 中可以读取到 partition4 和 partition9 消费的 offset，从 offset 位置接着往后消费即可。问题来了，若 FlinkKafkaConsumer 的并行度改变后，offset 信息如何恢复呢？</p><h4 id="Source-端并行度改变了，如何来恢复-offset"><a href="#Source-端并行度改变了，如何来恢复-offset" class="headerlink" title="Source 端并行度改变了，如何来恢复 offset"></a>Source 端并行度改变了，如何来恢复 offset</h4><p>subtask1 当前消费了 3 个 partition，而其他 subtask 仅消费 2 个 partition，当发现 subtask1 读取 Kafka 成为瓶颈后，需要调大 Consumer 的并行度，使得每个 subtask 最多仅消费 2 个 partition。将 Consumer 实例的并行度增大到 6 以后，分配器对 partition 重新分配给 6 个 subtask，计算后的 startIndex 为 0，表示 partition0 分配给 subtask0，后续的 partition 采用轮循策略，partition 与 subtask 的对应关系如下。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-19-122939.jpg" alt="img"></p><p>之前 subtask0 消费 partition 4 和 9，并行度调大以后，subtask0 被分配消费 partition 0 和 6。但是 Flink 任务从 Checkpoint 恢复后，能保证 subtask0 读取到 partition 0 和 6 的 offset 吗？这个就需要深入了解当 Flink 算子并行度改变后，Operator State 的 ListState 两种恢复策略。两种策略如下所示，在 initializeState 方法中执行相应 API 来恢复。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">OperatorStateStore stateStore = context.getOperatorStateStore();</span><br><span class="line"></span><br><span class="line"><span class="comment">// 通过 getListState 获取 ListState</span></span><br><span class="line">stateStore.getListState(ListStateDescriptor&lt;S&gt; var1);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 通过 getUnionListState 获取 ListState</span></span><br><span class="line">stateStore.getUnionListState(ListStateDescriptor&lt;S&gt; var1);</span><br></pre></td></tr></table></figure><p>当并行度改变后，getListState 恢复策略是均匀分配，将 ListState 中保存的所有元素均匀地分配到所有并行度中，每个 subtask 获取到其中一部分状态信息。</p><p>getUnionListState 策略是将所有的状态信息合并后，每个 subtask 都获取到全量的状态信息。在 FlinkKafkaConsumer 中，假如使用 getListState 来获取 ListState，采用均匀分配状态信息的策略，Flink 可能给 subtask0 分配了 partition0 和 partition1 的 offset 信息，但实际上分配器让 subtask0 去消费 partition0 和 partition6，此时 subtask0 并拿不到 partition 6 的 offset 信息，不知道该从 partition 6 哪个位置消费，所以均匀分配状态信息的策略并不能满足需求。</p><p>这里应该使用 getUnionListState 来获取 ListState，也就是说每个 subtask 都可以获取到所有 partition 的 offset 信息，然后根据分配器让 subtask 0 去消费 partition0 和 partition6 时，subtask0 只需要从全量的 offset 中拿到 partition0 和 partition6 的状态信息即可。</p><p>这么做会使得每个 subtask 获取到一些无用的 offset 的信息，但实际上这些 offset 信息占用的空间会比较小，所以该方案成本比较低。关于 OperatorState 的 ListState 两种获取方式请参考代码：</p><blockquote><p><a href="https://github.com/zhisheng17/flink-learning/blob/master/flink-learning-state/src/main/java/com/zhisheng/state/operator/state/UnionListStateExample.java">https://github.com/zhisheng17/flink-learning/blob/master/flink-learning-state/src/main/java/com/zhisheng/state/operator/state/UnionListStateExample.java</a></p></blockquote><p>FlinkKafkaConsumer 初始化时，恢复 offset 相关的源码如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// initializeState  方法中用于恢复 offset 状态信息</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">void</span> <span class="title">initializeState</span><span class="params">(FunctionInitializationContext context)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    OperatorStateStore stateStore = context.getOperatorStateStore();</span><br><span class="line">    <span class="comment">// 此处省略了兼容 Flink 1.2 之前状态 API 的场景</span></span><br><span class="line">    ...</span><br><span class="line">    <span class="comment">// 此处使用的 getUnionListState，而不是 getListState。因为重启后，可能并行度被改变了</span></span><br><span class="line">    <span class="keyword">this</span>.unionOffsetStates = stateStore.getUnionListState(<span class="keyword">new</span> ListStateDescriptor&lt;&gt;(</span><br><span class="line">            OFFSETS_STATE_NAME,</span><br><span class="line">            TypeInformation.of(<span class="keyword">new</span> TypeHint&lt;Tuple2&lt;KafkaTopicPartition, Long&gt;&gt;() &#123;&#125;)));</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (context.isRestored() &amp;&amp; !restoredFromOldState) &#123;</span><br><span class="line">        restoredState = <span class="keyword">new</span> TreeMap&lt;&gt;(<span class="keyword">new</span> KafkaTopicPartition.Comparator());</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 将状态中恢复的 offset 信息 put 到 TreeMap 类型的 restoredState 中，方便查询</span></span><br><span class="line">        <span class="keyword">for</span> (Tuple2&lt;KafkaTopicPartition, Long&gt; kafkaOffset : unionOffsetStates.get()) &#123;</span><br><span class="line">            restoredState.put(kafkaOffset.f0, kafkaOffset.f1);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// open 方法对 FlinkKafkaConsumer 做初始化</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(Configuration configuration)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    <span class="comment">// 创建 Kafka partition 的发现器，用于检测该 subtask 应该去消费哪些 partition</span></span><br><span class="line">    <span class="keyword">this</span>.partitionDiscoverer = createPartitionDiscoverer(</span><br><span class="line">            topicsDescriptor,</span><br><span class="line">            getRuntimeContext().getIndexOfThisSubtask(),</span><br><span class="line">            getRuntimeContext().getNumberOfParallelSubtasks());</span><br><span class="line">    <span class="keyword">this</span>.partitionDiscoverer.open();</span><br><span class="line">    <span class="comment">// subscribedPartitionsToStartOffsets 存储当前 subtask 需要消费的 partition 及对应的 offset 初始信息</span></span><br><span class="line">    subscribedPartitionsToStartOffsets = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line">    <span class="comment">//用 partition 发现器获取该 subtask 应该消费且新发现的 partition</span></span><br><span class="line">    <span class="keyword">final</span> List&lt;KafkaTopicPartition&gt; allPartitions = partitionDiscoverer.discoverPartitions();</span><br><span class="line">    <span class="comment">// restoredState 在 initializeState 时初始化，所以 != null 表示任务从 Checkpoint 处恢复</span></span><br><span class="line">    <span class="keyword">if</span> (restoredState != <span class="keyword">null</span>) &#123;</span><br><span class="line">        <span class="keyword">for</span> (KafkaTopicPartition partition : allPartitions) &#123;</span><br><span class="line">            <span class="comment">// 若分配给该 subtask 的 partition 在 restoredState 中不包含</span></span><br><span class="line">            <span class="comment">// 说明该 partition 是新创建的 partition，默认从 earliest 开始消费</span></span><br><span class="line">              <span class="comment">// 并添加到 restoredState 中</span></span><br><span class="line">            <span class="keyword">if</span> (!restoredState.containsKey(partition)) &#123;</span><br><span class="line">                restoredState.put(partition, KafkaTopicPartitionStateSentinel.EARLIEST_OFFSET);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (Map.Entry&lt;KafkaTopicPartition, Long&gt; restoredStateEntry : restoredState.entrySet()) &#123;</span><br><span class="line">            <span class="comment">// 遍历 restoredState，使用分配器检测当前的 partition 是否分配给当前的 subtask</span></span><br><span class="line">            <span class="comment">// assign 方法返回当前 partition 应该分配的 subtask index 编号</span></span><br><span class="line">            <span class="comment">// getRuntimeContext().getIndexOfThisSubtask()  返回当前 subtask 的 index 编号</span></span><br><span class="line">            <span class="keyword">if</span> (KafkaTopicPartitionAssigner.assign(</span><br><span class="line">                restoredStateEntry.getKey(), getRuntimeContext().getNumberOfParallelSubtasks())</span><br><span class="line">                    == getRuntimeContext().getIndexOfThisSubtask())&#123;</span><br><span class="line">                <span class="comment">// 如果当前遍历的 partition 分配给当前 subtask 来消费，则将 partition 信息加到  subscribedPartitionsToStartOffsets 中</span></span><br><span class="line">                subscribedPartitionsToStartOffsets.put(restoredStateEntry.getKey(), restoredStateEntry.getValue());</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="comment">// else 表示任务不是从 Checkpoint 处恢复，本次源码主要分析状态恢复，不考虑该情况</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>对 offset 信息快照相关的源码如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">void</span> <span class="title">snapshotState</span><span class="params">(FunctionSnapshotContext context)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    <span class="comment">// 把旧的 offset 信息从 unionOffsetStates 清除掉</span></span><br><span class="line">    unionOffsetStates.clear();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">final</span> AbstractFetcher&lt;?, ?&gt; fetcher = <span class="keyword">this</span>.kafkaFetcher;</span><br><span class="line">    <span class="comment">// 通过提取器从 Kafka 读取数据，若 fetcher == null 表示提取器还未初始化</span></span><br><span class="line">    <span class="keyword">if</span> (fetcher == <span class="keyword">null</span>) &#123;</span><br><span class="line">        <span class="comment">// Kafka 提取器还未初始化，说明还未从 Kafka 中读取数据</span></span><br><span class="line">                <span class="comment">// 所以遍历 subscribedPartitionsToStartOffsets，将 offset 的初始信息写入到状态中</span></span><br><span class="line">        <span class="keyword">for</span> (Map.Entry&lt;KafkaTopicPartition, Long&gt; subscribedPartition : subscribedPartitionsToStartOffsets.entrySet()) &#123;</span><br><span class="line">            unionOffsetStates.add(Tuple2.of(subscribedPartition.getKey(), subscribedPartition.getValue()));</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (offsetCommitMode == OffsetCommitMode.ON_CHECKPOINTS) &#123;</span><br><span class="line">            <span class="comment">// 将 offset put 到 pendingOffsetsToCommit，后续 Commit 到 Kafka </span></span><br><span class="line">            pendingOffsetsToCommit.put(context.getCheckpointId(), restoredState);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="comment">// 从 Kafka 提取器中获取该 subtask 订阅的 partition 当前消费的 offset 信息</span></span><br><span class="line">        HashMap&lt;KafkaTopicPartition, Long&gt; currentOffsets = fetcher.snapshotCurrentState();</span><br><span class="line">        <span class="keyword">if</span> (offsetCommitMode == OffsetCommitMode.ON_CHECKPOINTS) &#123;</span><br><span class="line">            <span class="comment">// 将 offset put 到 pendingOffsetsToCommit，后续 Commit 到 Kafka </span></span><br><span class="line">            pendingOffsetsToCommit.put(context.getCheckpointId(), currentOffsets);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (Map.Entry&lt;KafkaTopicPartition, Long&gt; kafkaTopicPartitionLongEntry : currentOffsets.entrySet()) &#123;</span><br><span class="line">            <span class="comment">// 将该 subtask 订阅的 partition 以及当前 partition 消费到的 offset 写入到状态中</span></span><br><span class="line">            unionOffsetStates.add(</span><br><span class="line">                    Tuple2.of(kafkaTopicPartitionLongEntry.getKey(), kafkaTopicPartitionLongEntry.getValue()));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>上述源码分析描述了，当 Checkpoint 时 FlinkKafkaConsumer 如何将 offset 信息保存到状态中，当任务从 Checkpoint 处恢复时 FlinkKafkaConsumer 如何从状态中获取相应的 offset 信息，并解答了当 Source 并行度改变时 FlinkKafkaConsumer 如何来恢复 offset 信息。</p><h4 id="如何实现自动发现当前消费-topic-下新增的-partition"><a href="#如何实现自动发现当前消费-topic-下新增的-partition" class="headerlink" title="如何实现自动发现当前消费 topic 下新增的 partition"></a>如何实现自动发现当前消费 topic 下新增的 partition</h4><p>当 FlinkKafkaConsumer 初始化时，每个 subtask 会订阅一批 partition，但是当 Flink 任务运行过程中，如果被订阅的 topic 创建了新的 partition，FlinkKafkaConsumer 如何实现动态发现新创建的 partition 并消费呢？</p><p>在使用 FlinkKafkaConsumer 时，可以通过 Properties 传递一些配置参数，当配置了参数FlinkKafkaConsumerBase.KEY_PARTITION<em>DISCOVERY_INTERVAL</em>MILLIS 时，就会开启 partition 的动态发现，该参数表示间隔多久检测一次是否有新创建的 partition。那具体实现原理呢？相关源码的 UML 图如下所示：</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-11-15-132311.png" alt="img"></p><p>笔者生产环境使用的 FlinkKafkaConsumer011，FlinkKafkaConsumer011 继承 FlinkKafkaConsumer09，FlinkKafkaConsumer09 继承 FlinkKafkaConsumerBase。将参数 KEY_PARTITION<em>DISCOVERY_INTERVAL_MILLIS 传递给 FlinkKafkaConsumer011 时，在 FlinkKafkaConsumer09 的构造器中会调用 getLong(checkNotNull(props, “props”), KEY_PARTITION_DISCOVERY_INTERVAL</em>MILLIS, PARTITION_DISCOVERY_DISABLED) 解析该参数，并最终赋值给 FlinkKafkaConsumerBase 的 discoveryIntervalMillis 属性。后续相关源码如下所示：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// FlinkKafkaConsumerBase 的 run 方法</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">(SourceContext&lt;T&gt; sourceContext)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">      ...</span><br><span class="line">    <span class="keyword">if</span> (discoveryIntervalMillis == PARTITION_DISCOVERY_DISABLED) &#123;</span><br><span class="line">            kafkaFetcher.runFetchLoop();</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="comment">// discoveryIntervalMillis 被设置了，则开启 PartitionDiscovery</span></span><br><span class="line">            runWithPartitionDiscovery();</span><br><span class="line">        &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// runWithPartitionDiscovery 方法会调用 createAndStartDiscoveryLoop 方法</span></span><br><span class="line"><span class="comment">// createAndStartDiscoveryLoop 方法内创建了一个线程去循环检测发现新分区</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">createAndStartDiscoveryLoop</span><span class="params">(AtomicReference&lt;Exception&gt; discoveryLoopErrorRef)</span> </span>&#123;</span><br><span class="line">    <span class="comment">//  创建一个线程去循环检测发现新分区</span></span><br><span class="line">    discoveryLoopThread = <span class="keyword">new</span> Thread(() -&gt; &#123;</span><br><span class="line">        <span class="keyword">while</span> (running) &#123;</span><br><span class="line">            <span class="keyword">final</span> List&lt;KafkaTopicPartition&gt; discoveredPartitions;</span><br><span class="line">            <span class="comment">//  用 partition 发现器获取该 subtask 应该消费且新发现的 partition</span></span><br><span class="line">            discoveredPartitions = partitionDiscoverer.discoverPartitions();</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 发现了新的 partition，则添加到 Kafka 提取器</span></span><br><span class="line">            <span class="keyword">if</span> (running &amp;&amp; !discoveredPartitions.isEmpty()) &#123;</span><br><span class="line">                <span class="comment">//  kafkaFetcher 添加 新发现的 partition</span></span><br><span class="line">                kafkaFetcher.addDiscoveredPartitions(discoveredPartitions);</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (running &amp;&amp; discoveryIntervalMillis != <span class="number">0</span>) &#123;</span><br><span class="line">                <span class="comment">//  sleep 设置的间隔时间</span></span><br><span class="line">                Thread.sleep(discoveryIntervalMillis);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;, <span class="string">&quot;Kafka Partition Discovery for &quot;</span> + getRuntimeContext().getTaskNameWithSubtasks());</span><br><span class="line"></span><br><span class="line">    discoveryLoopThread.start();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>discoveryLoopThread 线程中每间隔 discoveryIntervalMillis 时间会调用 partition 发现器获取该 subtask 应该消费且新发现的 partition，在 open 方法初始化时，同样也调用 partitionDiscoverer.discoverPartitions() 方法来获取新发现的 partition，partition 发现器的 discoverPartitions 方法第一次调用时，会返回该 subtask 所有的 partition，后续调用只会返回新发现的且应该被当前 subtask 消费的 partition。discoverPartitions 方法源码如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> List&lt;KafkaTopicPartition&gt; <span class="title">discoverPartitions</span><span class="params">()</span> <span class="keyword">throws</span> WakeupException, ClosedException </span>&#123;</span><br><span class="line">    List&lt;KafkaTopicPartition&gt; newDiscoveredPartitions;</span><br><span class="line">    <span class="comment">// 获取订阅的 Topic 的所有 partition </span></span><br><span class="line">    newDiscoveredPartitions = getAllPartitionsForTopics(topicsDescriptor.getFixedTopics());</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 剔除 旧的 partition 和 不应该被该 subtask 去消费的 partition</span></span><br><span class="line">    Iterator&lt;KafkaTopicPartition&gt; iter = newDiscoveredPartitions.iterator();</span><br><span class="line">    KafkaTopicPartition nextPartition;</span><br><span class="line">    <span class="keyword">while</span> (iter.hasNext()) &#123;</span><br><span class="line">        nextPartition = iter.next();</span><br><span class="line">        <span class="comment">// setAndCheckDiscoveredPartition 方法设计比较巧妙，</span></span><br><span class="line">          <span class="comment">// 将旧的 partition 和 不应该被该 subtask 消费的 partition，返回 false</span></span><br><span class="line">        <span class="comment">// 将这些partition 剔除，就是新发现的 partition</span></span><br><span class="line">        <span class="keyword">if</span> (!setAndCheckDiscoveredPartition(nextPartition)) &#123;</span><br><span class="line">            iter.remove();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> newDiscoveredPartitions;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// discoveredPartitions 中存放着所有发现的 partition</span></span><br><span class="line"><span class="keyword">private</span> Set&lt;KafkaTopicPartition&gt; discoveredPartitions = <span class="keyword">new</span> HashSet&lt;&gt;();</span><br><span class="line"></span><br><span class="line"><span class="comment">// setAndCheckDiscoveredPartition 方法实现</span></span><br><span class="line"><span class="comment">// 当参数的 partition 是新发现的 partition 且应该被当前 subtask 消费时，返回 true</span></span><br><span class="line"><span class="comment">// 旧的 partition 和 不应该被该 subtask 消费的 partition，返回 false</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">setAndCheckDiscoveredPartition</span><span class="params">(KafkaTopicPartition partition)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// discoveredPartitions 中不存在，表示发现了新的 partition，将其加入到 discoveredPartitions  </span></span><br><span class="line">    <span class="keyword">if</span> (!discoveredPartitions.contains(partition)) &#123;</span><br><span class="line">        discoveredPartitions.add(partition);</span><br><span class="line">        <span class="comment">// 再通过分配器来判断该 partition 是否应该被当前 subtask 去消费</span></span><br><span class="line">        <span class="keyword">return</span> KafkaTopicPartitionAssigner.assign(partition, numParallelSubtasks) == indexOfThisSubtask;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>上述代码中依赖 Set 类型的 discoveredPartitions 来判断 partition 是否是新的 partition，刚开始 discoveredPartitions 是一个空的 Set，所以任务初始化第一次调用发现器的 discoverPartitions 方法时，会把所有属于当前 subtask 的 partition 都返回，来保证所有属于当前 subtask 的 partition 都能被消费到。之后任务运行过程中，若创建了新的 partition，则新 partition 对应的那一个 subtask 会自动发现并从 earliest 位置开始消费，新创建的 partition 对其他 subtask 并不会产生影响。</p>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Flink 保证Exactly Once[1]</title>
      <link href="2019/12/10/Flink-%E4%BF%9D%E8%AF%81Exactly-Once-1/"/>
      <url>2019/12/10/Flink-%E4%BF%9D%E8%AF%81Exactly-Once-1/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p>在分布式场景下，应用程序随时可能出现任何形式的故障，例如：机器硬件故障、程序 OOM 等。当应用程序出现故障时，Flink 为了保证数据消费的 Exactly Once，需要有相应的故障容错能力。Flink 是通过周期性 Checkpoint 的方式来实现故障容错，这里使用的是基于 Chandy-Lamport 改进的算法。</p><a id="more"></a><h3 id="Flink-内部如何保证-Exactly-Once？"><a href="#Flink-内部如何保证-Exactly-Once？" class="headerlink" title="Flink 内部如何保证 Exactly Once？"></a>Flink 内部如何保证 Exactly Once？</h3><p>Flink 官网的定义是 Stateful Computations over Data Streams（数据流上的有状态计算），那到底什么是状态呢？举一个无状态计算的例子，比如：我们只是进行一个字符串拼接，输入 a，输出a_666；输入b，输出 b_666。无状态表示计算输出的结果跟之前的状态没关系，符合幂等性。</p><p>幂等性就是用户对于同一操作发起的一次请求或者多次请求的结果是一致的，不会因为多次点击而产生副作用。而计算 PV、UV 就属于有状态计算。实时计算 PV 时，每次都需要从某个存储介质的结果表中拿到之前的 PV 值，+1 后 set 到结果表中。有状态计算表示输出的结果跟之前的状态有关系，不符合幂等性，访问多次，PV 会增加。</p><h4 id="Flink-的-Checkpoint-功能简介"><a href="#Flink-的-Checkpoint-功能简介" class="headerlink" title="Flink 的 Checkpoint 功能简介"></a>Flink 的 Checkpoint 功能简介</h4><p><strong>Flink Checkpoint 机制的存在就是为了解决 Flink 任务在运行过程中由于各种原因导致任务失败后，能够正常恢复任务。</strong></p><p><strong>Checkpoint 是通过给程序做快照的方式使得将整个程序某些时刻的状态保存下来，当任务挂掉之后，默认从最近一次保存的完整快照处进行恢复任务。</strong></p><p>SnapShot 翻译为快照，是指将程序中某些信息存一份，后期可以用这些信息来恢复任务。对于一个 Flink 任务来讲，快照里面到底保存着什么信息呢？理论知识一般比较晦涩难懂，我们分析一个案例，用案例辅助大家理解快照里面到底存储什么信息。</p><blockquote><p>计算各个 app 的 PV，使用 Flink 该怎么统计呢？</p></blockquote><p>可以把要统计的 app_id 做为 key，对应的 PV 值做为 value，将统计的结果放到一个 Map 集合中，这个 Map 集合可以是内存里的 HashMap 或其他 kv 数据库，例如放到 Redis 的 key、value 结构中。从 Kafka 读取到一条条日志，由于要统计各 app 的 PV，所以我们需要从日志中解析出 app_id 字段，每来一条日志，只需要从 Map 集合将相应 app_id 的 PV 值拿出来，+1 后 put 到 Map 中，这样我们的 Map 中永远保存着所有 app 最新的 PV 数据。详细流程如下所示：</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-07-151541.jpg" alt="flink任务task图.png"></p><p>图中包含三部分：第一个是 Kafka 的一个名为 test 的 Topic，我们的数据来源于这个 Topic，第二个是 Flink 的 Source Task，是 Flink 应用程序读取数据的 Task，第三个是计算 PV 的 Flink Task，用于统计各个 app 的 PV 值，并将 PV 结果输出到 Map 集合。</p><p>Flink 的 Source Task 记录了当前消费到 test Topic 所有 partition 的 offset，为了方便理解 Checkpoint 的作用，这里先用一个 partition 进行讲解，假设名为 test 的 Topic 只有一个 partition0。</p><p>例：(0,60000) 表示 0 号 partition，目前消费到 offset 为 60000 的数据。Flink 的 PV task 记录了当前计算的各 app 的 PV 值，为了方便讲解，这里假设有两个 app：app1、app2。</p><p>例：(app1,50000) (app2,10000) 表示 app1 当前 PV 值为50000、app2 当前 PV 值为 10000。计算过程中，每来一条数据，只需要确定相应 app_id，将相应的 PV 值 +1 后 put 到 map 中即可。</p><p>该案例中，Checkpoint 到底记录了什么信息呢？记录的其实就是第 n 次 Checkpoint 消费的 offset 信息和各 app 的 PV 值信息，记录下发生 Checkpoint 当前的状态信息，并将该状态信息保存到相应的状态后端。（注：<strong>状态后端是保存状态的地方</strong>，决定状态如何保存，如何保证状态高可用，我们只需要知道，我们能从状态后端拿到 offset 信息和 PV 信息即可。状态后端必须是高可用的，否则我们的状态后端经常出现故障，会导致无法通过 Checkpoint 来恢复我们的应用程序）。下面列出了第 100 次 Checkpoint 的时候，状态后端保存的状态信息：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">chk-100</span><br><span class="line">    - offset：(0,60000)</span><br><span class="line">    - PV：(app1,50000) (app2,10000)</span><br></pre></td></tr></table></figure><p>该状态信息表示第 100 次 Checkpoint 的时候，partition 0 offset 消费到了 60000，PV 统计结果为 (app1,50000) (app2,10000)。如果任务挂了，如何恢复？</p><p>假如我们设置了一分钟进行一次 Checkpoint，第 100 次 Checkpoint 成功后，过了十秒钟，offset 已经消费到 (0,60100)，PV 统计结果变成了 (app1,50080) (app2,10020)，突然任务挂了，怎么办？</p><p>其实很简单，Flink 只需要从最近一次成功的 Checkpoint，也就是从第 100 次 Checkpoint 保存的 offset(0,60000) 处接着消费即可，当然 PV 值也要从第 100 次 Checkpoint 里保存的 PV 值 (app1,50000) (app2,10000) 进行累加，不能从 (app1,50080) (app2,10020) 处进行累加，因为 **partition 0 offset 消费到 60000 时，对应的 PV 统计结果为 (app1,50000) (app2,10000)**。</p><p>当然如果你想从 offset(0,60100)、PV(app1,50080)(app2,10020) 这个状态恢复，也是做不到的，因为那个时刻程序突然挂了，这个状态根本没有保存下来，只有在 Checkpoint 的时候，才会把这些完整的状态保存到状态后端，供我们恢复任务。我们能做的最高效方式就是从最近一次成功的 Checkpoint 处恢复，也就是一直所说的 chk-100。以上基本就是 Checkpoint 承担的工作，为了方便理解，描述的业务场景比较简单。</p><p>补充两个问题：计算 PV 的 task 在一直运行，它怎么知道什么时候去做 Checkpoint 呢？计算 PV 的 task 怎么保证它自己计算的 PV 值 (app1,50000) (app2,10000) 就是 offset(0,60000) 那一刻的统计结果呢？Flink 在数据中加了一个叫做 barrier 的东西，下图中红圈处就是两个 barrier。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-07-151548.jpg" alt="img"></p><p>barrier 从 Source Task 处生成，一直流到 Sink Task，期间所有的 Task 只要碰到 barrier，就会触发自身进行快照。如图所示，Checkpoint barrier n-1 处做的快照就是指 Job 从开始处理到 barrier n-1 所有的状态数据，barrier n 处做的快照就是指从 Job 开始到处理到 barrier n 所有的状态数据。</p><p>对应到 PV 案例中就是，Source Task 接收到 JobManager 的编号为 chk-100 的 Checkpoint 触发请求后，发现自己恰好接收到 Kafka offset(0,60000) 处的数据，所以会往 offset(0,60000) 数据之后 offset(0,60001) 数据之前插入一个barrier，然后自己开始做快照，也就是将 offset(0,60000) 保存到状态后端 chk-100 中。</p><p>然后，Source Task 会把 barrier 和我们要处理的数据一块往下游发送，当统计 PV 的 task 接收到 barrier 后，意味着 barrier 之前的数据已经被 PV task 处理完了，此时也会暂停处理 barrier 之后的数据，将自己内存中保存的 PV 信息 (app1,50000) (app2,10000) 保存到状态后端 chk-100 中。Flink 大概就是通过以上过程来保存快照的。</p><p>上述过程中，barrier 的作用就是为了把数据区分开，barrier 之前的数据是本次 Checkpoint 之前必须处理完的数据，barrier 之后的数据在本次 Checkpoint 之前不能被处理。</p><p>Checkpoint 过程中有一个同步做快照的环节不能处理 barrier 之后的数据，为什么呢？如果做快照的同时，也在处理数据，那么处理的数据可能会修改快照内容，所以先暂停处理数据，把内存中快照保存好后，再处理数据。</p><p>结合案例来讲就是，PV task 在对 (app1,50000)、(app2,10000) 做快照的同时，如果 barrier 之后的数据还在处理，可能会导致状态信息还没保存到磁盘，状态已经变成了 (app1,50001) (app2,10001)，导致我们最后快照里保存的 PV 值变成了 (app1,50001) (app2,10001)，这样如果从 Checkpoint 恢复任务时，我们从 offset 60000 开始消费，PV 值从 (app1,50001) (app2,10001) 开始累加，就会造成计算的 PV 结果偏高，结果不准确，就不能保证 Exactly Once。</p><p>所以，Checkpoint 同步做快照的过程中，不能处理 barrier 之后的数据。Checkpoint 将快照信息写入到磁盘后，为了保证快照信息的高可用，需要将快照上传到 HDFS，这个上传快照到 HDFS 的过程是异步进行的，这个过程也可以处理 barrier 之后的数据，处理 barrier 之后的数据不会影响到磁盘上的快照信息。</p><p>从 PV 案例再分析 Flink 是如何做 Checkpoint 并从 Checkpoint 处恢复任务的，首先 JobManager 端会向所有 SourceTask 发送 Checkpoint，Source Task 会在数据流中安插 Checkpoint barrier。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-07-151547.jpg" alt="单并行度 PV 案例 Checkpoint 过程图示1"></p><p>Source Task 安插好 barrier 后，会将 barrier 跟数据一块发送给下游，然后自身开始做快照，并将快照信息 offset(0,60000) 发送到高可用的持久化存储介质，例如 HDFS 上。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-07-151542.jpg" alt="单并行度 PV 案例 Checkpoint 过程图示2"></p><p>下游的 PV task 接收到 barrier 后，也会做快照，并将快照信息 PV：(app1,50000) (app2,10000) 发送到 HDFS。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-07-151543.jpg" alt="img"></p><p>假设第 100 次 Checkpoint 完成后，一段时间后任务挂了，Flink 任务会自动从状态后端恢复任务。Source Task 去读取自己需要的状态信息 offset(0,60000)，并从 offset 为 60000 的位置接着开始消费数据，PV task 也会去读取需要的状态信息 PV：(app1,50000) (app2,10000)，并在该状态值的基础上，往上累积计算 PV 值。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-07-151544.jpg" alt="img"></p><h4 id="多并行度、多-Operator-情况下，Checkpoint-的过程"><a href="#多并行度、多-Operator-情况下，Checkpoint-的过程" class="headerlink" title="多并行度、多 Operator 情况下，Checkpoint 的过程"></a>多并行度、多 Operator 情况下，Checkpoint 的过程</h4><p>上一节中讲述了单并行度情况下 Checkpoint 的过程，但是生产环境中，一般都是多并行度，而且算子也会比较多，这种情况下 Checkpoint 的过程就会变得复杂。分布式状态容错面临的问题与挑战：</p><ul><li>如何确保状态拥有<strong>精确一次</strong>的容错保证？</li><li>如何在分布式场景下替多个拥有本地状态的算子产生<strong>一个全域一致的快照</strong>？</li><li>如何在<strong>不中断运算</strong>的前提下产生快照？</li></ul><p>多并行度、多 Operator 实例的情况下，如何做全域一致的快照？所有的 Operator 运行过程中接收到所有上游算子发送 barrier 后，对自身的状态进行一次快照，保存到相应状态后端。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-07-151553.jpg" alt="img"></p><p>当任务从状态恢复时，每个 Operator 从状态后端读取自己相应的状态信息，数据源会从状态中保存的位置开始重新消费，后续的其他算子也会基于 Checkpoint 中保存的状态进行计算。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-07-151556.jpg" alt="多并行度下，任务从 Checkpoint 恢复图示"></p><p>整个 Checkpoint 的过程跟之前单并行度类似，图中有 4 个带状态的 Operator 实例，相应的状态后端就可以想象成 4 个格子。整个 Checkpoint 的过程可以当做 Operator 实例填自己格子的过程，Operator 实例将自身的状态写到状态后端中相应的格子，当所有的格子填满可以简单地认为一次完整的 Checkpoint 做完了。</p><p>上面只是快照的过程，Checkpoint 执行过程如下：</p><p>\1. JobManager 端的 CheckPointCoordinator 向所有 Source Task 发送 CheckPointTrigger，Source Task 会在数据流中安插 Checkpoint barrier。</p><p>\2. 当 task 收到所有的 barrier 后，向自己的下游继续传递 barrier，然后自身执行快照，并将自己的状态<strong>异步写入到持久化存储</strong>中。</p><ul><li>增量 CheckPoint 只是把最新的一部分数据更新写入到外部存储；</li><li>为了下游尽快开始做 CheckPoint，所以会先发送 barrier 到下游，自身再同步进行快照。</li></ul><p>\3. 当 task 对状态的快照信息完成备份后，会将备份数据的地址（state handle）通知给 JobManager 的 CheckPointCoordinator。</p><ul><li>如果 Checkpoint 的持续时长超过了 Checkpoint 设定的超时时间，CheckPointCoordinator 还没有收集完所有的 State Handle，CheckPointCoordinator就会认为本次 Checkpoint 失败，会把这次 Checkpoint 产生的所有状态数据全部删除。</li></ul><p>\4. CheckPointCoordinator 把整个 StateHandle 封装成 completed Checkpoint Meta，写入到 HDFS，整个 Checkpoint 结束。</p><h4 id="barrier-对齐"><a href="#barrier-对齐" class="headerlink" title="barrier 对齐"></a>barrier 对齐</h4><p>什么是 barrier 对齐？如图所示，当前的 Operator 实例接收上游两个流的数据，一个是字母流，一个是数字流。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-07-151558.jpg" alt="img"></p><p>当 Checkpoint 时，上游字母流和数字流都会往 Operator 实例发送 Checkpoint barrier，但是由于每个算子的执行速率不同，所以不可能保证上游两个流的 barrier 同时到达 Operator 实例，那图中的 Operator 实例到底什么时候进行快照呢？接收到任意一个 barrier 就可以开始进行快照了吗，还是接收到所有的 barrier 才能开始进行快照呢？</p><p>答案是：当一个 Operator 实例有多个输入流时，Operator 实例需要在做快照之前进行 barrier 对齐，等待所有输入流的 barrier 都到达。barrier 对齐的详细过程如下所示：</p><ol><li>对于一个有多个输入流的 Operator 实例，当 Operator 实例从其中一个输入流接收到 Checkpoint barrier n 时，就不能处理来自该流的任何数据记录了，直到它从其他所有输入流接收到 barrier n 为止，否则 <strong>Operator 实例 Checkpoint n 的快照会混入快照 n 的记录和快照 n+1 的记录</strong>。如上图中第 1 个小图所示，数字流的 barrier 先到达了。</li><li>接收到 barrier n 的流暂时被搁置，从这些流接收的记录不会被处理，而是放入输入缓冲区。图 2 中，我们可以看到虽然数字流对应的 barrier 已经到达了，但是 barrier 之后的 1、2、3 这些数据只能放到缓冲区中，等待字母流的 barrier 到达。</li><li>一旦最后所有输入流都接收到 barrier n，Operator 实例就会把 barrier 之前所有已经处理完成的数据和 barrier n 一块发送给下游。然后 Operator 实例就可以对状态信息进行快照。如图 3 所示，Operator 实例接收到上游所有流的 barrier n，此时 Operator 实例就可以将 barrier 和 barrier 之前的数据发送到下游，然后自身状态进行快照。</li><li>快照做完后，Operator 实例将继续处理缓冲区的记录，然后就可以处理输入流的数据。如图 4 所示，先处理完缓冲区数据，就可以正常处理输入流的数据了。</li></ol><p>上面的过程就是 Flink 在 Operator 实例有多个输入流的情况下，整个 barrier 对齐的过程。那什么是 barrier 不对齐呢？</p><p>barrier 不对齐是指当还有其他流的 barrier 还没到达时，为了提高 Operator 实例的处理性能，Operator 实例会直接处理 barrier 之后的数据，等到所有流的barrier 都到达后，就可以对该 Operator 做 Checkpoint 快照了。</p><p>对应到图中就是，barrier 不对齐时会直接把 barrier 之后的数据 1、2、3 直接处理掉，而<strong>不是</strong>放到缓冲区中等待其他的输入流的 barrier 到达，当所有输入流的 barrier 都到达后，才开始对 Operator 实例的状态信息进行快照，这样会导致做快照之前，Operator 实例已经处理了一些 barrier n 之后的数据。</p><p>Checkpoint 的目的是为了保存快照信息，如果 barrier 不对齐，那么 Operator 实例在做第 n 次 Checkpoint 之前，已经处理了一些 barrier n 之后的数据，当程序从第 n 次 Checkpoint 恢复任务时，程序会从第 n 次 Checkpoint 保存的 offset 位置开始消费数据，就会导致一些数据被处理了两次，就出现了重复消费。如果进行 barrier 对齐，就不会出现这种重复消费的问题，所以，<strong>barrier 对齐就可以实现 Exactly Once，barrier 不对齐就变成了 At Least Once。</strong></p><p>再结合计算 PV 的案例来证明一下，为什么 barrier 对齐就可以实现 Exactly Once，barrier 不对齐就变成了 At Least Once。之前的案例为了简单，描述的 Kafka topic 只有 1 个 partition，这里为了讲述 barrier 对齐，假设 topic 有 2 个 partittion，且计算的是我们平台的总 PV，也就是说不需要区分 app，每条一条数据，我们都需要将其 PV 值 +1 即可。如下图所示，Flink 应用程序有两个 Source Task，一个计算 PV 的 Task，这里计算 PV 的 Task 就出现了存在多个输入流的情况。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-07-151546.jpg" alt="img"></p><p>假设 barrier 不对齐，那么 Checkpoint 过程是怎么样呢？</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-07-151554.jpg" alt="img"></p><p>如左图所示，Source Subtask 0 和 Subtask 1 已经完成了快照操作，它们的状态信息为 offset(0,10000)(1,10005)，表示 partition0 消费到 offset 为 10000 的位置，partition 1 消费到 offset 为 10005 的位置。当 Source Subtask 1 的 barrier 到达 PV task 时，计算的 PV 结果为 20002，但 PV task 还没有接收到 Source Subtask 0 发送的 barrier，所以 PV task 还不能对自身状态信息进行快照。由于设置的 barrier 不对齐，所以此时 PV task 会继续处理 Source Subtask 0 和 Source Subtask 1 传来的数据。</p><p>很快，如右图所示，PV task 接收到 Source Subtask 0 发来的 barrier，但是 PV task 已经处理了 Source Subtask 1 barrier 之后的三条数据，所以 PV 值目前已经为 20008 了，这里的 PV=20008 实际上已经处理到 partition 1 offset 为 10008 的位置，此时 PV task 会对自身的状态信息（PV = 20008）做快照，整体的快照信息为 offset(0,10000)(1,10005) PV=20008。</p><p>接着程序在继续运行，过了 10 秒，由于某个服务器故障，导致我们的 Operator 实例有一个挂了，所以 Flink 会从最近一次 Checkpoint 保存的状态恢复。那具体是怎么恢复的呢？</p><p>Flink 同样会起三个 Operator 实例，我还称它们是 Source Subtask 0、Source Subtask 1 和 PV task。三个 Operator 会从状态后端读取保存的状态信息。Source Subtask 0 会从 partition 0 offset 为 10000 的位置开始消费，Source Subtask 1 会从 partition 1 offset 为 10005 的位置开始消费，PV task 会基于 PV=20008 进行累加统计。然后就会发现的 PV 值 20008 实际上已经包含了 partition 1 的offset 10005<del>10008 的数据，所以 partition 1 从 offset 10005 恢复任务时，partition1 的 offset 10005</del>10008 的数据被消费了两次，出现了重复消费的问题，所以 barrier 不对齐只能保证 At Least Once。</p><p>如果设置为 barrier 对齐，这里能保证 Exactly Once 吗？如下图所示，当 PV task 接收到 Source Subtask 1 的 barrier 后，并不会处理 Source Subtask 1 barrier 之后的数据，而是把这些数据放到 PV task 的输入缓冲区中，直到等到 Source Subtask 0 的 barrier 到达后，PV task 才会对自身状态信息进行快照。</p><p>此时 PV task 会把 PV=20005 保存到快照信息中，整体的快照状态信息为 offset(0,10000)(1,10005) PV=20005，当任务从 Checkpoint 恢复时，Source Subtask 0 会从 partition 0 offset 为 10000 的位置开始消费，Source Subtask 1 会从 partition 1 offset 为 10005 的位置开始消费，PV task 会基于 PV=20005 进行累加统计，所以 barrier 对齐能保证 Flink 内部的 Exactly Once。</p><p>在 Flink 应用程序中，当 Checkpoint 语义设置 Exactly Once 或 At Least Once 时，唯一的区别就是 barrier 对不对齐。当设置为 Exactly Once 时，就会 barrier 对齐，当设置为 At Least Once 时，就会 barrier 不对齐。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-07-151555.jpg" alt="img"></p><p>通过本案例，我们应该发现了 barrier 在 Flink 的 Checkpoint 中起着非常大的作用。barrier 告诉 Flink 应用程序，Checkpoint 之前哪些数据不应该被处理，barrier 对齐的过程其实就是为了防止 Flink 应用程序处理重复的数据。</p><p>总结一下，满足哪些条件时，会出现 barrier 对齐？在代码中设置了 Flink 的 Checkpoint 语义是 Exactly Once，其次 Operator 实例必须有多个输入流才会出现 barrier 对齐。</p><p>对齐，汉语词汇，释义为使两个以上事物配合或接触得整齐。由汉语解释可得对齐肯定需要两个以上事物，所以必须有多个输入流才可能存在对齐。</p><p>barrier 对齐就是上游多个流配合使得数据对齐的过程。言外之意：如果 Operator 实例只有一个输入流，就根本不存在 barrier 对齐，自己跟自己默认永远都是对齐的，所以当我们的应用程序从 Source 到 Sink 所有算子的并行度都是 1 的话，就算设置的 At Least Once，无形中也实现了 barrier 对齐，此时 Checkpoint 设置成 Exactly Once 和 At Least Once 一点区别都没有，都可以保证 Exactly Once。</p><p>看到这里你应该已经知道了哪种情况会出现重复消费了，也应该要掌握为什么 barrier 对齐就能保证 Exactly Once，为什么 barrier 不对齐就是 At Least Once。</p><p>barrier 对齐其实是要付出代价的，从 barrier 对齐的过程可以看出，PV task 明明可以更高效的处理数据，但因为 barrier 对齐，导致 Source Subtask 1 barrier 之后的数据被放到缓冲区中，暂时性地没有被处理，假如生产环境中，Source Subtask 0 的 barrier 迟迟没有到达，比 Source Subtask 1 延迟了 30 秒，那么这 30 秒期间，Source Subtask 1 barrier 之后的数据不能被处理，所以 PV task 相当于被闲置了。</p><p>所以，当我们的一些业务场景对 Exactly Once 要求不高时，我们可以设置 Flink 的 Checkpoint 语义是 At Least Once 来小幅度的提高应用程序的执行效率。Flink Web UI 的 Checkpoint 选项卡中可以看到 barrier 对齐的耗时，如果发现耗时比较长，且对 Exactly Once 语义要求不高时，可以考虑使用该优化方案。</p><p>前面提到如何在不中断运算的前提下产生快照？在 Flink 的 Checkpoint 过程中，无论下游算子有没有做完快照，只要上游算子将 barrier 发送到下游且上游算子自身已经做完快照时，那么上游算子就可以处理 barrier 之后的数据了，从而使得整个系统 Checkpoint 的过程影响面尽量缩到最小，来提升系统整体的吞吐量。</p><p>在整个 Checkpoint 的过程中，还存在一个问题，假设我们设置的 10 分钟一次 Checkpoint。在第 n 次 Checkpoint 成功后，过了 9 分钟，任务突然挂了，我们需要从最近一次成功的 Checkpoint 处恢复任务，也就是从 9 分钟之前的状态恢复任务，就需要把这 9分钟的数据全部再消费一次，成本比较大。</p><p>有的同学可能会想，那可以不可以设置为 100ms 就做一次 Checkpoint 呢？这样的话，当任务出现故障时，就不需要从 9 分钟前的状态进行恢复了，直接从 100ms 之前的状态恢复即可，恢复就会很快，不需要处理大量重复数据了。</p><p>但是，这样做会导致应用程序频繁的访问状态后端，一般我们为了高可用，会把状态里的数据比如 offset(0,60000) PV(app1,50000)(app2,10000) 信息保存到 HDFS 中，如果频繁访问 HDFS，肯定会造成吞吐量下降，所以一般我们的 Checkpoint 时间间隔可以设置为分钟级别，例如 1 分钟、3 分钟，对于状态很大的任务每次 Checkpoint 访问 HDFS 比较耗时，我们甚至可以设置为 5 分钟一次 Checkpoint，毕竟我们的应用程序挂的概率并不高，偶尔一次从 5 分钟前的状态恢复，我们是可以接受的。</p><p>可以根据业务场景合理地调节 Checkpoint 的间隔时长，对于状态很小的 Job Checkpoint 会很快，我们可以调小时间间隔，对于状态比较大的 Job Checkpoint 会比较慢，我们可以调大 Checkpoint 时间间隔。</p><p>有的同学可能还有疑问，明明说好的 Exactly Once，但在 Checkpoint 成功后 10s 发生了故障，从最近一次成功的 Checkpoint 处恢复时，由于发生故障前的 10s Flink 也在处理数据，所以 Flink 应用程序肯定是把一些数据重复处理了呀。</p><p>在面对任意故障时，不可能保证每个算子中用户定义的逻辑在每个事件中只执行一次，因为用户代码被部分执行的可能性是永远存在的。那么，当引擎声明 Exactly Once 处理语义时，它们能保证什么呢？如果不能保证用户逻辑只执行一次，那么哪些逻辑只执行一次？当引擎声明 Exactly Once 处理语义时，它们实际上是在说，它们可以保证引擎管理的状态更新只提交一次到持久的后端存储。换言之，无论以什么维度计算 PV、无论 Flink 应用程序发生多少次故障导致重启从 Checkpoint 恢复，Flink 都可以保证 PV 结果是准确的，不会因为各种任务重启而导致 PV 值计算偏高。</p><p>为了下游尽快做 Checkpoint，所以会先发送 barrier 到下游，自身再同步进行快照。这一步，如果向下发送 barrier 后，自己同步快照慢怎么办？下游已经同步好了，自己还没？可能会出现下游比上游快照还早的情况，但是这不影响快照结果，只是下游做快照更及时了，我只要保证下游把 barrier 之前的数据都处理了，并且不处理 barrier 之后的数据，然后做快照，那么下游也同样支持 Exactly Once。</p><p>这个问题不要从全局思考，单独思考上游和下游的实例，你会发现上下游的状态都是准确的，既没有丢，也没有重复计算。这里需要注意，如果有一个 Operator 的 Checkpoint 失败了或者因为 Checkpoint 超时也会导致失败，那么 JobManager 会认为整个 Checkpoint 失败。失败的 Checkpoint 是不能用来恢复任务的，必须所有的算子的 Checkpoint 都成功，那么这次 Checkpoint 才能认为是成功的，才能用来恢复任务。</p><p>对应到 PV 案例就是，PV task 做快照速度较快，PV=20005 较早地写入到了 HDFS，但是 offset(0,10000)(1,10005) 过了几秒才写入到 HDFS，这种情况就算出现了，也不会影响计算结果，因为我们的快照信息是完全正确的。</p><p>再分享一个案例，Flink 的 Checkpoint 语义设置了 Exactly Once，程序中设置了 1 分钟 1 次 Checkpoint，5 秒向 MySQL 写一次数据，并 commit。最后发现 MySQL 中数据重复了。为什么会重复呢？Flink 要求端对端的 Exactly Once 都必须实现 TwoPhaseCommitSinkFunction。如果你的 Checkpoint 成功了，过了 30 秒突然程序挂了，由于 5 秒 Commit 一次，所以在应用程序挂之前的 30 秒实际上已经写入了 6 批数据进入 MySQL。从 Checkpoint 处恢复时，之前提交的 6 批数据就会重复写入，所以出现了重复消费。</p><p>Flink 的 Exactly Once 有两种情况，一个是我们本节所讲的 Flink 内部的 Exactly Once，一个是端对端的 Exactly Once。关于端对端如何保证 Exactly Once，我们在下一节中深入分析。</p><h3 id="端对端如何保证-Exactly-Once？"><a href="#端对端如何保证-Exactly-Once？" class="headerlink" title="端对端如何保证 Exactly Once？"></a>端对端如何保证 Exactly Once？</h3><p>Flink 与外部存储介质之间进行数据交互统称为端对端或 end to end 数据传输。上一节讲述了 Flink 内部如何保证 Exactly Once，这一节来分析端对端的 Exactly Once。</p><p>正如上述 Flink 写 MySQL 的案例所示，在第 n 次 Checkpoint 结束后，第 n+1 次 Checkpoint 之前，如果 Flink 应用程序已经向外部的存储介质中成功写入并提交了一些数据后，Flink 应用程序由于某些原因挂了，导致任务从第 n 次 Checkpoint 处恢复。这种情况下，就会导致第 n 次 Checkpoint 结束后且任务失败之前往外部存储介质中写入的那一部分数据重复写入两次，可能会导致相同的数据在存储介质中存储了两份，从而端对端的一致性语义保证从 Exactly Once 退化为 At Least Once。</p><p>这里只考虑了数据重复的情况，为什么不考虑丢数据的情况呢？在写数据时可以对异常进行捕获增加重试策略，如果重试多次还没有成功可以让 Flink 任务失败，Flink 任务就会从最近一次成功的 Checkpoint 处恢复，就不会出现丢数据的情况，所以我们本节内容主要用来解决数据重复的问题。</p><p>针对上述端对端 Exactly Once 的问题，我们可以使用以下方案来解决：</p><ol><li>假如我们使用的存储介质支持按照全局主键去重，那么比较容易实现 Exactly Once，无论相同的数据往外部存储中写入了几次，外部存储都会进行去重，只保留一条数据。例如，app1 的 PV 值为 10，现在把（key=app1，value=10）往 Redis 中写入 10 次，只是说把 value 值覆盖了 10 次，并不会导致结果错误，这种方案属于幂等性写入。</li><li>我们上述案例中为什么会导致重复写入数据到外部存储呢？是因为在下一次 Checkpoint 之前如果任务失败时，一些数据已经成功写入到了外部存储中，没办法删除那些数据。既然问题是这样，那可以想办法把“向外部存储中提交数据”与 Checkpoint 强关联，两次 Checkpoint 之间不允许向外部存储介质中提交数据，Checkpoint 的时候再向外部存储提交。如果提交成功，则 Checkpoint 成功，提交失败，则 Checkpoint 也失败。这样在下一次 Checkpoint 之前，如果任务失败，也没有重复数据被提交到外部存储。这里只是描述一下大概思想，好多细节这里并没有详细描述，会在下文中详细描述。基于上述思想，Flink 实现了 TwoPhaseCommitSinkFunction，它提取了两阶段提交协议的通用逻辑，使得通过 Flink 来构建端到端的 Exactly Once 程序成为可能。它提供了一个抽象层，用户只需要实现少数方法就能实现端到端的 Exactly Once 语义。不过这种方案必须要求我们的输出端（Sink 端）必须支持事务。</li></ol><p>下面我们通过两部分来详细介绍上述两种方案。</p><h4 id="幂等性写入如何保证端对端的-Exactly-Once"><a href="#幂等性写入如何保证端对端的-Exactly-Once" class="headerlink" title="幂等性写入如何保证端对端的 Exactly Once"></a>幂等性写入如何保证端对端的 Exactly Once</h4><p>实时 ETL 当 HBase 做为 Sink 端时，就是典型的应用场景。把日志中的主键做为 HBase 的 rowkey，就可以保证数据不重复，实现比较简单，这里不多赘述。</p><p>继续探讨实时计算各 app PV 的案例，将统计结果以普通键值对的形式保存到 Redis 中供业务方查询。到底如何实现，才能保证 Redis 中的结果是精准的呢？在之前 Strom 或 Spark Streaming 的方案中，将统计的 PV 结果保存在 Redis 中，每来一条数据，从 Redis 中获取相应 app 对应的 PV 值然后内存中进行 +1 后，再将 PV 值 put 到 Redis 中。</p><p>例如：Redis 中保存 app1 的 PV 为 10，现在来了一条 app1 的日志，首先从 Redis 中获取 app1 的 PV 值 =10，内存中 10+1=11，将 (app1,11) put 到 Redis 中，这里的 11 就是我们统计的 app1 的 PV 结果。可以将这种方案优化为 incr 或 incrby，直接对 Redis 中的 10 进行累加，不需要手动在内存中进行累加操作。</p><p>当然 Flink 也可以用上述的这种方案来统计各 app 的 PV，但是上述方案并不能保证 Exactly Once，为什么呢？当第 n 次 Checkpoint 时，app1 的 PV 结果为 10000，第 n 次 Checkpoint 结束后运行了 10 秒，Redis 中 app1 的 PV 结果已经累加到了 10200。此时如果任务挂了，从第 n 次 Checkpoint 恢复任务时，会继续按照 Redis 中保存的 PV=10200 进行累加，但是正确的结果应该是从 PV=10000 开始累加。</p><p>如果按照上面的方案统计 PV，就可能会出现统计值偏高的情况。这里也证实了一点：并不是说 Flink 程序的 Checkpoint 语义设置为 Exactly Once，就能保证我们的统计结果或者各种输出结果都能满足 Exactly Once。为了编写真正满足 Exactly Once 的代码，我们需要对 Flink 的 Checkpoint 原理做一些了解，编写对 Exactly Once 友好的代码。</p><p>那如何编写代码才能使得最后在 Redis 中保存的 PV 结果满足 Exactly Once 呢？上一节中，讲述了 Flink 内部状态可以保证 Exactly Once，这里可以将统计的 PV 结果保存在 Flink 内部的状态里，每次基于状态进行累加操作，并将累加到的结果 put 到 Redis 中，这样当任务从 Checkpoint 处恢复时，并不是基于 Redis 中实时统计的 PV 值进行累加，而是基于 Checkpoint 中保存的 PV 值进行累加，Checkpoint 中会保存每次 Checkpoint 时对应的 PV 快照信息，例如：第 n 次 Checkpoint 会把当时 pv=10000 保存到快照信息里，同时状态后端还保存着一份实时的状态信息用于实时累加。</p><p>示例代码如下所示：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">final</span> StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"><span class="comment">// 1 分钟一次 Checkpoint</span></span><br><span class="line">env.enableCheckpointing(TimeUnit.MINUTES.toMillis(<span class="number">1</span>));</span><br><span class="line"></span><br><span class="line">CheckpointConfig checkpointConf = env.getCheckpointConfig();</span><br><span class="line"><span class="comment">// Checkpoint 语义 EXACTLY ONCE</span></span><br><span class="line">checkpointConf.setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE);</span><br><span class="line">checkpointConf.enableExternalizedCheckpoints(CheckpointConfig.ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION);</span><br><span class="line"></span><br><span class="line">Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">&quot;localhost:9092&quot;</span>);</span><br><span class="line">props.put(ConsumerConfig.GROUP_ID_CONFIG, <span class="string">&quot;app-pv-stat&quot;</span>);</span><br><span class="line"></span><br><span class="line">DataStreamSource&lt;String&gt; appInfoSource = env.addSource(<span class="keyword">new</span> FlinkKafkaConsumer011&lt;&gt;(</span><br><span class="line">        <span class="comment">// kafka topic， String 序列化</span></span><br><span class="line">        <span class="string">&quot;app-topic&quot;</span>,  <span class="keyword">new</span> SimpleStringSchema(), props));</span><br><span class="line"></span><br><span class="line"><span class="comment">// 按照 appId 进行 keyBy</span></span><br><span class="line">appInfoSource.keyBy((KeySelector&lt;String, String&gt;) appId -&gt; appId)</span><br><span class="line">        .map(<span class="keyword">new</span> RichMapFunction&lt;String, Tuple2&lt;String, Long&gt;&gt;() &#123;</span><br><span class="line">            <span class="keyword">private</span> ValueState&lt;Long&gt; pvState;</span><br><span class="line">            <span class="keyword">private</span> <span class="keyword">long</span> pv = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(Configuration parameters)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">super</span>.open(parameters);</span><br><span class="line">                <span class="comment">// 初始化状态</span></span><br><span class="line">                pvState = getRuntimeContext().getState(</span><br><span class="line">                        <span class="keyword">new</span> ValueStateDescriptor&lt;&gt;(<span class="string">&quot;pvStat&quot;</span>,</span><br><span class="line">                        TypeInformation.of(<span class="keyword">new</span> TypeHint&lt;Long&gt;() &#123;&#125;)));</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> Tuple2&lt;String, Long&gt; <span class="title">map</span><span class="params">(String appId)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="comment">// 从状态中获取该 app 的 PV 值，+1 后，update 到状态中</span></span><br><span class="line">                <span class="keyword">if</span>(<span class="keyword">null</span> == pvState.value())&#123;</span><br><span class="line">                    pv = <span class="number">1</span>;</span><br><span class="line">                &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                    pv = pvState.value();</span><br><span class="line">                    pv += <span class="number">1</span>;</span><br><span class="line">                &#125;</span><br><span class="line">                pvState.update(pv);</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> Tuple2&lt;&gt;(appId, pv);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;)</span><br><span class="line">        .print();</span><br><span class="line"></span><br><span class="line">env.execute(<span class="string">&quot;Flink PV stat&quot;</span>);</span><br></pre></td></tr></table></figure><p>详细代码请参考：</p><blockquote><p><a href="https://github.com/zhisheng17/flink-learning/blob/master/flink-learning-examples/src/main/java/com/zhisheng/examples/streaming/checkpoint/PvStatExactlyOnce.java">PvStatExactlyOnce.java</a></p></blockquote><p>代码中设置 1 分钟一次 Checkpoint，Checkpoint 语义 EXACTLY ONCE，从 Kafka 中读取数据，这里为了简化代码，所以 Kafka 中读取的直接就是 String 类型的 appId，按照 appId KeyBy 后，执行 RichMapFunction，RichMapFunction 的 open 方法中会初始化 ValueState<Long> 类型的 pvState，pvState 就是上文一直强调的状态信息，每次 Checkpoint 的时候，会把 pvState 的状态信息快照一份到 HDFS 来提供恢复。</p><p>这里按照 appId 进行 keyBy，所以每一个 appId 都会对应一个 pvState，pvState 里存储着该 appId 对应的 pv 值。每来一条数据都会执行一次 map 方法，当这条数据对应的 appId 是新 app 时，pvState 里就没有存储这个 appId 当前的 pv 值，将 pv 值赋值为 1，当 pvState 里存储的 value 不为 null 时，拿出 pv 值 +1后 update 到 pvState 里。map 方法再将 appId 和 pv 值发送到下游算子，下游直接调用了 print 进行输出，这里完全可以替换成相应的 RedisSink 或 HBaseSink。</p><p>本案例中计算 pv 的工作交给了 Flink 内部的 ValueState，不依赖外部存储介质进行累加，外部介质承担的角色仅仅是提供数据给业务方查询，所以无论下游使用什么形式的 Sink，只要 Sink 端能够按照主键去重，该统计方案就可以保证 Exactly Once。本案例使用的 ValueState，关于 State 的详细使用请参阅第 3.1 节。</p><h4 id="TwoPhaseCommitSinkFunction-如何保证端对端的-Exactly-Once"><a href="#TwoPhaseCommitSinkFunction-如何保证端对端的-Exactly-Once" class="headerlink" title="TwoPhaseCommitSinkFunction 如何保证端对端的 Exactly Once"></a>TwoPhaseCommitSinkFunction 如何保证端对端的 Exactly Once</h4><p>Flink 的源码中有这么一段注释：</p><blockquote><p>This is a recommended base class for all of the {@link SinkFunction} that intend to implement exactly-once semantic.</p></blockquote><p>意思是对于打算实现 Exactly Once 语义的所有 SinkFunction 都推荐继承该抽象类。在介绍 TwoPhaseCommitSinkFunction 之前，先了解一下 2PC 分布式一致性协议。</p><p>在分布式系统中，每一个机器节点虽然都能明确地知道自己在进行事务操作过程中的结果是成功或失败，但无法直接获取到其他分布式节点的操作结果。因此，当一个事务操作需要跨越多个分布式节点的时候，为了让每个节点都能够获取到其他节点的事务执行状况，需要引入一个“协调者（Coordinator）”节点来统一调度所有分布式节点的执行逻辑，这些被调度的分布式节点被称为“参与者（Participant）”。协调者负责调度参与者的行为，并最终决定这些参与者是否要把事务真正的提交。</p><p>普通的事务可以保证单个事务内所有操作要么全部成功，要么全部失败，而分布式系统中具体如何保证多台节点上执行的事务要么所有节点事务都成功，要么所有节点事务都失败呢？先了解一下 2PC 一致性协议。</p><p>2PC 是 Two-Phase Commit 的缩写，即两阶段提交。2PC 将分布式事务分为了两个阶段，分别是提交事务请求（投票）和执行事务提交。协调者会根据参与者在第一阶段的投票结果，来决定第二阶段是否真正的执行事务，具体流程如下。</p><p><strong>提交事务请求（投票）阶段</strong></p><ol><li>协调者向所有参与者发送 prepare 请求与事务内容，询问是否可以准备事务提交，并等待参与者的响应。</li><li>各参与者执行事务操作，并记录 Undo 日志（用于回滚）和 Redo日志（用于重放），但不真正提交。</li><li>参与者向协调者返回事务操作的执行结果，执行成功返回 Yes，否则返回 No。</li></ol><p><strong>执行事务提交阶段</strong></p><p>分为成功与失败两种情况。</p><p>若第一阶段所有参与者都返回 Yes，说明事务可以提交：</p><ol><li>协调者向所有参与者发送 Commit 请求。</li><li>参与者收到 Commit 请求后，会正式执行事务提交操作，并在提交完成后释放事务资源。</li><li>完成事务提交后，向协调者发送 Ack 消息。</li><li>协调者收到所有参与者的 Ack 消息，完成事务。</li><li>参与者收到 Commit 请求后，将事务真正地提交上去，并释放占用的事务资源，并向协调者返回 Ack。</li><li>协调者收到所有参与者的 Ack 消息，事务成功完成。</li></ol><p>若第一阶段有参与者返回 No 或者超时未返回，说明事务中断，需要回滚：</p><ol><li>协调者向所有参与者发送 Rollback 请求。</li><li>参与者收到 Rollback 请求后，根据 Undo 日志回滚到事务执行前的状态，释放占用的事务资源。</li><li>参与者在完成事务回滚后，向协调者返回 Ack。</li><li>协调者收到所有参与者的 Ack 消息，事务回滚完成。</li></ol><p>简单来讲，2PC 讲一个事务的处理过程分为了投票和执行两个阶段，其核心是每个事务都采用先尝试后提交的处理方式。下面分别图示出这两种情况：</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-07-151552.jpg" alt="img"></p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-07-151557.jpg" alt="img"></p><p>2PC 的优点：原理简单，实现方便。</p><p>2PC 的缺点：</p><ul><li>协调者单点问题：协调者在整个 2PC 协议中非常重要，一旦协调者故障，则 2PC 将无法运转。</li><li>过于保守：在 2PC 的阶段一，如果参与者出现故障而导致协调者无法获取到参与者的响应信息，这时协调者只能依靠自身的超时机制来判断是否需要中断事务，这种策略比较保守。换言之，2PC 没有涉及较为完善的容错机制，任意一个节点失败都会导致整个事务的失败。</li><li>同步阻塞：执行过程是完全同步的，各个参与者在等待其他参与者投票响应的的过程中，将无法进行其他任何操作。</li><li>数据不一致：在二阶段提交协议的阶段二，当协调者向所有的参与者发送 Commit 请求后，出现了局部网络异常或局部参与者机器故障等因素导致一部分的参与者执行了 Commit 操作，而发生故障的参与者没有执行 Commit，于是整个分布式系统便出现了数据不一致现象。</li></ul><p>Flink 的 TwoPhaseCommitSinkFunction 是基于 2PC 实现的。Flink 的 JobManager 对应到 2PC 中的协调者，Operator 实例对应到 2PC 中的参与者。TwoPhaseCommitSinkFunction 实现了 CheckpointedFunction 和 CheckpointListener 接口。</p><p>CheckpointedFunction 接口中有两个方法 snapshotState 和 initializeState，snapshotState 方法会在 Checkpoint 时且做快照之前被调用，initializeState 方法会在自定义 Function 初始化恢复状态时被调用。</p><p>CheckpointListener 接口中有一个 notifyCheckpointComplete 方法，Operator 实例的 Checkpoint 成功后，会反馈给 JobManager，当 JobManager 接收到所有 Operator 实例 Checkpoint 成功的通知后，就认为本次 Checkpoint 成功了，会给所有 Operator 实例发送一个 Checkpoint 完成的通知，Operator 实例接收到通知后，就会调用 notifyCheckpointComplete 方法。</p><p>TwoPhaseCommitSinkFunction定义了如下 5 个抽象方法：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 处理每一条数据</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">abstract</span> <span class="keyword">void</span> <span class="title">invoke</span><span class="params">(TXN transaction, IN value, Context context)</span> <span class="keyword">throws</span> Exception</span>;</span><br><span class="line"><span class="comment">// 开始一个事务，返回事务信息的句柄</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">abstract</span> TXN <span class="title">beginTransaction</span><span class="params">()</span> <span class="keyword">throws</span> Exception</span>;</span><br><span class="line"><span class="comment">// 预提交（即提交请求）阶段的逻辑</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">abstract</span> <span class="keyword">void</span> <span class="title">preCommit</span><span class="params">(TXN transaction)</span> <span class="keyword">throws</span> Exception</span>;</span><br><span class="line"><span class="comment">// 正式提交阶段的逻辑</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">abstract</span> <span class="keyword">void</span> <span class="title">commit</span><span class="params">(TXN transaction)</span></span>;</span><br><span class="line"><span class="comment">// 取消事务，Rollback 相关的逻辑</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">abstract</span> <span class="keyword">void</span> <span class="title">abort</span><span class="params">(TXN transaction)</span></span>;</span><br></pre></td></tr></table></figure><p>TwoPhaseCommitSinkFunction 里这些方法什么时候会被执行呢？如下图所示，在状态初始化的 initializeState 方法内或者每次 Checkpoint 的 snapshotState 方法内都会调用 beginTransaction 方法开启新的事务。开启新的事务后，Flink 开始处理数据，每来一条数据都会调用 invoke 方法，按照业务逻辑将数据添加到本次的事务中。等到下一次 Checkpoint 执行 snapshotState 时，会调用 preCommit 方法进行预提交，预提交一般会对事务进行 flush 操作，到这里为止可以理解为 2PC 的第一阶段。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-11-15-TwoPhaseCommitSinkFunction%20%E7%AC%AC%E4%B8%80%E9%98%B6%E6%AE%B5.png" alt="img">)</p><p>第一阶段运行期间无论是机器故障还是 invoke 失败或者 preCommit 对应预提交的 flush 失败都可以理解为 2PC 的第一阶段返回了 No，即投票失败就会执行 2PC 第二阶段的 Rollback，对应到 TwoPhaseCommitSinkFunction 中就是执行 abort 方法，abort 方法内一般会对本次事务进行 abortTransaction 操作。</p><p>只有当 2PC 的第一阶段所有参与者都完全成功，也就是说 Flink TwoPhaseCommitSinkFunction 对应的所有并行度在本次事务中 invoke 全部成功且 preCommit 对应预提交的 flush 也全部成功才认为 2PC 的第一阶段返回了Yes，即投票成功就会执行 2PC 第二阶段的 Commit，对应到 TwoPhaseCommitSinkFunction 中就是执行 Commit 方法，Commit 方法内一般会对本次事务进行 commitTransaction 操作，以上就是 Flink 中 TwoPhaseCommitSinkFunction 的大概执行流程。</p><p>在第一阶段结束时，数据被写入到了外部存储，但是当事务的隔离级别为读已提交（Read Committed）时，在外部存储中并读取不到我们写入的数据，因为并没有执行 Commit 操作。如下图所示，是第二阶段的两种情况。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-11-15-TwoPhaseCommitSinkFunction%20%E7%AC%AC%E4%BA%8C%E9%98%B6%E6%AE%B5.png" alt="img"></p><p>FlinkKafkaProducer011 继承了 TwoPhaseCommitSinkFunction，如下图所示，Flink 应用使用 FlinkKafkaProducer011 时，Checkpoint 的时候不仅要将快照保存到状态后端，还要执行 preCommit 操作将缓存中的数据 flush 到 Sink 端的 Kafka 中。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-07-151549.jpg" alt="img"></p><p>当所有的实例快照完成且所有 Sink 实例执行完 preCommit 操作时，会把快照完成的消息发送给 JobManager，JobManager 收到所有实例的 Checkpoint 完成消息时，就认为这次 Checkpoint 完成了，会向所有的实例发送 Checkpoint 完成的通知（Notify Checkpoint Completed），当 FlinkKafkaProducer011 接收到 Checkpoint 完成的消息时，就会执行 Commit 方法。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-07-151550.jpg" alt="img"></p><p>上文提到过 2PC 有一些缺点存在，关于协调者和参与者故障的问题，对应到 Flink 中如果节点发生故障会申请资源并从最近一次成功的 Checkpoint 处恢复任务，所以，节点故障的问题 Flink 已经解决了。关于 2PC 同步阻塞的问题，2PC 算法在没有等到第一阶段所有参与者的投票之前肯定是不能执行第二阶段的 Commit，所以基于 2PC 实现原理同步阻塞的问题没有办法解决，除非使用其他算法。</p><p>那数据不一致的问题呢？</p><p>在整个的第一阶段不会真正地提交数据到 Kafka，所以只要设置事务隔离识别为读已提交（Read Committed），那么第一阶段就不会导致数据不一致的问题。</p><p>那 Flink 的第二阶段呢？</p><p>Flink 中，Checkpoint 成功后，会由 JobManager 给所有的实例发送 Checkpoint 完成的通知，然后 KafkaSink 在 notifyCheckpointComplete 方法内执行 commit。假如现在执行第 n 次 Checkpoint，快照完成且预提交完成，我们认为第 n 次 Checkpoint 已经成功了，这里一定要记住无论第二阶段是否 commit 成功，Flink 都会认为第 n 次 Checkpoint 已经结束了，换言之 Flink 可能会出现第 n 次 Checkpoint 成功了，但是第 n 次 Checkpoint 对应的事务 commit 并没有成功。</p><p>当 Checkpoint 成功后，JobManager 会向所有的 KafkaSink 发送 Checkpoint 完成的通知，所有的 KafkaSink 接收到通知后才会执行 Commit 操作。假如 JobManager 发送通知时出现了故障，导致 KafkaSink 的所有并行度都没有收到通知或者只有其中一部分 KafkaSink 接收到了通知，最后有一部分的 KafkaSink 执行了 Commit，另外一部分 KafkaSink 并没有执行 Commit，此时出现了 Checkpoint 成功，但是数据并没有完整地提交到 Kafka 的情况，出现了数据不一致的问题。</p><p>那 Flink 如何解决这个问题呢？</p><p>在任务执行过程中，如果因为各种原因导致有任意一个 KafkaSink 没有 Commit 成功，就会认为 Flink 任务出现故障，就会从最近一次成功的 Checkpoint 处恢复任务，也就是从第 n 次 Checkpoint 处恢复，TwoPhaseCommitSinkFunction 将每次 Checkpoint 时需要 Commit 的事务保存在状态里，当从第 n 次 Checkpoint 恢复时会从状态中拿到第 n 次 Checkpoint 可能没有提交的事务并执行 Commit，通过这种方式来保证所有的 KafkaSink 都能将事务进行 Commit，从而解决了 2PC 协议中可能出现的数据不一致的问题。</p><p>也就是说 Flink 任务重启后，会检查之前 Checkpoint 是否有未提交的事务，如果有则执行 Commit，从而保证了 Checkpoint 之前的数据被完整地提交。</p><p>简单描述一下 FlinkKafkaProducer011 的实现原理：</p><ul><li>FlinkKafkaProducer011 继承了 TwoPhaseCommitSinkFunction，所有并行度在 initializeState 初始化状态时，会开启新的事务，并把状态里保存的之前未提交事务进行 commit。</li><li>接下来开始调用 invoke 方法处理数据，会把数据通过事务 api 发送到 Kafka。一段时间后，开始 Checkpoint，checkpoint 时 snapshotState 方法会被执行，snapshotState 方法会调用 preCommit 方法并把当前还未 Commit 的事务添加到状态中来提供故障容错。</li><li>snapshotState 方法执行完成后，会对自身状态信息进行快照并上传到 HDFS 上来提供恢复。所有的实例都将状态信息备份完成后就认为本次 Checkpoint 结束了，此时 JobManager 会向所有的实例发送 Checkpoint 完成的通知，各实例收到通知后，会调用 notifyCheckpointComplete 方法把未提交的事务进行 commit。</li><li>期间如果出现其中某个并行度出现故障，JobManager 会停止此任务，向所有的实例发送通知，各实例收到通知后，调用 close 方法，关闭 Kafka 事务 Producer。</li></ul><p>以上就是 FlinkKafkaProducer011 实现原理的简单描述，具体实现细节请参考源码。</p><p>TwoPhaseCommitSinkFunction 还存在一个问题，假如我们设置的一分钟一次 Checkpoint，事务隔离级别设置为读已提交时，那么我们这一分钟内写入的数据，都必须等到 Checkpoint 结束后，下游才能读取到，导致我们的 Flink 任务数据延迟了一分钟。所以我们要结合这个特性，合理的设置我们的 Checkpoint 周期。</p>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Flink 重启策略</title>
      <link href="2019/12/09/Flink-%E9%87%8D%E5%90%AF%E7%AD%96%E7%95%A5/"/>
      <url>2019/12/09/Flink-%E9%87%8D%E5%90%AF%E7%AD%96%E7%95%A5/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><h3 id="Flink-Job-常见重启错误"><a href="#Flink-Job-常见重启错误" class="headerlink" title="Flink Job 常见重启错误"></a>Flink Job 常见重启错误</h3><p>不知道大家是否有遇到过这样的问题：整个 Job 一直在重启，并且还会伴随着一些错误（可以通过 UI 查看 Exceptions 日志）</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-04-152844.png" alt="img"></p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-06-140519.png" alt="img"></p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-09-26-2019-05-14_00-59-25.png" alt="img"></p><p>其实遇到上面这种问题比较常见的，比如有时候因为数据的问题（不合规范、为 null 等），这时在处理这些脏数据的时候可能就会遇到各种各样的异常错误，比如空指针、数组越界、数据类型转换错误等。可能你会说只要过滤掉这种脏数据就行了，或者进行异常捕获就不会导致 Job 不断重启的问题了。</p><p>确实如此，如果做好了脏数据的过滤和异常的捕获，Job 的稳定性确实有保证，但是复杂的 Job 下每个算子可能都会产生出脏数据（包含源数据可能也会为空或者不合法的数据），你不可能在每个算子里面也用一个大的 try catch 做一个异常捕获，所以脏数据和异常简直就是防不胜防，不过我们还是要尽力的保证代码的健壮性，但是也要配置好 Flink Job 的 RestartStrategy（重启策略）。</p><h3 id="RestartStrategy"><a href="#RestartStrategy" class="headerlink" title="RestartStrategy"></a>RestartStrategy</h3><p>RestartStrategy，重启策略，在遇到机器或者代码等不可预知的问题时导致 Job 或者 Task 挂掉的时候，它会根据配置的重启策略将 Job 或者受影响的 Task 拉起来重新执行，以使得作业恢复到之前正常执行状态。Flink 中的重启策略决定了是否要重启 Job 或者 Task，以及重启的次数和每次重启的时间间隔。</p><h3 id="为什么需要-RestartStrategy？"><a href="#为什么需要-RestartStrategy？" class="headerlink" title="为什么需要 RestartStrategy？"></a>为什么需要 RestartStrategy？</h3><p>重启策略会让 Job 从上一次完整的 Checkpoint 处恢复状态，保证 Job 和挂之前的状态保持一致，另外还可以让 Job 继续处理数据，不会出现 Job 挂了导致消息出现大量堆积的问题，合理的设置重启策略可以减少 Job 不可用时间和避免人工介入处理故障的运维成本，因此重启策略对于 Flink Job 的稳定性来说有着举足轻重的作用。</p><h3 id="怎么配置-RestartStrategy？"><a href="#怎么配置-RestartStrategy？" class="headerlink" title="怎么配置 RestartStrategy？"></a>怎么配置 RestartStrategy？</h3><p>既然 Flink 中的重启策略作用这么大，那么该如何配置呢？其实如果 Flink Job 没有单独设置重启重启策略的话，则会使用集群启动时加载的默认重启策略，如果 Flink Job 中单独设置了重启策略则会覆盖默认的集群重启策略。默认重启策略可以在 Flink 的配置文件 <code>flink-conf.yaml</code> 中设置，由 <code>restart-strategy</code> 参数控制，有 fixed-delay（固定延时重启策略）、failure-rate（故障率重启策略）、none（不重启策略）三种可以选择，如果选择的参数不同，对应的其他参数也不同。下面分别介绍这几种重启策略和如何配置。</p><h4 id="FixedDelayRestartStrategy（固定延时重启策略）"><a href="#FixedDelayRestartStrategy（固定延时重启策略）" class="headerlink" title="FixedDelayRestartStrategy（固定延时重启策略）"></a>FixedDelayRestartStrategy（固定延时重启策略）</h4><p>FixedDelayRestartStrategy 是固定延迟重启策略，程序按照集群配置文件中或者程序中额外设置的重启次数尝试重启作业，如果尝试次数超过了给定的最大次数，程序还没有起来，则停止作业，另外还可以配置连续两次重启之间的等待时间，在 <code>flink-conf.yaml</code> 中可以像下面这样配置。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">restart-strategy:</span> <span class="string">fixed-delay</span></span><br><span class="line"><span class="attr">restart-strategy.fixed-delay.attempts:</span> <span class="number">3</span>  <span class="comment">#表示作业重启的最大次数，启用 checkpoint 的话是 Integer.MAX_VALUE，否则是 1。</span></span><br><span class="line"><span class="attr">restart-strategy.fixed-delay.delay:</span> <span class="number">10</span> <span class="string">s</span>  <span class="comment">#如果设置分钟可以类似 1 min，该参数表示两次重启之间的时间间隔，当程序与外部系统有连接交互时延迟重启可能会有帮助，启用 checkpoint 的话，延迟重启的时间是 10 秒，否则使用 akka.ask.timeout 的值。</span></span><br></pre></td></tr></table></figure><p>在程序中设置固定延迟重启策略的话如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">env.setRestartStrategy(RestartStrategies.fixedDelayRestart(</span><br><span class="line">  <span class="number">3</span>, <span class="comment">// 尝试重启的次数</span></span><br><span class="line">  Time.of(<span class="number">10</span>, TimeUnit.SECONDS) <span class="comment">// 延时</span></span><br><span class="line">));</span><br></pre></td></tr></table></figure><h4 id="FailureRateRestartStrategy（故障率重启策略）"><a href="#FailureRateRestartStrategy（故障率重启策略）" class="headerlink" title="FailureRateRestartStrategy（故障率重启策略）"></a>FailureRateRestartStrategy（故障率重启策略）</h4><p>FailureRateRestartStrategy 是故障率重启策略，在发生故障之后重启作业，如果固定时间间隔之内发生故障的次数超过设置的值后，作业就会失败停止，该重启策略也支持设置连续两次重启之间的等待时间。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">restart-strategy:</span> <span class="string">failure-rate</span></span><br><span class="line"><span class="attr">restart-strategy.failure-rate.max-failures-per-interval:</span> <span class="number">3</span>  <span class="comment">#固定时间间隔内允许的最大重启次数，默认 1</span></span><br><span class="line"><span class="attr">restart-strategy.failure-rate.failure-rate-interval:</span> <span class="number">5</span> <span class="string">min</span>  <span class="comment">#固定时间间隔，默认 1 分钟</span></span><br><span class="line"><span class="attr">restart-strategy.failure-rate.delay:</span> <span class="number">10</span> <span class="string">s</span> <span class="comment">#连续两次重启尝试之间的延迟时间，默认是 akka.ask.timeout </span></span><br></pre></td></tr></table></figure><p>可以在应用程序中这样设置来配置故障率重启策略：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">env.setRestartStrategy(RestartStrategies.failureRateRestart(</span><br><span class="line">  <span class="number">3</span>, <span class="comment">// 固定时间间隔允许 Job 重启的最大次数</span></span><br><span class="line">  Time.of(<span class="number">5</span>, TimeUnit.MINUTES), <span class="comment">// 固定时间间隔</span></span><br><span class="line">  Time.of(<span class="number">10</span>, TimeUnit.SECONDS) <span class="comment">// 两次重启的延迟时间</span></span><br><span class="line">));</span><br></pre></td></tr></table></figure><h4 id="NoRestartStrategy（不重启策略）"><a href="#NoRestartStrategy（不重启策略）" class="headerlink" title="NoRestartStrategy（不重启策略）"></a>NoRestartStrategy（不重启策略）</h4><p>NoRestartStrategy 作业不重启策略，直接失败停止，在 <code>flink-conf.yaml</code> 中配置如下：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">restart-strategy:</span> <span class="string">none</span></span><br></pre></td></tr></table></figure><p>在程序中如下设置即可配置不重启：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">env.setRestartStrategy(RestartStrategies.noRestart());</span><br></pre></td></tr></table></figure><h4 id="Fallback（备用重启策略）"><a href="#Fallback（备用重启策略）" class="headerlink" title="Fallback（备用重启策略）"></a>Fallback（备用重启策略）</h4><p>如果程序没有启用 Checkpoint，则采用不重启策略，如果开启了 Checkpoint 且没有设置重启策略，那么采用固定延时重启策略，最大重启次数为 Integer.MAX_VALUE。</p><p>在应用程序中配置好了固定延时重启策略，可以测试一下代码异常后导致 Job 失败后重启的情况，然后观察日志，可以看到 Job 重启相关的日志：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[flink-akka.actor.default-dispatcher-5] INFO org.apache.flink.runtime.executiongraph.ExecutionGraph - Try to restart or fail the job zhisheng default RestartStrategy example (a890361aed156610b354813894d02cd0) if no longer possible.</span><br><span class="line">[flink-akka.actor.default-dispatcher-5] INFO org.apache.flink.runtime.executiongraph.ExecutionGraph - Job zhisheng default RestartStrategy example (a890361aed156610b354813894d02cd0) switched from state FAILING to RESTARTING.</span><br><span class="line">[flink-akka.actor.default-dispatcher-5] INFO org.apache.flink.runtime.executiongraph.ExecutionGraph - Restarting the job zhisheng default RestartStrategy example (a890361aed156610b354813894d02cd0).</span><br></pre></td></tr></table></figure><p>最后重启次数达到配置的最大重启次数后 Job 还没有起来的话，则会停止 Job 并打印日志：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[flink-akka.actor.default-dispatcher-2] INFO org.apache.flink.runtime.executiongraph.ExecutionGraph - Could not restart the job zhisheng default RestartStrategy example (a890361aed156610b354813894d02cd0) because the restart strategy prevented it.</span><br></pre></td></tr></table></figure><p>Flink 中几种重启策略的设置如上，大家可以根据需要选择合适的重启策略，比如如果程序抛出了空指针异常，但是你配置的是一直无限重启，那么就会导致 Job 一直在重启，这样无非再浪费机器资源，这种情况下可以配置重试固定次数，每次隔多久重试的固定延时重启策略，这样在重试一定次数后 Job 就会停止，如果对 Job 的状态做了监控告警的话，那么你就会收到告警信息，这样也会提示你去查看 Job 的运行状况，能及时的去发现和修复 Job 的问题。</p><h3 id="RestartStrategy-源码剖析"><a href="#RestartStrategy-源码剖析" class="headerlink" title="RestartStrategy 源码剖析"></a>RestartStrategy 源码剖析</h3><p>再介绍重启策略应用程序代码配置的时候不知道你有没有看到设置重启策略都是使用 RestartStrategies 类，通过该类的方法就可以创建不同的重启策略，在 RestartStrategies 类中提供了五个方法用来创建四种不同的重启策略（有两个方法是创建 FixedDelay 重启策略的，只不过方法的参数不同），如下图所示：</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-08-151745.png" alt="img"></p><p>在每个方法内部其实调用的是 RestartStrategies 中的内部静态类，分别是 NoRestartStrategyConfiguration、FixedDelayRestartStrategyConfiguration、FailureRateRestartStrategyConfiguration、FallbackRestartStrategyConfiguration，这四个类都继承自 RestartStrategyConfiguration 抽象类。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-08-151617.png" alt="img"></p><p>上面是定义的四种重启策略的配置类，在 Flink 中是靠 RestartStrategyResolving 类中的 resolve 方法来解析 RestartStrategies.RestartStrategyConfiguration，然后根据配置使用 RestartStrategyFactory 创建 RestartStrategy。RestartStrategy 是一个接口，它有 canRestart 和 restart 两个方法，它有四个实现类： FixedDelayRestartStrategy、FailureRateRestartStrategy、ThrowingRestartStrategy、NoRestartStrategy。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-08-151311.png" alt="img"></p><h3 id="Failover-Strategies（故障恢复策略）"><a href="#Failover-Strategies（故障恢复策略）" class="headerlink" title="Failover Strategies（故障恢复策略）"></a>Failover Strategies（故障恢复策略）</h3><p>Flink 通过重启策略和故障恢复策略来控制 Task 重启：重启策略决定是否可以重启以及重启的间隔；故障恢复策略决定哪些 Task 需要重启。在 Flink 中支持两种不同的故障重启策略，该策略可以在 flink-conf.yaml 中的配置，默认为：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">jobmanager.execution.failover-strategy:</span> <span class="string">region</span></span><br></pre></td></tr></table></figure><p>该配置有两个可选值，full（重启所有的 Task）和 region（重启 pipelined region），在 Flink 1.9 中默认设置的恢复策略变成 region 了。</p><p>参考 Flink Issue：<a href="https://issues.apache.org/jira/browse/FLINK-13223">https://issues.apache.org/jira/browse/FLINK-13223</a></p><h4 id="重启所有的任务"><a href="#重启所有的任务" class="headerlink" title="重启所有的任务"></a>重启所有的任务</h4><p>在 full 故障恢复策略下，Task 发生故障时会重启作业中的所有 Task 来恢复，会造成一定的资源浪费，但却是恢复作业一致性的最安全策略，会在其他 Failover 策略失败时作为保底策略使用。</p><h4 id="基于-Region-的局部故障重启策略"><a href="#基于-Region-的局部故障重启策略" class="headerlink" title="基于 Region 的局部故障重启策略"></a>基于 Region 的局部故障重启策略</h4><p>基于 Region 的局部故障恢复策略会将作业中的 Task 划分为数个 Region，根据数据传输决定的，有数据传输的 Task 会被放在同一个 Region，不同 Region 之间无数据交换。如果有 Task 发生故障的时候，它会重启发生错误的 Task 所在 Region 的所有 Task，这种策略相对于重启所有的 Task 策略来说重启的 Task 数量会变少。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-08-131936.png" alt="img"></p><p>如上图如果 C2 Task 因为错误挂了，它会根据数据流往上找到 Source，然后根据 Source 可以知道数据流到下游的所有 Task，进而将这些 Task 重启（见下图）。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-08-140828.png" alt="img"></p><p>当然你会发现上面这种重启方式其实重启的 Task 数量还是不少，为了进一步减少需要重新启动的 Task 数量，可以使用某些类型的数据流交换，将 Task 运算的结果暂存在中间，然后如果有 Task 失败了，那么就往前去找中间结果，然后重启中间结果到数据流向的最后 Task 之间所有的 Task。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-08-144622.png" alt="img"></p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-08-144713.png" alt="img"></p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-08-145101.png" alt="img"></p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-08-150015.png" alt="img"></p><p>从上面四个图可以看到这样的话，故障恢复的需要重启的 Task 数量就降低了，但是适合这种的策略的场景是有限的，详情可以参考：</p><blockquote><p><a href="https://cwiki.apache.org/confluence/display/FLINK/FLIP-1+:+Fine+Grained+Recovery+from+Task+Failures">https://cwiki.apache.org/confluence/display/FLINK/FLIP-1+%3A+Fine+Grained+Recovery+from+Task+Failures</a></p></blockquote><p>在查看源码的时候还看到一种恢复策略是 RestartIndividualStrategy，这种策略只会重启挂掉的那个 Task，如果该 Task 没有包含数据源，这会导致它不能重流数据而导致一部分数据丢失，所以这种策略的使用是有局限性的，不能保证数据的一致性。</p>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Flink State 深入理解</title>
      <link href="2019/12/07/Flink-State-%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3/"/>
      <url>2019/12/07/Flink-State-%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p>Flink 是一款有状态的流处理框架</p><h1 id="State-概述"><a href="#State-概述" class="headerlink" title="State 概述"></a>State 概述</h1><h2 id="为什么需要-state？"><a href="#为什么需要-state？" class="headerlink" title="为什么需要 state？"></a>为什么需要 state？</h2><p>对于流处理系统，数据是一条一条被处理的，如果没有对数据处理的进度进行记录，那么如果这个处理数据的 Job 因为机器问题或者其他问题而导致重启，那么它是不知道上一次处理数据是到哪个地方了，这样的情况下如果是批数据，倒是可以很好的解决（重新将这份固定的数据再执行一遍），但是流数据那就麻烦了，你根本不知道什么在 Job 挂的那个时刻数据消费到哪里了？那么你重启的话该从哪里开始重新消费呢？你可以有以下选择（因为你可能也不确定 Job 挂的具体时间）：</p><ol><li>Job 挂的那个时间之前：如果是从 Job 挂之前开始重新消费的话，那么会导致部分数据（从新消费的时间点到之前 Job 挂的那个时间点之前的数据）重复消费</li><li>Job 挂的那个时间之后：如果是从 Job 挂之后开始消费的话，那么会导致部分数据（从 Job 挂的那个时间点到新消费的时间点产生的数据）丢失，没有消费</li></ol><p><img src="/images/flink/1.png"></p><p>为解决上面两种情况（数据重复消费或者数据没有消费）的发生，Flink state 诞生了，state 中存储着每条数据消费后数据的消费点（生产环境需要持久化这些状态）,当 Job 因为某种错误或者其他原因导致重启时，就能够从 checkpoint 中的 state 数据进行恢复</p><h2 id="state-种类"><a href="#state-种类" class="headerlink" title="state 种类"></a>state 种类</h2><p><strong>在 Flink 中有两个基本的 state：Keyed state 和 Operator state</strong></p><h3 id="Keyed-State"><a href="#Keyed-State" class="headerlink" title="Keyed State"></a>Keyed State</h3><p><strong>Keyed State 总是和具体的 key 相关联，也只能在 KeyedStream 的 function 和 operator 上使用。可以将 Keyed State 当作是 Operator State 的一种特例，但是它是被分区或分片的。每个 Keyed State 分区对应一个 key 的 Operator State，对于某个 key 在某个分区上有唯一的状态。</strong></p><h3 id="Operator-State"><a href="#Operator-State" class="headerlink" title="Operator State"></a>Operator State</h3><p>对 Operator State 而言，每个 operator state 都对应着一个并行实例。</p><p>Kafka Connector 就是一个很好的例子。每个 Kafka consumer 的并行实例都会持有一份topic partition 和 offset 的 map，这个 map 就是它的 Operator State。</p><p>当并行度发生变化时，Operator State 可以将状态在所有的并行实例中进行重分配，并且提供了多种方式来进行重分配。</p><p>当并行度发生变化时，Operator State 可以将状态在所有的并行实例中进行重分配，并且提供了多种方式来进行重分配。</p><hr><hr><h3 id="Raw-and-Managed-State"><a href="#Raw-and-Managed-State" class="headerlink" title="Raw and Managed State"></a>Raw and Managed State</h3><p><strong>Keyed State 和 Operator State 都有两种存在形式，即 Raw State（原始状态）和 Managed State（托管状态）。</strong></p><p><strong>Raw State</strong></p><blockquote><p>原始状态是算子保存它们自己的数据结构中的 state，当 checkpoint 时，原始状态会以字节流的形式写入进 checkpoint 中。Flink 并不知道 State 的数据结构长啥样，仅能看到原生的字节数组。</p></blockquote><p><strong>Managed State</strong></p><blockquote><p>托管状态可以使用 Flink 提供的数据结构来表示，例如内部哈希表或者 RocksDB。具体有 ValueState，ListState 等。Flink 会对这些状态进行编码然后将它们写入到 checkpoint 中。</p></blockquote><p>DataStream 的所有 function 都可以使用托管状态，但是原生状态只能在自定义 operator 的时候使用。相对于原生状态，推荐使用托管状态，因为如果使用托管状态，当并行度发生改变时，Flink 可以自动的帮你重分配 state，同时还可以更好的管理内存。</p><p>注意：如果你的托管状态需要特殊的序列化，目前 Flink 还不支持。</p><hr><hr><h3 id="使用托管-Keyed-State"><a href="#使用托管-Keyed-State" class="headerlink" title="使用托管 Keyed State"></a>使用托管 Keyed State</h3><p>托管的 Keyed State 接口提供对不同类型状态（这些状态的范围都是当前输入元素的 key）的访问，这意味着这种状态只能在通过 stream.keyBy() 创建的 KeyedStream 上使用。</p><p>我们首先来看一下有哪些可以使用的状态，然后再来看看它们在程序中是如何使用的：</p><ol><li><p>ValueState</p><p>保存一个可以更新和获取的值（每个 Key 一个 value），可以用 update(T) 来更新 value，可以用 value() 来获取 value。</p></li><li><p>ListState</p><p>保存一个值的列表，用 add(T) 或者 addAll(List) 来添加，用 Iterable get() 来获取。</p></li><li><p>ReducingState</p><p>保存一个值，这个值是状态的很多值的聚合结果，接口和 ListState 类似，但是可以用相应的 ReduceFunction 来聚合。</p></li><li><p>AggregatingState</p><p>保存很多值的聚合结果的单一值，与 ReducingState 相比，不同点在于聚合类型可以和元素类型不同，提供 AggregateFunction 来实现聚合。</p><p>FoldingState</p><p>与 AggregatingState 类似，除了使用 FoldFunction 进行聚合。</p><p><font color='red'>注意：FoldingState 已经不推荐使用，可以用 AggregatingState 来代替。</font></p></li><li><p>MapState: 保存一组映射，可以将 kv 放进这个状态，使用 put(UK, UV) 或者 putAll(Map) 添加，或者使用 get(UK) 获取。</p></li></ol><p>所有类型的状态都有一个 clear() 方法来清除当前的状态。</p><p>需要注意，上面的这些状态对象仅用来和状态打交道，状态不一定保存在内存中，也可以存储在磁盘或者其他地方。另外，你获取到的状态的值是取决于输入元素的 key，因此如果 key 不同，那么在一次调用用户函数中获得的值可能与另一次调用的值不同。</p><p>要使用一个状态对象，需要先创建一个 StateDescriptor，它包含了状态的名字（你可以创建若干个 state，但是它们必须要有唯一的值以便能够引用它们），状态的值的类型，或许还有一个用户定义的函数，比如 ReduceFunction。根据你想要使用的 state 类型，你可以创建 ValueStateDescriptor、ListStateDescriptor、ReducingStateDescriptor、FoldingStateDescriptor 或者 MapStateDescriptor。</p><p>状态只能通过 RuntimeContext 来获取，所以只能在 RichFunction 里面使用。RichFunction 中你可以通过 RuntimeContext 用下述方法获取状态：</p><ul><li>ValueState getState(ValueStateDescriptor)</li><li>ReducingState getReducingState(ReducingStateDescriptor)</li><li>ListState getListState(ListStateDescriptor)</li><li>AggregatingState getAggregatingState(AggregatingState)</li><li>FoldingState getFoldingState(FoldingStateDescriptor)</li><li>MapState getMapState(MapStateDescriptor)</li></ul><p>上面讲了这么多概念，那么来一个例子来看看如何使用状态：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CountWindowAverage</span> <span class="keyword">extends</span> <span class="title">RichFlatMapFunction</span>&lt;<span class="title">Tuple2</span>&lt;<span class="title">Long</span>, <span class="title">Long</span>&gt;, <span class="title">Tuple2</span>&lt;<span class="title">Long</span>, <span class="title">Long</span>&gt;&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//ValueState 使用方式，第一个字段是 count，第二个字段是运行的和 </span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">transient</span> ValueState&lt;Tuple2&lt;Long, Long&gt;&gt; sum;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">flatMap</span><span class="params">(Tuple2&lt;Long, Long&gt; input, Collector&lt;Tuple2&lt;Long, Long&gt;&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//访问状态的 value 值</span></span><br><span class="line">        Tuple2&lt;Long, Long&gt; currentSum = sum.value();</span><br><span class="line"></span><br><span class="line">        <span class="comment">//更新 count</span></span><br><span class="line">        currentSum.f0 += <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//更新 sum</span></span><br><span class="line">        currentSum.f1 += input.f1;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//更新状态</span></span><br><span class="line">        sum.update(currentSum);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//如果 count 等于 2, 发出平均值并清除状态</span></span><br><span class="line">        <span class="keyword">if</span> (currentSum.f0 &gt;= <span class="number">2</span>) &#123;</span><br><span class="line">            out.collect(<span class="keyword">new</span> Tuple2&lt;&gt;(input.f0, currentSum.f1 / currentSum.f0));</span><br><span class="line">            sum.clear();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(Configuration config)</span> </span>&#123;</span><br><span class="line">        ValueStateDescriptor&lt;Tuple2&lt;Long, Long&gt;&gt; descriptor =</span><br><span class="line">                <span class="keyword">new</span> ValueStateDescriptor&lt;&gt;(</span><br><span class="line">                        <span class="string">&quot;average&quot;</span>, <span class="comment">//状态名称</span></span><br><span class="line">                        TypeInformation.of(<span class="keyword">new</span> TypeHint&lt;Tuple2&lt;Long, Long&gt;&gt;() &#123;&#125;), <span class="comment">//类型信息</span></span><br><span class="line">                        Tuple2.of(<span class="number">0L</span>, <span class="number">0L</span>)); <span class="comment">//状态的默认值</span></span><br><span class="line">        sum = getRuntimeContext().getState(descriptor);<span class="comment">//获取状态</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">env.fromElements(Tuple2.of(<span class="number">1L</span>, <span class="number">3L</span>), Tuple2.of(<span class="number">1L</span>, <span class="number">5L</span>), Tuple2.of(<span class="number">1L</span>, <span class="number">7L</span>), Tuple2.of(<span class="number">1L</span>, <span class="number">4L</span>), Tuple2.of(<span class="number">1L</span>, <span class="number">2L</span>))</span><br><span class="line">        .keyBy(<span class="number">0</span>)</span><br><span class="line">        .flatMap(<span class="keyword">new</span> CountWindowAverage())</span><br><span class="line">        .print();</span><br><span class="line"></span><br><span class="line"><span class="comment">//结果会打印出 (1,4) 和 (1,5)</span></span><br></pre></td></tr></table></figure><p>这个例子实现了一个简单的计数器，我们使用元组的第一个字段来进行分组(这个例子中，所有的 key 都是 1)，这个 CountWindowAverage 函数将计数和运行时总和保存在一个 ValueState 中，一旦计数等于 2，就会发出平均值并清理 state，因此又从 0 开始。请注意，如果在第一个字段中具有不同值的元组，则这将为每个不同的输入 key保存不同的 state 值。</p><h3 id="State-TTL-存活时间"><a href="#State-TTL-存活时间" class="headerlink" title="State TTL(存活时间)"></a>State TTL(存活时间)</h3><h4 id="State-TTL-介绍"><a href="#State-TTL-介绍" class="headerlink" title="State TTL 介绍"></a>State TTL 介绍</h4><p>TTL 可以分配给任何类型的 Keyed state，如果一个状态设置了 TTL，那么当状态过期时，那么之前存储的状态值会被清除。所有的状态集合类型都支持单个入口的 TTL，这意味着 List 集合元素和 Map 集合都支持独立到期。为了使用状态 TTL，首先必须要构建 StateTtlConfig 配置对象，然后可以通过传递配置在 State descriptor 中启用 TTL 功能：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.state.StateTtlConfig;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.state.ValueStateDescriptor;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.time.Time;</span><br><span class="line"></span><br><span class="line">StateTtlConfig ttlConfig = StateTtlConfig</span><br><span class="line">    .newBuilder(Time.seconds(<span class="number">1</span>))</span><br><span class="line">    .setUpdateType(StateTtlConfig.UpdateType.OnCreateAndWrite)</span><br><span class="line">    .setStateVisibility(StateTtlConfig.StateVisibility.NeverReturnExpired)</span><br><span class="line">    .build();</span><br><span class="line"></span><br><span class="line">ValueStateDescriptor&lt;String&gt; stateDescriptor = <span class="keyword">new</span> ValueStateDescriptor&lt;&gt;(<span class="string">&quot;zhisheng&quot;</span>, String.class);</span><br><span class="line">stateDescriptor.enableTimeToLive(ttlConfig);    <span class="comment">//开启 ttl</span></span><br></pre></td></tr></table></figure><p>上面配置中有几个选项需要注意：</p><p>1、newBuilder 方法的第一个参数是必需的，它代表着状态存活时间。</p><p>2、UpdateType 配置状态 TTL 更新时（默认为 OnCreateAndWrite）：</p><ul><li>StateTtlConfig.UpdateType.OnCreateAndWrite: 仅限创建和写入访问时更新</li><li>StateTtlConfig.UpdateType.OnReadAndWrite: 除了创建和写入访问，还支持在读取时更新</li></ul><p>3、StateVisibility 配置是否在读取访问时返回过期值（如果尚未清除），默认是 NeverReturnExpired：</p><ul><li>StateTtlConfig.StateVisibility.NeverReturnExpired: 永远不会返回过期值</li><li>StateTtlConfig.StateVisibility.ReturnExpiredIfNotCleanedUp: 如果仍然可用则返回</li></ul><p>在 NeverReturnExpired 的情况下，过期状态表现得好像它不再存在，即使它仍然必须被删除。该选项对于在 TTL 之后必须严格用于读取访问的数据的用例是有用的，例如，应用程序使用隐私敏感数据.</p><p>另一个选项 ReturnExpiredIfNotCleanedUp 允许在清理之前返回过期状态。</p><p>注意：</p><ul><li>状态后端会存储上次修改的时间戳以及对应的值，这意味着启用此功能会增加状态存储的消耗，堆状态后端存储一个额外的 Java 对象，其中包含对用户状态对象的引用和内存中原始的 long 值。RocksDB 状态后端存储为每个存储值、List、Map 都添加 8 个字节。</li><li>目前仅支持参考 processing time 的 TTL</li><li>使用启用 TTL 的描述符去尝试恢复先前未使用 TTL 配置的状态可能会导致兼容性失败或者 StateMigrationException 异常。</li><li>TTL 配置并不是 Checkpoint 和 Savepoint 的一部分，而是 Flink 如何在当前运行的 Job 中处理它的方式。</li><li>只有当用户值序列化器可以处理 null 值时，具体 TTL 的 Map 状态当前才支持 null 值，如果序列化器不支持 null 值，则可以使用 NullableSerializer 来包装它（代价是需要一个额外的字节）。</li></ul><h4 id="清除过期-state"><a href="#清除过期-state" class="headerlink" title="清除过期 state"></a>清除过期 state</h4><p>默认情况下，过期值只有在显式读出时才会被删除，例如通过调用 ValueState.value()。</p><p>注意：这意味着默认情况下，如果未读取过期状态，则不会删除它，这可能导致状态不断增长，这个特性在 Flink 未来的版本可能会发生变化。</p><p>此外，你可以在获取完整状态快照时激活清理状态，这样就可以减少状态的大小。在当前实现下不清除本地状态，但是在从上一个快照恢复的情况下，它不会包括已删除的过期状态，你可以在 StateTtlConfig 中这样配置：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.state.StateTtlConfig;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.time.Time;</span><br><span class="line"></span><br><span class="line">StateTtlConfig ttlConfig = StateTtlConfig</span><br><span class="line">    .newBuilder(Time.seconds(<span class="number">1</span>))</span><br><span class="line">    .cleanupFullSnapshot()</span><br><span class="line">    .build();</span><br></pre></td></tr></table></figure><p>此配置不适用于 RocksDB 状态后端中的增量 checkpoint。对于现有的 Job，可以在 StateTtlConfig 中随时激活或停用此清理策略，例如，从保存点重启后。</p><p>除了在完整快照中清理外，你还可以在后台激活清理。如果使用的后端支持以下选项，则会激活 StateTtlConfig 中的默认后台清理：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.state.StateTtlConfig;</span><br><span class="line">StateTtlConfig ttlConfig = StateTtlConfig</span><br><span class="line">    .newBuilder(Time.seconds(<span class="number">1</span>))</span><br><span class="line">    .cleanupInBackground()</span><br><span class="line">    .build();</span><br></pre></td></tr></table></figure><p>要在后台对某些特殊清理进行更精细的控制，可以按照下面的说明单独配置它。目前，堆状态后端依赖于增量清理，RocksDB 后端使用压缩过滤器进行后台清理。</p><p>我们再来看看 TTL 对应着的类 StateTtlConfig 类中的具体实现，这样我们才能更加的理解其使用方式。</p><p>在该类中的属性有如下：</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-23-143816.png" alt="img"></p><ul><li>DISABLED：它默认创建了一个 UpdateType 为 Disabled 的 StateTtlConfig</li><li>UpdateType：这个是一个枚举，包含 Disabled（代表 TTL 是禁用的，状态不会过期）、OnCreateAndWrite、OnReadAndWrite 可选</li><li>StateVisibility：这也是一个枚举，包含了 ReturnExpiredIfNotCleanedUp、NeverReturnExpired</li><li>TimeCharacteristic：这是时间特征，其实是只有 ProcessingTime 可选</li><li>Time：设置 TTL 的时间，这里有两个参数 unit 和 size</li><li>CleanupStrategies：TTL 清理策略，在该类中又有字段 isCleanupInBackground（是否在后台清理） 和相关的清理 strategies（包含 FULL<em>STATE</em>SCAN<em>SNAPSHOT、INCREMENTAL</em>CLEANUP 和 ROCKSDB<em>COMPACTION</em>FILTER），同时该类中还有 CleanupStrategy 接口，它的实现类有 EmptyCleanupStrategy（不清理，为空）、IncrementalCleanupStrategy（增量的清除）、RocksdbCompactFilterCleanupStrategy（在 RocksDB 中自定义压缩过滤器）。</li></ul><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-23-144111.png" alt="img"></p><h3 id="如何使用托管-Operator-State"><a href="#如何使用托管-Operator-State" class="headerlink" title="如何使用托管 Operator State"></a>如何使用托管 Operator State</h3><p>为了使用托管的 Operator State，必须要有一个有状态的函数，这个函数可以实现 CheckpointedFunction 或者 ListCheckpointed 接口。</p><p>下面分别讲一下如何使用：</p><h4 id="CheckpointedFunction"><a href="#CheckpointedFunction" class="headerlink" title="CheckpointedFunction"></a>CheckpointedFunction</h4><p>如果是实现 CheckpointedFunction 接口的话，那么我们先来看下这个接口里面有什么方法呢：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//当请求 checkpoint 快照时，将调用此方法</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">snapshotState</span><span class="params">(FunctionSnapshotContext context)</span> <span class="keyword">throws</span> Exception</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">//在分布式执行期间创建并行功能实例时，将调用此方法。 函数通常在此方法中设置其状态存储数据结构</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">initializeState</span><span class="params">(FunctionInitializationContext context)</span> <span class="keyword">throws</span> Exception</span>;</span><br></pre></td></tr></table></figure><p>当有请求执行 checkpoint 的时候，snapshotState() 方法就会被调用，initializeState() 方法会在每次初始化用户定义的函数时或者从更早的 checkpoint 恢复的时候被调用，因此 initializeState() 不仅是不同类型的状态被初始化的地方，而且还是 state 恢复逻辑的地方。</p><p>目前，List 类型的托管状态是支持的，状态被期望是一个可序列化的对象的 List，彼此独立，这样便于重分配，换句话说，这些对象是可以重新分配的 non-keyed state 的最小粒度，根据状态的访问方法，定义了重新分配的方案：</p><ul><li>Even-split redistribution：每个算子会返回一个状态元素列表，整个状态在逻辑上是所有列表的连接。在重新分配或者恢复的时候，这个状态元素列表会被按照并行度分为子列表，每个算子会得到一个子列表。这个子列表可能为空，或包含一个或多个元素。举个例子，如果使用并行性 1，算子的检查点状态包含元素 element1 和 element2，当将并行性增加到 2 时，element1 可能最终在算子实例 0 中，而 element2 将转到算子实例 1 中。</li><li>Union redistribution：每个算子会返回一个状态元素列表，整个状态在逻辑上是所有列表的连接。在重新分配或恢复的时候，每个算子都会获得完整的状态元素列表。</li></ul><p>如下示例是一个有状态的 SinkFunction 使用 CheckpointedFunction 来发送到外部之前缓存数据，使用了Even-split策略。</p><p>下面是一个有状态的 SinkFunction 的示例，它使用 CheckpointedFunction 来缓存数据，然后再将这些数据发送到外部系统，使用了 Even-split 策略：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">BufferingSink</span> <span class="keyword">implements</span> <span class="title">SinkFunction</span>&lt;<span class="title">Tuple2</span>&lt;<span class="title">String</span>, <span class="title">Integer</span>&gt;&gt;, <span class="title">CheckpointedFunction</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">int</span> threshold;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">transient</span> ListState&lt;Tuple2&lt;String, Integer&gt;&gt; checkpointedState;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> List&lt;Tuple2&lt;String, Integer&gt;&gt; bufferedElements;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">BufferingSink</span><span class="params">(<span class="keyword">int</span> threshold)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.threshold = threshold;</span><br><span class="line">        <span class="keyword">this</span>.bufferedElements = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">invoke</span><span class="params">(Tuple2&lt;String, Integer&gt; value, Context contex)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        bufferedElements.add(value);</span><br><span class="line">        <span class="keyword">if</span> (bufferedElements.size() == threshold) &#123;</span><br><span class="line">            <span class="keyword">for</span> (Tuple2&lt;String, Integer&gt; element: bufferedElements) &#123;</span><br><span class="line">                <span class="comment">//将数据发到外部系统</span></span><br><span class="line">            &#125;</span><br><span class="line">            bufferedElements.clear();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">snapshotState</span><span class="params">(FunctionSnapshotContext context)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        checkpointedState.clear();</span><br><span class="line">        <span class="keyword">for</span> (Tuple2&lt;String, Integer&gt; element : bufferedElements) &#123;</span><br><span class="line">            checkpointedState.add(element);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">initializeState</span><span class="params">(FunctionInitializationContext context)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        ListStateDescriptor&lt;Tuple2&lt;String, Integer&gt;&gt; descriptor =</span><br><span class="line">            <span class="keyword">new</span> ListStateDescriptor&lt;&gt;(</span><br><span class="line">                <span class="string">&quot;buffered-elements&quot;</span>,</span><br><span class="line">                TypeInformation.of(<span class="keyword">new</span> TypeHint&lt;Tuple2&lt;String, Integer&gt;&gt;() &#123;&#125;));</span><br><span class="line"></span><br><span class="line">        checkpointedState = context.getOperatorStateStore().getListState(descriptor);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (context.isRestored()) &#123;</span><br><span class="line">            <span class="keyword">for</span> (Tuple2&lt;String, Integer&gt; element : checkpointedState.get()) &#123;</span><br><span class="line">                bufferedElements.add(element);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>initializeState 方法将 FunctionInitializationContext 作为参数，它用来初始化 non-keyed 状态。注意状态是如何初始化的，类似于 Keyed state，StateDescriptor 包含状态名称和有关状态值的类型的信息：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">ListStateDescriptor&lt;Tuple2&lt;String, Integer&gt;&gt; descriptor =</span><br><span class="line">    <span class="keyword">new</span> ListStateDescriptor&lt;&gt;(</span><br><span class="line">        <span class="string">&quot;buffered-elements&quot;</span>,</span><br><span class="line">        TypeInformation.of(<span class="keyword">new</span> TypeHint&lt;Tuple2&lt;Long, Long&gt;&gt;() &#123;&#125;));</span><br><span class="line"></span><br><span class="line">checkpointedState = context.getOperatorStateStore().getListState(descriptor);</span><br></pre></td></tr></table></figure><h4 id="ListCheckpointed"><a href="#ListCheckpointed" class="headerlink" title="ListCheckpointed"></a>ListCheckpointed</h4><p>是一种受限的 CheckpointedFunction，只支持 List 风格的状态和 even-spit 的重分配策略。该接口里面的方法有：</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-23-144503.png" alt="img"></p><ul><li>snapshotState(): 获取函数的当前状态。状态必须返回此函数先前所有的调用结果。</li><li>restoreState(): 将函数或算子的状态恢复到先前 checkpoint 的状态。此方法在故障恢复后执行函数时调用。如果函数的特定并行实例无法恢复到任何状态，则状态列表可能为空。</li></ul><h3 id="Stateful-Source-Functions"><a href="#Stateful-Source-Functions" class="headerlink" title="Stateful Source Functions"></a>Stateful Source Functions</h3><p>与其他算子相比，有状态的 source 函数需要注意的地方更多，比如为了保证状态的更新和结果的输出原子性，用户必须在 source 的 context 上加锁。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">CounterSource</span> <span class="keyword">extends</span> <span class="title">RichParallelSourceFunction</span>&lt;<span class="title">Long</span>&gt; <span class="keyword">implements</span> <span class="title">ListCheckpointed</span>&lt;<span class="title">Long</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//一次语义的当前偏移量</span></span><br><span class="line">    <span class="keyword">private</span> Long offset = <span class="number">0L</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//作业取消标志</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">volatile</span> <span class="keyword">boolean</span> isRunning = <span class="keyword">true</span>;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">(SourceContext&lt;Long&gt; ctx)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">final</span> Object lock = ctx.getCheckpointLock();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> (isRunning) &#123;</span><br><span class="line">            <span class="comment">//输出和状态更新是原子性的</span></span><br><span class="line">            <span class="keyword">synchronized</span> (lock) &#123;</span><br><span class="line">                ctx.collect(offset);</span><br><span class="line">                offset += <span class="number">1</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">cancel</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        isRunning = <span class="keyword">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> List&lt;Long&gt; <span class="title">snapshotState</span><span class="params">(<span class="keyword">long</span> checkpointId, <span class="keyword">long</span> checkpointTimestamp)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> Collections.singletonList(offset);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">restoreState</span><span class="params">(List&lt;Long&gt; state)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">for</span> (Long s : state)</span><br><span class="line">            offset = s;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>或许有些算子想知道什么时候 checkpoint 全部做完了，可以参考使用 org.apache.flink.runtime.state.CheckpointListener 接口来实现，在该接口里面有 notifyCheckpointComplete 方法。</p><hr><h3 id="Broadcast-State"><a href="#Broadcast-State" class="headerlink" title="Broadcast State"></a>Broadcast State</h3><h4 id="Broadcast-State-如何使用"><a href="#Broadcast-State-如何使用" class="headerlink" title="Broadcast State 如何使用"></a>Broadcast State 如何使用</h4><p>前面提到了两种 Operator state 支持的动态扩展方法：even-split redistribution 和 union redistribution。Broadcast State 是 Flink 支持的另一种扩展方式，它用来支持将某一个流的数据广播到下游所有的 Task 中，数据都会存储在下游 Task 内存中，接收到广播的数据流后就可以在操作中利用这些数据，一般我们会将一些规则数据进行这样广播下去，然后其他的 Task 也都能根据这些规则数据做配置，更常见的就是规则动态的更新，然后下游还能够动态的感知。</p><p>Broadcast state 的特点是：</p><ul><li>使用 Map 类型的数据结构</li><li>仅适用于同时具有广播流和非广播流作为数据输入的特定算子</li><li>可以具有多个不同名称的 Broadcast state</li></ul><p>那么我们该如何使用 Broadcast State 呢？下面通过一个例子来讲解一下，在这个例子中，我要广播的数据是监控告警的通知策略规则，然后下游拿到我这个告警通知策略去判断哪种类型的告警发到哪里去，该使用哪种方式来发，静默时间多长等。</p><p>第一个数据流是要处理的数据源，流中的对象具有告警或者恢复的事件，其中用一个 type 字段来标识哪个事件是告警，哪个事件是恢复，然后还有其他的字段标明是哪个集群的或者哪个项目的，简单代码如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">DataStreamSource&lt;AlertEvent&gt; alertData = env.addSource(<span class="keyword">new</span> FlinkKafkaConsumer011&lt;&gt;(<span class="string">&quot;alert&quot;</span>,</span><br><span class="line">        <span class="keyword">new</span> AlertEventSchema(),</span><br><span class="line">        parameterTool.getProperties()));</span><br></pre></td></tr></table></figure><p>然后第二个数据流是要广播的数据流，它是告警通知策略数据（定时从 MySQL 中读取的规则表），简单代码如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">DataStreamSource&lt;Rule&gt; alarmdata = env.addSource(<span class="keyword">new</span> GetAlarmNotifyData());</span><br><span class="line"></span><br><span class="line"><span class="comment">// MapState 中保存 (RuleName, Rule) ，在描述类中指定 State name</span></span><br><span class="line">MapStateDescriptor&lt;String, Rule&gt; ruleStateDescriptor = <span class="keyword">new</span> MapStateDescriptor&lt;&gt;(</span><br><span class="line">            <span class="string">&quot;RulesBroadcastState&quot;</span>,</span><br><span class="line">            BasicTypeInfo.STRING_TYPE_INFO,</span><br><span class="line">            TypeInformation.of(<span class="keyword">new</span> TypeHint&lt;Rule&gt;() &#123;&#125;));</span><br><span class="line"></span><br><span class="line"><span class="comment">// alarmdata 使用 MapStateDescriptor 作为参数广播，得到广播流</span></span><br><span class="line">BroadcastStream&lt;Rule&gt; ruleBroadcastStream = alarmdata.broadcast(ruleStateDescriptor);</span><br></pre></td></tr></table></figure><p>然后你要做的是将两个数据流进行连接，连接后再根据告警规则数据流的规则数据进行处理（这个告警的逻辑很复杂，我们这里就不再深入讲），伪代码大概如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">alertData.connect(ruleBroadcastStream)</span><br><span class="line">    .process(</span><br><span class="line">        <span class="keyword">new</span> KeyedBroadcastProcessFunction&lt;AlertEvent, Rule&gt;() &#123;</span><br><span class="line">            <span class="comment">//根据告警规则的数据进行处理告警事件</span></span><br><span class="line">        &#125;</span><br><span class="line">    )</span><br><span class="line">    <span class="comment">//可能还有更多的操作</span></span><br></pre></td></tr></table></figure><p><code>alertData.connect(ruleBroadcastStream)</code> 该 connect 方法将两个流连接起来后返回一个 BroadcastConnectedStream 对象，如果对 BroadcastConnectedStream 不太清楚的可以回看下文章 <a href="https://gitbook.cn/gitchat/column/undefined/topic/5db6a754f6a6211cb9616526">4如何使用 DataStream API 来处理数据？</a> 再次复习一下。BroadcastConnectedStream 调用 process() 方法执行处理逻辑，需要指定一个逻辑实现类作为参数，具体是哪种实现类取决于非广播流的类型：</p><ul><li>如果非广播流是 keyed stream，需要实现 KeyedBroadcastProcessFunction</li><li>如果非广播流是 non-keyed stream，需要实现 BroadcastProcessFunction</li></ul><p>那么该怎么获取这个 Broadcast state 呢，它需要通过上下文来获取:</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ctx.getBroadcastState(ruleStateDescriptor)</span><br></pre></td></tr></table></figure><h4 id="BroadcastProcessFunction-和-KeyedBroadcastProcessFunction"><a href="#BroadcastProcessFunction-和-KeyedBroadcastProcessFunction" class="headerlink" title="BroadcastProcessFunction 和 KeyedBroadcastProcessFunction"></a>BroadcastProcessFunction 和 KeyedBroadcastProcessFunction</h4><p>这两个抽象函数有两个相同的需要实现的接口:</p><ul><li>processBroadcastElement()：处理广播流中接收的数据元</li><li>processElement()：处理非广播流数据的方法</li></ul><p>用于处理非广播流是 non-keyed stream 的情况:</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">BroadcastProcessFunction</span>&lt;<span class="title">IN1</span>, <span class="title">IN2</span>, <span class="title">OUT</span>&gt; <span class="keyword">extends</span> <span class="title">BaseBroadcastProcessFunction</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="keyword">void</span> <span class="title">processElement</span><span class="params">(IN1 value, ReadOnlyContext ctx, Collector&lt;OUT&gt; out)</span> <span class="keyword">throws</span> Exception</span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="keyword">void</span> <span class="title">processBroadcastElement</span><span class="params">(IN2 value, Context ctx, Collector&lt;OUT&gt; out)</span> <span class="keyword">throws</span> Exception</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>用于处理非广播流是 keyed stream 的情况</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">KeyedBroadcastProcessFunction</span>&lt;<span class="title">KS</span>, <span class="title">IN1</span>, <span class="title">IN2</span>, <span class="title">OUT</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="keyword">void</span> <span class="title">processElement</span><span class="params">(IN1 value, ReadOnlyContext ctx, Collector&lt;OUT&gt; out)</span> <span class="keyword">throws</span> Exception</span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="keyword">void</span> <span class="title">processBroadcastElement</span><span class="params">(IN2 value, Context ctx, Collector&lt;OUT&gt; out)</span> <span class="keyword">throws</span> Exception</span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onTimer</span><span class="params">(<span class="keyword">long</span> timestamp, OnTimerContext ctx, Collector&lt;OUT&gt; out)</span> <span class="keyword">throws</span> Exception</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>可以看到这两个接口提供的上下文对象有所不同。非广播方（processElement）使用 ReadOnlyContext，而广播方（processBroadcastElement）使用 Context。这两个上下文对象（简称 ctx）通用的方法接口有：</p><ul><li>访问 Broadcast state：ctx.getBroadcastState(MapStateDescriptorstateDescriptor)</li><li>查询数据元的时间戳：ctx.timestamp()</li><li>获取当前水印：ctx.currentWatermark()</li><li>获取当前处理时间：ctx.currentProcessingTime()</li><li>向旁侧输出（side-outputs）发送数据：ctx.output(OutputTag outputTag, X value)</li></ul><p>这两者不同之处在于对 Broadcast state 的访问限制：广播方对其具有读和写的权限（read-write），非广播方只有读的权限（read-only），为什么要这么设计呢，主要是为了保证 Broadcast state 在算子的所有并行实例中是相同的。由于 Flink 中没有跨任务的通信机制，在一个任务实例中的修改不能在并行任务间传递，而广播端在所有并行任务中都能看到相同的数据元，只对广播端提供可写的权限。同时要求在广播端的每个并行任务中，对接收数据的处理是相同的。如果忽略此规则会破坏 State 的一致性保证，从而导致不一致且难以诊断的结果。也就是说，processBroadcast() 的实现逻辑必须在所有并行实例中具有相同的确定性行为。</p><h4 id="使用-Broadcast-state-需要注意"><a href="#使用-Broadcast-state-需要注意" class="headerlink" title="使用 Broadcast state 需要注意"></a>使用 Broadcast state 需要注意</h4><p>前面介绍了 Broadcast state，并将 BroadcastProcessFunction 和 KeyedBroadcastProcessFunction 做了个对比，那么接下来强调一下使用 Broadcast state 时需要注意的事项：</p><ul><li>没有跨任务的通信，这就是为什么只有广播方可以修改 Broadcast state 的原因。</li><li>用户必须确保所有任务以相同的方式为每个传入的数据元更新 Broadcast state，否则可能导致结果不一致。</li><li>跨任务的 Broadcast state 中的事件顺序可能不同，虽然广播的元素可以保证所有元素都将转到所有下游任务，但元素到达的顺序可能不一致。因此，Broadcast state 更新不能依赖于传入事件的顺序。</li><li>所有任务都会把 Broadcast state 存入 checkpoint，虽然 checkpoint 发生时所有任务都具有相同的 Broadcast state。这是为了避免在恢复期间所有任务从同一文件中进行恢复（避免热点），然而代价是 state 在 checkpoint 时的大小成倍数（并行度数量）增加。</li><li>Flink 确保在恢复或改变并行度时不会有重复数据，也不会丢失数据。在具有相同或改小并行度后恢复的情况下，每个任务读取其状态 checkpoint。在并行度增大时，原先的每个任务都会读取自己的状态，新增的任务以循环方式读取前面任务的检查点。</li><li>不支持 RocksDB state backend，Broadcast state 在运行时保存在内存中。</li></ul><h3 id="Queryable-State"><a href="#Queryable-State" class="headerlink" title="Queryable State"></a>Queryable State</h3><p>Queryable State，顾名思义，就是可查询的状态。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-06-29-075631.jpg" alt="img"></p><p>传统管理这些状态的方式是通过将计算后的状态结果存储在第三方 KV 存储中，然后由第三方应用去获取这些 KV 状态，但是在 Flink 种，现在有了 Queryable State，意味着允许用户对流的内部状态进行实时查询。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-06-29-091521.jpg" alt="img"></p><p>那么就不再像其他流计算框架，需要将结果存储到其他外部存储系统才能够被查询到，这样我们就可以不再需要等待状态写入外部存储（这块可能是其他系统的主要瓶颈之一），甚至可以做到无需任何数据库就可以让用户直接查询到数据，这使得数据获取到的时间会更短，更及时，如果你有这块的需求（需要将某些状态数据进行展示，比如数字大屏），那么就强烈推荐使用 Queryable State。目前可查询的 state 主要针对可分区的 state，如 keyed state 等。</p><p>在 Flink 源码中，为此还专门有一个 module 来讲 Queryable State 呢！</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-10-23-144649.png" alt="img"></p><p>那么我们该如何使用 Queryable State 呢？有如下两种方式 ：</p><ul><li>QueryableStateStream, 将 KeyedStream 转换为 QueryableStateStream，类似于 Sink，后续不能进行任何转换操作</li><li>StateDescriptor#setQueryable(String queryableStateName)，将 Keyed State 设置为可查询的 （不支持 Operator State）</li></ul><p>外部应用在查询 Flink 应用程序内部状态的时候要使用 QueryableStateClient, 提交异步查询请求来获取状态。如何使状态可查询呢，假如已经创建了一个状态可查询的 Job，并通过 JobClient 提交 Job，那么它在 Flink 内部的具体实现如下图（图片来自 <a href="http://vishnuviswanath.com/flink_queryable_state1.html">Queryable States in ApacheFlink - How it works</a>）所示：</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-06-29-073842.jpg" alt="img"></p><p>上面讲解了让 State 可查询的原理，如果要在 Flink 集群中使用的话，首先得将 Flink 安装目录下 opt 里面的 <code>flink-queryable-state-runtime_2.11-1.9.0.jar</code> 复制到 lib 目录下，默认 lib 目录是不包含这个 jar 的。</p><p>然后你可以像下面这样操作让状态可查询：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Reducing state</span></span><br><span class="line">ReducingStateDescriptor&lt;Tuple2&lt;Integer, Long&gt;&gt; reducingState = <span class="keyword">new</span> ReducingStateDescriptor&lt;&gt;(</span><br><span class="line">        <span class="string">&quot;zhisheng&quot;</span>,</span><br><span class="line">        <span class="keyword">new</span> SumReduce(),</span><br><span class="line">        source.getType());</span><br><span class="line"></span><br><span class="line"><span class="keyword">final</span> String queryName = <span class="string">&quot;zhisheng&quot;</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">final</span> QueryableStateStream&lt;Integer, Tuple2&lt;Integer, Long&gt;&gt; queryableState =</span><br><span class="line">        dataStream.keyBy(<span class="keyword">new</span> KeySelector&lt;Tuple2&lt;Integer, Long&gt;, Integer&gt;() &#123;</span><br><span class="line">            <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = -<span class="number">4126824763829132959L</span>;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> Integer <span class="title">getKey</span><span class="params">(Tuple2&lt;Integer, Long&gt; value)</span> </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> value.f0;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;).asQueryableState(queryName, reducingState);</span><br></pre></td></tr></table></figure><p>除了上面的 Reducing，你还可以使用 ValueState、FoldingState，还可以直接通过asQueryableState(queryName），注意不支持 ListState，调用 asQueryableState 方法后会返回 QueryableStateStream，接着无需再做其他操作。</p><p>那么用户如果定义了 Queryable State 的话，该怎么来查询对应的状态呢？下面来看看具体逻辑：</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-06-29-074814.jpg" alt="img"></p><p>简单来说，当用户在 Job 中定义了 queryable state 之后，就可以在外部通过QueryableStateClient 来查询对应的状态实时值，你可以创建如下方法：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//创建 Queryable State Client</span></span><br><span class="line">QueryableStateClient client = <span class="keyword">new</span> QueryableStateClient(host, port);</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">QueryableStateClient</span><span class="params">(<span class="keyword">final</span> InetAddress remoteAddress, <span class="keyword">final</span> <span class="keyword">int</span> remotePort)</span> </span>&#123;</span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">this</span>.client = <span class="keyword">new</span> Client&lt;&gt;(</span><br><span class="line">            <span class="string">&quot;Queryable State Client&quot;</span>, <span class="number">1</span>,</span><br><span class="line">            messageSerializer, <span class="keyword">new</span> DisabledKvStateRequestStats());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在 QueryableStateClient 中有几个不同参数的 getKvState 方法，参数可有 JobID、queryableStateName、key、namespace、keyTypeInfo、namespaceTypeInfo、StateDescriptor，其实内部最后调用的是一个私有的 getKvState 方法：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> CompletableFuture&lt;KvStateResponse&gt; <span class="title">getKvState</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">final</span> JobID jobId, <span class="keyword">final</span> String queryableStateName,</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">final</span> <span class="keyword">int</span> keyHashCode, <span class="keyword">final</span> <span class="keyword">byte</span>[] serializedKeyAndNamespace)</span> </span>&#123;</span><br><span class="line">    ...</span><br><span class="line">    <span class="comment">//构造 KV state 查询的请求</span></span><br><span class="line">    KvStateRequest request = <span class="keyword">new</span> KvStateRequest(jobId, queryableStateName, keyHashCode, serializedKeyAndNamespace);</span><br><span class="line">    <span class="comment">//这个 client 是在构造 QueryableStateClient 中赋值的，这个 client 是 Client&lt;KvStateRequest, KvStateResponse&gt;，发送请求后会返回 CompletableFuture&lt;KvStateResponse&gt;</span></span><br><span class="line">    <span class="keyword">return</span> client.sendRequest(remoteAddress, request);</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在 Flink 源码中专门有一个 QueryableStateOptions 类来设置可查询状态相关的配置，有如下这些配置。</p><p>服务器端：</p><ul><li>queryable-state.proxy.ports：可查询状态代理的服务器端口范围的配置参数，默认是 9069</li><li>queryable-state.proxy.network-threads：客户端代理的网络线程数，默认是 0</li><li>queryable-state.proxy.query-threads：客户端代理的异步查询线程数，默认是 0</li><li>queryable-state.server.ports：可查询状态服务器的端口范围，默认是 9067</li><li>queryable-state.server.network-threads：KvState 服务器的网络线程数</li><li>queryable-state.server.query-threads：KvStateServerHandler 的异步查询线程数</li><li>queryable-state.enable：是否启用可查询状态代理和服务器</li></ul><p>客户端：</p><ul><li>queryable-state.client.network-threads：KvState 客户端的网络线程数</li></ul><p><strong>注意</strong>：</p><p>可查询状态的生命周期受限于 Job 的生命周期，例如，任务在启动时注册可查询状态，在清理的时候会注销它。在未来的版本中，可能会将其解耦，以便在任务完成后仍可以允许查询到任务的状态。</p>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Spark 任务调度机制</title>
      <link href="2019/12/02/Spark%20%E4%BB%BB%E5%8A%A1%E8%B0%83%E5%BA%A6%E6%9C%BA%E5%88%B6/"/>
      <url>2019/12/02/Spark%20%E4%BB%BB%E5%8A%A1%E8%B0%83%E5%BA%A6%E6%9C%BA%E5%88%B6/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p>在生产环境下， Spark 集群的部署方式一般为 YARN-Cluster 模式，因此本文基于YARN-Cluster 模式</p><a id="more"></a><h1 id="Spark-任务提交流程"><a href="#Spark-任务提交流程" class="headerlink" title="Spark 任务提交流程"></a><strong>Spark 任务提交流程</strong></h1><p>YARN-Cluster 模式中提交 Spark 应用程序</p><p>首先通过 Client 向 ResourceManager 请求启动一个 Application，同时检查是否有足够的资源满足 Application 的需求，如果资源条件满足，则准备 ApplicationMaster 的启动上下文，交给ResourceManager，并循环监控 Application 状态。</p><p>当提交的资源队列中有资源时，ResourceManager 会在某个 NodeManager 上启动 ApplicationMaster 进程，ApplicationMaster 会单独启动 Driver 后台线程，当 Driver 启动后，ApplicationMaster 会通过本地的 RPC 连接 Driver ，并开始向 ResourceManager 申请 Container 资源运行 Executor 进程, 当 ResourceManager 返回 Container 资源，ApplicationMaster 则在对应的 Container 上启动 Executor 。</p><p>Driver 线程主要是初始化 SparkContext 对象，准备运行所需的上下文，然后一方面保持与 ApplicationMaster 的 RPC 连接，通过 ApplicationMaster 申请资源，另一方面根据用户业务逻辑开始调度任务，将任务下发到已有的空闲 Executor 上。</p><p>当 ResourceManager 向 ApplicationMaster 返 回 Container 资源时 ， ApplicationMaster 就尝试在对应的 Container 上启动 Executor 进程，Executor 进程起来后， 会向 Driver 反向注册， 注册成功后保持与 Driver 的心跳，同时等待 Driver 分发任务，当分发的任务执行完毕后，将任务状态上报给 Driver。</p><h1 id="Spark-任务调度概述"><a href="#Spark-任务调度概述" class="headerlink" title="Spark 任务调度概述"></a>Spark 任务调度概述</h1><p>Driver 线程初始化 SparkContext 对象，准备运行所需的上下文，一方面保持与 ApplicationMaster 的 RPC 连接，通过 ApplicationMaster 申请资源，另一方面根据用户业务逻辑开始调度任务，将任务下发到已有的空闲 Executor 上。</p><p><strong>Driver 会根据用户程序逻辑准备任务，并根据 Executor 资源情况逐步分发任务。</strong> 在详细阐述任务调度前，首先说明下 Spark 里的几个概念。一个 Spark 应用程序包括Job、Stage 以及 Task 三个概念：</p><ul><li>Job 是以 Action 方法为界，遇到一个 Action 方法则触发一个Job</li><li>Stage 是 Job 的子集，以宽依赖为界。遇到 Shuffle 做一次划分</li><li>Task 是 Stage 的子集，以并行度(分区数)来衡量,分区数是多少，则有多少个 task</li></ul><p>Spark 的任务调度总体来说分两路进行，一路是 Stage 级的调度， 一路是 Task 级的调度，总体调度流程如下图所示：</p><img src="../images/spark/6.png" alt="" style="zoom:50%;" /><p>Spark RDD 通过其 Transactions 操作，形成了 RDD 血缘关系图，即 DAG ，最后通过 Action 的调用，触发 Job 并调度执行。</p><p><strong>DAGScheduler 负责 Stage 级的调度，主要是将 Job 切分成若干 Stages，并将每个 Stage 打包成 TaskSet 交给 TaskScheduler 调度。</strong></p><p><strong>TaskScheduler 负责 Task 级的调度，将 DAGScheduler 给过来的 TaskSet 按照指定的调度策略分发到 Executor 上执行，调度过程中 SchedulerBackend 负责提供可用资源， 其中 SchedulerBackend 有多种实现， 分别对接不同的资源管理系统。</strong></p><h1 id="Spark-Stage-级调度"><a href="#Spark-Stage-级调度" class="headerlink" title="Spark Stage 级调度"></a>Spark Stage 级调度</h1><p>DAGScheduler 是实现了面向 stage 的调度，它可以为每个 Job 计算出一个 DAG，追踪 RDD 和 stage 的输出是否被持久化，并且寻找到一个最优调度机制来运行 Job.</p><ol><li><p>接收用户提交的 Job；</p></li><li><p>将 Job 划分为不同 stage 的 DAG图，记录哪些 RDD、Stage 被物化存储，并在每一个 stage 内产生一系列的 task，并封装成 TaskSet；</p></li><li><p>要保证相互依赖的 Job/stage 能够得到顺利的调度执行，DAGScheduler 必然需要监控当前Job / Stage乃至Task的完成情况。</p></li><li><p>结合当前的缓存情况，决定每个 Task 的最佳位置(移动计算而不是移动数据，任务在数据所在的节点上运行)，将 TaskSet 提交给 TaskScheduler;</p><p>DAGScheduler 找到哪些 RDDs 已经被 cache 了来避免重计算它们，而且同样地记住哪些ShuffleMapStages 已经生成了输出文件来避免重建一个 shuffle 的 map 侧计算任务。</p></li><li><p>重新提交 Shuffle 输出丢失的 Stage 给 TaskScheduler</p><p>处理由于 shuffle 输出文件丢失导致的失败，在这种情况下，旧的 stage 可能会被重新提交。一个 stage 内部的失败，如果不是由于 shuffle 文件丢失导致的，会被 TaskScheduler 处理，它会被多次重试每一个 task，直到最后一个。实在不行，才会被取消整个 stage。</p></li></ol><h3 id="Stage-划分"><a href="#Stage-划分" class="headerlink" title="Stage 划分"></a>Stage 划分</h3><p>SparkContext 将 Job 提交给 DAGScheduler，DAGScheduler 将一个 Job 划分为若干 Stages ，<strong>具体划分策略是，以 Shuffle 为界，划分 Stage ,由最终的 RDD 不断通过依赖回溯判断父依赖是否是宽依赖，窄依赖的 RDD 被划分到同一个 Stage 中，进行 pipeline 式的计算，划分的 Stages 分两类，一类叫做 ResultStage 为 DAG 下游的 Stage，由 Action 方法决定； 另一类叫做 ShuffleMapStage，其为下游 Stage 准备数据。</strong></p><h3 id="生成-Job，提交-Stage"><a href="#生成-Job，提交-Stage" class="headerlink" title="生成 Job，提交 Stage"></a><strong>生成 Job，提交 Stage</strong></h3><p><strong>一个 Stage 是否被提交，需要判断它的父 Stage 是否执行，只有在父 Stage 执行完毕才能提交当前 Stage，如果一个 Stage 没有父 Stage，那么从该 Stage 开始提交。Stage 提交时会将 Task 信息[分区信息以及方法等]序列化并被打包成 TaskSet 交给 TaskScheduler，一个 Partition 对应一个 Task。</strong></p><h1 id="Spark-Task-级调度"><a href="#Spark-Task-级调度" class="headerlink" title="Spark Task 级调度"></a>Spark Task 级调度</h1><p>Spark Task 的调度是由 TaskScheduler 来完成。DAGScheduler 将 Stage 打包到 TaskSet 交给 TaskScheduler，TaskScheduler 会将 TaskSet 封装为 TaskSetManager 加入到调度队列中。</p><img src="../images/spark/7.png" alt="" style="zoom:50%;" /><p><strong>TaskSetManager 负责监控管理同一个 Stage 中的 Tasks，TaskScheduler 就是以 TaskSetManager 为单元来调度任务 。</strong></p><p>TaskScheduler 初始化后会启动 SchedulerBackend，它负责跟外界打交道，接收 Executor 的注册信息，并维护 Executor 的状态，同时它在启动后会定期地去询问 TaskScheduler 是否有任务要运行，也就是说， 它会定期地问 TaskScheduler “我有这么余量，你要不要啊”，TaskScheduler 在  SchedulerBackend 问它的时候，会从调度队列中按照指定的调度策略选择 TaskSetManager 去运行。</p><p>SchedulerBackend负责与Cluster Manager交互，取得分配给 Application 的资源，并将资源传给TaskScheduler，由 TaskScheduler 为 Task 最终分配计算资源</p><h3 id="调度策略"><a href="#调度策略" class="headerlink" title="调度策略"></a>调度策略</h3><p>TaskScheduler 会先把 DAGScheduler 提交过来的 TaskSet 封装成 TaskSetManager 放到任务队列里，然后再从任务队列里按照一定的规则把它们取出来放在 SchedulerBackend 给过来的 Executor 上运行。这个调度过程实际上还是比较粗粒度的，是面向 TaskSetManager 的。</p><p>调度队列的层次结构如下图所示</p><img src="../images/spark/8.png" alt="" style="zoom:50%;" /><p>TaskScheduler 支持两种调度策略，一种是 FIFO，也是默认的调度策略，另一种是 FAIR。</p><p>在 TaskScheduler 初始化过程中会实例化 rootPool，表示树的根节点， 是 Pool 类型。</p><ul><li><p><strong>FIFO 调度策略</strong></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span>[spark] <span class="class"><span class="keyword">class</span> <span class="title">FIFOSchedulingAlgorithm</span> <span class="keyword">extends</span> <span class="title">SchedulingAlgorithm</span> </span>&#123;</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">comparator</span></span>(s1: <span class="type">Schedulable</span>, s2: <span class="type">Schedulable</span>): <span class="type">Boolean</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> priority1 = s1.priority <span class="comment">// jobId</span></span><br><span class="line">    <span class="keyword">val</span> priority2 = s2.priority</span><br><span class="line">    <span class="keyword">var</span> res = math.signum(priority1 - priority2)</span><br><span class="line">    <span class="keyword">if</span> (res == <span class="number">0</span>) &#123;</span><br><span class="line">      <span class="keyword">val</span> stageId1 = s1.stageId</span><br><span class="line">      <span class="keyword">val</span> stageId2 = s2.stageId</span><br><span class="line">      res = math.signum(stageId1 - stageId2)</span><br><span class="line">    &#125;</span><br><span class="line">    res &lt; <span class="number">0</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>比较 s1和 s2 所属的 JobId，值越小，优先级越高</li><li>如果两个 JobId 的优先级相同， 则对 s1，s2所属的 StageId 进行比，值越小，优先级越高</li></ul></li><li><p><strong>Fair 调度策略</strong></p><p>FAIR 模式中有一个 Root Pool 和多个子 Pool，各个子 Pool 中 存储着所有待分配的 TaskSetManager 。</p><img src="../images/spark/9.png" alt="" style="zoom:50%;" /><p>可以通过在 Properties 中指定 spark.scheduler.pool 属性，指定某个调度池作为 TaskSetManager 的父调度池，如果根调度池不存在此属性值对应的调度池，会创建以此属性值为名称的调度池作为 TaskSetManager 的父调度池，并将此调度池作为根调度池的子调度池。</p><p>在 FAIR 模式中，需要先对 子Pool 进行排序，再对 子Pool 里面的 TaskSetManager 进行排序，因为 Pool 和 TaskSetManager 都继承了 Schedulable 特质，因此使用相同的排序算法 。</p><p>每个要排序的对象包含三个属性 : runningTasks 值[正在运行的 Task 数]、 minShare 值、 weight 值，比较时会综合考量三个属性值。</p><p>注意，minShare 、weight 的值均在公平调度配置文件 fairscheduler.xml 中被指定， 调度池在构建阶段会读取此文件的相关配置。</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">  <span class="tag">&lt;<span class="name">allocations</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">pool</span> <span class="attr">name</span>=<span class="string">&quot;production&quot;</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">schedulingMode</span>&gt;</span>FAIR<span class="tag">&lt;/<span class="name">schedulingMode</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">weight</span>&gt;</span>1<span class="tag">&lt;/<span class="name">weight</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">minShare</span>&gt;</span>2<span class="tag">&lt;/<span class="name">minShare</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">pool</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">pool</span> <span class="attr">name</span>=<span class="string">&quot;test&quot;</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">schedulingMode</span>&gt;</span>FIFO<span class="tag">&lt;/<span class="name">schedulingMode</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">weight</span>&gt;</span>2<span class="tag">&lt;/<span class="name">weight</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">minShare</span>&gt;</span>3<span class="tag">&lt;/<span class="name">minShare</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">pool</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">allocations</span>&gt;</span></span><br></pre></td></tr></table></figure><ul><li><p><strong>runningTasks 比 minShare 小的先执行</strong></p><blockquote><p>如果 A 对象的 runningTasks 大于它的 minShare，B 对象的 runningTasks 小于它的 minShare,那么 B 排在 A 前面</p></blockquote></li><li><p><strong>minShare 使用率低的先执行</strong></p><blockquote><p>如果A，B 对象的 runningTasks 都小于它的 minShare ，那么就比较 runningTasks 和 minShare 的比值 [minShare使用率]谁小谁排前面</p></blockquote></li><li><p><strong>权重使用率低的先执行</strong></p><blockquote><p>如果A、B 对象 的 runningTasks 都大于它们的 minShare ，那么就比较 runningTasks 与 weight 的比值(权重使用率),谁小谁排前面。</p></blockquote></li><li><p><strong>如果上述比较均相等，则比较名字</strong></p></li></ul><p>FAIR 模式排序完成后，所有的 TaskSetManager 被放入一个 ArrayBuffer 里，之后依次被取出并发送给 Executor 执行 。</p></li></ul><p>从调度队列中拿到 TaskSetManager 后，由于 TaskSetManager 封装了一个 Stage 的所有 Task， 并负责管理调度这些 Task，接下来 TaskSetManager 按照一定的规则逐个取出 Task 给 TaskScheduler，TaskScheduler 提交给 SchedulerBackend 去发到 Executor 执行。</p><h3 id="本地化调度"><a href="#本地化调度" class="headerlink" title="本地化调度"></a>本地化调度</h3><p>DAGScheduler 划分 Stage, 通过调用 submitStage 来提交一个 Stage 对应的 tasks， submitStage 会调用 submitMissingTasks，submitMissingTasks 确定每个需要计算的 task 的 preferredLocations，通过调用 getPreferrdeLocations() 得到 partition 的优先位置，由于一个 partition 对应一个task， 此 partition 的优先位置就是 task 的优先位置，对于要提交到 TaskScheduler 的 TaskSet 中的每一个 task ，该 task 优先位置与其对应的 partition 对应的优先位置一致 </p><p>根据每个 task 的优先位置，确定 task 的 Locality 级别，Locality一共有五种，优先级由高到低顺序</p><table><thead><tr><th>PROCESS_LOCAL</th><th>进程本地化，task 和数据在同一个 Executor 中，性能最好。</th></tr></thead><tbody><tr><td><strong>NODE_LOCAL</strong></td><td>节点本地化，task 和数据在同一个节点中，但是 task 和数据不在同一个 Executor 中，数据需要在进程间进行传输。</td></tr><tr><td><strong>RACK_LOCAL</strong></td><td>机架本地化，task 和数据在同一个机架的两个节点上，数据需要通过网络在节点之间进行传输。</td></tr><tr><td><strong>NO_PREF</strong></td><td>数据从哪里访问都一样快，不需要位置优先</td></tr><tr><td><strong>ANY</strong></td><td>task 和数据不在一个机架中，性能最差。</td></tr></tbody></table><p>在调度执行时，Spark 总是会尽量让每个 task 以最高的本地性级别来启动，当一个 task 以X本地性级别启动，但是该本地性级别对应的所有节点都没有空闲资源而启动失败，此时并不会马上降低本地性级别启动而是在某个时间长度内再次以 X 本地性级别来启动该 task，若超过限时时间则降级启动，去尝试下一个本地性级别，依次类推。</p><p>可以通过调大每个类别的最大容忍延迟时间，在等待阶段对应的 Executor 可能 就会有相应的资源去执行此 task，这就在在一定程度上提到了运行性能。</p><h3 id="失败重试与黑名单机制"><a href="#失败重试与黑名单机制" class="headerlink" title="失败重试与黑名单机制"></a>失败重试与黑名单机制</h3><p>除了选择合适的 Task 调度机制外，还需要监控 Task 的执行状态，与外部通信的是 SchedulerBackend。</p><p><strong>Task 被提交到 Executor 启动执行后，Executor 会将执行状态上报给 SchedulerBackend， SchedulerBackend 则通知该 Task 对应的 TaskSetManager，TaskSetManager 获取得知 Task 的执行状态，对于失败的 Task，TaskSetManager 会记录失败次数，如果失败次数还没有超过最大重试次数，则把该 Task 放回待调度的 Task 池子中，否则整个 Application 失败。</strong></p><p>在记录 Task 失败次数过程中，会记录其上一次失败所在的 ExecutorId 和 Host，下次调度该 Task 时，会使用黑名单机制，避免再次被调度到上一次失败的节点上，起到一定的容错作用。</p><p><strong>黑名单记录 Task 上一次失败所在的 ExecutorId 和 Host，以及其对应的 “拉黑时间”.</strong></p><p><strong>“拉黑时间”是指这段时间内不要再往这个节点上调度这个 Task 了。</strong></p>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Spark Shuffle 机制</title>
      <link href="2019/11/28/Spark%20Shuffle/"/>
      <url>2019/11/28/Spark%20Shuffle/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p>shuffle 的性能高低直接影响了整个程序的性能和吞吐量。因为在分布式情况下，reduce task 需要跨节点去拉取其它节点上的 map task 结果。这一过程将会产生网络资源消耗和内存，磁盘 IO 的消耗。</p><a id="more"></a><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a><strong>概述</strong></h2><p>Shuffle 描述着数据从 map task 输出到 reduce task 输入的这段过程。</p><p>Shuffle 是连接 Map 和 Reduce 之间的桥梁， Map 的输出要用到 Reduce 中必须经过 shuffle 这个环节.</p><p>shuffle 的性能高低直接影响了整个程序的性能和吞吐量。因为在分布式情况下，reduce task 需要跨节点去拉取其它节点上的 map task 结果。这一过程将会产生网络资源消耗和内存，磁盘 IO 的消耗。</p><p>通常 shuffle 分为两部分：Map阶段的数据准备和  Reduce 阶段的数据拷贝处理。</p><p>一般将在 map 端的 shuffle 称之为 Shuffle Write， 在Reduce 端的 Shuffle 称之为 Shuffle Read</p><h2 id="导致-Shuffle-操作算子"><a href="#导致-Shuffle-操作算子" class="headerlink" title="导致 Shuffle 操作算子"></a><strong>导致 Shuffle 操作算子</strong></h2><h3 id="重分区类的操作"><a href="#重分区类的操作" class="headerlink" title="重分区类的操作"></a><strong>重分区类的操作</strong></h3><p>重分区类算子一般会 shuffle，因为需要在整个集群中，对之前所有的分区的数据进行随机，均匀的打乱，然后把数据放入下游新的指定数量的分区内。</p><p>比如 repartition、repartitionAndSortWithinPartitions等</p><h3 id="byKey-类的操作"><a href="#byKey-类的操作" class="headerlink" title="byKey 类的操作"></a><strong>byKey 类的操作</strong></h3><p>比如 reduceByKey、groupByKey、sortByKey 等，对一个 key 进行聚合操作时要保证集群中，所有节点上相同的 key 分配到同一个节点上进行处理</p><h3 id="Join-类的操作"><a href="#Join-类的操作" class="headerlink" title="Join 类的操作"></a><strong>Join 类的操作</strong></h3><p>比如 join、cogroup 等。两个 rdd 进行 join，就必须将相同 key 的数据，shuffle 到同一个节点上，然后进行相同 key 的两个 rdd 数据操作。</p><h2 id="Shuffle-原理"><a href="#Shuffle-原理" class="headerlink" title="Shuffle 原理"></a><strong>Shuffle 原理</strong></h2><p><strong>ShuffleMapStage 的结束伴随着 shuffle 文件的写磁盘</strong></p><p><strong>ResultStage基本上对应代码中的 action 算子， 即将一个函数应用在 RDD 的各个 partition 的数据集上，意味着一个 job 的运行结束</strong></p><p>在划分 stage 时， 最后一个 stage 称为 finalStage， 它本质上是一个 ResultStage</p><h3 id="HashShuffle"><a href="#HashShuffle" class="headerlink" title="HashShuffle"></a><strong>HashShuffle</strong></h3><p>通常 shuffle 分为两部分：write 阶段的数据准备和 read 阶段的数据拷贝处理。</p><h4 id="shuffle-write"><a href="#shuffle-write" class="headerlink" title="shuffle write"></a>shuffle write</h4><blockquote><p>shuffle write 阶段，ShuffleMapStage 结束后，每一个 task 中的数据按照 key 进行分类，根据 <strong>hash 算法</strong>，<strong>将相同的 key 写入到一个磁盘文件中，而每一个磁盘文件都只属于下游 stage 的一个 task。在将数据写入磁盘之前，会先将数据写入到内存缓冲，当内存缓冲填满之后，溢写到磁盘文件中。</strong></p></blockquote><h4 id="shuffle-read"><a href="#shuffle-read" class="headerlink" title="shuffle read"></a>shuffle read</h4><blockquote><p>shuffle read，通常就是一个 stage 刚开始时要做的事情。此时该 stage 的每一个 task 需要将上一个 stage 的计算结果中的所有相同 key，从各个节点上通过网络都拉取到自己所在的节点上，然后进行 key 的聚合或连接等操作。由于 shuffle write 的过程中，task 给下游 stage 的每个 task 都创建了一个磁盘文件，因此 shuffle read 的过程中，每个 task 只要从上游 stage 的所有 task 所在节点上，拉取属于自己的那一个磁盘文件即可.</p></blockquote><blockquote><p>shuffle read 的拉取过程是一边拉取一边进行聚合的。每个 shuffle read task 都会有一个自己的buffer 缓冲，每次都只能拉取与 buffer 缓冲相同大小的数据，然后通过内存中的一个 Map 进行聚合等操作。聚合完一批数据后，再拉取下一批数据，并放到 buffer 缓冲中进行聚合操作。以此类推，直到最后将所有数据到拉取完，并得到最终的结果。</p></blockquote><p>这种策略的不足在于，<strong>下游有几个 task，上游的每一个 task 都就都需要创建几个临时文件</strong>，每个文件中只存储 key 取 hash 之后相同的数据，导致了当下游的 task 任务过多的时候，上游会堆积大量的小文件.</p><p><img src="../images/spark/2.png" alt="屏幕快照 2020-02-21 下午8.24.28"></p><ol><li>Shuffle 前在磁盘上会产生海量的小文件，此时会产生大量耗时低效的 IO 操作</li><li>内存不够用，由于内存中需要保存海量文件操作信息和临时信息，如果数据处理的规模比较庞大的话，内存不可承受，会出现 OOM 等问题。</li></ol><h3 id="优化之后的-HashShuffle"><a href="#优化之后的-HashShuffle" class="headerlink" title="优化之后的 HashShuffle"></a>优化之后的 HashShuffle</h3><p>这里说的优化，是指我们可以设置一个参数，spark.shuffle.consolidateFiles。该参数默认值为false，将其设置为 true 即可开启优化机制。通常来说，如果我们使用HashShuffleManager，那么都建议开启这个选项。</p><p>开启这个机制之后，在 shuffle write 过程中，task 就不是为下游 stage 的每个 task 创建一个磁盘文件。出现 shuffleFileGroup 的概念。一个 Executor 上有多少个 CPU core ，就可以并行执行多少个 task。第一批并行执行的每个 task 都会创建一个 shuffleFileGroup，并将数据写入对应的磁盘文件内。<strong>当 Executor 的 CPU core 接着执行下一批 task 时，下一批 task 就会复用之前已有的 shuffleFileGroup ，包括其中的磁盘文件。</strong>而不会写入新的磁盘文件中。</p><p>consolidate 机制允许不同的 task 复用同一批磁盘文件，这样就可以有效将多个 task 的磁盘文件进行一定程度上的合并，从而大幅度减少磁盘文件的数量，进而提升 shuffle write 的性能。</p><img src="../images/spark/3.png" alt="" style="zoom:50%;" /><h2 id="SortShuffle"><a href="#SortShuffle" class="headerlink" title="SortShuffle"></a><strong>SortShuffle</strong></h2><p>SortShuffleManager 的运行机制主要分成两种，一种是普通运行机制，另一种是 bypass 运 行 机 制 。 当 shuffle read task 的 数量小于等于 spark.shuffle.sort.bypassMergeThreshold 参数的值时[默认为 200 ]， 就会启用 bypass 机制。</p><h3 id="普通-SortShuffle"><a href="#普通-SortShuffle" class="headerlink" title="普通 SortShuffle"></a><strong>普通 SortShuffle</strong></h3><p>Task 将数据会先写入一个内存数据结构。</p><p><font color='blue'>[根据不同的 shuffle 算子，可能选用不同的数据结构。如果是 reduceByKey 这种聚合类的 shuffle 算子，那么会选用 Map 数据结构，一边通过 Map 进行聚合，一边写入内存；如果是 join 这种普通的 shuffle 算子，那么会选用 Array 数据结构，直接写入内存。]</font></p><p>每写一条数据进入内存数据结构之后，就会判断是否达到了某个临界值,<strong>如果达到了临界值的话，就会尝试的将内存数据结构中的数据溢写到磁盘</strong>,然后清空内存数据结构。</p><p>注：此时的临界值为动态变化的，并非固定值</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">maybeSpill</span></span>(collection: <span class="type">C</span>, currentMemory: <span class="type">Long</span>): <span class="type">Boolean</span> = &#123;</span><br><span class="line">  <span class="keyword">var</span> shouldSpill = <span class="literal">false</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (elementsRead % <span class="number">32</span> == <span class="number">0</span> &amp;&amp; currentMemory &gt;= myMemoryThreshold) &#123;</span><br><span class="line">    <span class="comment">// Claim up to double our current memory from the shuffle memory pool</span></span><br><span class="line">    <span class="keyword">val</span> amountToRequest = <span class="number">2</span> * currentMemory - myMemoryThreshold</span><br><span class="line">    <span class="keyword">val</span> granted = acquireMemory(amountToRequest)</span><br><span class="line">    myMemoryThreshold += granted</span><br><span class="line">    <span class="comment">// If we were granted too little memory to grow further (either tryToAcquire returned 0,</span></span><br><span class="line">    <span class="comment">// or we already had more memory than myMemoryThreshold), spill the current collection</span></span><br><span class="line">    shouldSpill = currentMemory &gt;= myMemoryThreshold</span><br><span class="line">  &#125;</span><br><span class="line">  shouldSpill = shouldSpill || _elementsRead &gt; numElementsForceSpillThreshold</span><br><span class="line">  <span class="comment">// Actually spill</span></span><br><span class="line">  <span class="keyword">if</span> (shouldSpill) &#123;</span><br><span class="line">    _spillCount += <span class="number">1</span></span><br><span class="line">    logSpillage(currentMemory)</span><br><span class="line">    spill(collection)</span><br><span class="line">    _elementsRead = <span class="number">0</span></span><br><span class="line">    _memoryBytesSpilled += currentMemory</span><br><span class="line">    releaseMemory()</span><br><span class="line">  &#125;</span><br><span class="line">  shouldSpill</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>在溢写到磁盘文件之前，会先根据 key 对内存数据结构中已有的数据进行排序</strong>，排序之后，会分批将数据写入磁盘文件。</p><p><font color='grey'>默认的 batch 数量是 10000 条，也就是说，排序好的数据，会以每批次 1 万条数据的形式分批写入磁盘文件，写入磁盘文件是通过Java 的 BufferedOutputStream 实现的。BufferedOutputStream 是 Java 的缓冲输出流，首先会将数据缓冲在内存中，当内存缓冲满溢之后再一次写入磁盘文件中，这样可以减少磁盘IO次数，提升性能。</font></p><p>一个 Task 将所有数据写入内存数据结构的过程中，会发生多次磁盘溢写，产生多个临时文件，最后会将之前所有的临时文件都进行合并，最后会合并成为一个大文件。<strong>最终只剩下两个文件，一个是合并之后的数据文件，一个是索引文件</strong>,索引文件标识了下游各个 Task 的数据在文件中的 start offset 与 end offset。最终再由下游的 task 根据索引文件读取相应的数据文件。</p><img src="../images/spark/5.png" alt="" style="zoom:50%;" /><p><font color = 'blue'> <strong>SortShuffleManager 由于有一个磁盘文件 merge 的过程，因此大大减少了文件数量。 比如第一个 stage 有 50 个 task ， 总共有 10 个 Executor ， 每个 Executor 执行 5个 task ，而第二个 stage 有 100 个 task 。由于每个 task 最终只有一个磁盘 文件，因此 此时每个 Executor 上只有 5 个磁盘文件， 所有 Executor 只有 50 个磁盘文件。</strong></font></p><h3 id="bypassSortShuffle"><a href="#bypassSortShuffle" class="headerlink" title="bypassSortShuffle"></a>bypassSortShuffle</h3><p>此时 Task 会为每个下游 Task 都创建一个临时磁盘文件，并将数据按 key 进行 hash 然后根据 key 的hash 值，将 key 写入对应的磁盘文件之中。当然，写入磁盘文件时也是先写入内存缓冲，缓冲写满之后再溢写到磁盘文件的。最后，同样会将所有临时磁盘文件都合并成一个磁盘文件，并创建一个单独的索引文件。</p><p>该过程的磁盘写机制其实跟未经优化的 HashShuffleManager 是一模一样的,因为都要创建数量惊人的磁盘文件， 只是在最后会做一个磁盘文件的合并而已,因此产生少量的最终磁盘文件,也让该机制相对未经优化的 HashShuffleManager 来说，shuffle read 的性能会更好。</p><p>而该机制与普通 SortShuffleManager 运行机制的不同在于</p><ol><li>磁盘写机制不同 </li><li>不会进行排序 </li></ol><p>也就是说,启用该机制的最大好处在于,shuffle write 过程中,不需要进行数据的排序操作,也就节省掉了这部分的性能开销。</p><p><strong>触发条件</strong></p><ol><li><p>shuffle map task 的数量小于 spark.shuffle.sort.bypassMergeThreshold 参数的值[默认200]</p></li><li><p>不是聚合类的shuffle 算子[比如groupByKey]</p></li></ol><h2 id="Spark-Shuffle-vs-MR-Shuffle"><a href="#Spark-Shuffle-vs-MR-Shuffle" class="headerlink" title="Spark Shuffle vs MR Shuffle"></a>Spark Shuffle vs MR Shuffle</h2><h3 id="Shuffle-管理器"><a href="#Shuffle-管理器" class="headerlink" title="Shuffle 管理器"></a>Shuffle 管理器</h3><p>Hadoop 2.7.x Shuffle 过程是 sort-based 过程，在 shuffle 过程中会发生排序行为</p><p>Spark 2.2.x Spark ShuffleManager 分为HashShuffleManager和 SortShuffleManager。Spark 1.2 后 默认为SortShuffleManager，在普通模式下，shuffle 过程中会发生排序行为；Spark 可以根据业务场景需要进行ShuffleManager 选择Hash Shuffle Manager / Sort ShuffleManager[普通模式和bypass模式]。</p><h3 id="Shuffle-过程排序次数"><a href="#Shuffle-过程排序次数" class="headerlink" title="Shuffle 过程排序次数"></a>Shuffle 过程排序次数</h3><ul><li>Hadoop Shuffle 过程总共会发生 3 次排序行为，详细分别如下：<ul><li>第一次排序行为：在 map 阶段，由环形缓冲区溢出到磁盘上时，落地磁盘的文件会按照 key 进行分区和排序，属于分区内有序，排序算法为快速排序</li><li>第二次排序行为：在 map 阶段，对溢出的文件进行 combiner合并过程中，需要对溢出的小文件进行归并排序、合并，排序算法为归并排序；</li><li>第三次排序行为：在 reduce 阶段，reduce task 将不同 maptask 端文件拉去到同一个reduce 分区后，对文件进行合并，归并排序，排序算法为归并排序；</li></ul></li><li>Spark Shuffle 过程在满足Shuffle Manager 为 SortShuffleManager ，且运行模式为普通模式的情况下才会发生排序行为，排序行为发生在数据结构中保存数据内存达到阈值，在溢出磁盘文件之前会对内存数据结构中数据进行排序；<ul><li>Spark 中 Sorted-Based Shuffle 在 Mapper 端是进行排序的，包括 partition 的排序和每个partition 内部元素进行排序。但是在 Reducer 端没有进行排序，所以 job 的结果默认情况下不是排序的。</li><li>Sorted-Based Shuffle  采用 Tim-Sort 排序算法，好处是可以极为高效的使用 Mapper 端的排序成果完成全局排序。</li></ul></li></ul><h3 id="Shuffle-逻辑流划分"><a href="#Shuffle-逻辑流划分" class="headerlink" title="Shuffle 逻辑流划分"></a>Shuffle 逻辑流划分</h3><ul><li><p>Hadoop Shuffle 过程可以划分为：map()，spill，merge，shuffle，sort，reduce()等，是按照流程顺次执行的，属于Push类型；</p></li><li><p>Spark Shuffle过程是由算子进行驱动，由于Spark的算子懒加载特性，属于Pull类型，整个Shuffle过程可以划分为Shuffle Write 和Shuffle Read两个阶段；</p></li></ul><h3 id="数据结构不同"><a href="#数据结构不同" class="headerlink" title="数据结构不同"></a>数据结构不同</h3><ul><li>Hadoop 是基于文件的数据结构</li><li>Spark是基于RDD的数据结构，计算性能要比 Hadoop 要高</li></ul><h3 id="Shuffle-Fetch-后数据存放位置"><a href="#Shuffle-Fetch-后数据存放位置" class="headerlink" title="Shuffle Fetch 后数据存放位置"></a>Shuffle Fetch 后数据存放位置</h3><ul><li>Hadoop reduce 端将 map task 的文件拉去到同一个 reduce 分区，是将文件进行归并排序、合并，将文件直接保存在磁盘上</li><li>Spark Shuffle Read 拉取来的数据首先肯定是放在 Reducer 端的内存缓存区中的，实现是内存+磁盘的方式，当然也可以通过 Spark.shuffle.spill=false 来设置只能使用内存。使用 ExternalAppendOnlyMap的方式时候如果内存使用达到一定临界值，会首先尝试在内存中扩大 ExternalAppendOnlyMap，如果不能扩容的话才会spill到磁盘。</li></ul><h3 id="Fetch-操作与数据计算粒度"><a href="#Fetch-操作与数据计算粒度" class="headerlink" title="Fetch 操作与数据计算粒度"></a>Fetch 操作与数据计算粒度</h3><ul><li>Hadoop 的 MapReduce 是粗粒度的，Hadoop Shuffle Reducer Fetch到的数据 record先暂时被存放到Buffer 中，当 Buffer 快满时才进行 combine() 操作</li><li>Spark 的 Shuffle Fetch 是细粒度的，Reducer 是对 Map 端数据 Record 边拉去边聚合</li></ul><h2 id="Spark-Shuffle-调优"><a href="#Spark-Shuffle-调优" class="headerlink" title="Spark Shuffle 调优"></a>Spark Shuffle 调优</h2><p>大多数 Spark 作业的性能主要就是消耗在了 shuffle 环节，因为该环节包含了大量的磁盘IO、序列化、网络数据传输等操作。因此，如果要让作业的性能更上一层楼，就有必要对 shuffle 过程进行调优。</p><h3 id="shuffle-相关参数调优"><a href="#shuffle-相关参数调优" class="headerlink" title="shuffle 相关参数调优"></a>shuffle 相关参数调优</h3><ol><li><p><strong>spark.shuffle.file.buffer</strong></p><p>默认值：32k</p><p>参数说明：该参数用于设置 shuffle write task 的 BufferedOutputStream 的 buffer 缓冲大小。将数据写到磁盘文件之前，会先写入 buffer 缓冲中，待缓冲写满之后，才会溢写到磁盘。<br>调优建议：如果作业可用的内存资源较为充足的话，可以适当增加这个参数的大小（比如64k），从而减少shuffle write过程中溢写磁盘文件的次数，也就可以减少磁盘IO次数，进而提升性能。在实践中发现，合理调节该参数，性能会有1%~5%的提升。</p></li><li><p><strong>spark.reducer.maxSizeInFlight</strong></p><p>默认值：48m<br>参数说明：该参数用于设置shuffle read task的buffer缓冲大小，而这个buffer缓冲决定了每次能够拉取多少数据。<br>调优建议：如果作业可用的内存资源较为充足的话，可以适当增加这个参数的大小（比如96m），从而减少拉取数据的次数，也就可以减少网络传输的次数，进而提升性能。在实践中发现，合理调节该参数，性能会有1%~5%的提升。</p></li><li><p><strong>spark.shuffle.io.maxRetries</strong></p><p>默认值：3<br>参数说明：shuffle read task 从 shuffle write task 所在节点拉取属于自己的数据时，如果因为网络异常导致拉取失败，是会自动进行重试的。该参数就代表了可以重试的最大次数。如果在指定次数之内拉取还是没有成功，就可能会导致作业执行失败。<br>调优建议：对于那些包含了特别耗时的 shuffle 操作的作业，建议增加重试最大次数（比如60次），以避免由于 JVM 的 full gc 或者网络不稳定等因素导致的数据拉取失败。在实践中发现，对于针对超大数据量（数十亿~上百亿）的 shuffle 过程，调节该参数可以大幅度提升稳定性。</p></li><li><p><strong>spark.shuffle.io.retryWait</strong></p><p>默认值：5s<br>参数说明：该参数代表了每次重试拉取数据的等待间隔，默认是 5s。<br>调优建议：建议加大间隔时长（比如60s），以增加 shuffle 操作的稳定性。</p></li></ol>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Spark 数据倾斜</title>
      <link href="2019/11/23/Spark%20%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C/"/>
      <url>2019/11/23/Spark%20%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p>Spark 中的数据倾斜问题主要指 shuffle 过程中由于不同的 key 对应的数据量不同导致的不同 task 所处理的数据量不同的问题。</p><a id="more"></a><h2 id="表现"><a href="#表现" class="headerlink" title="表现"></a>表现</h2><ol><li>Spark 作业的大部分 task 都执行迅速，只有有限的几个 task 执行的非常慢，此时可能出现了数据倾斜，作业可以运行，但是运行的非常慢。</li><li>原本能够正常执行的 Spark 作业，突然出现 OOM[内存溢出] 异常</li></ol><h2 id="定位数据倾斜"><a href="#定位数据倾斜" class="headerlink" title="定位数据倾斜"></a>定位数据倾斜</h2><p>在 Spark 中，同一个 Stage 的不同 Partition 可以并行处理，而具有依赖关系的不同 Stage 之间是串行处理的。假设某个 Spark Job 分为 Stage 0 和 Stage 1 两个 Stage，且 Stage 1 依赖于 Stage 0，那 Stage 0 完全处理结束之前不会处理 Stage 1。而 Stage 0 可能包含 N 个Task，这 N 个Task可以并行进行。如果其中 N-1 个 Task 都在10秒内完成，而另外一个 Task 却耗时1分钟，那该 Stage 的总时间至少为1分钟。换句话说，一个 Stage 所耗费的时间，主要由最慢的那个 Task 决定。<br>由于同一个 Stage 内的所有 Task 执行相同的计算，在排除不同计算节点计算能力差异的前提下，不同Task之间耗时的差异主要由该 Task 所处理的数据量决定。<br>Stage 的数据来源主要分为如下两类</p><ol><li>从数据源直接读取。如读取 HDFS，Kafka</li><li>读取上一个 Stage 的 Shuffle 数据</li></ol><p>常用并且可能会触发 shuffle 操作的算子有：distinct，groupByKey，reduceByKey，aggregateByKey，join 等。出现数据倾斜，很有可能就是使用了这些算子中的某一个导致的。</p><ol><li><p>如果我们是 yarn-client 模式提交，我们可以在本地直接查看 log，在 log 中定位到当前运行到了哪个 stage</p></li><li><p>如果用的 yarn-cluster 模式提交的话，我们可以通过 spark web UI 来查看当前运行到了哪个 stage。</p></li></ol><p>无论用的哪种模式我们都可以在 Spark web UI 上面查看到当前这个 stage 的各个 task 的数据量和运行时间，从而能够进一步确定是不是 task 的数据分配不均导致的数据倾斜。</p><p>当确定了发生数据倾斜的 stage 后，我们可以找出会触发 shuffle 的算子，推算出发生倾斜的那个 stage 对应代码。触发 shuffle 操作的除了上面提到的那些算子外，还要注意使用 spark sql 的某些 sql 语句，比如 group by 等。</p><h2 id="解决策略"><a href="#解决策略" class="headerlink" title="解决策略"></a>解决策略</h2><h4 id="数据源的数据倾斜"><a href="#数据源的数据倾斜" class="headerlink" title="数据源的数据倾斜"></a>数据源的数据倾斜</h4><p>尽量避免数据源的数据倾斜，以 Spark Stream 通过 DirectStream 方式读取 Kafka 数据为例。由于Kafka 的每一个 Partition 对应 Spark 的一个 Task（Partition），所以 Kafka 内相关 Topic 的各 Partition 之间数据是否平衡，直接决定 Spark 处理该数据时是否会产生数据倾斜。</p><p>Kafka 某一 Topic 内消息在不同 Partition 之间的分布，主要由 Producer 端所使用的 Partition 实现类决定。如果使用随机 Partitioner，则每条消息会随机发送到一个 Partition 中，从而从概率上来讲，各 Partition 间的数据会达到平衡。此时源直接读取 Kafka 数据的 Stage 不会产生数据倾斜。<br>但很多时候，业务场景可能会要求将具备同一特征的数据顺序消费，此时就需要将具有相同特征的数据放于同一个Partition中。一个典型的场景是，需要将同一个用户相关的 PV 信息置于同一个 Partition 中。此时，如果产生了数据倾斜，则需要通过其它方式处理。</p><h4 id="过滤异常数据"><a href="#过滤异常数据" class="headerlink" title="过滤异常数据"></a><strong>过滤异常数据</strong></h4><p>如果导致数据倾斜的 key 是异常数据，那么简单的过滤掉就可以了。</p><p>首先要对 key 进行分析，判断是哪些 key 造成数据倾斜。然后对这些 key 对应的记录进行分析:</p><ol><li>空值或者异常值之类的，大多是这个原因引起</li><li>无效数据，大量重复的测试数据或是对结果影响不大的有效数据</li><li>有效数据，业务导致的正常数据分布</li></ol><p>解决方案</p><p>对于第 1，2 种情况，直接对数据进行过滤即可。第3种情况则需要特殊的处理，具体我们下面详细介绍</p><h3 id="调整并行度分散同一个-Task-的不同-Key"><a href="#调整并行度分散同一个-Task-的不同-Key" class="headerlink" title="调整并行度分散同一个 Task 的不同 Key"></a>调整并行度分散同一个 Task 的不同 Key</h3><p>Spark 在做 Shuffle 时，默认使用 HashPartitioner 对数据进行分区。如果并行度设置的不合适，可能造成大量不相同的 Key 对应的数据被分配到了同一个 Task 上，造成该 Task 所处理的数据远大于其它 Task，从而造成数据倾斜。<br>如果调整 Shuffle 时的并行度，使得原本被分配到同一 Task 的不同 Key 发配到不同 Task 上处理，则可降低原 Task 所需处理的数据量，从而缓解数据倾斜问题造成的短板效应。</p><p><strong>优势</strong><br>实现简单，可在需要 Shuffle 的操作算子上直接设置并行度或者使用<code>spark.default.parallelism</code>设置。如果是Spark SQL，还可通过<code>SET spark.sql.shuffle.partitions=[num_tasks]</code>设置并行度。可用最小的代价解决问题。一般如果出现数据倾斜，都可以通过这种方法先试验几次，如果问题未解决，再尝试其它方法。</p><p><img src="https://www.iteblog.com/pic/spark/changeparallelism.png" alt="spark change parallelism"></p><p><strong>劣势</strong><br>适用场景少，只能将分配到同一Task 的不同 Key 分散开，但对于同一 Key 倾斜严重的情况该方法并不适用。并且该方法一般只能缓解数据倾斜，没有彻底消除问题。从实践经验来看，其效果一般。</p><p><strong>方案实践经验</strong></p><p>该方案通常无法彻底解决数据倾斜，因为如果出现一些极端情况，比如某个key对应的数据量有100万，那么无论你的task数量增加到多少，这个对应着100万数据的key肯定还是会分配到一个task中去处理，因此注定还是会发生数据倾斜的。所以这种方案只能说是在发现数据倾斜时尝试使用的第一种手段，尝试去用嘴简单的方法缓解数据倾斜而已，或者是和其他方案结合起来使用。</p><h3 id="自定义-Partitioner"><a href="#自定义-Partitioner" class="headerlink" title="自定义 Partitioner"></a>自定义 Partitioner</h3><p><strong>适用场景</strong><br>大量不同的 Key 被分配到了相同的 Task 造成该 Task 数据量过大。</p><p><strong>解决方案</strong><br>使用自定义的 Partitioner 实现类代替默认的 HashPartitioner，尽量将所有不同的 Key 均匀分配到不同的Task中。</p><p><strong>优势</strong><br>不影响原有的并行度设计。如果改变并行度，后续 Stage 的并行度也会默认改变，可能会影响后续 Stage。</p><p><strong>劣势</strong><br>适用场景有限，只能将不同 Key 分散开，对于同一 Key 对应数据集非常大的场景不适用。效果与调整并行度类似，只能缓解数据倾斜而不能完全消除数据倾斜。而且需要根据数据特点自定义专用的 Partitioner，不够灵活。</p><h3 id="两阶段聚合（局部聚合-全局聚合）"><a href="#两阶段聚合（局部聚合-全局聚合）" class="headerlink" title="两阶段聚合（局部聚合+全局聚合）"></a>两阶段聚合（局部聚合+全局聚合）</h3><p>在 Spark 中使用 groupByKey 和 reduceByKey 这两个算子会进行 shuffle 操作。这时候如果 map 端的文件每个 key 的数据量偏差很大，很容易会造成数据倾斜。</p><p>我们可以先对需要操作的数据中的 key 拼接上随机数进行打散分组，这样原来是一个 key 的数据可能会被分到多个 key 上，然后进行一次聚合，聚合完之后将原来拼在 key 上的随机数去掉，再进行聚合，这样对数据倾斜会有比较好的效果。</p><p>这个方案的核心实现思路就是进行两阶段聚合。第一次是局部聚合，先给每个 key 都打上一个随机数，比如10以内的随机数，此时原先一样的key就变成不一样的了，比如(hello, 1) (hello, 1) (hello, 1) (hello, 1)，就会变成(1_hello, 1) (1_hello, 1) (2_hello, 1) (2_hello, 1)。接着对打上随机数后的数据，执行 reduceByKey 等聚合操作，进行局部聚合，那么局部聚合结果，就会变成了(1_hello, 2) (2_hello, 2)。然后将各个key的前缀给去掉，就会变成(hello,2)(hello,2)，再次进行全局聚合操作，就可以得到最终结果了，比如(hello, 4)。</p><p><strong>方案缺点：</strong>仅仅适用于聚合类的 shuffle 操作，适用范围相对较窄。如果是 join 类的 shuffle 操作，还得用其他的解决方案。</p><h3 id="将-reduce-join-转换为-map-join"><a href="#将-reduce-join-转换为-map-join" class="headerlink" title="将 reduce join 转换为 map join"></a>将 reduce join 转换为 map join</h3><p>通过 Spark 的 Broadcast 机制，将 Reduce 侧 Join 转化为 Map 侧 Join，避免 Shuffle 从而完全消除 Shuffle 带来的数据倾斜。</p><p>两个 RDD 在进行 join 时会有 shuffle 操作，如果每个 key 对应的数据分布不均匀也会有数据倾斜发生。</p><p>这种情况下，如果两个 RDD 中某个 RDD 的数据量不大，可以将该 RDD 的数据提取出来，然后做成广播变量，将数据量大的那个 RDD 做 map 算子操作，然后在 map 算子内和广播变量进行 join，这样可以避免了 join 过程中的 shuffle，也就避免了 shuffle 过程中可能会出现的数据倾斜现象。</p><p><strong>适用场景</strong><br>参与 Join 的一边数据集足够小，可被加载进 Driver 并通过 Broadcast 方法广播到各个 Executor 中。</p><p><strong>解决方案</strong><br>在 Java/Scala 代码中将小数据集数据拉取到 Driver，然后通过 broadcast 方案将小数据集的数据广播到各 Executor。或者在使用 SQL 前，将 broadcast 的阈值调整得足够多，从而使用 broadcast 生效。进而将 Reduce 侧 Join 替换为 Map 侧 Join。</p><p><a href="https://www.iteblog.com/pic/spark/mapjoin.png"><img src="https://www.iteblog.com/pic/spark/mapjoin.png" alt="spark map join"></a></p><h4 id="案例"><a href="#案例" class="headerlink" title="案例"></a>案例</h4><p><strong>优势</strong><br>避免了 Shuffle，彻底消除了数据倾斜产生的条件，可极大提升性能。</p><p><strong>劣势</strong><br>要求参与 Join 的一侧数据集足够小，并且主要适用于 Join 的场景，不适合聚合的场景，适用条件有限。</p><h3 id="为-skew-的-key-增加随机前-后缀，拆分-join-再-union"><a href="#为-skew-的-key-增加随机前-后缀，拆分-join-再-union" class="headerlink" title="为 skew 的 key 增加随机前/后缀，拆分 join 再 union"></a>为 skew 的 key 增加随机前/后缀，<strong>拆分 join 再 union</strong></h3><p>为数据量特别大的 Key 增加随机前/后缀，使得原来 Key 相同的数据变为 Key 不相同的数据，从而使倾斜的数据集分散到不同的 Task 中，彻底解决数据倾斜问题。Join 另一则的数据中，与倾斜 Key 对应的部分数据，与随机前缀集作笛卡尔乘积，从而保证无论数据倾斜侧倾斜Key如何加前缀，都能与之正常 Join。</p><p><strong>适用场景</strong><br>两张表都比较大，无法使用Map则Join。其中一个RDD有少数几个Key的数据量过大，另外一个RDD的Key分布较为均匀。</p><p><strong>解决方案</strong><br>将有数据倾斜的RDD中倾斜Key对应的数据集单独抽取出来加上随机前缀，另外一个RDD每条数据分别与随机前缀结合形成新的RDD（相当于将其数据增到到原来的N倍，N即为随机前缀的总个数），然后将二者Join并去掉前缀。然后将不包含倾斜Key的剩余数据进行Join。最后将两次Join的结果集通过union合并，即可得到全部Join结果。</p><p><img src="https://www.iteblog.com/pic/spark/randomprefix.png" alt="spark random prefix"></p><p><strong>优势</strong><br>相对于 Map 则Join，更能适应大数据集的 Join。如果资源充足，倾斜部分数据集与非倾斜部分数据集可并行进行，效率提升明显。且只针对倾斜部分的数据做数据扩展，增加的资源消耗有限。</p><p><strong>劣势</strong><br>如果倾斜 Key 非常多，则另一侧数据膨胀非常大，此方案不适用。而且此时对倾斜Key与非倾斜Key分开处理，需要扫描数据集两遍，增加了开销。</p><h3 id="大表-key-加盐，小表扩大-N-倍-join"><a href="#大表-key-加盐，小表扩大-N-倍-join" class="headerlink" title="大表 key 加盐，小表扩大 N 倍 join"></a><strong>大表 key 加盐，小表扩大 N 倍 join</strong></h3><h4 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h4><p>如果出现数据倾斜的 Key 比较多，上一种方法将这些大量的倾斜 Key 分拆出来，意义不大。此时更适合直接对存在数据倾斜的数据集全部加上随机前缀，然后对另外一个不存在严重数据倾斜的数据集整体与随机前缀集作笛卡尔乘积（即将数据量扩大N倍）。</p><p><img src="https://www.iteblog.com/pic/spark/randomprefixandenlargesmalltable.png" alt="spark random prefix"></p><p><strong>适用场景</strong><br>一个数据集存在的倾斜Key比较多，另外一个数据集数据分布比较均匀。</p><p><strong>优势</strong><br>对大部分场景都适用，效果不错。</p><p><strong>劣势</strong><br>需要将一个数据集整体扩大N倍，会增加资源消耗。</p>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Spark Shuffle</title>
      <link href="2019/11/23/HDFS%E6%A6%82%E8%BF%B0/"/>
      <url>2019/11/23/HDFS%E6%A6%82%E8%BF%B0/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><h1 id="1-产生背景"><a href="#1-产生背景" class="headerlink" title="1.产生背景"></a>1.产生背景</h1><h1 id="2-HDFS-优缺点"><a href="#2-HDFS-优缺点" class="headerlink" title="2.HDFS 优缺点"></a>2.<code>HDFS</code> 优缺点</h1><h2 id="2-1-HDFS-的优点"><a href="#2-1-HDFS-的优点" class="headerlink" title="2.1.HDFS 的优点"></a>2.1.<code>HDFS</code> 的优点</h2><h3 id="2-1-1-处理超大文件"><a href="#2-1-1-处理超大文件" class="headerlink" title="2.1.1. 处理超大文件"></a>2.1.1. 处理超大文件</h3><p>超大文件通常是指百<code>MB</code>、甚至数百<code>TB</code>大小的文件。目前在实际应用中，HDFS 已经能用来存储管理 PB 级的数据了。</p><h3 id="2-1-2-流式的访问数据"><a href="#2-1-2-流式的访问数据" class="headerlink" title="2.1.2. 流式的访问数据"></a>2.1.2. 流式的访问数据</h3><p><code>HDFS</code> 的设计建立在 **”一次写入、多次读写”**任务的基础上。这意味着一个数据集一旦由数据源生成，就会被复制分发到不同的存储节点中，然后响应各种各样的数据分析任务请求。</p><h3 id="2-1-3-运行于廉价的商用机器集群上"><a href="#2-1-3-运行于廉价的商用机器集群上" class="headerlink" title="2.1.3. 运行于廉价的商用机器集群上"></a>2.1.3. 运行于廉价的商用机器集群上</h3><p>只须运行在低廉的商用硬件集群上，而无需在昂贵的高可用性机器上</p><h2 id="2-2-HDFS-的缺点"><a href="#2-2-HDFS-的缺点" class="headerlink" title="2.2. HDFS 的缺点"></a>2.2. <code>HDFS</code> 的缺点</h2><h3 id="2-2-1-不适合低延迟数据访问"><a href="#2-2-1-不适合低延迟数据访问" class="headerlink" title="2.2.1. 不适合低延迟数据访问"></a>2.2.1. 不适合低延迟数据访问</h3><p>如果要处理一些用户要求时间比较短的低延迟应用请求，则HDFS不适合。HDFS是为了处理大型数据集分析任务的，主要是为达到高的数据吞吐量而设计的，这就可能要求以高延迟作为代价。对于那些有低延时要求的应用程序，HBase是一个更好的选择。</p><h3 id="2-2-2-无法高效存储大量的小文件"><a href="#2-2-2-无法高效存储大量的小文件" class="headerlink" title="2.2.2. 无法高效存储大量的小文件"></a>2.2.2. 无法高效存储大量的小文件</h3><p>因为NameNode把文件系统的元数据放置在内存中，所有文件系统所能容纳的文件数目是由NameNode的内存大小来决定。还有一个问题就是，因为MapTask的数量是由Splits来决定的，所以用MR处理大量的小文件时，就会产生过多的MapTask，线程管理开销将会增加作业时间。当Hadoop处理很多小文件(文件大小小于HDFS中Block大小)的时候，由于FileInputFormat不会对小文件进行划分，所以每一个小文件都会被当做一个Split并分配一个Map任务，导致效率底下。</p><h3 id="2-2-3-不支持多用户写入及任意修改文件"><a href="#2-2-3-不支持多用户写入及任意修改文件" class="headerlink" title="2.2.3. 不支持多用户写入及任意修改文件"></a>2.2.3. 不支持多用户写入及任意修改文件</h3><p>在HDFS的一个文件中只有一个写入者，而且写操作只能在文件末尾完成，即只能执行追加操作，目前HDFS还不支持多个户对同一文件的写操作，以及在文件任意位置进行修改。但是支持文件追加</p><h1 id="3-HDFS-架构"><a href="#3-HDFS-架构" class="headerlink" title="3. HDFS 架构"></a>3. <code>HDFS</code> 架构</h1><h2 id="3-1-Hadoop1-x"><a href="#3-1-Hadoop1-x" class="headerlink" title="3.1. Hadoop1.x"></a>3.1. <code>Hadoop1.x</code></h2><h3 id="3-1-1-NameNode"><a href="#3-1-1-NameNode" class="headerlink" title="3.1.1. NameNode"></a>3.1.1. <code>NameNode</code></h3><p>NameNode又称为名称节点，是负责管理分布式文件系统的命名空间，保存了两个核心的数据结构，即FsImage 和EditLog 。你可以把它理解成大管家，它不负责存储具体的数据。</p><p>HDFS也是文件系统，它也有metadata；由NameNode**<strong>储存在其内存</strong></p><p>文件、block、目录占用大概150Byte字节的元数据；所以HDFS适合储存大文件，不适合储存小文件</p><h3 id="3-1-2-Secondary-NameNode"><a href="#3-1-2-Secondary-NameNode" class="headerlink" title="3.1.2. Secondary NameNode"></a>3.1.2. <code>Secondary NameNode</code></h3><blockquote><p>它出现在 <code>Hadoop1.x</code> 版本中，又称辅助<code>NameNode</code>，在 <code>Hadoop2.x</code> 以后的版本中此角色消失。如果充当<code>Datanode</code> 节点的一台机器宕机或者损害，其数据不会丢失，因为备份数据还存在于其他的 <code>Datanode</code> 中。但是，如果充当 <code>Namenode</code> 节点的机器宕机或损害导致文件系统无法使用，那么文件系统上的所有文件将会丢失，因为我们不知道如何根据 <code>Datanode</code> 的块重建文件。因此，对 <code>NameNode</code> 实现容错非常重要。<code>Hadoop</code> 提供了两种机制实现高容错性。</p></blockquote><blockquote><ul><li><p>辅助 <code>NameNode</code> 工作，分担其工作量。比如定期合并 <code>FsImage</code> 和 <code>Edits</code> ，防止编辑日志文件过大，并且能保证其信息与 <code>NameNode</code> 信息保持一致，并推送给<code>NameNode</code>。</p></li><li><p>紧急情况下，可以辅助恢复 <code>NameNode</code>。</p></li></ul></blockquote><blockquote><p><strong><code>SecondaryNameNode</code> 一般在另一台单独的物理计算机上运行，因为它需要占用大量 <code>CPU</code> 时间来与<code>namenode</code> 进行合并操作，一般情况是单独开一个线程来执行操作过程。但是，<code>SecondaryNameNode</code> 保存的信息永远是滞后于<code>NameNode</code>，所以在 <code>NameNode</code> 失效时，难免会丢失部分数据。</strong></p></blockquote><blockquote><p>在这种情况下，一般把存储在 <code>NFS</code> 上的 <code>Namenode</code> 元数据复制到 <code>SecondaryNameNode</code> 并作为新的 <code>NameNode</code>。<code>SecondaryNameNode</code> 不是 <code>NameNode</code> 的备份，可以作为备份。<code>SecondaryNameNode</code> 主要工作是帮助 <code>NameNode</code> 合并 <code>edits</code> 和<code>fsimag</code>e，减少 <code>Namenode</code> 的启动时间</p></blockquote><blockquote><p><strong><code>SecondaryNameNode</code> 执行合并的时机决定于</strong>：</p><ul><li>配置文件设置的时间间隔 **<code>fs.checkpoint.period</code>**，默认为 <code>3600</code> 秒。</li><li>配置文件设置 <code>edits log</code> 大小**<code>fs.checkpoint.size</code>**,规定 <code>edits</code> 文件的最大值默认是<code>64MB</code>。</li></ul></blockquote><ul><li><p>第一阶段：<code>NameNode</code> 启动</p><ul><li>第一次启动 <code>NameNode</code> 格式化后，创建 <code>FsImage</code> 和 <code>Edits</code> 文件。如果不是第一次启动，直接加载编辑日志和镜像文件到内存。</li><li>客户端对元数据进行增删改的请求</li><li><code>NameNode</code> 记录操作日志，更新滚动日志。</li><li><code>NameNode</code> 在内存中对数据进行增删改查</li></ul></li><li><p>第二阶段：<code>Secondary NameNode</code>工作</p><ul><li><strong><code>Secondary NameNode</code> 请求  <code>NameNode</code> 是否需要  <code>checkpoint</code>。直接带回<code>NameNode</code>是否需要检查结果。</strong>[<code>checkpoint</code> 触发条件：定时时间和 <code>Edits</code> 数据已满]</li><li><code>Secondary NameNode</code> 请求执行 <code>checkpoint</code>。</li><li><code>NameNode</code> 滚动正在写的 <code>Edits</code> 日志</li><li>将滚动前的编辑日志和镜像文件拷贝到 <code>Secondary NameNode</code></li><li><code>SecondaryNameNode</code>加载编辑日志和镜像文件到内存，并合并。生成新的镜像文件<code>fsimage.chkpoint</code></li><li>拷贝 <code>fsimage.chkpoint</code> 到 <code>NameNode</code></li><li><code>NameNode</code> 将 <code>fsimage.chkpoint</code> 重新命名成 `fsimage``</li></ul></li></ul><h3 id="3-1-3-DataNode"><a href="#3-1-3-DataNode" class="headerlink" title="3.1.3. DataNode"></a>3.1.3. <code>DataNode</code></h3><blockquote><p><code>Datanode</code> 用来具体的存储文件，维护了<code>blockId</code> 与 <code>datanode</code> 本地文件的映射。 需要不断的与<code>namenode</code> 节点通信，来告知其自己的信息，方便 <code>namenode</code> 来管控整个系统。</p></blockquote><h3 id="3-1-4-Client"><a href="#3-1-4-Client" class="headerlink" title="3.1.4. Client"></a>3.1.4. <code>Client</code></h3><blockquote><p>客户端是用户和 <code>HDFS</code> 进行交互的手段，<code>HDFS</code> 提供了各种各样的客户端，包括命令行接口、<code>Java API</code> <code>Thrift</code>接口等。</p></blockquote><ul><li><p>职责</p><ul><li><p>文件切分。文件上传 <code>HDFS</code> 的时候，<code>client</code> 将文件切分成一个一个的 <code>Block</code>，然后进行上传。</p></li><li><p>和 <code>NameNode</code> 进行交互，获取文件的位置信息。</p></li><li><p>和 <code>DataNode</code> 进行交互，读取或写入文件</p></li><li><p>提供一些命令来管理 <code>HDFS</code>，如 <code>NameNode</code>格式化</p></li><li><p>提供一些命令来访问<code>HDFS</code>，如 <code>NameNode</code> 增删改查操作。</p></li></ul></li></ul><h2 id="3-2-Hadoop2-x"><a href="#3-2-Hadoop2-x" class="headerlink" title="3.2. Hadoop2.x"></a>3.2. <code>Hadoop2.x</code></h2><h3 id="3-2-1-Active-NameNode"><a href="#3-2-1-Active-NameNode" class="headerlink" title="3.2.1. Active NameNode"></a>3.2.1. <code>Active NameNode</code></h3><h3 id="3-2-2-StandBy-NameNode"><a href="#3-2-2-StandBy-NameNode" class="headerlink" title="3.2.2. StandBy NameNode"></a>3.2.2. <code>StandBy NameNode</code></h3>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
            <tag> Hadoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark Shuffle</title>
      <link href="2019/11/23/Hadoop%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0%5B3%5D-NameNode%E5%90%AF%E5%8A%A8%E6%B5%81%E7%A8%8B/"/>
      <url>2019/11/23/Hadoop%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0%5B3%5D-NameNode%E5%90%AF%E5%8A%A8%E6%B5%81%E7%A8%8B/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p>详细讲述 HDFS NameNode 启动流程</p><a id="more"></a><h1 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h1><p>Namenode 实体在代码实现中主要对应于三个类</p><ol><li><p>NameNode</p><p>NameNode 类负责管理 Namenode 配置，RPC 接口以及 HTTP 接口等，</p></li><li><p>NameNodeRpcServer</p><p>用于接收和处理所有的 RPC 请求</p></li><li><p>FSNamesystem</p><p>负责实现 Namenode 的所有逻辑</p></li></ol><p>Namenode 的启动操作是在 NameNode 类中执行的，main() 方法首先调用 createNameNode() 创建一个NameNode 对象，创建成功后调用 NameNode.join() 方法等待 RPC 服务结束。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String argv[])</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (DFSUtil.parseHelpArgument(argv, NameNode.USAGE, System.out, <span class="keyword">true</span>)) &#123;</span><br><span class="line">    System.exit(<span class="number">0</span>);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    StringUtils.startupShutdownMessage(NameNode.class, argv, LOG);</span><br><span class="line">    <span class="comment">//TODO</span></span><br><span class="line">    NameNode namenode = createNameNode(argv, <span class="keyword">null</span>);</span><br><span class="line">    <span class="keyword">if</span> (namenode != <span class="keyword">null</span>) &#123;</span><br><span class="line">      namenode.join();</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; <span class="keyword">catch</span> (Throwable e) &#123;</span><br><span class="line">    LOG.error(<span class="string">&quot;Failed to start namenode.&quot;</span>, e);</span><br><span class="line">    terminate(<span class="number">1</span>, e);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>createNameNode() 方法会根据启动 Namenode 时传入的启动选项，调用对应的方法执行操<br>作。</p><ul><li>FORMAT:格式化当前 Namenode,调用 format() 方法执行格式化操作。</li><li>FINALIZE:提交上一次升级，目前 Namenode 命令不再支持“－finalize”选项，建<br>议用户使用“hdfs dfsadmin-finalizeUpgrade”命令进行提交操作。</li><li>ROLLBACK:回滚上一次升级，调用 doRollback0方法执行回滚操作</li><li>BOOTSTRAPSTANDBY:拷贝 Active Namenode 的最新命名空间数据到 Standby<br>Namenode,调用 BootstrapStandby.run方法执行操作。</li><li>INITIALIZESHAREDEDITS:初始化editlog 的共享存储空间，并从Active Namenode<br>中拷贝足够的 editlog 数据，使得Standby 节点能够顺利启动。 这里调用了静态方法<br>initializeSharedEdits() 执行操作。</li><li>BACKUP:启动 backup节点，这里直接构造一个BackupNode对象并返回。</li><li>CHECKPOINT:启动 checkpoint 节点，也是直接构造BackupNode 对象并返回。</li><li>RECOVER:恢复损坏的元数据以及文件系统，这里调用了 doRecoveryO方法执行操作。</li><li>默认：正常启动NameNode</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//正常启动NameNode</span></span><br><span class="line"><span class="keyword">default</span>: &#123;</span><br><span class="line">  <span class="comment">//初始化 metric 系统</span></span><br><span class="line">  DefaultMetricsSystem.initialize(<span class="string">&quot;NameNode&quot;</span>);</span><br><span class="line">  <span class="keyword">return</span> <span class="keyword">new</span> NameNode(conf);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="正常启动-NameNode"><a href="#正常启动-NameNode" class="headerlink" title="正常启动 NameNode"></a>正常启动 NameNode</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//TODO</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="title">NameNode</span><span class="params">(Configuration conf, NamenodeRole role)</span></span></span><br><span class="line"><span class="function">    <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">this</span>.conf = conf;</span><br><span class="line">  <span class="keyword">this</span>.role = role;</span><br><span class="line">  setClientNamenodeAddress(conf);<span class="comment">//RPC地址</span></span><br><span class="line">  String nsId = getNameServiceId(conf);<span class="comment">//命名空间的id</span></span><br><span class="line">  String namenodeId = HAUtil.getNameNodeId(conf, nsId);</span><br><span class="line">  <span class="keyword">this</span>.haEnabled = HAUtil.isHAEnabled(conf, nsId);</span><br><span class="line">  state = createHAState(getStartupOption(conf));</span><br><span class="line">  <span class="keyword">this</span>.allowStaleStandbyReads = HAUtil.shouldAllowStandbyReads(conf);</span><br><span class="line">  <span class="keyword">this</span>.haContext = createHAContext();</span><br><span class="line"></span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="comment">//初始化联邦一些配置</span></span><br><span class="line">    initializeGenericKeys(conf, nsId, namenodeId);</span><br><span class="line">    <span class="comment">//TODO 初始化 namenode</span></span><br><span class="line">    initialize(conf);</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      haContext.writeLock();</span><br><span class="line">      state.prepareToEnterState(haContext);</span><br><span class="line">      <span class="comment">//TODO active \ standBy</span></span><br><span class="line">      state.enterState(haContext);</span><br><span class="line">    &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">      haContext.writeUnlock();</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">    <span class="keyword">this</span>.stop();</span><br><span class="line">    <span class="keyword">throw</span> e;</span><br><span class="line">  &#125; <span class="keyword">catch</span> (HadoopIllegalArgumentException e) &#123;</span><br><span class="line">    <span class="keyword">this</span>.stop();</span><br><span class="line">    <span class="keyword">throw</span> e;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">this</span>.started.set(<span class="keyword">true</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="初始化成员变量"><a href="#初始化成员变量" class="headerlink" title="初始化成员变量"></a>初始化成员变量</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">this</span>.conf = conf;</span><br><span class="line"><span class="keyword">this</span>.role = role;</span><br><span class="line">setClientNamenodeAddress(conf);<span class="comment">//RPC地址</span></span><br><span class="line">String nsId = getNameServiceId(conf);<span class="comment">//命名空间的id</span></span><br><span class="line">String namenodeId = HAUtil.getNameNodeId(conf, nsId);</span><br><span class="line"><span class="keyword">this</span>.haEnabled = HAUtil.isHAEnabled(conf, nsId);</span><br><span class="line">state = createHAState(getStartupOption(conf));</span><br><span class="line"><span class="keyword">this</span>.allowStaleStandbyReads = HAUtil.shouldAllowStandbyReads(conf);</span><br><span class="line"><span class="keyword">this</span>.haContext = createHAContext();</span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//NameNode核心成员变量用来管理元数据（实现对DataNode、Block的管理以及读写日志）</span></span><br><span class="line"><span class="keyword">protected</span> FSNamesystem namesystem;</span><br><span class="line"><span class="comment">//保存配置文件的信息</span></span><br><span class="line"><span class="keyword">protected</span> <span class="keyword">final</span> Configuration conf;</span><br><span class="line"><span class="comment">//保存NameNode的角色信息</span></span><br><span class="line"><span class="keyword">protected</span> <span class="keyword">final</span> NamenodeRole role;</span><br><span class="line"></span><br><span class="line"><span class="comment">//保存NameNode的状态（HA）</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">volatile</span> HAState state;</span><br><span class="line"><span class="comment">//是否开启了高可用(HA)</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">boolean</span> haEnabled;</span><br><span class="line"><span class="comment">//高可用上下文</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> HAContext haContext;</span><br><span class="line"><span class="comment">//NameNode核心成员变量提供RPC服务（提供RPC服务是DataNode和NameNode通信和外部命令管理NameNode的窗口）</span></span><br><span class="line"><span class="keyword">private</span> NameNodeRpcServer rpcServer;</span><br><span class="line"><span class="keyword">protected</span> <span class="keyword">final</span> <span class="keyword">boolean</span> allowStaleStandbyReads;</span><br><span class="line"><span class="keyword">private</span> AtomicBoolean started = <span class="keyword">new</span> AtomicBoolean(<span class="keyword">false</span>); </span><br></pre></td></tr></table></figure><h4 id="初始化联邦"><a href="#初始化联邦" class="headerlink" title="初始化联邦"></a>初始化联邦</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//初始化联邦一些配置</span></span><br><span class="line">initializeGenericKeys(conf, nsId, namenodeId);</span><br></pre></td></tr></table></figure><h4 id="初始化-NameNode"><a href="#初始化-NameNode" class="headerlink" title="初始化 NameNode"></a>初始化 NameNode</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 初始化 namenode</span></span><br><span class="line">initialize(conf);</span><br></pre></td></tr></table></figure><p>NameNode.initialize()构造 HTTP 服务器，构造 RPC 服务器，初始化<br>FSNamesystem 对象，最后调用 startCommonServices() 启动 HTTP 服务器、RPC服务器。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">initialize</span><span class="params">(Configuration conf)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">  <span class="comment">//设置metrics的监控间隔</span></span><br><span class="line">  <span class="keyword">if</span> (conf.get(HADOOP_USER_GROUP_METRICS_PERCENTILES_INTERVALS) == <span class="keyword">null</span>) &#123;</span><br><span class="line">    String intervals = conf.get(DFS_METRICS_PERCENTILES_INTERVALS_KEY);</span><br><span class="line">    <span class="keyword">if</span> (intervals != <span class="keyword">null</span>) &#123;</span><br><span class="line">      conf.set(HADOOP_USER_GROUP_METRICS_PERCENTILES_INTERVALS,</span><br><span class="line">        intervals);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//设置权限，根据 hadoop.security.authentication 获取认证方式及规则</span></span><br><span class="line">  UserGroupInformation.setConfiguration(conf);</span><br><span class="line">  <span class="comment">//用户安全登录</span></span><br><span class="line">  loginAsNameNodeUser(conf);</span><br><span class="line"></span><br><span class="line">  <span class="comment">//初始化度量系统，用于度量namenode服务状态</span></span><br><span class="line">  NameNode.initMetrics(conf, <span class="keyword">this</span>.getRole());</span><br><span class="line">  StartupProgressMetrics.register(startupProgress);</span><br><span class="line">  <span class="keyword">if</span> (NamenodeRole.NAMENODE == role) &#123;</span><br><span class="line">    <span class="comment">//TODO 1. 启动 httpServer 对外发布50070</span></span><br><span class="line">    startHttpServer(conf);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">this</span>.spanReceiverHost = SpanReceiverHost.getInstance(conf);</span><br><span class="line">  <span class="comment">//TODO 2. 加载元数据</span></span><br><span class="line">  loadNamesystem(conf);</span><br><span class="line">  <span class="comment">//TODO 2. 启动 RPC</span></span><br><span class="line">  rpcServer = createRpcServer(conf);</span><br><span class="line">  <span class="comment">//如果RPC的地址是null，那么创建一个RPC的地址</span></span><br><span class="line">  <span class="keyword">if</span> (clientNamenodeAddress == <span class="keyword">null</span>) &#123;</span><br><span class="line">    <span class="comment">// This is expected for MiniDFSCluster. Set it now using </span></span><br><span class="line">    <span class="comment">// the RPC server&#x27;s bind address.</span></span><br><span class="line">    clientNamenodeAddress = </span><br><span class="line">        NetUtils.getHostPortString(rpcServer.getRpcAddress());</span><br><span class="line">    LOG.info(<span class="string">&quot;Clients are to use &quot;</span> + clientNamenodeAddress + <span class="string">&quot; to access&quot;</span></span><br><span class="line">        + <span class="string">&quot; this namenode/service.&quot;</span>);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">if</span> (NamenodeRole.NAMENODE == role) &#123;</span><br><span class="line">    httpServer.setNameNodeAddress(getNameNodeAddress());<span class="comment">//给httpServer设置RPC的地址</span></span><br><span class="line">    httpServer.setFSImage(getFSImage());<span class="comment">//设置FSImage快照</span></span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  pauseMonitor = <span class="keyword">new</span> JvmPauseMonitor(conf);</span><br><span class="line">  pauseMonitor.start();</span><br><span class="line">  metrics.getJvmMetrics().setPauseMonitor(pauseMonitor);</span><br><span class="line">  <span class="comment">//TODO 4. 启动一些公共服务[最重要]</span></span><br><span class="line">  startCommonServices(conf);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="启动-HttpServer"><a href="#启动-HttpServer" class="headerlink" title="启动 HttpServer"></a>启动 HttpServer</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (NamenodeRole.NAMENODE == role) &#123;</span><br><span class="line">  <span class="comment">//TODO 启动 httpServer 对外发布50070</span></span><br><span class="line">  startHttpServer(conf);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">startHttpServer</span><span class="params">(<span class="keyword">final</span> Configuration conf)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">  httpServer = <span class="keyword">new</span> NameNodeHttpServer(conf, <span class="keyword">this</span>, getHttpServerBindAddress(conf));</span><br><span class="line">  httpServer.start();</span><br><span class="line">  httpServer.setStartupProgress(startupProgress);</span><br><span class="line"> </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">start</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    <span class="comment">//HTTP_ONLY</span></span><br><span class="line">    HttpConfig.Policy policy = DFSUtil.getHttpPolicy(conf);</span><br><span class="line">    <span class="comment">//localhost | 0.0.0.0</span></span><br><span class="line">    <span class="keyword">final</span> String infoHost = bindAddress.getHostName();</span><br><span class="line"></span><br><span class="line">    <span class="comment">//得到 namenode 的 web 端地址 localhost:50070</span></span><br><span class="line">    <span class="keyword">final</span> InetSocketAddress httpAddr = bindAddress;</span><br><span class="line">    <span class="comment">//得到https的服务地址：localhost:50470</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">final</span> String httpsAddrString = conf.getTrimmed(</span><br><span class="line">        DFSConfigKeys.DFS_NAMENODE_HTTPS_ADDRESS_KEY,</span><br><span class="line">        DFSConfigKeys.DFS_NAMENODE_HTTPS_ADDRESS_DEFAULT);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//把https的地址封装成InetSocketAddress</span></span><br><span class="line">    InetSocketAddress httpsAddr = NetUtils.createSocketAddr(httpsAddrString);</span><br><span class="line">    <span class="comment">//构造https的地址</span></span><br><span class="line">    <span class="keyword">if</span> (httpsAddr != <span class="keyword">null</span>) &#123;</span><br><span class="line">      <span class="comment">//TODO</span></span><br><span class="line"><span class="comment">//      final String bindHost =</span></span><br><span class="line"><span class="comment">//      conf.getTrimmed(DFSConfigKeys.DFS_NAMENODE_HTTPS_BIND_HOST_KEY);</span></span><br><span class="line">      <span class="keyword">final</span> String bindHost = <span class="string">&quot;localhost&quot;</span>;</span><br><span class="line">      <span class="keyword">if</span> (bindHost != <span class="keyword">null</span> &amp;&amp; !bindHost.isEmpty()) &#123;</span><br><span class="line">        httpsAddr = <span class="keyword">new</span> InetSocketAddress(bindHost, httpsAddr.getPort());</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    HttpServer2.Builder builder = DFSUtil.httpServerTemplateForNNAndJN(conf,</span><br><span class="line">        httpAddr, httpsAddr, <span class="string">&quot;hdfs&quot;</span>,</span><br><span class="line">        DFSConfigKeys.DFS_NAMENODE_KERBEROS_INTERNAL_SPNEGO_PRINCIPAL_KEY,</span><br><span class="line">        DFSConfigKeys.DFS_NAMENODE_KEYTAB_FILE_KEY);</span><br><span class="line"></span><br><span class="line">    httpServer = builder.build();</span><br><span class="line">    <span class="keyword">if</span> (policy.isHttpsEnabled()) &#123;</span><br><span class="line">      <span class="comment">// 构建datanode的https服务的端口</span></span><br><span class="line">      InetSocketAddress datanodeSslPort = NetUtils.createSocketAddr(conf.getTrimmed(</span><br><span class="line">          DFSConfigKeys.DFS_DATANODE_HTTPS_ADDRESS_KEY, infoHost + <span class="string">&quot;:&quot;</span></span><br><span class="line">              + DFSConfigKeys.DFS_DATANODE_HTTPS_DEFAULT_PORT));</span><br><span class="line">      httpServer.setAttribute(DFSConfigKeys.DFS_DATANODE_HTTPS_PORT_KEY,</span><br><span class="line">          datanodeSslPort.getPort());</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    initWebHdfs(conf);</span><br><span class="line">    <span class="comment">//绑定属性</span></span><br><span class="line">    httpServer.setAttribute(NAMENODE_ATTRIBUTE_KEY, nn);</span><br><span class="line">    httpServer.setAttribute(JspHelper.CURRENT_CONF, conf);</span><br><span class="line">    <span class="comment">//TODO 在这里绑定很多功能</span></span><br><span class="line">    setupServlets(httpServer, conf);</span><br><span class="line">    httpServer.start();<span class="comment">//启动对外发布50070</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">int</span> connIdx = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">if</span> (policy.isHttpEnabled()) &#123;</span><br><span class="line">      httpAddress = httpServer.getConnectorAddress(connIdx++);</span><br><span class="line">      conf.set(DFSConfigKeys.DFS_NAMENODE_HTTP_ADDRESS_KEY,</span><br><span class="line">          NetUtils.getHostPortString(httpAddress));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (policy.isHttpsEnabled()) &#123;</span><br><span class="line">      httpsAddress = httpServer.getConnectorAddress(connIdx);</span><br><span class="line">      conf.set(DFSConfigKeys.DFS_NAMENODE_HTTPS_ADDRESS_KEY,</span><br><span class="line">          NetUtils.getHostPortString(httpsAddress));</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><img src="/Users/zxc/Documents/hexo/source/_posts/Hadoop源码学习[3]-NameNode启动流程.assets/hdfs3.png" alt="hdfs3" style="zoom:50%;" /><h3 id="绑定功能属性"><a href="#绑定功能属性" class="headerlink" title="绑定功能属性"></a>绑定功能属性</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">initWebHdfs(conf);</span><br><span class="line"><span class="comment">//绑定属性</span></span><br><span class="line">httpServer.setAttribute(NAMENODE_ATTRIBUTE_KEY, nn);</span><br><span class="line">httpServer.setAttribute(JspHelper.CURRENT_CONF, conf);</span><br><span class="line"><span class="comment">//TODO 在这里绑定很多功能</span></span><br><span class="line">setupServlets(httpServer, conf);</span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">setupServlets</span><span class="params">(HttpServer2 httpServer, Configuration conf)</span> </span>&#123;</span><br><span class="line">  httpServer.addInternalServlet(<span class="string">&quot;startupProgress&quot;</span>,</span><br><span class="line">      StartupProgressServlet.PATH_SPEC, StartupProgressServlet.class);</span><br><span class="line">  httpServer.addInternalServlet(<span class="string">&quot;getDelegationToken&quot;</span>,</span><br><span class="line">      GetDelegationTokenServlet.PATH_SPEC, </span><br><span class="line">      GetDelegationTokenServlet.class, <span class="keyword">true</span>);</span><br><span class="line">  httpServer.addInternalServlet(<span class="string">&quot;renewDelegationToken&quot;</span>, </span><br><span class="line">      RenewDelegationTokenServlet.PATH_SPEC, </span><br><span class="line">      RenewDelegationTokenServlet.class, <span class="keyword">true</span>);</span><br><span class="line">  httpServer.addInternalServlet(<span class="string">&quot;cancelDelegationToken&quot;</span>, </span><br><span class="line">      CancelDelegationTokenServlet.PATH_SPEC, </span><br><span class="line">      CancelDelegationTokenServlet.class, <span class="keyword">true</span>);</span><br><span class="line">  httpServer.addInternalServlet(<span class="string">&quot;fsck&quot;</span>, <span class="string">&quot;/fsck&quot;</span>, FsckServlet.class,</span><br><span class="line">      <span class="keyword">true</span>);</span><br><span class="line">  httpServer.addInternalServlet(<span class="string">&quot;imagetransfer&quot;</span>, ImageServlet.PATH_SPEC,</span><br><span class="line">      ImageServlet.class, <span class="keyword">true</span>);</span><br><span class="line">  httpServer.addInternalServlet(<span class="string">&quot;listPaths&quot;</span>, <span class="string">&quot;/listPaths/*&quot;</span>,</span><br><span class="line">      ListPathsServlet.class, <span class="keyword">false</span>);</span><br><span class="line">  httpServer.addInternalServlet(<span class="string">&quot;data&quot;</span>, <span class="string">&quot;/data/*&quot;</span>,</span><br><span class="line">      FileDataServlet.class, <span class="keyword">false</span>);</span><br><span class="line">  httpServer.addInternalServlet(<span class="string">&quot;checksum&quot;</span>, <span class="string">&quot;/fileChecksum/*&quot;</span>,</span><br><span class="line">      FileChecksumServlets.RedirectServlet.class, <span class="keyword">false</span>);</span><br><span class="line">  httpServer.addInternalServlet(<span class="string">&quot;contentSummary&quot;</span>, <span class="string">&quot;/contentSummary/*&quot;</span>,</span><br><span class="line">      ContentSummaryServlet.class, <span class="keyword">false</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><img src="/Users/zxc/Documents/hexo/source/_posts/Hadoop源码学习[3]-NameNode启动流程.assets/hdfs5.png" alt="hdfs5" style="zoom:60%;" /><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//NameNode核心成员变量用来管理元数据（实现对DataNode、Block的管理以及读写日志）</span></span><br><span class="line"><span class="keyword">protected</span> FSNamesystem namesystem;</span><br><span class="line"><span class="comment">//保存配置文件的信息</span></span><br><span class="line"><span class="keyword">protected</span> <span class="keyword">final</span> Configuration conf;</span><br><span class="line"><span class="comment">//保存NameNode的角色信息</span></span><br><span class="line"><span class="keyword">protected</span> <span class="keyword">final</span> NamenodeRole role;</span><br><span class="line"></span><br><span class="line"><span class="comment">//保存NameNode的状态（HA）</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">volatile</span> HAState state;</span><br><span class="line"><span class="comment">//是否开启了高可用(HA)</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">boolean</span> haEnabled;</span><br><span class="line"><span class="comment">//高可用上下文</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> HAContext haContext;</span><br><span class="line"><span class="comment">//NameNode核心成员变量提供RPC服务（提供RPC服务是DataNode和NameNode通信和外部命令管理NameNode的窗口）</span></span><br><span class="line"><span class="keyword">private</span> NameNodeRpcServer rpcServer;</span><br><span class="line"><span class="keyword">protected</span> <span class="keyword">final</span> <span class="keyword">boolean</span> allowStaleStandbyReads;</span><br><span class="line"><span class="keyword">private</span> AtomicBoolean started = <span class="keyword">new</span> AtomicBoolean(<span class="keyword">false</span>); </span><br></pre></td></tr></table></figure><h1 id="3-加载元数据"><a href="#3-加载元数据" class="headerlink" title="3.加载元数据"></a>3.加载元数据</h1><p><img src="/Users/zxc/Documents/hexo/source/_posts/Hadoop%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0%5B3%5D-NameNode%E5%90%AF%E5%8A%A8%E6%B5%81%E7%A8%8B.assets/hdfs6-2540565.png" alt="hdfs6"></p><h1 id="4-创建-RPC"><a href="#4-创建-RPC" class="headerlink" title="4.创建 RPC"></a>4.创建 RPC</h1><h3 id="4-1-注册-RPC，然后给引擎注册协议-Protocol"><a href="#4-1-注册-RPC，然后给引擎注册协议-Protocol" class="headerlink" title="4.1.注册 RPC，然后给引擎注册协议 Protocol"></a>4.1.<strong>注册 RPC，然后给引擎注册协议 Protocol</strong></h3><h3 id="4-2-分别实例化-ServiceRpcServer-和-ClientRpcServer"><a href="#4-2-分别实例化-ServiceRpcServer-和-ClientRpcServer" class="headerlink" title="4.2.分别实例化 ServiceRpcServer 和 ClientRpcServer"></a>4.2.<strong>分别实例化 ServiceRpcServer 和 ClientRpcServer</strong></h3><h3 id="4-2-1-ServiceRpcServer"><a href="#4-2-1-ServiceRpcServer" class="headerlink" title="4.2.1.ServiceRpcServer"></a>4.2.1.ServiceRpcServer</h3><p>用来 NameNode 和 DataNode 之间的通信（注册、心跳、握手、数据块的汇报）</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">this</span>.serviceRpcServer = <span class="keyword">new</span> RPC.Builder(conf)</span><br><span class="line">    .setProtocol(</span><br><span class="line">        org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolPB.class)</span><br><span class="line">    .setInstance(clientNNPbService)</span><br><span class="line">    .setBindAddress(bindHost)</span><br><span class="line">    .setPort(serviceRpcAddr.getPort()).setNumHandlers(serviceHandlerCount)</span><br><span class="line">    .setVerbose(<span class="keyword">false</span>)</span><br><span class="line">    .setSecretManager(namesystem.getDelegationTokenSecretManager())</span><br><span class="line">    .build();</span><br></pre></td></tr></table></figure><h3 id="4-2-2-ClientRpcServer"><a href="#4-2-2-ClientRpcServer" class="headerlink" title="4.2.2.ClientRpcServer"></a>4.2.2.ClientRpcServer</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">this</span>.clientRpcServer = <span class="keyword">new</span> RPC.Builder(conf)</span><br><span class="line">    .setProtocol(</span><br><span class="line">        org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolPB.class)</span><br><span class="line">    .setInstance(clientNNPbService).setBindAddress(bindHost)</span><br><span class="line">    .setPort(rpcAddr.getPort()).setNumHandlers(handlerCount)</span><br><span class="line">    .setVerbose(<span class="keyword">false</span>)</span><br><span class="line">    .setSecretManager(namesystem.getDelegationTokenSecretManager()).build();</span><br></pre></td></tr></table></figure><h1 id="5-启动公共服务"><a href="#5-启动公共服务" class="headerlink" title="5.启动公共服务"></a>5.启动公共服务</h1><h2 id="5-1-进行资源检查的前期准备"><a href="#5-1-进行资源检查的前期准备" class="headerlink" title="5.1.进行资源检查的前期准备"></a>5.1.进行资源检查的前期准备</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Create a NameNodeResourceChecker, which will check the edits dirs and any</span></span><br><span class="line"><span class="comment"> * additional dirs to check set in &lt;code&gt;conf&lt;/code&gt;.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">NameNodeResourceChecker</span><span class="params">(Configuration conf)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">  <span class="keyword">this</span>.conf = conf;</span><br><span class="line">  <span class="comment">//volumes 存放待检查的路径</span></span><br><span class="line">  volumes = <span class="keyword">new</span> HashMap&lt;String, CheckedVolume&gt;();</span><br><span class="line">  <span class="comment">//100M</span></span><br><span class="line">  duReserved = conf.getLong(DFSConfigKeys.DFS_NAMENODE_DU_RESERVED_KEY,</span><br><span class="line">      DFSConfigKeys.DFS_NAMENODE_DU_RESERVED_DEFAULT);</span><br><span class="line">  <span class="comment">//获取本地目录列表</span></span><br><span class="line">  Collection&lt;URI&gt; extraCheckedVolumes = Util.stringCollectionAsURIs(</span><br><span class="line">          conf.getTrimmedStringCollection(DFSConfigKeys.DFS_NAMENODE_CHECKED_VOLUMES_KEY));</span><br><span class="line"></span><br><span class="line">  <span class="comment">//获取共享目录列表dfs.namenode.shared.edits.dir（HA场景下的journalNode的目录列表）</span></span><br><span class="line">  Collection&lt;URI&gt; localEditDirs = Collections2.filter(</span><br><span class="line">      FSNamesystem.getNamespaceEditsDirs(conf),<span class="comment">//拿到edits的目录</span></span><br><span class="line">      <span class="keyword">new</span> Predicate&lt;URI&gt;() &#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">apply</span><span class="params">(URI input)</span> </span>&#123;</span><br><span class="line">          <span class="keyword">if</span> (input.getScheme().equals(NNStorage.LOCAL_URI_SCHEME)) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">          &#125;</span><br><span class="line">          <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Add all the local edits dirs, marking some as required if they are</span></span><br><span class="line">  <span class="comment">// configured as such.</span></span><br><span class="line">  <span class="keyword">for</span> (URI editsDirToCheck : localEditDirs) &#123;</span><br><span class="line">    addDirToCheck(editsDirToCheck,</span><br><span class="line">        FSNamesystem.getRequiredNamespaceEditsDirs(conf).contains(</span><br><span class="line">            editsDirToCheck));</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// All extra checked volumes are marked &quot;required&quot;</span></span><br><span class="line">  <span class="keyword">for</span> (URI extraDirToCheck : extraCheckedVolumes) &#123;</span><br><span class="line">    addDirToCheck(extraDirToCheck, <span class="keyword">true</span>);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//返回配置的目录空间，最小可用的容忍度，模式1</span></span><br><span class="line">  minimumRedundantVolumes = conf.getInt(</span><br><span class="line">      DFSConfigKeys.DFS_NAMENODE_CHECKED_VOLUMES_MINIMUM_KEY,</span><br><span class="line">      DFSConfigKeys.DFS_NAMENODE_CHECKED_VOLUMES_MINIMUM_DEFAULT);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="5-2-检查可用资源是否足够：如果不够，日志打印警告信息，然后进入安全模式"><a href="#5-2-检查可用资源是否足够：如果不够，日志打印警告信息，然后进入安全模式" class="headerlink" title="5.2.检查可用资源是否足够：如果不够，日志打印警告信息，然后进入安全模式"></a>5.2.检查可用资源是否足够：如果不够，日志打印警告信息，然后进入安全模式</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/** </span></span><br><span class="line"><span class="comment"> * Start services common to both active and standby states</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">startCommonServices</span><span class="params">(Configuration conf, HAContext haContext)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">  <span class="keyword">this</span>.registerMBean(); <span class="comment">// register the MBean for the FSNamesystemState</span></span><br><span class="line">  writeLock();</span><br><span class="line">  <span class="keyword">this</span>.haContext = haContext;</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="comment">//TODO 1 进行资源检查的前期准备</span></span><br><span class="line">    nnResourceChecker = <span class="keyword">new</span> NameNodeResourceChecker(conf);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//TODO 2 检查可用资源是否足够：如果不够，日志打印警告信息，然后进入安全模式</span></span><br><span class="line">    checkAvailableResources();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span> safeMode != <span class="keyword">null</span> &amp;&amp; !isPopulatingReplQueues();</span><br><span class="line"></span><br><span class="line">    StartupProgress prog = NameNode.getStartupProgress();</span><br><span class="line">    <span class="comment">// 目前NameNode启动，进入到safemode阶段，处于一个等待汇报blocks的状态</span></span><br><span class="line">    prog.beginPhase(Phase.SAFEMODE);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//TODO 处于一个等待汇报blocks的状态</span></span><br><span class="line">    prog.setTotal(Phase.SAFEMODE, STEP_AWAITING_REPORTED_BLOCKS,</span><br><span class="line">      getCompleteBlocksTotal());</span><br><span class="line"></span><br><span class="line">    <span class="comment">//TODO 3 设置所有的block，用于后面判断是否进入安全模式</span></span><br><span class="line">    setBlockTotal();</span><br><span class="line"></span><br><span class="line">    <span class="comment">//TODO 4 激活BlockManager</span></span><br><span class="line">    blockManager.activate(conf);</span><br><span class="line">  &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">    writeUnlock();</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  registerMXBean();</span><br><span class="line">  DefaultMetricsSystem.instance().register(<span class="keyword">this</span>);</span><br><span class="line">  <span class="keyword">if</span> (inodeAttributeProvider != <span class="keyword">null</span>) &#123;</span><br><span class="line">    inodeAttributeProvider.start();</span><br><span class="line">    dir.setINodeAttributeProvider(inodeAttributeProvider);</span><br><span class="line">  &#125;</span><br><span class="line">  snapshotManager.registerMXBean();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="5-2-安全模式"><a href="#5-2-安全模式" class="headerlink" title="5.2.安全模式"></a>5.2.安全模式</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/** </span></span><br><span class="line"><span class="comment"> * There is no need to enter safe mode </span></span><br><span class="line"><span class="comment"> * if DFS is empty or &#123;<span class="doctag">@link</span> #threshold&#125; == 0</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">boolean</span> <span class="title">needEnter</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * 1: threshold=0.999f 这个不为null &amp;&amp; 汇报过来的数据块数 &lt; 安全阈值(1000=999)</span></span><br><span class="line"><span class="comment">   * 2: 存活的 datanode 数量如果&lt;=0</span></span><br><span class="line"><span class="comment">   * 3: 如果磁盘空间&lt;100m</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * */</span></span><br><span class="line">  <span class="keyword">return</span> (threshold != <span class="number">0</span> &amp;&amp; blockSafe &lt; blockThreshold) ||</span><br><span class="line">    (datanodeThreshold != <span class="number">0</span> &amp;&amp; getNumLiveDataNodes() &lt; datanodeThreshold) ||</span><br><span class="line">    (!nameNodeHasResourcesAvailable());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="6-总结"><a href="#6-总结" class="headerlink" title="6.总结"></a>6.总结</h1><p><img src="/Users/zxc/Documents/hexo/source/_posts/Hadoop%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0%5B3%5D-NameNode%E5%90%AF%E5%8A%A8%E6%B5%81%E7%A8%8B.assets/hdfs1.png" alt="hdfs1"></p><h2 id="1-2-简介"><a href="#1-2-简介" class="headerlink" title="1.2. 简介"></a>1.2. 简介</h2><p>安全模式是HDFS的一种工作状态，处于安全模式的状态下，只向客户端提供文件的只读视图，不接受对命名空间的修改；同时NameNode节点也不会进行数据块的复制或者删除，如：副本的数量小于正常水平。</p><h2 id="1-3-相关命令"><a href="#1-3-相关命令" class="headerlink" title="1.3. 相关命令"></a>1.3. 相关命令</h2><ul><li><p><code>hadoop dfsadmin -safemode leave</code></p><blockquote><p>强制 <code>NameNode</code> 退出安全模式</p></blockquote></li><li><p><code>hadoop dfsadmin -safemode enter</code>  </p><blockquote><p> 进入安全模式</p></blockquote></li><li><p><code>hadoop dfsadmin -safemode get</code>   </p><blockquote><p> 查看安全模式状态</p></blockquote></li><li><p><code>hadoop dfsadmin -safemode wait </code></p><blockquote><p>等待一直到安全模式结束</p></blockquote></li></ul>]]></content>
      
      
      <categories>
          
          <category> Hadoop 源码阅读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
            <tag> Hadoop </tag>
            
            <tag> Hadoop 源码阅读 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark 存储模块</title>
      <link href="2019/11/22/Spark%20%E5%85%B1%E4%BA%AB%E5%8F%98%E9%87%8F/"/>
      <url>2019/11/22/Spark%20%E5%85%B1%E4%BA%AB%E5%8F%98%E9%87%8F/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p>在执行 Spark 的应用程序时，Spark 集群会启动 Driver 和 Executor 两种 JVM 进程，前者为主控进程，负责创建 Spark 上下文，提交 Spark 作业[Job]，并将作业转化为计算任务[Task]，在各个 Executor 进程间协调任务的调度；后者负责在工作节点上执行具体的计算任务，并将结果返回给 Driver， 同时为需要持久化的 RDD 提供存储功能。由于 Driver 的内存管理相对来说较为简单，本节主要对 Executor 的内存管理进行分析，下文中的 Spark 内存均特指 Executor 的内存。</p><a id="more"></a><h2 id="Execuor-内存模型"><a href="#Execuor-内存模型" class="headerlink" title="Execuor 内存模型"></a>Execuor 内存模型</h2><h3 id="堆内和堆外内存"><a href="#堆内和堆外内存" class="headerlink" title="堆内和堆外内存"></a>堆内和堆外内存</h3><p>作为一个 JVM 进程，Executor 的内存管理建立在 JVM 的内存管理之上，Spark 对 JVM 的堆内 [On-heap]空间进行了更为详细的分配，以充分利用内存。</p><p>同时，Spark 引入了堆外[Off-heap]内存，使之可以直接在工作节点的系统内存中开辟空间，进一步优化了内存的使用。</p><p><strong>堆内内存受到 JVM 统一管理，堆外内存是直接向操作系统进行内存的申请和释放。</strong></p><h3 id="堆内内存"><a href="#堆内内存" class="headerlink" title="堆内内存"></a>堆内内存</h3><p>堆内内存的大小，由 Spark 应用程序启动时的 –executor-memory 或 spark.executor.memory 参数配置。 </p><p>Executor 内运行的并发任务共享 JVM 堆内内存， 这些任务在缓存 RDD 数据和广播 Broadcast 数据时占用的内存被规划为存储 [Storage] 内存， 而这些任务在执行 Shuffle 时占用的内存被规划为执行 [Execution] 内存，剩余的部分不做特殊规划，Spark 内部的对象实例，或者用户定义的 Spark 应用程序中的对象实例，均占用剩余的空间 。不同的管理模式下， 这三部分占用的空间大小各不相同 </p><h3 id="堆外内存"><a href="#堆外内存" class="headerlink" title="堆外内存"></a>堆外内存</h3><p>JVM 对于内存的清理无法准确指定时间点，因此无法实现精确的释放。<strong>为了进一步优化内存的使用以及提高 Shuffle 时排序的效率，Spark 引入了堆外[Off-heap]内存，使之可以直接在工作节点的系统内存中开辟空间，存储经过序列化的二进制数据。</strong>由于内存的申请和释放不再通过 JVM 机制，而是直接向操作系统申请，减少了不必要的内存开销，以及频繁的 GC 扫描和回收，提升了处理性能。而且序列化的数据占用的空间可以被精确计算，所以相比堆内内存来说，堆外内存可以被精确地申请和释放，降低了管理的难度，也降低了误差 </p><p>在默认情况下堆外内存并不启用，可通过配置 spark.memory.offHeap.enabled 参数启用，并由 spark.memory.offHeap.size 参数设定堆外空间的大小。除了没有 other 空间，堆外内存与堆内内存的划分方式相同，所有运行中的并发任务共享存储内存和执行内存 。</p><h2 id="内存空间分配"><a href="#内存空间分配" class="headerlink" title="内存空间分配"></a>内存空间分配</h2><h3 id="静态内存管理"><a href="#静态内存管理" class="headerlink" title="静态内存管理"></a>静态内存管理</h3><p>在 Spark 最初采用的静态内存管理机制下，**[存储内存]<strong>、</strong>[执行内存]<strong>和</strong>[其他内存]**的大小在 Spark 应用程序运行期间均为固定的，但用户可以应用程序启动前进行配置。</p><img src="/images/spark/009.png" alt="" style="zoom:90%;" /><ul><li><p>可用的存储内存</p><p> systemMaxMemory * spark.storage.memoryFraction * spark.storage.safetyFraction</p></li><li><p>可用的执行内存</p><p>systemMaxMemory * spark.shuffle.memoryFraction * spark.shuffle.safetyFraction</p></li></ul><p>其中 systemMaxMemory 取决于当前 JVM 堆内内存的大小，最后可用的执行内存或者存储内存要在此基础上与各自的 memoryFraction 参数和 safetyFraction 参数相乘得出。 </p><p>上述计算公式中的两个 safetyFraction 参数，其意义在于在逻辑上预留出 [1-safetyFraction] 这么一块保险区域，降低因实际内存超出当前预设范围而导致 OOM 的风险。</p><p>值得注意的是，这个预留的保险区域仅仅是一种逻辑上的规划，在具体使用时 Spark 并没有区别对待，和”其它内存”一样交给了 JVM 去管理。</p><p><strong>Storage 内存和 Execution 内存都有预留空间，目的是防止 OOM ，因为 Spark 堆内内存大小的记录是不准确的，需要留出保险区域。</strong></p><p>堆外的空间分配较为简单，只有存储内存和执行内存。可用的执行内存和存储内存占用的空间大小直接由参数 spark.memory.storageFraction 决定，由于堆外内存占用的空间可以被精确计算，所以无需再设定保险区域 </p><img src="/images/spark/12.png" alt="" style="zoom:90%;" /><p>静态内存管理机制实现起来较为简单，但如果用户不熟悉 Spark 的存储机制，或没有根据具体的数据规模和计算任务或做相应的配置，很容易造成存储内存和执行内存中的一方剩余大量的空间，而另一方却早早被占满，不得不淘汰或移出旧的内容以存储新的内容。由于新的内存管理机制的出现，这种方式目前已经很少有开发者使用，出于兼容旧版本的应用程序的目的，Spark 仍然保留了它的实现。</p><h3 id="统一内存管理"><a href="#统一内存管理" class="headerlink" title="统一内存管理"></a>统一内存管理</h3><p><strong>Spark1.6 之后引入的统一内存管理机制，与静态内存管理的区别在于<font color='blue'>存储内存和执行内存共享同一块空间，可以动态占用对方的空闲区域</font></strong> </p><img src="/images/spark/10.png" alt="" style="zoom:90%;" /><p>其中最重要的优化在于动态占用机制， 其规则如下：</p><img src="/images/spark/20.png" alt="" style="zoom:90%;" /><ul><li><p>设定基本的存储内存和执行内存区域（spark.storage.storageFraction参数），该设定确定了双方各自拥有的空间的范围；</p></li><li><p>双方的空间都不足时，则存储到硬盘</p><p>若己方空间不足而对方空余时，可借用对方的空间; [注：存储空间不足是指不足以放下一个完整的Block]</p></li><li><p>执行内存的空间被对方占用后，可让对方将占用的部分转存到磁盘，然后”归还”借用的空间；</p></li><li><p>存储内存的空间被对方占用后，无法让对方 “归还”，因为需要考虑 Shuffle 过程中的很多因素，实现起来较为复杂。</p></li></ul><h2 id="存储内存管理"><a href="#存储内存管理" class="headerlink" title="存储内存管理"></a>存储内存管理</h2><h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><p>Storage 管理着 Spark 应用在运行过程中产生的各种数据。比如 RDD 缓存，shuffle 过程中缓存及写入磁盘的数据，广播变量等。</p><p>Storage 模块在逻辑上以 Block 为基本存储单位，RDD 的每个 Partition 经过处理后唯一对应一个 Block。Driver 端 BlockManager 负责整个 Spark 应用程序的 Block 的元数据信息的管理和维护，而 Executor 端的 BlockManager 需要将 Block 的更新等状态上报到 Master，同时接收 Master 的命令， 例如新增或删除一个 RDD。</p><ul><li><p>BlockManager</p><blockquote><p>BlockManager 是 整个 Spark 底层负责数据存储与管理的一个组件 ， Driver 和 Executor 的所有数据都由对应的 BlockManager 进行管理。</p><p>Driver 上有 BlockManager Master ，负责对各个节点上的 BlockManager 内部管理的数据的元数据进行维护， 比如 block 的增删改等操作， 都会在这里维护好元数据 的变更。</p><p>每个节点都有一个 BlockManager，每个 BlockManager 创建之后， 第一件事即使去向 BlockManag erMaster 进行注册。</p></blockquote></li><li><p>CacheManager</p><blockquote><p>CacheManager 管理 spark 的缓存，而缓存可以基于内存的缓存，也可以是基于磁盘的缓存；<br>CacheManager 需要通过 BlockManager 来操作数据</p></blockquote></li></ul><h3 id="RDD-的持久化机制"><a href="#RDD-的持久化机制" class="headerlink" title="RDD 的持久化机制"></a>RDD 的持久化机制</h3><p>弹性分布式数据集 RDD 作为 Spark 最根本的数据抽象，是只读的分区记录的集合，基于在稳定物理存储中的数据集上创建，或者在其他已有的 RDD 上执行转换[Transformation]操作产生一个新的 RDD。转换后的 RDD 与 原始的 RDD 之间产生的依赖关系构成了血统[Lineag]。凭借血统，Spark 保证了每一个 RDD 都可以被重新恢复。</p><p>Task 在启动之初读取一个分区时，会先判断这个分区是否已经被持久化，如果没有则需要检查 Checkpoint 或按照血统重新计算。所以如果一个 RDD 要执行多次 action 操作， 可以在第一次 action 操作中使用 persist 或 cache 方法，在内存或磁盘中持久化或缓存这个 RDD，从而在后面的行动时提升计算速度。</p><blockquote><p>其中 cache 这个方法是个 Tranformation ,当第一次遇到 action 算子的时才会进行持久化</p><p>cache 内部调用了 persist(StorageLevel.MEMORY_ONLY)方法，所以执行 cache 算子其实就是执行了 persist 算子且持久化级别为 MEMORY_ONLY。 故缓存是一种特殊的持久化。堆内和堆外存储内存的设计，便可以对缓存 RDD 时使用的内存做统一的规划和管理。</p></blockquote><p>RDD 的持久化由 Spark 的 Storage 模块负责，实现了 RDD 与物理存储的解耦合。 </p><p>在对 RDD 持久化时，Spark 规定了 MEMORY_ONLY 、MEMORY_AND_DISK 等 7 种不同的存储级别 ， 而存储级别是以下 5 个变量的组合：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">StorageLevel</span> <span class="title">private</span>(<span class="params"></span></span></span><br><span class="line"><span class="class"><span class="params">  private var _useDisk: <span class="type">Boolean</span>, //磁盘</span></span></span><br><span class="line"><span class="class"><span class="params">  private var _useMemory: <span class="type">Boolean</span>, //这里其实是指堆内内存</span></span></span><br><span class="line"><span class="class"><span class="params">  private var _useOffHeap: <span class="type">Boolean</span>, //堆外内存</span></span></span><br><span class="line"><span class="class"><span class="params">  private var _deserialized: <span class="type">Boolean</span>, //是否为非序列化</span></span></span><br><span class="line"><span class="class"><span class="params">  private var _replication: <span class="type">Int</span> = 1 //副本个数</span></span></span><br><span class="line"><span class="class"><span class="params"></span>)</span></span><br></pre></td></tr></table></figure><table><thead><tr><th>存储级别</th><th>含义</th></tr></thead><tbody><tr><td>MEMORY_ONLY</td><td>以非序列化的 Java 对象的方式持久化在 JVM 内存中。如果内存无法完全存储 RDD 所有的 partition，那么那些没有持久化的 partition 就会在下一次需要使用它们的时候，重新被计算</td></tr><tr><td>MEMORY_AND_DISK</td><td>同上，但是当 RDD 某些 partition 无法存储在内存中时，会持久化到磁盘中。下次需要使用这些 partition 时，需要从磁盘上读取</td></tr><tr><td>MEMORY_ONLY_SER</td><td>同 MEMORY_ONLY，但是会使用 Java 序列化方式，将 Java 对象序列化后进行持久化。可以减少内存开销，但是需要进行反序列化，因此会加大 CPU 开销</td></tr><tr><td>MEMORY_AND_DISK_SER</td><td>同 MEMORY_AND_DISK，但是使用序列化方式持久化 Java 对象</td></tr><tr><td>DISK_ONLY</td><td>使用非序列化 Java 对象的方式持久化，完全存储到磁盘上</td></tr><tr><td>MEMORY_ONLY_2  MEMORY_AND_DISK_2</td><td>如果是尾部加了 2 的持久化级别，表示将持久化数据复用一份，保存到其他节点，从而在数据丢失时，不需要再次计算，只需要使用备份数据即可</td></tr></tbody></table><h3 id="RDD的缓存过程"><a href="#RDD的缓存过程" class="headerlink" title="RDD的缓存过程"></a>RDD的缓存过程</h3><p>RDD 在缓存到存储内存之前，数据项 [Record]的对象实例在逻辑上占用了 JVM 堆内内存的 other 部分的空间，同一 Partition 的不同数据项的存储空间并不连续。</p><p>缓存到存储内存之后， Partition 被转换成 Block，Record 在堆内或堆外存储内存中占用一块连续的空间。将 Partition 由不连续的存储空间转换为连续存储空间的过程，Spark 称之为”展开” [Unroll] </p><p>Block 有序列化和非序列化两种存储格式，具体以哪种方式取决于该 RDD 的存储级别。非序列化的 Block 用一个数组存储所有的对象实例<strong>，序列化的 Block 则用字节缓冲区 ByteBuffer 来存储二进制数据。</strong></p><p>每个 Executor 的 Storage 模块用一个 LinkedHashMap 来管理堆内和堆外存储内存中所有的 Block ，对这个 LinkedHashMap 新增和删除间接记录了内存的申请和释放。</p><p>因为不能保证存储空间可以一次容纳 Iterator 中的所有数据，当前的计算任务在 Unroll 时要向 MemoryManager 申请足够的 Unroll 空间来临时占位，空间不足则 Unroll 失败，空间足够时可以继续进行。在静态内存管理时，Spark 在存储内存中专门划分了一块 Unroll 空间，其大小是固定的，统一内存管理时则没有对 Unroll 空间进行特别区分，对于序列化的 Partition ，其所需的 Unroll 空间可以直接累加计算，一次申请。</p><blockquote><p>对于序列化的 Partition，其所需的 Unroll 空间可以直接累加计算，一次申请。</p></blockquote><blockquote><p>对于非序列化的 Partition 则要在遍历 Record 的过程中依次申请，即每读取一条 Record，采样估算其所需的 Unroll 空间并进行申请，空间不足时可以中断，释放已占用的 Unroll 空间。</p></blockquote><p>如果最终 Unroll 成功， 当前 Partition 所占用的 Unroll 空间被转换为正常的缓存 RDD 的存储空间 </p><img src="/images/spark/30.png" alt="" style="zoom:90%;" /><h3 id="淘汰与落盘"><a href="#淘汰与落盘" class="headerlink" title="淘汰与落盘"></a>淘汰与落盘</h3><p><strong>由于同一个 Executor 的所有的计算任务共享有限的存储内存空间，当有新的 Block 需要缓存但是剩余空间不足且无法动态占用时，就要对 LinkedHashMap 中 的旧 Block 进行淘汰，而被淘汰的 Block 如果其存储级别中同时包含存储到磁盘的要求，则要对其进行落盘，否则直接删除该 Block。</strong></p><ul><li>被淘汰的旧 Block 要与新 Block 的 MemoryMode 相同，即同属于堆外或堆内内存</li><li>新旧 Block 不能属于同一个RDD，避免循环淘汰</li><li>旧 Block 所属 RDD 不能处于被读状态，避免引发一致性问题</li><li>遍历 LinkedHashMap中 Block，按照最近最少使用 LRU 的顺序淘汰，直到满足新 Block 所需的空间。 其中 LRU 是 LinkedHashMap 的特性。</li></ul><h3 id="执行内存管理"><a href="#执行内存管理" class="headerlink" title="执行内存管理"></a>执行内存管理</h3><p>执行内存主要用来存储任务在执行 Shuffle 时占用的内存，Shuffle 是按照一定规则对 RDD 数据重新分区的过程， Shuffle 的 Write 和 Read 两阶段对执行内存的使用.</p><h3 id="Shuffle-Write"><a href="#Shuffle-Write" class="headerlink" title="Shuffle Write"></a>Shuffle Write</h3><p>在 map 端会采用 ExternalSorter 进行外排，在内存中存储数据时主要占用堆内执行空间。</p><h3 id="Shuffle-Read"><a href="#Shuffle-Read" class="headerlink" title="Shuffle Read"></a>Shuffle Read</h3><ul><li>在对 reduce端的数据进行聚合时， 要将数据交给 Aggregator处理， 在内存中存储数据时占用堆内执行空间。</li><li>如果需要进行最终结果排序，则要将再次将数据交给 ExternalSorter处理，占用堆内执行空间</li></ul><p>在 ExternalSorter 和 Aggregator 中， Spark 会使用一种叫 AppendOnlyMap 的哈希表在堆内执行内存中存储数据 ， 但在 Shuffle 过程中所有数据并不能都保存到该哈希表中， 当这个哈希表占用的内存会进行周期性地采样估算， 当其大到一定程度， 无法再从 MemoryManager 申请到新的执行内存时， Spark 就会将其全部内容存储到磁盘文件中， 这 个过程被称为溢存 [Spill] ， 溢存到磁盘的文件最后会被归 并 [Merge] </p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>Spark 的存储内存和执行内存有着截然不同的管理方式</p><ul><li><p>对于存储内存来说，<strong>Spark</strong> 用一个 **LinkedHashMap **来集中管理所有的 Block，Block 由需要缓存的 RDD 的 Partition 转化而成；</p></li><li><p>对于执行内存，Spark 用 <strong>AppendOnlyMap</strong> 来存储 Shuffle 过程中的数据， 在 Tungsten 排序中甚至抽象成为页式内存管理，开辟了全新的 JVM 内存管理机制 。</p></li></ul><h2 id="Spark-Shuffle-内存使用"><a href="#Spark-Shuffle-内存使用" class="headerlink" title="Spark Shuffle 内存使用"></a>Spark Shuffle 内存使用</h2><p>在使用 Spark 进行计算时，我们经常会碰到作业 (Job) Out Of Memory(OOM) 的情况，而且很大一部分情况是发生在 Shuffle 阶段。那么在 Spark Shuffle 中具体是哪些地方会使用比较多的内存而有可能导致 OOM 呢？ 为此，本文将围绕以上问题梳理 Spark 内存管理和 Shuffle 过程中与内存使用相关的知识；然后，简要分析下在 Spark Shuffle 中有可能导致 OOM 的原因。</p><h2 id="OOM"><a href="#OOM" class="headerlink" title="OOM"></a>OOM</h2><p>内存不够，数据太多就会抛出 OOM的 Exeception，主要有driver OOM和 executor OOM两种</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">java</span><span class="selector-class">.lang</span><span class="selector-class">.OutOfMemoryError</span>: <span class="selector-tag">Java</span> <span class="selector-tag">heap</span> <span class="selector-tag">space</span></span><br></pre></td></tr></table></figure><h3 id="driver-OOM"><a href="#driver-OOM" class="headerlink" title="driver OOM"></a><strong>driver OOM</strong></h3><ul><li><strong>用户在 Driver 端口生成大对象, 比如创建了一个大的集合数据结构</strong></li><li><strong>使用了collect 等操作，将所有 executor 的数据聚合到 driver 导致</strong></li></ul><p>一般是使用了collect 等操作，将所有 executor 的数据聚合到 driver 导致。尽量不要使用 collect操作即可。</p><h3 id="executor-OOM"><a href="#executor-OOM" class="headerlink" title="executor OOM"></a><strong>executor OOM</strong></h3><h3 id="数据倾斜导致内存溢出"><a href="#数据倾斜导致内存溢出" class="headerlink" title="数据倾斜导致内存溢出"></a><strong>数据倾斜导致内存溢出</strong></h3><p>数据不平衡除了有可能导致内存溢出外，也有可能导致性能的问题，调用 repartition 重新分区</p><h3 id="Reduce-OOM"><a href="#Reduce-OOM" class="headerlink" title="Reduce OOM"></a>Reduce OOM</h3><p>reduce task 去 map 端获取数据，reduce一边拉取数据一边聚合，reduce端有一块聚合内存[executor memory * 0.2],也就是这块内存不够<br><strong>解决方法</strong></p><ul><li>增加 reduce 聚合操作的内存的比例</li><li>增加 Executor memory 的大小 <strong>–executor-memory 5G</strong></li><li>减少 reduce task 每次拉取的数据量 设置 spak.reducer.maxSizeInFlight 24m, 拉取的次数就多了，因此建立连接的次数增多，有可能会连接不上[正好赶上 map task 端进行GC]</li></ul><h3 id="shuffle-后内存溢出"><a href="#shuffle-后内存溢出" class="headerlink" title="shuffle 后内存溢出"></a><strong>shuffle 后内存溢出</strong></h3><p> shuffle 后单个文件过大导致内存溢出。在 Spark 中，join，reduceByKey 这一类型的过程，都会有shuffle的过程，在shuffle的使用，需要传入一个partitioner，大部分 Spark 中的 shuffle 操作，默认的 partitioner 都是 HashPatitioner，默认值是父 RDD 中最大的分区数,这个参数通过spark.default.parallelism 控制 [在spark-sql中用spark.sql.shuffle.partitions] </p><p>spark.default.parallelism 参数只对 HashPartitioner 有效，所以如果是别的 Partitioner 或者自己实现的 Partitioner 就不能使用 spark.default.parallelism 这个参数来控制 shuffle 的并发量了。如果是别的partitioner 导致的 shuffle 内存溢出，就需要从 partitioner 的代码增加 partitions 的数量</p><h3 id="coalesce-调用导致内存溢出"><a href="#coalesce-调用导致内存溢出" class="headerlink" title="coalesce 调用导致内存溢出"></a><strong>coalesce 调用导致内存溢出</strong></h3><p>因为 hdfs 中不适合存小问题，所以 Spark 计算后如果产生的文件太小，调用 coalesce 合并文件再存入 hdfs中。但会导致一个问题，例如在 coalesce 之前有100个文件，这也意味着能够有100个 Task，现在调用coalesce(10)，最后只产生10个文件，因为 coalesce 并不是 shuffle 操作，这意味着 coalesce并不是先执行100个 Task，再将 Task 的执行结果合并成10个，而是从头到位只有10个 Task 在执行，原本100个文件是分开执行的，现在每个 Task 同时一次读取10个文件，使用的内存是原来的10倍，这导致了OOM。</p><p>解决这个问题的方法是令程序按照我们想的先执行100个 Task 再将结果合并成10个文件，这个问题同样可以通过repartition 解决，调用 repartition(10)</p><h3 id="standalone-模式下资源分配不均匀导致内存溢出"><a href="#standalone-模式下资源分配不均匀导致内存溢出" class="headerlink" title="standalone 模式下资源分配不均匀导致内存溢出"></a><strong>standalone 模式下资源分配不均匀导致内存溢出</strong></h3><p>在 standalone 的模式下如果配置了 –total-executor-cores 和 –executor-memory 这两个参数，但是没有配置 –executor-cores 参数，有可能导致，每个 Executor 的 memory 是一样的，但是 cores 的数量不同，那么在 cores 数量多的 Executor 中，由于能够同时执行多个Task，就容易导致内存溢出的情况。</p><p>这种情况的解决方法就是同时配置–executor-cores或者spark.executor.cores参数，确保Executor资源分配均匀。</p><h3 id="map-过程产生大量对象导致内存溢出"><a href="#map-过程产生大量对象导致内存溢出" class="headerlink" title="map 过程产生大量对象导致内存溢出"></a><strong>map 过程产生大量对象导致内存溢出</strong></h3><p>这种溢出的原因是在单个 map 中产生了大量的对象导致的</p><p>例如：rdd.map(x=&gt;for(i &lt;- 1 to 10000) yield i.toString)，这个操作在rdd中，每个对象都产生了10000 个对象，这肯定很容易产生内存溢出的问题。</p><p>针对这种问题，在不增加内存的情况下，可以通过减少每个 Task 的大小，以便达到每个 Task 即使产生大量的对象 Executor 的内存也能够装得下。具体做法可以在会产生大量对象的 map 操作之前调用 repartition方法，分区成更小的块传入map。</p><p>例如：rdd.repartition(10000).map(x=&gt;for(i &lt;- 1 to 10000) yield i.toString)</p><h2 id="参数"><a href="#参数" class="headerlink" title="参数"></a><strong>参数</strong></h2><h3 id="spark-driver-memory"><a href="#spark-driver-memory" class="headerlink" title="spark.driver.memory"></a>spark.driver.memory</h3><p>用来设置 Driver 的内存。在 Spark 程序中，SparkContext，DAGScheduler 都是运行在Driver端的。对应Stage 切分也是在 Driver 端运行，如果用户自己写的程序有过多的步骤，切分出过多的 Stage，这部分信息消耗的是 Driver 的内存，这个时候就需要调大 Driver 的内存</p>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Spark 存储模块</title>
      <link href="2019/11/22/Spark%20%E5%AD%98%E5%82%A8%E6%A8%A1%E5%9D%97/"/>
      <url>2019/11/22/Spark%20%E5%AD%98%E5%82%A8%E6%A8%A1%E5%9D%97/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p>在执行 Spark 的应用程序时，Spark 集群会启动 Driver 和 Executor 两种 JVM 进程，前者为主控进程，负责创建 Spark 上下文，提交 Spark 作业[Job]，并将作业转化为计算任务[Task]，在各个 Executor 进程间协调任务的调度；后者负责在工作节点上执行具体的计算任务，并将结果返回给 Driver， 同时为需要持久化的 RDD 提供存储功能。由于 Driver 的内存管理相对来说较为简单，本节主要对 Executor 的内存管理进行分析，下文中的 Spark 内存均特指 Executor 的内存。</p><a id="more"></a><h2 id="Execuor-内存模型"><a href="#Execuor-内存模型" class="headerlink" title="Execuor 内存模型"></a>Execuor 内存模型</h2><h3 id="堆内和堆外内存"><a href="#堆内和堆外内存" class="headerlink" title="堆内和堆外内存"></a>堆内和堆外内存</h3><p>作为一个 JVM 进程，Executor 的内存管理建立在 JVM 的内存管理之上，Spark 对 JVM 的堆内 [On-heap]空间进行了更为详细的分配，以充分利用内存。</p><p>同时，Spark 引入了堆外[Off-heap]内存，使之可以直接在工作节点的系统内存中开辟空间，进一步优化了内存的使用。</p><p><strong>堆内内存受到 JVM 统一管理，堆外内存是直接向操作系统进行内存的申请和释放。</strong></p><h3 id="堆内内存"><a href="#堆内内存" class="headerlink" title="堆内内存"></a>堆内内存</h3><p>堆内内存的大小，由 Spark 应用程序启动时的 –executor-memory 或 spark.executor.memory 参数配置。 </p><p>Executor 内运行的并发任务共享 JVM 堆内内存， Spark 对堆内内存的管理是一种逻辑上的“规划式”的管理，Executor 端的堆内内存区域在逻辑上被划分为以下四个区域。</p><ol><li><p>执行内存 (Execution Memory) : 主要用于存放 Shuffle、Join、Sort、Aggregation 等计算过程中的临时数据</p></li><li><p>存储内存 (Storage Memory) : 主要用于存储 spark 的 cache 数据，例如 RDD 的缓存、unroll 数据</p><p>主要用于存储 spark 的 cache 数据，例如 RDD 的缓存、广播（Broadcast）数据、和 unroll 数据。内存占比为 UsableMemory * spark.memory.fraction * spark.memory.storageFraction，Spark 2+ 中，默认初始状态下 Storage Memory 和 Execution Memory 均约占系统总内存的30%（1 * 0.6 * 0.5 = 0.3）。</p></li><li><p>用户内存（User Memory）: 主要用于存储 RDD 转换操作所需要的数据，例如 RDD 依赖等信息；</p></li><li><p>预留内存（Reserved Memory）: 系统预留内存，会用来存储 Spark 内部对象。</p><p>系统预留内存，用来存储 Spark 内部对象。其大小在代码中是写死的，其值等于 300MB，这个值是不能修改的</p></li></ol><h3 id="堆外内存"><a href="#堆外内存" class="headerlink" title="堆外内存"></a>堆外内存</h3><p>Spark 对于堆内内存的清理无法准确指定时间点，因此无法实现精确的释放。<strong>为了进一步优化内存的使用 Spark 引入了堆外内存，使之可以直接在工作节点的系统内存中开辟空间，存储经过序列化的二进制数据。</strong>由于内存的申请和释放不再通过 JVM 机制，而是直接向操作系统申请，减少了不必要的内存开销，以及频繁的 GC 扫描和回收，提升了处理性能。而且序列化的数据占用的空间可以被精确计算，所以相比堆内内存来说，堆外内存可以被精降低了管理的难度，也降低了误差。 </p><p>在默认情况下堆外内存并不启用，可通过配置 spark.memory.offHeap.enabled 参数启用，并由 spark.memory.offHeap.size 参数设定堆外空间的大小。除了没有 other 空间，堆外内存与堆内内存的划分方式相同，所有运行中的并发任务共享存储内存和执行内存。</p><h2 id="内存空间分配"><a href="#内存空间分配" class="headerlink" title="内存空间分配"></a>内存空间分配</h2><h3 id="静态内存管理"><a href="#静态内存管理" class="headerlink" title="静态内存管理"></a>静态内存管理</h3><p>在 Spark 最初采用的静态内存管理机制下，**[存储内存]<strong>、</strong>[执行内存]<strong>和</strong>[其他内存]**的大小在 Spark 应用程序运行期间均为固定的，但用户可以应用程序启动前进行配置。</p><img src="../images/spark/009.png" alt="" style="zoom:90%;" /><ul><li><p>可用的存储内存</p><p> systemMaxMemory * spark.storage.memoryFraction * spark.storage.safetyFraction</p></li><li><p>可用的执行内存</p><p>systemMaxMemory * spark.shuffle.memoryFraction * spark.shuffle.safetyFraction</p></li></ul><p>其中 systemMaxMemory 取决于当前 JVM 堆内内存的大小，最后可用的执行内存或者存储内存要在此基础上与各自的 memoryFraction 参数和 safetyFraction 参数相乘得出。 </p><p>上述计算公式中的两个 safetyFraction 参数，其意义在于在逻辑上预留出 [1-safetyFraction] 这么一块保险区域，降低因实际内存超出当前预设范围而导致 OOM 的风险。</p><p>值得注意的是，这个预留的保险区域仅仅是一种逻辑上的规划，在具体使用时 Spark 并没有区别对待，和”其它内存”一样交给了 JVM 去管理。</p><p><strong>Storage 内存和 Execution 内存都有预留空间，目的是防止 OOM ，因为 Spark 堆内内存大小的记录是不准确的，需要留出保险区域。</strong></p><p>堆外的空间分配较为简单，只有存储内存和执行内存。可用的执行内存和存储内存占用的空间大小直接由参数 spark.memory.storageFraction 决定，由于堆外内存占用的空间可以被精确计算，所以无需再设定保险区域 </p><img src="../images/spark/12.png" alt="" style="zoom:90%;" /><p>静态内存管理机制实现起来较为简单，但如果用户不熟悉 Spark 的存储机制，或没有根据具体的数据规模和计算任务或做相应的配置，很容易造成存储内存和执行内存中的一方剩余大量的空间，而另一方却早早被占满，不得不淘汰或移出旧的内容以存储新的内容。由于新的内存管理机制的出现，这种方式目前已经很少有开发者使用，出于兼容旧版本的应用程序的目的，Spark 仍然保留了它的实现。</p><h3 id="统一内存管理"><a href="#统一内存管理" class="headerlink" title="统一内存管理"></a>统一内存管理</h3><p><strong>Spark1.6 之后引入的统一内存管理机制，与静态内存管理的区别在于<font color='blue'>存储内存和执行内存共享同一块空间，可以动态占用对方的空闲区域</font></strong> </p><img src="../images/spark/10.png" alt="" style="zoom:90%;" /><p>其中最重要的优化在于动态占用机制， 其规则如下：</p><img src="../images/spark/20.png" alt="" style="zoom:90%;" /><ul><li><p>设定基本的存储内存和执行内存区域（spark.storage.storageFraction参数），该设定确定了双方各自拥有的空间的范围；</p></li><li><p>双方的空间都不足时，则存储到硬盘</p><p>若己方空间不足而对方空余时，可借用对方的空间; [注：存储空间不足是指不足以放下一个完整的Block]</p></li><li><p>执行内存的空间被对方占用后，可让对方将占用的部分转存到磁盘，然后”归还”借用的空间；</p></li><li><p>存储内存的空间被对方占用后，无法让对方 “归还”，因为需要考虑 Shuffle 过程中的很多因素，实现起来较为复杂。</p><p>Storage 内存的空间被对方占用后，目前的实现是无法让对方”归还”，因为需要考虑 Shuffle 过程中的很多因素，实现起来较为复杂；而且 Shuffle 过程产生的文件在后面一定会被使用到，而 Cache 在内存的数据不一定在后面使用。在 <a href="http://www.linuxprobe.com/wp-content/uploads/2017/04/unified-memory-management-spark-10000.pdf">Unified Memory Management in Spark 1.6</a> 中详细讲解了为何选择这种策略，简单总结如下:</p><ul><li>数据清除的开销 : 驱逐 storage 内存的开销取决于 storage level，MEMORY_ONLY 可能是最昂贵的，因为需要重新计算，MEMORY_AND_DISK_SER 正好相反，只涉及到磁盘IO。溢写 execution 内存到磁盘的开销并不昂贵，因为 execution 存储的数据格式紧凑(compact format)，序列化开销低。并且，清除的 storage 内存可能不会被用到，但是，可以预见的是，驱逐的 execution 内存是必然会再被读到内存的，频繁的驱除重读 execution 内存将导致昂贵的开销。</li><li>实现的复杂度 : storage 内存的驱逐是容易实现的，只需要使用已有的方法，drop 掉 block。execution 则复杂的多，首先，execution 以 page 为单位管理这部分内存，并且确保相应的操作至少有 one page ，如果把这 one page 内存驱逐了，对应的操作就会处于饥饿状态。此外，还需要考虑 execution 内存被驱逐的情况下，等待 cache 的 block 如何处理。</li></ul></li></ul><h2 id="存储内存管理"><a href="#存储内存管理" class="headerlink" title="存储内存管理"></a>存储内存管理</h2><h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><p>Storage 模块在逻辑上以 Block 为基本存储单位，RDD 的每个 Partition 经过处理后唯一对应一个 Block。</p><p>BlockManager 是 整个 Spark 底层负责数据存储与管理的一个组件 ， Driver 和 Executor 的所有数据都由对应的 BlockManager 进行管理。</p><p>Driver 端 BlockManager 负责整个 Spark 应用程序的 Block 的元数据信息的管理和维护，而 Executor 端的 BlockManager 需要将 Block 的更新等状态上报到 Master，同时接收 Master 的命令， 例如新增或删除一个 RDD。</p><h3 id="RDD-的持久化机制"><a href="#RDD-的持久化机制" class="headerlink" title="RDD 的持久化机制"></a>RDD 的持久化机制</h3><p>Task 在启动之初读取一个分区时，会先判断这个分区是否已经被持久化，如果没有则需要检查 Checkpoint 或按照血统重新计算。所以如果一个 RDD 要执行多次 action 操作， 可以在第一次 action 操作中使用 persist 或 cache 方法，在内存或磁盘中持久化或缓存这个 RDD，从而在后面的行动时提升计算速度。</p><blockquote><p>其中 cache 这个方法是个 Tranformation ,当第一次遇到 action 算子的时才会进行持久化</p><p>cache 内部调用了 persist(StorageLevel.MEMORY_ONLY)方法，所以执行 cache 算子其实就是执行了 persist 算子且持久化级别为 MEMORY_ONLY。 故缓存是一种特殊的持久化。堆内和堆外存储内存的设计，便可以对缓存 RDD 时使用的内存做统一的规划和管理。</p></blockquote><p>RDD 的持久化由 Spark 的 Storage 模块负责，实现了 RDD 与物理存储的解耦合。 </p><p>在对 RDD 持久化时，Spark 规定了 MEMORY_ONLY 、MEMORY_AND_DISK 等 7 种不同的存储级别 ， 而存储级别是以下 5 个变量的组合：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">StorageLevel</span> <span class="title">private</span>(<span class="params"></span></span></span><br><span class="line"><span class="class"><span class="params">  private var _useDisk: <span class="type">Boolean</span>, //磁盘</span></span></span><br><span class="line"><span class="class"><span class="params">  private var _useMemory: <span class="type">Boolean</span>, //这里其实是指堆内内存</span></span></span><br><span class="line"><span class="class"><span class="params">  private var _useOffHeap: <span class="type">Boolean</span>, //堆外内存</span></span></span><br><span class="line"><span class="class"><span class="params">  private var _deserialized: <span class="type">Boolean</span>, //是否为非序列化</span></span></span><br><span class="line"><span class="class"><span class="params">  private var _replication: <span class="type">Int</span> = 1 //副本个数</span></span></span><br><span class="line"><span class="class"><span class="params"></span>)</span></span><br></pre></td></tr></table></figure><table><thead><tr><th>存储级别</th><th>含义</th></tr></thead><tbody><tr><td>MEMORY_ONLY</td><td>以非序列化的 Java 对象的方式持久化在 JVM 内存中。如果内存无法完全存储 RDD 所有的 partition，那么那些没有持久化的 partition 就会在下一次需要使用它们的时候，重新被计算</td></tr><tr><td>MEMORY_AND_DISK</td><td>同上，但是当 RDD 某些 partition 无法存储在内存中时，会持久化到磁盘中。下次需要使用这些 partition 时，需要从磁盘上读取</td></tr><tr><td>MEMORY_ONLY_SER</td><td>同 MEMORY_ONLY，但是会使用 Java 序列化方式，将 Java 对象序列化后进行持久化。可以减少内存开销，但是需要进行反序列化，因此会加大 CPU 开销</td></tr><tr><td>MEMORY_AND_DISK_SER</td><td>同 MEMORY_AND_DISK，但是使用序列化方式持久化 Java 对象</td></tr><tr><td>DISK_ONLY</td><td>使用非序列化 Java 对象的方式持久化，完全存储到磁盘上</td></tr><tr><td>MEMORY_ONLY_2  MEMORY_AND_DISK_2</td><td>如果是尾部加了 2 的持久化级别，表示将持久化数据复用一份，保存到其他节点，从而在数据丢失时，不需要再次计算，只需要使用备份数据即可</td></tr></tbody></table><h3 id="RDD的缓存过程"><a href="#RDD的缓存过程" class="headerlink" title="RDD的缓存过程"></a>RDD的缓存过程</h3><p>RDD 在缓存之前，数据项 [Record]的对象实例在逻辑上占用了 JVM 堆内内存的 other 部分的空间，同一Partition 的不同数据项的存储空间并不连续。缓存到存储内存之后， Partition 被转换成 Block，Record 在堆内或堆外存储内存中占用一块连续的空间。将 Partition 由不连续的存储空间转换为连续存储空间的过程，Spark 称之为”展开” [Unroll] </p><p>Block 有序列化和非序列化两种存储格式，具体以哪种方式取决于该 RDD 的存储级别。非序列化的 Block 用一个数组存储所有的对象实例<strong>，序列化的 Block 则用字节缓冲区 ByteBuffer 来存储二进制数据。</strong></p><p>每个 Executor 的 Storage 模块用一个 LinkedHashMap 来管理堆内和堆外存储内存中所有的 Block ，对这个 LinkedHashMap 新增和删除间接记录了内存的申请和释放。</p><p>因为不能保证存储空间可以一次容纳 Iterator 中的所有数据，当前的计算任务在 Unroll 时要向 MemoryManager 申请足够的 Unroll 空间来临时占位，空间不足则 Unroll 失败，空间足够时可以继续进行。在静态内存管理时，Spark 在存储内存中专门划分了一块 Unroll 空间，其大小是固定的，统一内存管理时则没有对 Unroll 空间进行特别区分，对于序列化的 Partition ，其所需的 Unroll 空间可以直接累加计算，一次申请。</p><blockquote><p>对于序列化的 Partition，其所需的 Unroll 空间可以直接累加计算，一次申请。</p></blockquote><blockquote><p>对于非序列化的 Partition 则要在遍历 Record 的过程中依次申请，即每读取一条 Record，采样估算其所需的 Unroll 空间并进行申请，空间不足时可以中断，释放已占用的 Unroll 空间。</p></blockquote><p>如果最终 Unroll 成功， 当前 Partition 所占用的 Unroll 空间被转换为正常的缓存 RDD 的存储空间 </p><img src="../images/spark/30.png" alt="" style="zoom:50%;" /><h3 id="淘汰与落盘"><a href="#淘汰与落盘" class="headerlink" title="淘汰与落盘"></a>淘汰与落盘</h3><p><strong>由于同一个 Executor 的所有的计算任务共享有限的存储内存空间，当有新的 Block 需要缓存但是剩余空间不足且无法动态占用时，就要对 LinkedHashMap 中 的旧 Block 进行淘汰，而被淘汰的 Block 如果其存储级别中同时包含存储到磁盘的要求，则要对其进行落盘，否则直接删除该 Block。</strong></p><ul><li>被淘汰的旧 Block 要与新 Block 的 MemoryMode 相同，即同属于堆外或堆内内存</li><li>新旧 Block 不能属于同一个RDD，避免循环淘汰</li><li>旧 Block 所属 RDD 不能处于被读状态，避免引发一致性问题</li><li>遍历 LinkedHashMap中 Block，按照最近最少使用 LRU 的顺序淘汰，直到满足新 Block 所需的空间。 其中 LRU 是 LinkedHashMap 的特性。</li></ul><h3 id="执行内存管理"><a href="#执行内存管理" class="headerlink" title="执行内存管理"></a>执行内存管理</h3><p>执行内存主要用来存储任务在执行 Shuffle 时占用的内存，Shuffle 是按照一定规则对 RDD 数据重新分区的过程， Shuffle 的 Write 和 Read 两阶段对执行内存的使用.</p><h3 id="Shuffle-Write"><a href="#Shuffle-Write" class="headerlink" title="Shuffle Write"></a>Shuffle Write</h3><p>在 map 端会采用 ExternalSorter 进行外排，在内存中存储数据时主要占用堆内执行空间。</p><h3 id="Shuffle-Read"><a href="#Shuffle-Read" class="headerlink" title="Shuffle Read"></a>Shuffle Read</h3><ul><li>在对 reduce端的数据进行聚合时， 要将数据交给 Aggregator处理， 在内存中存储数据时占用堆内执行空间。</li><li>如果需要进行最终结果排序，则要将再次将数据交给 ExternalSorter处理，占用堆内执行空间</li></ul><p>在 ExternalSorter 和 Aggregator 中， Spark 会使用一种叫 AppendOnlyMap 的哈希表在堆内执行内存中存储数据 ， 但在 Shuffle 过程中所有数据并不能都保存到该哈希表中， 当这个哈希表占用的内存会进行周期性地采样估算， 当其大到一定程度， 无法再从 MemoryManager 申请到新的执行内存时， Spark 就会将其全部内容存储到磁盘文件中， 这 个过程被称为溢存 [Spill] ， 溢存到磁盘的文件最后会被归 并 [Merge] </p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>Spark 的存储内存和执行内存有着截然不同的管理方式</p><ul><li><p>对于存储内存来说，<strong>Spark</strong> 用一个 **LinkedHashMap **来集中管理所有的 Block，Block 由需要缓存的 RDD 的 Partition 转化而成；</p></li><li><p>对于执行内存，Spark 用 <strong>AppendOnlyMap</strong> 来存储 Shuffle 过程中的数据， 在 Tungsten 排序中甚至抽象成为页式内存管理，开辟了全新的 JVM 内存管理机制 。</p></li></ul><h2 id="Spark-Shuffle-内存使用"><a href="#Spark-Shuffle-内存使用" class="headerlink" title="Spark Shuffle 内存使用"></a>Spark Shuffle 内存使用</h2><p>在使用 Spark 进行计算时，我们经常会碰到作业 (Job) Out Of Memory(OOM) 的情况，而且很大一部分情况是发生在 Shuffle 阶段。那么在 Spark Shuffle 中具体是哪些地方会使用比较多的内存而有可能导致 OOM 呢？ 为此，本文将围绕以上问题梳理 Spark 内存管理和 Shuffle 过程中与内存使用相关的知识；然后，简要分析下在 Spark Shuffle 中有可能导致 OOM 的原因。</p><h2 id="OOM"><a href="#OOM" class="headerlink" title="OOM"></a>OOM</h2><p>内存不够，数据太多就会抛出 OOM 的 Exeception，主要有 driver OOM 和 executor OOM两种</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">java</span><span class="selector-class">.lang</span><span class="selector-class">.OutOfMemoryError</span>: <span class="selector-tag">Java</span> <span class="selector-tag">heap</span> <span class="selector-tag">space</span></span><br></pre></td></tr></table></figure><h3 id="driver-OOM"><a href="#driver-OOM" class="headerlink" title="driver OOM"></a><strong>driver OOM</strong></h3><ul><li><strong>用户在 Driver 端口生成大对象, 比如创建了一个大的集合数据结构</strong></li><li><strong>使用了collect 等操作，将所有 executor 的数据聚合到 driver 导致</strong></li></ul><p>一般是使用了collect 等操作，将所有 executor 的数据聚合到 driver 导致。尽量不要使用 collect 操作即可。</p><h3 id="executor-OOM"><a href="#executor-OOM" class="headerlink" title="executor OOM"></a><strong>executor OOM</strong></h3><h3 id="数据倾斜导致内存溢出"><a href="#数据倾斜导致内存溢出" class="headerlink" title="数据倾斜导致内存溢出"></a><strong>数据倾斜导致内存溢出</strong></h3><p>数据不平衡除了有可能导致内存溢出外，也有可能导致性能的问题，调用 repartition 重新分区</p><h3 id="Reduce-OOM"><a href="#Reduce-OOM" class="headerlink" title="Reduce OOM"></a>Reduce OOM</h3><p>reduce task 去 map 端获取数据，reduce一边拉取数据一边聚合，reduce端有一块聚合内存[executor memory * 0.2],也就是这块内存不够<br><strong>解决方法</strong></p><ul><li>增加 reduce 聚合操作的内存的比例</li><li>增加 Executor memory 的大小 <strong>–executor-memory 5G</strong></li><li>减少 reduce task 每次拉取的数据量 设置 spak.reducer.maxSizeInFlight 24m, 拉取的次数就多了，因此建立连接的次数增多，有可能会连接不上[正好赶上 map task 端进行GC]</li></ul><h3 id="shuffle-后内存溢出"><a href="#shuffle-后内存溢出" class="headerlink" title="shuffle 后内存溢出"></a><strong>shuffle 后内存溢出</strong></h3><p>shuffle 后单个文件过大导致内存溢出。在 Spark 中，join，reduceByKey 这一类型的过程，都会有shuffle 的过程，在 shuffle 的使用，需要传入一个 partitioner，大部分 Spark 中的 shuffle 操作，默认的 partitioner 都是 HashPatitioner，默认值是父 RDD 中最大的分区数,这个参数通过spark.default.parallelism 控制 [在spark-sql中用spark.sql.shuffle.partitions] </p><p>spark.default.parallelism 参数只对 HashPartitioner 有效，所以如果是别的 Partitioner 或者自己实现的 Partitioner 就不能使用 spark.default.parallelism 这个参数来控制 shuffle 的并发量了。如果是别的partitioner 导致的 shuffle 内存溢出，就需要从 partitioner 的代码增加 partitions 的数量</p><h3 id="coalesce-调用导致内存溢出"><a href="#coalesce-调用导致内存溢出" class="headerlink" title="coalesce 调用导致内存溢出"></a><strong>coalesce 调用导致内存溢出</strong></h3><p>因为 hdfs 中不适合存小问题，所以 Spark 计算后如果产生的文件太小，调用 coalesce 合并文件再存入 hdfs中。但会导致一个问题，例如在 coalesce 之前有100个文件，这也意味着能够有100个 Task，现在调用coalesce(10)，最后只产生10个文件，因为 coalesce 并不是 shuffle 操作，这意味着 coalesce并不是先执行100个 Task，再将 Task 的执行结果合并成10个，而是从头到位只有10个 Task 在执行，原本100个文件是分开执行的，现在每个 Task 同时一次读取10个文件，使用的内存是原来的10倍，这导致了OOM。</p><p>解决这个问题的方法是令程序按照我们想的先执行100个 Task 再将结果合并成10个文件，这个问题同样可以通过repartition 解决，调用 repartition(10)</p><h3 id="standalone-模式下资源分配不均匀导致内存溢出"><a href="#standalone-模式下资源分配不均匀导致内存溢出" class="headerlink" title="standalone 模式下资源分配不均匀导致内存溢出"></a><strong>standalone 模式下资源分配不均匀导致内存溢出</strong></h3><p>在 standalone 的模式下如果配置了 –total-executor-cores 和 –executor-memory 这两个参数，但是没有配置 –executor-cores 参数，有可能导致，每个 Executor 的 memory 是一样的，但是 cores 的数量不同，那么在 cores 数量多的 Executor 中，由于能够同时执行多个Task，就容易导致内存溢出的情况。</p><p>这种情况的解决方法就是同时配置–executor-cores或者spark.executor.cores参数，确保Executor资源分配均匀。</p><h3 id="map-过程产生大量对象导致内存溢出"><a href="#map-过程产生大量对象导致内存溢出" class="headerlink" title="map 过程产生大量对象导致内存溢出"></a><strong>map 过程产生大量对象导致内存溢出</strong></h3><p>这种溢出的原因是在单个 map 中产生了大量的对象导致的</p><p>例如：rdd.map(x=&gt;for(i &lt;- 1 to 10000) yield i.toString)，这个操作在rdd中，每个对象都产生了10000 个对象，这肯定很容易产生内存溢出的问题。</p><p>针对这种问题，在不增加内存的情况下，可以通过减少每个 Task 的大小，以便达到每个 Task 即使产生大量的对象 Executor 的内存也能够装得下。具体做法可以在会产生大量对象的 map 操作之前调用 repartition方法，分区成更小的块传入map。</p><p>例如：rdd.repartition(10000).map(x=&gt;for(i &lt;- 1 to 10000) yield i.toString)</p><h2 id="参数"><a href="#参数" class="headerlink" title="参数"></a><strong>参数</strong></h2><h3 id="spark-driver-memory"><a href="#spark-driver-memory" class="headerlink" title="spark.driver.memory"></a>spark.driver.memory</h3><p>用来设置 Driver 的内存。在 Spark 程序中，SparkContext，DAGScheduler 都是运行在Driver端的。对应Stage 切分也是在 Driver 端运行，如果用户自己写的程序有过多的步骤，切分出过多的 Stage，这部分信息消耗的是 Driver 的内存，这个时候就需要调大 Driver 的内存</p>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Spark 存储模块</title>
      <link href="2019/11/22/Spark%20%E5%AE%B9%E9%94%99%E6%9C%BA%E5%88%B6/"/>
      <url>2019/11/22/Spark%20%E5%AE%B9%E9%94%99%E6%9C%BA%E5%88%B6/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p>在执行 Spark 的应用程序时，Spark 集群会启动 Driver 和 Executor 两种 JVM 进程，前者为主控进程，负责创建 Spark 上下文，提交 Spark 作业[Job]，并将作业转化为计算任务[Task]，在各个 Executor 进程间协调任务的调度；后者负责在工作节点上执行具体的计算任务，并将结果返回给 Driver， 同时为需要持久化的 RDD 提供存储功能。由于 Driver 的内存管理相对来说较为简单，本节主要对 Executor 的内存管理进行分析，下文中的 Spark 内存均特指 Executor 的内存。</p><a id="more"></a><h2 id="Execuor-内存模型"><a href="#Execuor-内存模型" class="headerlink" title="Execuor 内存模型"></a>Execuor 内存模型</h2><h3 id="堆内和堆外内存"><a href="#堆内和堆外内存" class="headerlink" title="堆内和堆外内存"></a>堆内和堆外内存</h3><p>作为一个 JVM 进程，Executor 的内存管理建立在 JVM 的内存管理之上，Spark 对 JVM 的堆内 [On-heap]空间进行了更为详细的分配，以充分利用内存。</p><p>同时，Spark 引入了堆外[Off-heap]内存，使之可以直接在工作节点的系统内存中开辟空间，进一步优化了内存的使用。</p><p><strong>堆内内存受到 JVM 统一管理，堆外内存是直接向操作系统进行内存的申请和释放。</strong></p><h3 id="堆内内存"><a href="#堆内内存" class="headerlink" title="堆内内存"></a>堆内内存</h3><p>堆内内存的大小，由 Spark 应用程序启动时的 –executor-memory 或 spark.executor.memory 参数配置。 </p><p>Executor 内运行的并发任务共享 JVM 堆内内存， 这些任务在缓存 RDD 数据和广播 Broadcast 数据时占用的内存被规划为存储 [Storage] 内存， 而这些任务在执行 Shuffle 时占用的内存被规划为执行 [Execution] 内存，剩余的部分不做特殊规划，Spark 内部的对象实例，或者用户定义的 Spark 应用程序中的对象实例，均占用剩余的空间 。不同的管理模式下， 这三部分占用的空间大小各不相同 </p><h3 id="堆外内存"><a href="#堆外内存" class="headerlink" title="堆外内存"></a>堆外内存</h3><p>JVM 对于内存的清理无法准确指定时间点，因此无法实现精确的释放。<strong>为了进一步优化内存的使用以及提高 Shuffle 时排序的效率，Spark 引入了堆外[Off-heap]内存，使之可以直接在工作节点的系统内存中开辟空间，存储经过序列化的二进制数据。</strong>由于内存的申请和释放不再通过 JVM 机制，而是直接向操作系统申请，减少了不必要的内存开销，以及频繁的 GC 扫描和回收，提升了处理性能。而且序列化的数据占用的空间可以被精确计算，所以相比堆内内存来说，堆外内存可以被精确地申请和释放，降低了管理的难度，也降低了误差 </p><p>在默认情况下堆外内存并不启用，可通过配置 spark.memory.offHeap.enabled 参数启用，并由 spark.memory.offHeap.size 参数设定堆外空间的大小。除了没有 other 空间，堆外内存与堆内内存的划分方式相同，所有运行中的并发任务共享存储内存和执行内存 。</p><h2 id="内存空间分配"><a href="#内存空间分配" class="headerlink" title="内存空间分配"></a>内存空间分配</h2><h3 id="静态内存管理"><a href="#静态内存管理" class="headerlink" title="静态内存管理"></a>静态内存管理</h3><p>在 Spark 最初采用的静态内存管理机制下，**[存储内存]<strong>、</strong>[执行内存]<strong>和</strong>[其他内存]**的大小在 Spark 应用程序运行期间均为固定的，但用户可以应用程序启动前进行配置。</p><img src="/images/spark/009.png" alt="" style="zoom:90%;" /><ul><li><p>可用的存储内存</p><p> systemMaxMemory * spark.storage.memoryFraction * spark.storage.safetyFraction</p></li><li><p>可用的执行内存</p><p>systemMaxMemory * spark.shuffle.memoryFraction * spark.shuffle.safetyFraction</p></li></ul><p>其中 systemMaxMemory 取决于当前 JVM 堆内内存的大小，最后可用的执行内存或者存储内存要在此基础上与各自的 memoryFraction 参数和 safetyFraction 参数相乘得出。 </p><p>上述计算公式中的两个 safetyFraction 参数，其意义在于在逻辑上预留出 [1-safetyFraction] 这么一块保险区域，降低因实际内存超出当前预设范围而导致 OOM 的风险。</p><p>值得注意的是，这个预留的保险区域仅仅是一种逻辑上的规划，在具体使用时 Spark 并没有区别对待，和”其它内存”一样交给了 JVM 去管理。</p><p><strong>Storage 内存和 Execution 内存都有预留空间，目的是防止 OOM ，因为 Spark 堆内内存大小的记录是不准确的，需要留出保险区域。</strong></p><p>堆外的空间分配较为简单，只有存储内存和执行内存。可用的执行内存和存储内存占用的空间大小直接由参数 spark.memory.storageFraction 决定，由于堆外内存占用的空间可以被精确计算，所以无需再设定保险区域 </p><img src="/images/spark/12.png" alt="" style="zoom:90%;" /><p>静态内存管理机制实现起来较为简单，但如果用户不熟悉 Spark 的存储机制，或没有根据具体的数据规模和计算任务或做相应的配置，很容易造成存储内存和执行内存中的一方剩余大量的空间，而另一方却早早被占满，不得不淘汰或移出旧的内容以存储新的内容。由于新的内存管理机制的出现，这种方式目前已经很少有开发者使用，出于兼容旧版本的应用程序的目的，Spark 仍然保留了它的实现。</p><h3 id="统一内存管理"><a href="#统一内存管理" class="headerlink" title="统一内存管理"></a>统一内存管理</h3><p><strong>Spark1.6 之后引入的统一内存管理机制，与静态内存管理的区别在于<font color='blue'>存储内存和执行内存共享同一块空间，可以动态占用对方的空闲区域</font></strong> </p><img src="/images/spark/10.png" alt="" style="zoom:90%;" /><p>其中最重要的优化在于动态占用机制， 其规则如下：</p><img src="/images/spark/20.png" alt="" style="zoom:90%;" /><ul><li><p>设定基本的存储内存和执行内存区域（spark.storage.storageFraction参数），该设定确定了双方各自拥有的空间的范围；</p></li><li><p>双方的空间都不足时，则存储到硬盘</p><p>若己方空间不足而对方空余时，可借用对方的空间; [注：存储空间不足是指不足以放下一个完整的Block]</p></li><li><p>执行内存的空间被对方占用后，可让对方将占用的部分转存到磁盘，然后”归还”借用的空间；</p></li><li><p>存储内存的空间被对方占用后，无法让对方 “归还”，因为需要考虑 Shuffle 过程中的很多因素，实现起来较为复杂。</p></li></ul><h2 id="存储内存管理"><a href="#存储内存管理" class="headerlink" title="存储内存管理"></a>存储内存管理</h2><h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><p>Storage 管理着 Spark 应用在运行过程中产生的各种数据。比如 RDD 缓存，shuffle 过程中缓存及写入磁盘的数据，广播变量等。</p><p>Storage 模块在逻辑上以 Block 为基本存储单位，RDD 的每个 Partition 经过处理后唯一对应一个 Block。Driver 端 BlockManager 负责整个 Spark 应用程序的 Block 的元数据信息的管理和维护，而 Executor 端的 BlockManager 需要将 Block 的更新等状态上报到 Master，同时接收 Master 的命令， 例如新增或删除一个 RDD。</p><ul><li><p>BlockManager</p><blockquote><p>BlockManager 是 整个 Spark 底层负责数据存储与管理的一个组件 ， Driver 和 Executor 的所有数据都由对应的 BlockManager 进行管理。</p><p>Driver 上有 BlockManager Master ，负责对各个节点上的 BlockManager 内部管理的数据的元数据进行维护， 比如 block 的增删改等操作， 都会在这里维护好元数据 的变更。</p><p>每个节点都有一个 BlockManager，每个 BlockManager 创建之后， 第一件事即使去向 BlockManag erMaster 进行注册。</p></blockquote></li><li><p>CacheManager</p><blockquote><p>CacheManager 管理 spark 的缓存，而缓存可以基于内存的缓存，也可以是基于磁盘的缓存；<br>CacheManager 需要通过 BlockManager 来操作数据</p></blockquote></li></ul><h3 id="RDD-的持久化机制"><a href="#RDD-的持久化机制" class="headerlink" title="RDD 的持久化机制"></a>RDD 的持久化机制</h3><p>弹性分布式数据集 RDD 作为 Spark 最根本的数据抽象，是只读的分区记录的集合，基于在稳定物理存储中的数据集上创建，或者在其他已有的 RDD 上执行转换[Transformation]操作产生一个新的 RDD。转换后的 RDD 与 原始的 RDD 之间产生的依赖关系构成了血统[Lineag]。凭借血统，Spark 保证了每一个 RDD 都可以被重新恢复。</p><p>Task 在启动之初读取一个分区时，会先判断这个分区是否已经被持久化，如果没有则需要检查 Checkpoint 或按照血统重新计算。所以如果一个 RDD 要执行多次 action 操作， 可以在第一次 action 操作中使用 persist 或 cache 方法，在内存或磁盘中持久化或缓存这个 RDD，从而在后面的行动时提升计算速度。</p><blockquote><p>其中 cache 这个方法是个 Tranformation ,当第一次遇到 action 算子的时才会进行持久化</p><p>cache 内部调用了 persist(StorageLevel.MEMORY_ONLY)方法，所以执行 cache 算子其实就是执行了 persist 算子且持久化级别为 MEMORY_ONLY。 故缓存是一种特殊的持久化。堆内和堆外存储内存的设计，便可以对缓存 RDD 时使用的内存做统一的规划和管理。</p></blockquote><p>RDD 的持久化由 Spark 的 Storage 模块负责，实现了 RDD 与物理存储的解耦合。 </p><p>在对 RDD 持久化时，Spark 规定了 MEMORY_ONLY 、MEMORY_AND_DISK 等 7 种不同的存储级别 ， 而存储级别是以下 5 个变量的组合：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">StorageLevel</span> <span class="title">private</span>(<span class="params"></span></span></span><br><span class="line"><span class="class"><span class="params">  private var _useDisk: <span class="type">Boolean</span>, //磁盘</span></span></span><br><span class="line"><span class="class"><span class="params">  private var _useMemory: <span class="type">Boolean</span>, //这里其实是指堆内内存</span></span></span><br><span class="line"><span class="class"><span class="params">  private var _useOffHeap: <span class="type">Boolean</span>, //堆外内存</span></span></span><br><span class="line"><span class="class"><span class="params">  private var _deserialized: <span class="type">Boolean</span>, //是否为非序列化</span></span></span><br><span class="line"><span class="class"><span class="params">  private var _replication: <span class="type">Int</span> = 1 //副本个数</span></span></span><br><span class="line"><span class="class"><span class="params"></span>)</span></span><br></pre></td></tr></table></figure><table><thead><tr><th>存储级别</th><th>含义</th></tr></thead><tbody><tr><td>MEMORY_ONLY</td><td>以非序列化的 Java 对象的方式持久化在 JVM 内存中。如果内存无法完全存储 RDD 所有的 partition，那么那些没有持久化的 partition 就会在下一次需要使用它们的时候，重新被计算</td></tr><tr><td>MEMORY_AND_DISK</td><td>同上，但是当 RDD 某些 partition 无法存储在内存中时，会持久化到磁盘中。下次需要使用这些 partition 时，需要从磁盘上读取</td></tr><tr><td>MEMORY_ONLY_SER</td><td>同 MEMORY_ONLY，但是会使用 Java 序列化方式，将 Java 对象序列化后进行持久化。可以减少内存开销，但是需要进行反序列化，因此会加大 CPU 开销</td></tr><tr><td>MEMORY_AND_DISK_SER</td><td>同 MEMORY_AND_DISK，但是使用序列化方式持久化 Java 对象</td></tr><tr><td>DISK_ONLY</td><td>使用非序列化 Java 对象的方式持久化，完全存储到磁盘上</td></tr><tr><td>MEMORY_ONLY_2  MEMORY_AND_DISK_2</td><td>如果是尾部加了 2 的持久化级别，表示将持久化数据复用一份，保存到其他节点，从而在数据丢失时，不需要再次计算，只需要使用备份数据即可</td></tr></tbody></table><h3 id="RDD的缓存过程"><a href="#RDD的缓存过程" class="headerlink" title="RDD的缓存过程"></a>RDD的缓存过程</h3><p>RDD 在缓存到存储内存之前，数据项 [Record]的对象实例在逻辑上占用了 JVM 堆内内存的 other 部分的空间，同一 Partition 的不同数据项的存储空间并不连续。</p><p>缓存到存储内存之后， Partition 被转换成 Block，Record 在堆内或堆外存储内存中占用一块连续的空间。将 Partition 由不连续的存储空间转换为连续存储空间的过程，Spark 称之为”展开” [Unroll] </p><p>Block 有序列化和非序列化两种存储格式，具体以哪种方式取决于该 RDD 的存储级别。非序列化的 Block 用一个数组存储所有的对象实例<strong>，序列化的 Block 则用字节缓冲区 ByteBuffer 来存储二进制数据。</strong></p><p>每个 Executor 的 Storage 模块用一个 LinkedHashMap 来管理堆内和堆外存储内存中所有的 Block ，对这个 LinkedHashMap 新增和删除间接记录了内存的申请和释放。</p><p>因为不能保证存储空间可以一次容纳 Iterator 中的所有数据，当前的计算任务在 Unroll 时要向 MemoryManager 申请足够的 Unroll 空间来临时占位，空间不足则 Unroll 失败，空间足够时可以继续进行。在静态内存管理时，Spark 在存储内存中专门划分了一块 Unroll 空间，其大小是固定的，统一内存管理时则没有对 Unroll 空间进行特别区分，对于序列化的 Partition ，其所需的 Unroll 空间可以直接累加计算，一次申请。</p><blockquote><p>对于序列化的 Partition，其所需的 Unroll 空间可以直接累加计算，一次申请。</p></blockquote><blockquote><p>对于非序列化的 Partition 则要在遍历 Record 的过程中依次申请，即每读取一条 Record，采样估算其所需的 Unroll 空间并进行申请，空间不足时可以中断，释放已占用的 Unroll 空间。</p></blockquote><p>如果最终 Unroll 成功， 当前 Partition 所占用的 Unroll 空间被转换为正常的缓存 RDD 的存储空间 </p><img src="/images/spark/30.png" alt="" style="zoom:90%;" /><h3 id="淘汰与落盘"><a href="#淘汰与落盘" class="headerlink" title="淘汰与落盘"></a>淘汰与落盘</h3><p><strong>由于同一个 Executor 的所有的计算任务共享有限的存储内存空间，当有新的 Block 需要缓存但是剩余空间不足且无法动态占用时，就要对 LinkedHashMap 中 的旧 Block 进行淘汰，而被淘汰的 Block 如果其存储级别中同时包含存储到磁盘的要求，则要对其进行落盘，否则直接删除该 Block。</strong></p><ul><li>被淘汰的旧 Block 要与新 Block 的 MemoryMode 相同，即同属于堆外或堆内内存</li><li>新旧 Block 不能属于同一个RDD，避免循环淘汰</li><li>旧 Block 所属 RDD 不能处于被读状态，避免引发一致性问题</li><li>遍历 LinkedHashMap中 Block，按照最近最少使用 LRU 的顺序淘汰，直到满足新 Block 所需的空间。 其中 LRU 是 LinkedHashMap 的特性。</li></ul><h3 id="执行内存管理"><a href="#执行内存管理" class="headerlink" title="执行内存管理"></a>执行内存管理</h3><p>执行内存主要用来存储任务在执行 Shuffle 时占用的内存，Shuffle 是按照一定规则对 RDD 数据重新分区的过程， Shuffle 的 Write 和 Read 两阶段对执行内存的使用.</p><h3 id="Shuffle-Write"><a href="#Shuffle-Write" class="headerlink" title="Shuffle Write"></a>Shuffle Write</h3><p>在 map 端会采用 ExternalSorter 进行外排，在内存中存储数据时主要占用堆内执行空间。</p><h3 id="Shuffle-Read"><a href="#Shuffle-Read" class="headerlink" title="Shuffle Read"></a>Shuffle Read</h3><ul><li>在对 reduce端的数据进行聚合时， 要将数据交给 Aggregator处理， 在内存中存储数据时占用堆内执行空间。</li><li>如果需要进行最终结果排序，则要将再次将数据交给 ExternalSorter处理，占用堆内执行空间</li></ul><p>在 ExternalSorter 和 Aggregator 中， Spark 会使用一种叫 AppendOnlyMap 的哈希表在堆内执行内存中存储数据 ， 但在 Shuffle 过程中所有数据并不能都保存到该哈希表中， 当这个哈希表占用的内存会进行周期性地采样估算， 当其大到一定程度， 无法再从 MemoryManager 申请到新的执行内存时， Spark 就会将其全部内容存储到磁盘文件中， 这 个过程被称为溢存 [Spill] ， 溢存到磁盘的文件最后会被归 并 [Merge] </p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>Spark 的存储内存和执行内存有着截然不同的管理方式</p><ul><li><p>对于存储内存来说，<strong>Spark</strong> 用一个 **LinkedHashMap **来集中管理所有的 Block，Block 由需要缓存的 RDD 的 Partition 转化而成；</p></li><li><p>对于执行内存，Spark 用 <strong>AppendOnlyMap</strong> 来存储 Shuffle 过程中的数据， 在 Tungsten 排序中甚至抽象成为页式内存管理，开辟了全新的 JVM 内存管理机制 。</p></li></ul><h2 id="Spark-Shuffle-内存使用"><a href="#Spark-Shuffle-内存使用" class="headerlink" title="Spark Shuffle 内存使用"></a>Spark Shuffle 内存使用</h2><p>在使用 Spark 进行计算时，我们经常会碰到作业 (Job) Out Of Memory(OOM) 的情况，而且很大一部分情况是发生在 Shuffle 阶段。那么在 Spark Shuffle 中具体是哪些地方会使用比较多的内存而有可能导致 OOM 呢？ 为此，本文将围绕以上问题梳理 Spark 内存管理和 Shuffle 过程中与内存使用相关的知识；然后，简要分析下在 Spark Shuffle 中有可能导致 OOM 的原因。</p><h2 id="OOM"><a href="#OOM" class="headerlink" title="OOM"></a>OOM</h2><p>内存不够，数据太多就会抛出 OOM的 Exeception，主要有driver OOM和 executor OOM两种</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">java</span><span class="selector-class">.lang</span><span class="selector-class">.OutOfMemoryError</span>: <span class="selector-tag">Java</span> <span class="selector-tag">heap</span> <span class="selector-tag">space</span></span><br></pre></td></tr></table></figure><h3 id="driver-OOM"><a href="#driver-OOM" class="headerlink" title="driver OOM"></a><strong>driver OOM</strong></h3><ul><li><strong>用户在 Driver 端口生成大对象, 比如创建了一个大的集合数据结构</strong></li><li><strong>使用了collect 等操作，将所有 executor 的数据聚合到 driver 导致</strong></li></ul><p>一般是使用了collect 等操作，将所有 executor 的数据聚合到 driver 导致。尽量不要使用 collect操作即可。</p><h3 id="executor-OOM"><a href="#executor-OOM" class="headerlink" title="executor OOM"></a><strong>executor OOM</strong></h3><h3 id="数据倾斜导致内存溢出"><a href="#数据倾斜导致内存溢出" class="headerlink" title="数据倾斜导致内存溢出"></a><strong>数据倾斜导致内存溢出</strong></h3><p>数据不平衡除了有可能导致内存溢出外，也有可能导致性能的问题，调用 repartition 重新分区</p><h3 id="Reduce-OOM"><a href="#Reduce-OOM" class="headerlink" title="Reduce OOM"></a>Reduce OOM</h3><p>reduce task 去 map 端获取数据，reduce一边拉取数据一边聚合，reduce端有一块聚合内存[executor memory * 0.2],也就是这块内存不够<br><strong>解决方法</strong></p><ul><li>增加 reduce 聚合操作的内存的比例</li><li>增加 Executor memory 的大小 <strong>–executor-memory 5G</strong></li><li>减少 reduce task 每次拉取的数据量 设置 spak.reducer.maxSizeInFlight 24m, 拉取的次数就多了，因此建立连接的次数增多，有可能会连接不上[正好赶上 map task 端进行GC]</li></ul><h3 id="shuffle-后内存溢出"><a href="#shuffle-后内存溢出" class="headerlink" title="shuffle 后内存溢出"></a><strong>shuffle 后内存溢出</strong></h3><p> shuffle 后单个文件过大导致内存溢出。在 Spark 中，join，reduceByKey 这一类型的过程，都会有shuffle的过程，在shuffle的使用，需要传入一个partitioner，大部分 Spark 中的 shuffle 操作，默认的 partitioner 都是 HashPatitioner，默认值是父 RDD 中最大的分区数,这个参数通过spark.default.parallelism 控制 [在spark-sql中用spark.sql.shuffle.partitions] </p><p>spark.default.parallelism 参数只对 HashPartitioner 有效，所以如果是别的 Partitioner 或者自己实现的 Partitioner 就不能使用 spark.default.parallelism 这个参数来控制 shuffle 的并发量了。如果是别的partitioner 导致的 shuffle 内存溢出，就需要从 partitioner 的代码增加 partitions 的数量</p><h3 id="coalesce-调用导致内存溢出"><a href="#coalesce-调用导致内存溢出" class="headerlink" title="coalesce 调用导致内存溢出"></a><strong>coalesce 调用导致内存溢出</strong></h3><p>因为 hdfs 中不适合存小问题，所以 Spark 计算后如果产生的文件太小，调用 coalesce 合并文件再存入 hdfs中。但会导致一个问题，例如在 coalesce 之前有100个文件，这也意味着能够有100个 Task，现在调用coalesce(10)，最后只产生10个文件，因为 coalesce 并不是 shuffle 操作，这意味着 coalesce并不是先执行100个 Task，再将 Task 的执行结果合并成10个，而是从头到位只有10个 Task 在执行，原本100个文件是分开执行的，现在每个 Task 同时一次读取10个文件，使用的内存是原来的10倍，这导致了OOM。</p><p>解决这个问题的方法是令程序按照我们想的先执行100个 Task 再将结果合并成10个文件，这个问题同样可以通过repartition 解决，调用 repartition(10)</p><h3 id="standalone-模式下资源分配不均匀导致内存溢出"><a href="#standalone-模式下资源分配不均匀导致内存溢出" class="headerlink" title="standalone 模式下资源分配不均匀导致内存溢出"></a><strong>standalone 模式下资源分配不均匀导致内存溢出</strong></h3><p>在 standalone 的模式下如果配置了 –total-executor-cores 和 –executor-memory 这两个参数，但是没有配置 –executor-cores 参数，有可能导致，每个 Executor 的 memory 是一样的，但是 cores 的数量不同，那么在 cores 数量多的 Executor 中，由于能够同时执行多个Task，就容易导致内存溢出的情况。</p><p>这种情况的解决方法就是同时配置–executor-cores或者spark.executor.cores参数，确保Executor资源分配均匀。</p><h3 id="map-过程产生大量对象导致内存溢出"><a href="#map-过程产生大量对象导致内存溢出" class="headerlink" title="map 过程产生大量对象导致内存溢出"></a><strong>map 过程产生大量对象导致内存溢出</strong></h3><p>这种溢出的原因是在单个 map 中产生了大量的对象导致的</p><p>例如：rdd.map(x=&gt;for(i &lt;- 1 to 10000) yield i.toString)，这个操作在rdd中，每个对象都产生了10000 个对象，这肯定很容易产生内存溢出的问题。</p><p>针对这种问题，在不增加内存的情况下，可以通过减少每个 Task 的大小，以便达到每个 Task 即使产生大量的对象 Executor 的内存也能够装得下。具体做法可以在会产生大量对象的 map 操作之前调用 repartition方法，分区成更小的块传入map。</p><p>例如：rdd.repartition(10000).map(x=&gt;for(i &lt;- 1 to 10000) yield i.toString)</p><h2 id="参数"><a href="#参数" class="headerlink" title="参数"></a><strong>参数</strong></h2><h3 id="spark-driver-memory"><a href="#spark-driver-memory" class="headerlink" title="spark.driver.memory"></a>spark.driver.memory</h3><p>用来设置 Driver 的内存。在 Spark 程序中，SparkContext，DAGScheduler 都是运行在Driver端的。对应Stage 切分也是在 Driver 端运行，如果用户自己写的程序有过多的步骤，切分出过多的 Stage，这部分信息消耗的是 Driver 的内存，这个时候就需要调大 Driver 的内存</p>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Spark Shuffle</title>
      <link href="2019/11/20/Hadoop%E5%B0%8F%E6%96%87%E4%BB%B6%E5%A4%84%E7%90%86/"/>
      <url>2019/11/20/Hadoop%E5%B0%8F%E6%96%87%E4%BB%B6%E5%A4%84%E7%90%86/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p>HDFS上每个文件都要在 NameNode 上建立一个索引，这个索引的大小约为 <strong>150byte</strong>，这样当小文件比较多的时候，就会产生很多的索引文件，一方面会大量占用 **NameNode **的内存空间，另一方面就是索引文件过大使得索引速度变慢。</p><a id="more"></a><h1 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h1><h2 id="小文件"><a href="#小文件" class="headerlink" title="小文件"></a>小文件</h2><blockquote></blockquote><h2 id="小文件弊端"><a href="#小文件弊端" class="headerlink" title="小文件弊端"></a>小文件弊端</h2><p>HDFS上每个文件都要在 NameNode 上建立一个索引，这个索引的大小约为 <strong>150byte</strong>，这样当小文件比较多的时候，就会产生很多的索引文件，一方面会大量占用 **NameNode **的内存空间，另一方面就是索引文件过大使得索引速度变慢。</p><h1 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h1><h2 id="Hadoop-Archive"><a href="#Hadoop-Archive" class="headerlink" title="Hadoop Archive"></a>Hadoop Archive</h2><h2 id="Sequence-File"><a href="#Sequence-File" class="headerlink" title="Sequence File"></a>Sequence File</h2><h2 id="CombineFileInputFormat"><a href="#CombineFileInputFormat" class="headerlink" title="CombineFileInputFormat"></a>CombineFileInputFormat</h2><h2 id="开启-JVM-重用"><a href="#开启-JVM-重用" class="headerlink" title="开启 JVM 重用"></a>开启 JVM 重用</h2><p>Hadoop的默认配置通常是使用派生JVM来执行map和Reduce任务的。这时JVM的启动过程可能会造成相当大的开销，尤其是执行的job包含有成百上千task任务的情况。JVM重用可以使得JVM实例在同一个job中重新使用N次</p>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
            <tag> Hadoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark 调优</title>
      <link href="2019/11/19/Spark%20%E4%BC%98%E5%8C%96/"/>
      <url>2019/11/19/Spark%20%E4%BC%98%E5%8C%96/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p>在执行 Spark 的应用程序时，Spark集群会启动 Driver 和 Executor 两种 JVM 进程，前者为主控进程，负责创建 Spark 上下文，提交 Spark 作业[Job]，并将作业转化为计算任务[Task]，在各个 Executor 进程间协调任务的调度；后者负责在工作节点上执行具体的计算任务，并将结果返回给 Driver， 同时为需要持久化的 RDD 提供存储功能。由于 Driver 的内存管理相对来说较为简单，本节主要对 Executor 的内存管理进行分析，下文中的 Spark 内存均特指 Executor 的内存。</p><a id="more"></a><h3 id=""><a href="#" class="headerlink" title=""></a></h3>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>智慧出行服务体系建设的开发之订单和轨迹监控</title>
      <link href="2019/11/13/%E6%99%BA%E6%85%A7%E5%87%BA%E8%A1%8C%E6%9C%8D%E5%8A%A1%E4%BD%93%E7%B3%BB%E5%BB%BA%E8%AE%BE%E7%9A%84%E5%BC%80%E5%8F%91%E4%B9%8B%E8%AE%A2%E5%8D%95%E5%92%8C%E8%BD%A8%E8%BF%B9%E7%9B%91%E6%8E%A7/"/>
      <url>2019/11/13/%E6%99%BA%E6%85%A7%E5%87%BA%E8%A1%8C%E6%9C%8D%E5%8A%A1%E4%BD%93%E7%B3%BB%E5%BB%BA%E8%AE%BE%E7%9A%84%E5%BC%80%E5%8F%91%E4%B9%8B%E8%AE%A2%E5%8D%95%E5%92%8C%E8%BD%A8%E8%BF%B9%E7%9B%91%E6%8E%A7/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><h2 id="1、实时订单统计"><a href="#1、实时订单统计" class="headerlink" title="1、实时订单统计"></a>1、实时订单统计</h2><p>订单数据处理流程:</p><p>1.shell播放脚本读取订单数据到指定订单文件中.</p><p>2.使用flume监听订单文件，实时将订单数据发送到Kafka.</p><p>3.使用Spark streaming处理统计订单数据和乘车人数保存到redis中.</p><p>4.页面请求Java中台相应restful接口，restful接口查询redis中的数据返回页面，然后页面渲染显示.</p><p><img src="/images/didi/1571121383981.png" alt="1571121383981"></p><h3 id="1-1-订单数据回放"><a href="#1-1-订单数据回放" class="headerlink" title="1.1 订单数据回放"></a>1.1 订单数据回放</h3><p> 1.安装配置 Flume</p><p>flume agent配置:</p><p>代理名称：配置如下:a1(按照业务功能自定义一个名称即可)</p><p><img src="/images/didi/1571053796134.png" alt="1571053796134"></p><p>配置文件内容如下:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">a1.sources=r1</span><br><span class="line">a1.sinks=k1</span><br><span class="line">a1.channels=c1</span><br><span class="line">a1.sources.r1.type=exec</span><br><span class="line"><span class="meta">#</span><span class="bash">先使用tail -F的方式，随后做优化</span></span><br><span class="line">a1.sources.r1.command=tail -F /root/order/order</span><br><span class="line">a1.sources.r1.fileHeader=true</span><br><span class="line">a1.sinks.k1.type=org.apache.flume.sink.kafka.KafkaSink</span><br><span class="line">a1.sinks.k1.topic=hai_kou_order_topic</span><br><span class="line">a1.sinks.k1.brokerList=cdh-node01:9092,cdh-node02:9092,cdh-node03:9092</span><br><span class="line">a1.sinks.k1.batchSize=20</span><br><span class="line">a1.sinks.k1.requiredAcks=1</span><br><span class="line">a1.sinks.k1.producer.linger.ms=1</span><br><span class="line">a1.sinks.k1.producer.compression.type=snappy</span><br><span class="line">a1.channels.c1.type=memory</span><br><span class="line">a1.channels.c1.capacity=1000</span><br><span class="line">a1.channels.c1.transactionCapacity=100</span><br><span class="line">a1.sources.r1.channels=c1</span><br><span class="line">a1.sinks.k1.channel=c1</span><br></pre></td></tr></table></figure><p>2.kafka manager工具安装</p><p>此工具主要用作kafka主题消息的监控，主题增加，删除等操作.</p><p>3.消费kafka中的订单数据数据代码实现.</p><h3 id="1-2-数据回放的断点续传解决方案"><a href="#1-2-数据回放的断点续传解决方案" class="headerlink" title="1.2 数据回放的断点续传解决方案"></a>1.2 数据回放的断点续传解决方案</h3><p>问题背景:</p><p> 通常我们使用flume和kafka集成，都是使用flume监控文件,会在配置source时的命令，例如:tail -F 文件名,这种方式依然会存在一个问题，但flume的agent进程由于各种原因挂掉一段时间之后，</p><p>解决方案:</p><p>1.第一种方案,是在使用tail -F命令的地方修改</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a1.sources.r2.command=</span><br><span class="line">tail  -n +$(tail -n1 /root/log) -F /root/data/nginx.log | awk &#x27;ARGIND==1&#123;i=$0;next&#125;&#123;i++;if($0~/^tail/)&#123;i=0&#125;;print $0;print i &gt;&gt; &quot;/root/log&quot;;fflush(&quot;&quot;)&#125;&#x27; /root/log</span><br></pre></td></tr></table></figure><p>2.第二种方案,高版本的flume可以使用tailDir Souce</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">a1.sources.s1.type = TAILDIR</span><br><span class="line">a1.sources.s1.positionFile = /home/dev/flume/flume-1.8.0/log/taildir_position.json</span><br><span class="line">a1.sources.s1.filegroups = f1</span><br><span class="line">a1.sources.s1.filegroups.f1 = /home/dev/log/moercredit/logstash.log</span><br><span class="line">a1.sources.s1.headers.f1.headerKey1 = aaa</span><br><span class="line">a1.sources.s1.fileHeader = true</span><br></pre></td></tr></table></figure><h3 id="1-3-实时订单数据统计（订单情况、乘车人数情况）"><a href="#1-3-实时订单数据统计（订单情况、乘车人数情况）" class="headerlink" title="1.3 实时订单数据统计（订单情况、乘车人数情况）"></a>1.3 实时订单数据统计（订单情况、乘车人数情况）</h3><p>OrderStreamingProcessor类</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.cartravel.spark</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.cartravel.common.&#123;<span class="type">Constants</span>, <span class="type">TopicName</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.serialization.<span class="type">StringDeserializer</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.kafka010._</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.kafka010.<span class="type">LocationStrategies</span>.<span class="type">PreferConsistent</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.kafka010.<span class="type">ConsumerStrategies</span>.<span class="type">Subscribe</span></span><br><span class="line"><span class="keyword">import</span> org.apache.log4j.<span class="type">Level</span></span><br><span class="line"><span class="keyword">import</span> org.apache.log4j.<span class="type">Logger</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> scala.collection.<span class="type">JavaConversions</span>._</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * 订单数据流处理程</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">OrderStreamingProcessor</span> </span>&#123;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Order</span>(<span class="params">oderId: <span class="type">String</span></span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">object</span> <span class="title">OrderStreamingProcessor</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">import</span> org.apache.spark._</span><br><span class="line">    <span class="keyword">import</span> org.apache.spark.streaming._</span><br><span class="line">    <span class="comment">//设置Spark程序在控制台中的日志打印级别</span></span><br><span class="line">    <span class="type">Logger</span>.getLogger(<span class="string">&quot;org&quot;</span>).setLevel(<span class="type">Level</span>.<span class="type">WARN</span>)</span><br><span class="line">    <span class="comment">//local[*]使用本地模式运行，*表示内部会自动计算CPU核数，也可以直接指定运行线程数比如2，就是local[2]</span></span><br><span class="line">    <span class="comment">//表示使用两个线程来模拟spark集群</span></span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">&quot;OrderMonitor&quot;</span>).setMaster(<span class="string">&quot;local[1]&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//初始化Spark Streaming环境</span></span><br><span class="line">    <span class="keyword">val</span> streamingContext = <span class="keyword">new</span> <span class="type">StreamingContext</span>(conf, <span class="type">Seconds</span>(<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment">//设置检查点</span></span><br><span class="line">    streamingContext.checkpoint(<span class="string">&quot;/sparkapp/tmp&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//&quot;auto.offset.reset&quot; -&gt; &quot;earliest&quot;</span></span><br><span class="line">    <span class="comment">//</span></span><br><span class="line">    <span class="keyword">val</span> kafkaParams = <span class="type">Map</span>[<span class="type">String</span>, <span class="type">Object</span>](</span><br><span class="line"><span class="comment">//      &quot;bootstrap.servers&quot; -&gt; &quot;192.168.21.173:6667,192.168.21.174:6667,192.168.21.175:6667&quot;,</span></span><br><span class="line">      <span class="string">&quot;bootstrap.servers&quot;</span> -&gt; <span class="type">Constants</span>.<span class="type">KAFKA_BOOTSTRAP_SERVERS</span>,</span><br><span class="line">      <span class="string">&quot;key.deserializer&quot;</span> -&gt; classOf[<span class="type">StringDeserializer</span>],</span><br><span class="line">      <span class="string">&quot;value.deserializer&quot;</span> -&gt; classOf[<span class="type">StringDeserializer</span>],</span><br><span class="line">      <span class="string">&quot;group.id&quot;</span> -&gt; <span class="string">&quot;test0001&quot;</span>,</span><br><span class="line">      <span class="string">&quot;auto.offset.reset&quot;</span> -&gt; <span class="string">&quot;latest&quot;</span>,</span><br><span class="line">      <span class="string">&quot;enable.auto.commit&quot;</span> -&gt; (<span class="literal">false</span>: java.lang.<span class="type">Boolean</span>)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> topics = <span class="type">Array</span>(</span><br><span class="line">      <span class="type">TopicName</span>.<span class="type">HAI_KOU_ORDER_TOPIC</span>.getTopicName,</span><br><span class="line">      <span class="type">TopicName</span>.<span class="type">CHENG_DU_ORDER_TOPIC</span>.getTopicName,</span><br><span class="line">      <span class="type">TopicName</span>.<span class="type">XI_AN_ORDER_TOPIC</span>.getTopicName</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    topics.foreach(println(_))</span><br><span class="line">    println(<span class="string">&quot;topics:&quot;</span> + topics)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> stream = <span class="type">KafkaUtils</span>.createDirectStream[<span class="type">String</span>, <span class="type">String</span>](</span><br><span class="line">      streamingContext,</span><br><span class="line">      <span class="type">PreferConsistent</span>,</span><br><span class="line">      <span class="type">Subscribe</span>[<span class="type">String</span>, <span class="type">String</span>](topics, kafkaParams)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    stream.count().print();</span><br><span class="line"></span><br><span class="line">    <span class="comment">//实时统计订单总数</span></span><br><span class="line">    <span class="keyword">val</span> ordersDs = stream.map(record =&gt; &#123;</span><br><span class="line">      <span class="comment">//主题名称</span></span><br><span class="line">      <span class="keyword">val</span> topicName = record.topic()</span><br><span class="line">      <span class="keyword">val</span> orderInfo = record.value()</span><br><span class="line"></span><br><span class="line">      <span class="comment">//订单信息解析器</span></span><br><span class="line">      <span class="keyword">var</span> orderParser: <span class="type">OrderParser</span> = <span class="literal">null</span>;</span><br><span class="line"></span><br><span class="line">      <span class="comment">//不同主题的订单进行不同的处理</span></span><br><span class="line">      topicName <span class="keyword">match</span> &#123;</span><br><span class="line">        <span class="keyword">case</span> <span class="string">&quot;hai_kou_order_topic&quot;</span> =&gt; &#123;</span><br><span class="line">          orderParser = <span class="keyword">new</span> <span class="type">HaiKouOrderParser</span>();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">case</span> <span class="string">&quot;cheng_du_order_topic&quot;</span> =&gt; &#123;</span><br><span class="line">          orderParser = <span class="keyword">new</span> <span class="type">ChengDuOrderParser</span>()</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">case</span> <span class="string">&quot;xi_an_order_topic&quot;</span> =&gt; &#123;</span><br><span class="line">          orderParser = <span class="keyword">new</span> <span class="type">XiAnOrderParser</span>()</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">case</span> _ =&gt; &#123;</span><br><span class="line">          orderParser = <span class="literal">null</span>;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      println(<span class="string">&quot;orderParser:&quot;</span> + orderParser)</span><br><span class="line">      <span class="keyword">if</span> (<span class="literal">null</span> != orderParser) &#123;</span><br><span class="line">        <span class="keyword">val</span> order = orderParser.parser(orderInfo)</span><br><span class="line">        println(<span class="string">&quot;parser order:&quot;</span> + order)</span><br><span class="line">        order</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="literal">null</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">//订单计数,对于每个订单出现一次计数1</span></span><br><span class="line">    <span class="keyword">val</span> orderCountRest = ordersDs.map(order =&gt; &#123;</span><br><span class="line">      <span class="keyword">if</span> (<span class="literal">null</span> == order) &#123;</span><br><span class="line">        (<span class="string">&quot;&quot;</span>, <span class="number">0</span>)</span><br><span class="line">      &#125; <span class="keyword">else</span> <span class="keyword">if</span> (order.getClass == classOf[<span class="type">ChengDuTravelOrder</span>]) &#123;</span><br><span class="line">        (<span class="type">Constants</span>.<span class="type">CITY_CODE_CHENG_DU</span> + <span class="string">&quot;_&quot;</span> + order.createDay, <span class="number">1</span>)</span><br><span class="line">      &#125; <span class="keyword">else</span> <span class="keyword">if</span> (order.getClass == classOf[<span class="type">XiAnTravelOrder</span>]) &#123;</span><br><span class="line">        (<span class="type">Constants</span>.<span class="type">CITY_CODE_XI_AN</span> + <span class="string">&quot;_&quot;</span> + order.createDay, <span class="number">1</span>)</span><br><span class="line">      &#125; <span class="keyword">else</span> <span class="keyword">if</span> (order.getClass == classOf[<span class="type">HaiKouTravelOrder</span>]) &#123;</span><br><span class="line">        (<span class="type">Constants</span>.<span class="type">CITY_CODE_HAI_KOU</span> + <span class="string">&quot;_&quot;</span> + order.createDay, <span class="number">1</span>)</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        (<span class="string">&quot;&quot;</span>, <span class="number">0</span>)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;).updateStateByKey((currValues: <span class="type">Seq</span>[<span class="type">Int</span>], state: <span class="type">Option</span>[<span class="type">Int</span>]) =&gt; &#123;</span><br><span class="line">      <span class="keyword">var</span> count = currValues.sum + state.getOrElse(<span class="number">0</span>);</span><br><span class="line">      <span class="type">Some</span>(count)</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">      * 乘车人数统计</span></span><br><span class="line"><span class="comment">      * 如果是成都或者西安的订单，数据中没有乘车人数字段，所有按照默认一单一人的方式进行统计</span></span><br><span class="line"><span class="comment">      * 海口的订单数据中有乘车人数字段，就按照具体数进行统计</span></span><br><span class="line"><span class="comment">      */</span></span><br><span class="line">    <span class="keyword">val</span> passengerCountRest = ordersDs.map(order =&gt; &#123;</span><br><span class="line">      <span class="keyword">if</span> (<span class="literal">null</span> == order) &#123;</span><br><span class="line">        (<span class="string">&quot;&quot;</span>, <span class="number">0</span>)</span><br><span class="line">      &#125; <span class="keyword">else</span> <span class="keyword">if</span> (order.getClass == classOf[<span class="type">ChengDuTravelOrder</span>]) &#123;</span><br><span class="line">        (<span class="type">Constants</span>.<span class="type">CITY_CODE_CHENG_DU</span> + <span class="string">&quot;_&quot;</span> + order.createDay, <span class="number">1</span>)</span><br><span class="line">      &#125; <span class="keyword">else</span> <span class="keyword">if</span> (order.getClass == classOf[<span class="type">XiAnTravelOrder</span>]) &#123;</span><br><span class="line">        (<span class="type">Constants</span>.<span class="type">CITY_CODE_XI_AN</span> + <span class="string">&quot;_&quot;</span> + order.createDay, <span class="number">1</span>)</span><br><span class="line">      &#125; <span class="keyword">else</span> <span class="keyword">if</span> (order.getClass == classOf[<span class="type">HaiKouTravelOrder</span>]) &#123;</span><br><span class="line">        <span class="keyword">var</span> passengerCount = order.asInstanceOf[<span class="type">HaiKouTravelOrder</span>].passengerCount.toInt</span><br><span class="line">        <span class="comment">//scala不支持类似java中的三目运算符，可以使用下面的操作方式</span></span><br><span class="line">        passengerCount = <span class="keyword">if</span>(passengerCount&gt;<span class="number">0</span>) passengerCount <span class="keyword">else</span> <span class="number">1</span></span><br><span class="line">        (<span class="type">Constants</span>.<span class="type">CITY_CODE_HAI_KOU</span> + <span class="string">&quot;_&quot;</span> + order.createDay,passengerCount)</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        (<span class="string">&quot;&quot;</span>, <span class="number">0</span>)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;).updateStateByKey((currValues: <span class="type">Seq</span>[<span class="type">Int</span>], state: <span class="type">Option</span>[<span class="type">Int</span>]) =&gt; &#123;</span><br><span class="line">      <span class="keyword">var</span> count = currValues.sum + state.getOrElse(<span class="number">0</span>);</span><br><span class="line">      <span class="type">Some</span>(count)</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    orderCountRest.foreachRDD(orderCountRDD=&gt;&#123;</span><br><span class="line">      <span class="keyword">import</span> com.cartravel.util.<span class="type">JedisUtil</span></span><br><span class="line">      <span class="keyword">val</span> jedisUtil = <span class="type">JedisUtil</span>.getInstance()</span><br><span class="line">      <span class="keyword">val</span> jedis = jedisUtil.getJedis</span><br><span class="line">      <span class="keyword">val</span> orderCountRest = orderCountRDD.collect()</span><br><span class="line">      println(<span class="string">&quot;orderCountRest:&quot;</span>+orderCountRest)</span><br><span class="line">      orderCountRest.foreach(countrest=&gt;&#123;</span><br><span class="line">        println(<span class="string">&quot;countrest:&quot;</span>+countrest._1+<span class="string">&quot;,&quot;</span>+countrest._2)</span><br><span class="line">        <span class="keyword">if</span>(<span class="literal">null</span>!=countrest)&#123;</span><br><span class="line">          jedis.hset(<span class="type">Constants</span>.<span class="type">ORDER_COUNT</span>, countrest._1, countrest._2 + <span class="string">&quot;&quot;</span>)</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;)</span><br><span class="line">      jedisUtil.returnJedis(jedis)</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    passengerCountRest.foreachRDD(passengerCountRdd=&gt;&#123;</span><br><span class="line">      <span class="keyword">import</span> com.cartravel.util.<span class="type">JedisUtil</span></span><br><span class="line">      <span class="keyword">val</span> jedisUtil = <span class="type">JedisUtil</span>.getInstance()</span><br><span class="line">      <span class="keyword">val</span> jedis = jedisUtil.getJedis</span><br><span class="line"></span><br><span class="line">      <span class="keyword">val</span> passengerCountRest = passengerCountRdd.collect()</span><br><span class="line">      passengerCountRest.foreach(countrest=&gt;&#123;</span><br><span class="line">        jedis.hset(<span class="type">Constants</span>.<span class="type">PASSENGER_COUNT</span>, countrest._1, countrest._2 + <span class="string">&quot;&quot;</span>)</span><br><span class="line">      &#125;)</span><br><span class="line">      jedisUtil.returnJedis(jedis)</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//启动sparkstreaming程序</span></span><br><span class="line">    streamingContext.start();</span><br><span class="line">    streamingContext.awaitTermination();</span><br><span class="line">    streamingContext.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="2、全域订单轨迹监控"><a href="#2、全域订单轨迹监控" class="headerlink" title="2、全域订单轨迹监控"></a>2、全域订单轨迹监控</h2><h3 id="2-1-实时订单轨迹监控"><a href="#2-1-实时订单轨迹监控" class="headerlink" title="2.1 实时订单轨迹监控"></a>2.1 实时订单轨迹监控</h3><p>​    盖亚数据计划开放的开源数据集中是已经生成的订单轨迹数据所以是不知道订单什么时候结束，真实的业务场景中是有开始和技术的标志位，但是我们可以在数据中认为的设置开始和技术标记，可以这么做在数据的开始开可以设置start字符串在数据的技术可以设置end技术的字符串，使用start和end字符串作为订单轨迹数据的开始和结束.</p><p>实现流程:</p><p><img src="/images/didi/1571215041738.png" alt="1571215041738"></p><p>消费轨迹数据</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br></pre></td><td class="code"><pre><span class="line">com.cartravel.kafka.<span class="keyword">package</span> com.cartravel.kafka;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.cartravel.common.Constants;</span><br><span class="line"><span class="keyword">import</span> com.cartravel.common.Order;</span><br><span class="line"><span class="keyword">import</span> com.cartravel.common.TopicName;</span><br><span class="line"><span class="keyword">import</span> com.cartravel.util.HBaseUtil;</span><br><span class="line"><span class="keyword">import</span> com.cartravel.util.JedisUtil;</span><br><span class="line"><span class="keyword">import</span> com.cartravel.util.ObjUtil;</span><br><span class="line"><span class="keyword">import</span> org.apache.commons.lang.StringUtils;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.client.Put;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.client.Table;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.Consumer;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecord;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecords;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.KafkaConsumer;</span><br><span class="line"><span class="keyword">import</span> org.apache.log4j.Level;</span><br><span class="line"><span class="keyword">import</span> org.apache.log4j.Logger;</span><br><span class="line"><span class="keyword">import</span> redis.clients.jedis.Jedis;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.BufferedOutputStream;</span><br><span class="line"><span class="keyword">import</span> java.io.FileOutputStream;</span><br><span class="line"><span class="keyword">import</span> java.io.PrintWriter;</span><br><span class="line"><span class="keyword">import</span> java.text.SimpleDateFormat;</span><br><span class="line"><span class="keyword">import</span> java.util.*;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">GpsConsumer</span> <span class="keyword">implements</span> <span class="title">Runnable</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> Logger log = Logger.getLogger(GpsConsumer.class);</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span>  KafkaConsumer&lt;String, String&gt; consumer;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> String topic;</span><br><span class="line">    <span class="comment">//计数消费到的消息条数</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">int</span> count = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">private</span> FileOutputStream file = <span class="keyword">null</span>;</span><br><span class="line">    <span class="keyword">private</span> BufferedOutputStream out = <span class="keyword">null</span>;</span><br><span class="line">    <span class="keyword">private</span> PrintWriter printWriter = <span class="keyword">null</span>;</span><br><span class="line">    <span class="keyword">private</span> String lineSeparator = <span class="keyword">null</span>;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> batchNum = <span class="number">0</span>;</span><br><span class="line">    JedisUtil instance = <span class="keyword">null</span>;</span><br><span class="line">    Jedis jedis = <span class="keyword">null</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> String cityCode = <span class="string">&quot;&quot;</span>;</span><br><span class="line">    <span class="keyword">private</span> Map&lt;String, String&gt; gpsMap = <span class="keyword">new</span> HashMap&lt;String, String&gt;();</span><br><span class="line">    SimpleDateFormat sdf = <span class="keyword">new</span> SimpleDateFormat(<span class="string">&quot;yyyy-MM-dd HH:mm:ss&quot;</span>);</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">GpsConsumer</span><span class="params">(String topic, String groupId)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (topic.equalsIgnoreCase(TopicName.CHENG_DU_GPS_TOPIC.getTopicName())) &#123;</span><br><span class="line">            cityCode = Constants.CITY_CODE_CHENG_DU;</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (topic.equalsIgnoreCase(TopicName.XI_AN_GPS_TOPIC.getTopicName())) &#123;</span><br><span class="line">            cityCode = Constants.CITY_CODE_XI_AN;</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (topic.equalsIgnoreCase(TopicName.HAI_KOU_ORDER_TOPIC.getTopicName())) &#123;</span><br><span class="line">            cityCode = Constants.CITY_CODE_HAI_KOU;</span><br><span class="line">        &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> IllegalArgumentException(topic+<span class="string">&quot;,主题名称不合法!&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line"></span><br><span class="line">        <span class="comment">//dev-hdp</span></span><br><span class="line"><span class="comment">//        props.put(&quot;bootstrap.servers&quot;, &quot;192.168.21.173:6667,192.168.21.174:6667,192.168.21.175:6667&quot;);</span></span><br><span class="line">        <span class="comment">//dev-cdh</span></span><br><span class="line"><span class="comment">//        props.put(&quot;bootstrap.servers&quot;, &quot;192.168.21.177:9092,192.168.21.178:9092,192.168.21.179:9092&quot;);</span></span><br><span class="line">        <span class="comment">//pro-cdh</span></span><br><span class="line">        props.put(<span class="string">&quot;bootstrap.servers&quot;</span>, Constants.KAFKA_BOOTSTRAP_SERVERS);</span><br><span class="line"></span><br><span class="line"><span class="comment">//        props.put(&quot;bootstrap.servers&quot;, &quot;192.168.21.178:9092&quot;);</span></span><br><span class="line"></span><br><span class="line">        props.put(<span class="string">&quot;group.id&quot;</span>, groupId);</span><br><span class="line">        props.put(<span class="string">&quot;enable.auto.commit&quot;</span>, <span class="string">&quot;true&quot;</span>);</span><br><span class="line">        props.put(<span class="string">&quot;auto.offset.reset&quot;</span>, <span class="string">&quot;earliest&quot;</span>);</span><br><span class="line"><span class="comment">//        props.put(&quot;auto.offset.reset&quot;, &quot;latest&quot;);</span></span><br><span class="line">        props.put(<span class="string">&quot;session.timeout.ms&quot;</span>, <span class="string">&quot;30000&quot;</span>);</span><br><span class="line">        props.put(<span class="string">&quot;key.deserializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;</span>);</span><br><span class="line">        props.put(<span class="string">&quot;value.deserializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;</span>);</span><br><span class="line"></span><br><span class="line">        consumer = <span class="keyword">new</span> KafkaConsumer&lt;String,String&gt;(props);</span><br><span class="line">        <span class="keyword">this</span>.topic = topic;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">                doWork();</span><br><span class="line">            &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">                e.printStackTrace();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">doWork</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        batchNum++;</span><br><span class="line">        consumer.subscribe(Collections.singletonList(<span class="keyword">this</span>.topic));</span><br><span class="line">        ConsumerRecords&lt;String, String&gt; records = consumer.poll(<span class="number">1000</span>);</span><br><span class="line">        System.out.println(<span class="string">&quot;第&quot;</span> + batchNum + <span class="string">&quot;批次,&quot;</span> + records.count());</span><br><span class="line">        <span class="comment">//司机ID</span></span><br><span class="line">        String driverId = <span class="string">&quot;&quot;</span>;</span><br><span class="line">        <span class="comment">//订单ID</span></span><br><span class="line">        String orderId = <span class="string">&quot;&quot;</span>;</span><br><span class="line">        <span class="comment">//经度</span></span><br><span class="line">        String lng = <span class="string">&quot;&quot;</span>;</span><br><span class="line">        <span class="comment">//维度</span></span><br><span class="line">        String lat = <span class="string">&quot;&quot;</span>;</span><br><span class="line">        <span class="comment">//时间戳</span></span><br><span class="line">        String timestamp = <span class="string">&quot;&quot;</span>;</span><br><span class="line">        Order order = <span class="keyword">null</span>;</span><br><span class="line">        Order startEndTimeOrder = <span class="keyword">null</span>;</span><br><span class="line">        Object tmpOrderObj = <span class="keyword">null</span>;</span><br><span class="line">        <span class="keyword">if</span> (records.count() &gt; <span class="number">0</span>) &#123;</span><br><span class="line">            Table table = HBaseUtil.getTable(Constants.HTAB_GPS);</span><br><span class="line">            JedisUtil instance = JedisUtil.getInstance();</span><br><span class="line">            jedis = instance.getJedis();</span><br><span class="line">            List&lt;Put&gt; puts = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">            String rowkey = <span class="string">&quot;&quot;</span>;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (gpsMap.size() &gt; <span class="number">0</span>) &#123;</span><br><span class="line">                gpsMap.clear();</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="comment">//表不存在时创建表</span></span><br><span class="line">            <span class="keyword">if</span> (!HBaseUtil.tableExists(Constants.HTAB_GPS)) &#123;</span><br><span class="line">                HBaseUtil.createTable(HBaseUtil.getConnection(), Constants.HTAB_GPS, Constants.DEFAULT_FAMILY);</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; record : records) &#123;</span><br><span class="line">                count++;</span><br><span class="line">                log.warn(<span class="string">&quot;Received message: (&quot;</span> + record.key() + <span class="string">&quot;, &quot;</span> + record.value() + <span class="string">&quot;) at offset &quot;</span> +</span><br><span class="line">                        record.offset() + <span class="string">&quot;,count:&quot;</span> + count);</span><br><span class="line">                String value = record.value();</span><br><span class="line">                <span class="keyword">if</span> (value.contains(<span class="string">&quot;,&quot;</span>)) &#123;</span><br><span class="line">                    order = <span class="keyword">new</span> Order();</span><br><span class="line">                    String[] split = value.split(<span class="string">&quot;,&quot;</span>);</span><br><span class="line">                    driverId = split[<span class="number">0</span>];</span><br><span class="line">                    orderId = split[<span class="number">1</span>];</span><br><span class="line">                    timestamp = split[<span class="number">2</span>];</span><br><span class="line">                    lng = split[<span class="number">3</span>];</span><br><span class="line">                    lat = split[<span class="number">4</span>];</span><br><span class="line"></span><br><span class="line">                    rowkey = orderId + <span class="string">&quot;_&quot;</span> + timestamp;</span><br><span class="line">                    gpsMap.put(<span class="string">&quot;CITYCODE&quot;</span>, cityCode);</span><br><span class="line">                    gpsMap.put(<span class="string">&quot;DRIVERID&quot;</span>, driverId);</span><br><span class="line">                    gpsMap.put(<span class="string">&quot;ORDERID&quot;</span>, orderId);</span><br><span class="line">                    gpsMap.put(<span class="string">&quot;TIMESTAMP&quot;</span>, timestamp + <span class="string">&quot;&quot;</span>);</span><br><span class="line">                    gpsMap.put(<span class="string">&quot;TIME&quot;</span>, sdf.format(<span class="keyword">new</span> Date(Long.parseLong(timestamp+<span class="string">&quot;000&quot;</span>))));</span><br><span class="line">                    gpsMap.put(<span class="string">&quot;LNG&quot;</span>, lng);</span><br><span class="line">                    gpsMap.put(<span class="string">&quot;LAT&quot;</span>, lat);</span><br><span class="line"></span><br><span class="line">                    order.setOrderId(orderId);</span><br><span class="line"></span><br><span class="line">                    puts.add(HBaseUtil.createPut(rowkey, Constants.DEFAULT_FAMILY.getBytes(), gpsMap));</span><br><span class="line"></span><br><span class="line">                    <span class="comment">//1.存入实时订单单号</span></span><br><span class="line">                    jedis.sadd(Constants.REALTIME_ORDERS, cityCode + <span class="string">&quot;_&quot;</span> + orderId);</span><br><span class="line">                    <span class="comment">//2.存入实时订单的经纬度信息</span></span><br><span class="line">                    jedis.lpush(cityCode + <span class="string">&quot;_&quot;</span> + orderId, lng + <span class="string">&quot;,&quot;</span> + lat);</span><br><span class="line">                    <span class="comment">//3.存入订单的开始结束时间信息</span></span><br><span class="line"></span><br><span class="line">                    <span class="keyword">byte</span>[] orderBytes = jedis.hget(Constants.ORDER_START_ENT_TIME.getBytes()</span><br><span class="line">                            , orderId.getBytes());</span><br><span class="line"></span><br><span class="line">                    <span class="keyword">if</span> (orderBytes != <span class="keyword">null</span>) &#123;</span><br><span class="line">                        tmpOrderObj = ObjUtil.deserialize(orderBytes);</span><br><span class="line">                    &#125;</span><br><span class="line"></span><br><span class="line">                    <span class="keyword">if</span> (<span class="keyword">null</span> != tmpOrderObj) &#123;</span><br><span class="line">                        startEndTimeOrder = (Order) tmpOrderObj;</span><br><span class="line">                        startEndTimeOrder.setEndTime(Long.parseLong(timestamp+<span class="string">&quot;000&quot;</span>));</span><br><span class="line">                        jedis.hset(Constants.ORDER_START_ENT_TIME.getBytes(), orderId.getBytes(),</span><br><span class="line">                                ObjUtil.serialize(startEndTimeOrder));</span><br><span class="line">                    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                        <span class="comment">//第一次写入订单的开始时间,开始时间和结束时间一样</span></span><br><span class="line">                        order.setStartTime(Long.parseLong(timestamp));</span><br><span class="line">                        order.setEndTime(Long.parseLong(timestamp));</span><br><span class="line">                        jedis.hset(Constants.ORDER_START_ENT_TIME.getBytes(), orderId.getBytes(),</span><br><span class="line">                                ObjUtil.serialize(order));</span><br><span class="line">                    &#125;</span><br><span class="line">                    hourOrderInfoGather(jedis,gpsMap);</span><br><span class="line">                &#125; <span class="keyword">else</span> <span class="keyword">if</span> (value.contains(<span class="string">&quot;end&quot;</span>)) &#123;</span><br><span class="line">                    jedis.lpush(cityCode + <span class="string">&quot;_&quot;</span> + orderId, value);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            table.put(puts);</span><br><span class="line">            instance.returnJedis(jedis);</span><br><span class="line">        &#125;</span><br><span class="line">        log.warn(<span class="string">&quot;正常结束...&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 统计城市的每小时的订单信息和订单数</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> Exception</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span>  <span class="keyword">void</span> <span class="title">hourOrderInfoGather</span><span class="params">(Jedis jedis,Map&lt;String, String&gt; gpsMap)</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">        String time = gpsMap.get(<span class="string">&quot;TIME&quot;</span>);</span><br><span class="line">        String orderId = gpsMap.get(<span class="string">&quot;ORDERID&quot;</span>);</span><br><span class="line">        String day = time.substring(<span class="number">0</span>,time.indexOf(<span class="string">&quot; &quot;</span>));</span><br><span class="line">        String hour = time.split(<span class="string">&quot; &quot;</span>)[<span class="number">1</span>].substring(<span class="number">0</span>,<span class="number">2</span>);</span><br><span class="line">        <span class="comment">//redis表名,小时订单统计</span></span><br><span class="line">        String hourOrderCountTab = cityCode+<span class="string">&quot;_&quot;</span>+day+<span class="string">&quot;_hour_order_count&quot;</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//redis表名,小时订单ID</span></span><br><span class="line">        String hourOrderField = cityCode+<span class="string">&quot;_&quot;</span>+day+<span class="string">&quot;_&quot;</span>+hour;</span><br><span class="line">        String hourOrder = cityCode+<span class="string">&quot;_order&quot;</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">int</span> hourOrderCount = <span class="number">0</span>;</span><br><span class="line">        <span class="comment">//redis set集合中存放每小时内的所有订单id</span></span><br><span class="line">        <span class="keyword">if</span>(!jedis.sismember(hourOrder,orderId))&#123;</span><br><span class="line">            <span class="comment">//使用set存储小时订单id</span></span><br><span class="line">            jedis.sadd(hourOrder,orderId);</span><br><span class="line">            String hourOrdernum = jedis.hget(hourOrderCountTab, hourOrderField);</span><br><span class="line">            <span class="keyword">if</span>(StringUtils.isEmpty(hourOrdernum))&#123;</span><br><span class="line">                hourOrderCount = <span class="number">1</span>;</span><br><span class="line">            &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">                hourOrderCount = Integer.parseInt(hourOrdernum)+<span class="number">1</span>;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="comment">//HashMap 存储每个小时的订单总数</span></span><br><span class="line">            jedis.hset(hourOrderCountTab,hourOrderField,hourOrderCount+<span class="string">&quot;&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        Logger.getLogger(<span class="string">&quot;org.apache.kafka&quot;</span>).setLevel(Level.INFO);</span><br><span class="line">        <span class="comment">//kafka主题</span></span><br><span class="line">        String topic = <span class="string">&quot;cheng_du_gps_topic&quot;</span>;</span><br><span class="line">        <span class="comment">//消费组id</span></span><br><span class="line">        String groupId = <span class="string">&quot;cheng_du_gps_consumer_01&quot;</span>;</span><br><span class="line"></span><br><span class="line">        GpsConsumer gpsConsumer = <span class="keyword">new</span> GpsConsumer(topic, groupId);</span><br><span class="line">        Thread start = <span class="keyword">new</span> Thread(gpsConsumer);</span><br><span class="line">        start.start();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="2-2-历史订单轨迹回放"><a href="#2-2-历史订单轨迹回放" class="headerlink" title="2.2 历史订单轨迹回放"></a>2.2 历史订单轨迹回放</h3><p><a href="https://lbs.amap.com/api/javascript-api/example/marker/replaying-historical-running-data">高德轨迹回放示例</a></p><p>功能实现流程:</p><p><img src="/images/didi/1571216562760.png" alt="1571216562760"></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line">com.cartravel.ordermonitor.TrackMonitorController</span><br><span class="line"></span><br><span class="line"> <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 查询订单历史轨迹点</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> wrapper</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@PostMapping(&quot;/historyTrackPoints&quot;)</span></span><br><span class="line">    <span class="keyword">public</span> ResultModel&lt;List&lt;TrackPoint&gt;&gt; historyTrackPoints(<span class="meta">@RequestBody</span> QueryWrapper wrapper) &#123;</span><br><span class="line">        <span class="keyword">long</span> startTime = System.currentTimeMillis();</span><br><span class="line">        logger.info(<span class="string">&quot;【查询图形(点线面)】&quot;</span>);</span><br><span class="line">        ResultModel&lt;List&lt;TrackPoint&gt;&gt; result = <span class="keyword">new</span> ResultModel&lt;List&lt;TrackPoint&gt;&gt;();</span><br><span class="line">        Object tmpOrderObj = <span class="keyword">null</span>;</span><br><span class="line">        Order startEndTimeOrder = <span class="keyword">null</span>;</span><br><span class="line">        List&lt;TrackPoint&gt; list = <span class="keyword">null</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            String orderId = wrapper.getOrderId();</span><br><span class="line">            JedisUtil instance = JedisUtil.getInstance();</span><br><span class="line">            Jedis jedis = instance.getJedis();</span><br><span class="line"></span><br><span class="line">            <span class="keyword">byte</span>[] orderBytes = jedis.hget(Constants.ORDER_START_ENT_TIME.getBytes()</span><br><span class="line">                    , orderId.getBytes());</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (orderBytes != <span class="keyword">null</span>) &#123;</span><br><span class="line">                tmpOrderObj = ObjUtil.deserialize(orderBytes);</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (<span class="keyword">null</span> != tmpOrderObj) &#123;</span><br><span class="line">                startEndTimeOrder = (Order) tmpOrderObj;</span><br><span class="line">                String starttime = startEndTimeOrder.getStartTime() + <span class="string">&quot;&quot;</span>;</span><br><span class="line">                String enttime = startEndTimeOrder.getEndTime() + <span class="string">&quot;&quot;</span>;</span><br><span class="line"></span><br><span class="line">                String tableName = Constants.HTAB_GPS;</span><br><span class="line">                list = HBaseUtil.getRest(tableName, wrapper.getOrderId(),</span><br><span class="line">                        starttime, enttime, TrackPoint.class);</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            result.setSuccess(<span class="keyword">true</span>);</span><br><span class="line">            result.setData(list);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            result.setMsg(e.getMessage());</span><br><span class="line">        &#125;</span><br><span class="line">        logger.info(<span class="string">&quot;【查询订单历史轨迹点】msg:&#123;&#125;,time:&#123;&#125;&quot;</span>, result.getMsg(),</span><br><span class="line">                System.currentTimeMillis() - startTime);</span><br><span class="line">        <span class="keyword">return</span> result;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><h1 id=""><a href="#" class="headerlink" title=""></a></h1>]]></content>
      
      
      <categories>
          
          <category> 滴滴智慧出行 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 滴滴智慧出行 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>HDFS HA 机制解析</title>
      <link href="2019/11/12/HDFS%20HA%E6%9C%BA%E5%88%B6%E8%A7%A3%E6%9E%90/"/>
      <url>2019/11/12/HDFS%20HA%E6%9C%BA%E5%88%B6%E8%A7%A3%E6%9E%90/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p><code>Hadoop</code>的高可用是 <code>Hadoop 2.X</code> 版本及以上的特性</p><p><code>Hadoop HA</code> 通过 <code>zookeeper</code> 来实现 <code>namenode</code> 的高可用</p><a id="more"></a><h1 id="1-概述"><a href="#1-概述" class="headerlink" title="1.概述"></a>1.概述</h1><blockquote><p><code>Hadoop</code>的高可用是 <code>Hadoop 2.X</code> 版本及以上的特性</p><p><code>Hadoop HA</code> 通过 <code>zookeeper</code> 来实现 <code>namenode</code> 的高可用</p></blockquote><h1 id="2-关键技术"><a href="#2-关键技术" class="headerlink" title="2. 关键技术"></a>2. 关键技术</h1><p>保持主和备 <code>NameNode</code> 的状态同步</p><blockquote><p>并让 <code>Standby</code> 在 <code>Active</code> 挂掉后迅速提供服务 <code>Namenode</code> 启动比较耗时，包括加载<code>fsimage</code> 和 <code>editlog</code>[获取<code>file to block</code>信息]、处理所有 <code>datanode</code> 第一次<code>blockreport</code> [获取 <code>block to datanode</code> 信息]，保持 <code>NN</code> 的状态同步，需要这两部分信息同步。</p></blockquote><p>脑裂 <code>split-brain</code></p><blockquote><p>指在一个高可用系统中，当联系着的两个节点断开联系时，本来为一个整体的系统，分裂为两个独立节点，这时两个节点开始争抢共享资源，结果会导致系统混乱，数据损坏。</p></blockquote><p><code>NameNode</code> 切换对外透明</p><blockquote><p>主 <code>Namenode</code> 切换到另外一台机器时，不应该导致正在连接的客户端失败，主要包括 <code>Client</code>，<code>Datanode</code>与<code>NameNode</code> 的连接</p></blockquote><h1 id="3-架构"><a href="#3-架构" class="headerlink" title="3. 架构"></a>3. 架构</h1><h2 id="3-1-主备NameNode"><a href="#3-1-主备NameNode" class="headerlink" title="3.1. 主备NameNode"></a>3.1. 主备<code>NameNode</code></h2><blockquote><p><strong>解决单点问题，主 <code>NameNode</code> 对外提供服务，<code>StandBy NameNode</code> 同步``Active NameNode<code> 元数据，以待切换；所有</code>DataNode<code>同时向两个</code>NameNode` 汇报数据块信息</strong></p></blockquote><blockquote><p>两种切换选择</p><ul><li>手动切换:通过命令实现主备之间的切换，可以用 <code>HDFS</code> 升级等场合</li><li>自由切换:基于 <code>Zookeeper</code> 实现；<code>Zookeeper FailOver Controller</code> ：监控 <code>NameNode</code> 健康状态并向<code>Zookeeper</code> 注册 <code>NameNode</code> ，<code>NameNode</code> 挂掉后，<code>ZKFC </code>为 <code>NameNode</code> 竞争锁，获得<code>ZKFC</code>锁的<code>NameNode</code> 变为 <code>active</code></li></ul></blockquote><h2 id="3-2-Journal-Node-集群"><a href="#3-2-Journal-Node-集群" class="headerlink" title="3.2. Journal Node 集群"></a>3.2. <code>Journal Node</code> 集群</h2><blockquote><p><strong><code>Journal node</code> 是根据 <code>paxos</code> 思想来设计的，只有写到一半以上返回成功，就算本次写成功。所以 <code>Journal </code>需要部署 <code>3 </code>台组成一个集群，核心思想是过半 <code>Quorum</code>，异步写到多个<code> Journal Node</code>。</strong></p></blockquote><h1 id="4-机制"><a href="#4-机制" class="headerlink" title="4. 机制"></a>4. 机制</h1><blockquote><p> <code>Active Namenode</code> 每接收到事务请求时，都会先写日志</p></blockquote><p>批量刷磁盘</p><p>这个应该说是写日志的通用做法，如果每来一条日志都刷磁盘，效率很低，如果批量刷盘，就能合并很多小 <code>IO</code></p><p>双缓冲区切换</p><p>bufCurrent 日志写入缓冲区 </p><p>bufReady 即将刷磁盘的缓冲区</p><p>如果没有双缓冲区，我们写日志缓冲区满了，就要强制刷磁盘，我们知道刷磁盘不仅是写到操作系统内核缓冲区，还要刷到磁盘设备上，这是相当费时的操作，引入双缓冲区，在刷磁盘操作和写日志操作可以并发执行，大大提高了Namenode的吞吐量。</p>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
            <tag> Hadoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>HDFS 读写机制</title>
      <link href="2019/11/10/HDFS%E8%AF%BB%E5%86%99%E5%8F%8A%E5%85%B6%E7%9B%B8%E5%85%B3%E6%9C%BA%E5%88%B6/"/>
      <url>2019/11/10/HDFS%E8%AF%BB%E5%86%99%E5%8F%8A%E5%85%B6%E7%9B%B8%E5%85%B3%E6%9C%BA%E5%88%B6/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p>在 HDFS中可能同时有多个客户端在同一时刻写文件，如果不进行控制的话，有可能多个客户端会并发的写一个文件，所以需要进行控制，一般的想法是用一个互斥锁，在某一时刻只有一个客户端进行写操作，但是在分布式系统中有如下问题：</p><a id="more"></a><h1 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h1><h2 id="HDFS-租约机制"><a href="#HDFS-租约机制" class="headerlink" title="HDFS 租约机制"></a>HDFS 租约机制</h2><p>在 HDFS中可能同时有多个客户端在同一时刻写文件，如果不进行控制的话，有可能多个客户端会并发的写一个文件，所以需要进行控制，一般的想法是用一个互斥锁，在某一时刻只有一个客户端进行写操作，但是在分布式系统中有如下问题：</p><ul><li>每次写文件前，客户端需要向 master获取锁情况，他们之间的网络通讯太频繁。</li><li>当某个客户端获取锁之后和 master 失去联系，这个锁一直被该客户端占据，master和其他客户端不能获得锁，后续操作中断。</li></ul><p>在 HDFS 中使用了租约解决上面的问题：</p><ul><li>当写一个文件时，客户端向 NameNode 请求一个租约，租约有个时间期限，在时间期限内客户端可以写租约中管理的文件，一个文件只可能在一个租约内，所以只可能有一个客户端写。</li><li>在租约的有效时间内，客户端不需要向 NameNode 询问是否有写文件的权限，客户端会一直持有，当客户端一直正常的时候，客户端在租约过期的时候会续约。</li><li>当客户端在持有租约期间如果发生异常，和 NameNode 失去联系，在租约期满以后 NameNode 会发现客户端异常，新的租约会赋给其他正常的客户端，当异常客户端已经写了一部分数据， HDFS 为了分辨这些无用的数据，每次写的时候会增加版本号，异常客户端写的数据版本号过低，可以安全的删除掉。</li></ul><h2 id="CRC-校验"><a href="#CRC-校验" class="headerlink" title="CRC 校验"></a>CRC 校验</h2><h1 id="写数据流程"><a href="#写数据流程" class="headerlink" title="写数据流程"></a>写数据流程</h1><h2 id="概述-1"><a href="#概述-1" class="headerlink" title="概述"></a>概述</h2><ul><li><p>客户端向 NameNode 请求上传文件，NameNode 判断、检查目标文件是否已存在、判断 client 是否有权限、父目录是否存在。</p></li><li><p>NameNode 返回是否可以上传。</p></li><li><p>客户端请求第一个 block 上传到哪几个 DataNode 服务器上。</p></li><li><p>NameNode 返回给客户端存储块文件的 DataNode 列表。</p><p><font color='grey'>NameNode 会根据客户端的配置来查询 DataNode 信息，如果使用默认配置，那么最终结果会返回同一个机架的两个 DataNode 和另一个机架的 DataNode。这称为 <font color='blue'><strong>“机架感知”</strong> </font>策略。如：NameNode 返回3个 DataNode 节点，分别为 dn1、dn2、dn3。</font></p><p><font color='grey'><strong>在大多数情况下，副本系数是 3，HDFS 的存放策略是将一个副本存放在本机架的本节点上，一个副本放在同一机架的另一个节点上，最后一个副本放在不同机架的节点上。</strong></font></p><p><font color='grey'>这种策略减少了机架间的数据传输，这就提高了写操作的效率。机架的错误远远比节点的错误少，所以这个策略不会影响到数据的可靠性和可用性。同时，因为数据块只放在两个 [不是三个] 不同的机架上，所以此策略减少了读取数据时需要的网络传输总带宽。</font></p><p><font color='grey'>在这种策略下，副本并不是均匀分布在不同的机架上。三分之一的副本在一个节点上，三分之二的副本在一个机架上，其他副本均匀分布在剩下的机架中，这一策略在不损害数据可靠性和读取性能的情况下改进了写的性能。</font></p></li><li><p>客户端发出请求建立 pipeline。客户端请求 dn1 上传数据，dn1 收到请求会继续调用 dn2，然后 dn2调用dn3，将这个通信管道建立完成</p></li><li><p>客户端先将数据写入数据块 chunk 中（512Byte），当写满 chunk 之后会计算当前 chunk 的校验值checksums [4Byte]，同时将 checksums 写入chunk[chunk总大小为512Byte+4Byte],将chunk写入数据包 package</p><p><font color='grey'>客户端在开始传输数据块之前会把数据缓存在本地，当缓存大小超过了一个数据块的大小，会在客户端和第一个 datanode 建立连接开始流式的传输数据，这个 datanode 会一小部分一小部分[4K]的接收数据然后写入本地仓库，同时会把这些数据传输到第二个datanode，第二个 datanode 也同样一小部分一小部分的接收数据并写入本地仓库，同时传输给第三个 datanode (在流式复制时，逐级传输和响应采用响应队列来等待传输结果。队列响应完成后返回给客户端)，依次类推。这样逐级调用和返回之后，待这个数据块传输完成客户端后告诉 namenode 数据块传输完成，这时候 namenode 才会更新元数据信息记录操作日志。</font></p></li><li><p>第一个数据块传输完成后会使用同样的方式传输下面的数据块直到整个文件上传完成。</p></li></ul><h1 id="读数据"><a href="#读数据" class="headerlink" title="读数据"></a>读数据</h1><h2 id="概述-2"><a href="#概述-2" class="headerlink" title="概述"></a>概述</h2><ul><li>客户端向 NameNode 请求下载文件，NameNode 通过查询元数据，找到文件块所在的 DataNode 地址。</li><li>挑选一台 DataNode[就近原则，然后随机]服务器，请求读取数据。如果第一个 DataNode 无法连接，客户端将自动连接下一个 DataNode</li><li>DataNode 开始传输数据给客户端 [从磁盘里面读取数据放入流，以packet为单位来做校验]</li><li>客户端以 packet 为单位接收，先在本地缓存，然后写入目标文件。</li></ul><h1 id="细节"><a href="#细节" class="headerlink" title="细节"></a>细节</h1><h2 id="写流程"><a href="#写流程" class="headerlink" title="写流程"></a>写流程</h2><h3 id="HDFS-从客户端写入到-DataNode-时，ACK是否三个备份都写成功之后再确认成功操作？"><a href="#HDFS-从客户端写入到-DataNode-时，ACK是否三个备份都写成功之后再确认成功操作？" class="headerlink" title="HDFS 从客户端写入到  DataNode 时，ACK是否三个备份都写成功之后再确认成功操作？"></a>HDFS 从客户端写入到  DataNode 时，ACK是否三个备份都写成功之后再确认成功操作？</h3><p>不是，只要成功写入的节点数量达到 <strong>dfs.replication.min</strong>[默认为1]，那么就任务是写成功的</p><p>正常情况下</p><ul><li><p>在进行写操作的时候[以默认备份 3 份为例]，DataNode_1 接受数据后，首先将数据写入  buffer ，再将数据写入 DataNode_2，写入成功后将 buffer 中的数据写入本地磁盘，并等待 ACK 信息</p></li><li><p>重复上一个步骤，DataNode_2 写入本地磁盘后，等待 ACK 信息</p></li><li><p>如果 ACK 都成功返回后，发送给 Client，本次写入成功</p></li><li><p>如果一个节点或多个节点写入失败：</p></li></ul><p>只要成功写入的节点数量达到 dfs.replication.min [默认为1]，那么就任务是写成功的。然后 NameNode 会通过异步的方式将 block 复制到其他节点，使数据副本达到dfs.replication 参数配置的个数</p>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
            <tag> Hadoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>一条 SQL 查询语句是如何执行的？</title>
      <link href="2019/11/03/%E4%B8%80%E6%9D%A1SQL%E6%9F%A5%E8%AF%A2%E8%AF%AD%E5%8F%A5%E6%98%AF%E5%A6%82%E4%BD%95%E6%89%A7%E8%A1%8C%E7%9A%84%EF%BC%9F/"/>
      <url>2019/11/03/%E4%B8%80%E6%9D%A1SQL%E6%9F%A5%E8%AF%A2%E8%AF%AD%E5%8F%A5%E6%98%AF%E5%A6%82%E4%BD%95%E6%89%A7%E8%A1%8C%E7%9A%84%EF%BC%9F/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p>总体来说，MySQL 可以分为 Server 层和存储引擎层两部分。Server 层包括连接器、查询缓存、分析器、优化器、执行器等，涵盖 MySQL 的大多数核心服务功能，以及所有的内置函数（如日期、时间、数学和加密函数等），所有跨存储引擎的功能都在这一层实现，比如存储过程、触发器、视图等。而存储引擎层负责数据的存储和提取。其架构模式是插件式的，支持 InnoDB、MyISAM、Memory 等多个存储引擎。现在最常用的存储引擎是 InnoDB，它从 MySQL 5.5.5 版本开始成为了默认存储引擎。执行 create table 建表的时候，如果不指定引擎类型，默认使用的就是 InnoDB。不过，你也可以通过指定存储引擎的类型来选择别的引擎，比如在 create table 语句中使用 engine=memory, 来指定使用内存引擎创建表。不同存储引擎的表数据存取方式不同，不同的存储引擎共用一个 Server 层，也就是从连接器到执行器的部分。</p><h1 id="连接器"><a href="#连接器" class="headerlink" title="连接器"></a>连接器</h1><p>第一步，连接到数据库上，连接器负责跟客户端建立连接、获取权限、维持和管理连接。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql -h$ip -P$port -u$user -p</span><br></pre></td></tr></table></figure><h1 id="查询缓存"><a href="#查询缓存" class="headerlink" title="查询缓存"></a>查询缓存</h1><p>连接建立完成后，开始执行 select 语句。执行逻辑就会来到第二步：查询缓存。</p><p>MySQL 拿到一个查询请求后，会先到查询缓存看看，之前是不是执行过这条语句。之前执行过的语句及其结果可能会以 <code>key-value</code> 对的形式，被直接缓存在内存中。key 是查询的语句，value 是查询的结果。如果查询能够直接在这个缓存中找到 key，那么这个 value 就会被直接返回给客户端。如果语句不在查询缓存中，就会继续后面的执行阶段。执行完成后，执行结果会被存入查询缓存中。</p><p>如果查询命中缓存，MySQL 不需要执行后面的复杂操作，就可以直接返回结果，这个效率会很高。</p><p><strong><font color = "blue">注: 大多数情况下建议不使用查询缓存</font></strong></p><blockquote><p>因为查询缓存往往弊大于利。查询缓存的失效非常频繁，只要有对一个表的更新，这个表上所有的查询缓存都会被清空。因此很可能你费劲地把结果存起来，还没使用呢，就被一个更新全清空了。对于更新压力大的数据库来说，查询缓存的命中率会非常低。除非业务就是有一张静态表，很长时间才会更新一次。比如，一个系统配置表，那这张表上的查询才适合使用查询缓存。MySQL 将参数 <code>query_cache_type</code> 设置成 <code>DEMAND</code>，这样对于默认的 SQL 语句都不使用查询缓存。而对于确定要使用查询缓存的语句，可以用 <code>SQL_CACHE</code> 显式指定</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; select SQL_CACHE * from T where ID&#x3D;10</span><br></pre></td></tr></table></figure><p><font color = 'red'>MySQL 8.0 版本直接将查询缓存的整块功能删掉了，也就是说 8.0 开始彻底没有这个功能了</font></p><h1 id="分析器"><a href="#分析器" class="headerlink" title="分析器"></a>分析器</h1><p>如果没有命中查询缓存，就要开始真正执行语句了。首先，MySQL 需要对 SQL 语句做解析。分析器先会做“词法分析”。输入的是由多个字符串和空格组成的一条 SQL 语句，MySQL 需要识别出里面的字符串分别是什么，代表什么。MySQL 从输入的”select”这个关键字识别出来，这是一个查询语句。它也要把字符串 “T” 识别成 “表名 T”,识别以后，要做“语法分析”。根据词法分析的结果，语法分析器会根据语法规则，判断输入的这个 SQL 语句是否满足 MySQL 语法。如果你的语句不对，就会收到 <code>You have an error in your SQL syntax</code>的错误提醒</p><h1 id="优化器"><a href="#优化器" class="headerlink" title="优化器"></a>优化器</h1><p>经过了分析器，MySQL 就知道要做什么了。在开始执行之前，还要先经过优化器的处理。优化器是在表里面有多个索引的时候，决定使用哪个索引；或者在一个语句有多表关联（join）的时候，决定各个表的连接顺序。</p><p>比如执行下面这样的语句，这个语句是执行两个表的 join:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; select * from t1 join t2 using(ID) where t1.c&#x3D;10 and t2.d&#x3D;20;</span><br></pre></td></tr></table></figure><p>既可以先从表 t1 里面取出 c=10 的记录的 ID 值，再根据 ID 值关联到表 t2，再判断 t2 里面 d 的值是否等于 20。也可以先从表 t2 里面取出 d=20 的记录的 ID 值，再根据 ID 值关联到 t1，再判断 t1 里面 c 的值是否等于 10。</p><p>这两种执行方法的逻辑结果是一样的，但是执行的效率会有不同，而优化器的作用就是决定选择使用哪一个方案。</p><p>优化器阶段完成后，语句的执行方案就确定下来，然后进入执行器阶段。</p><h1 id="执行器"><a href="#执行器" class="headerlink" title="执行器"></a>执行器</h1><p>MySQL 通过分析器知道了要做什么，通过优化器知道了该怎么做，于是就进入了执行器阶段，开始执行语句。</p><p>开始执行的时候，要先判断一下表 T 有没有执行查询的权限，如果没有，就会返回没有权限的错误，如下所示 (在工程实现上，如果命中查询缓存，会在查询缓存返回结果的时候，做权限验证。查询也会在优化器之前调用 precheck 验证权限)。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; select * from T where ID &#x3D; 10;ERROR 1142 (42000): SELECT command denied to user &#39;b&#39;@&#39;localhost&#39; for table</span><br></pre></td></tr></table></figure><p>‘T’ 如果有权限，就打开表继续执行。打开表的时候，执行器就会根据表的引擎定义，去使用这个引擎提供的接口。</p><p>比如我们这个例子中的表 T 中，ID 字段没有索引，那么执行器的执行流程是这样的：</p><ol><li>调用 InnoDB 引擎接口取这个表的第一行，判断 ID 值是不是 10，如果不是则跳过，如果是则将这行存在结果集中；</li><li>调用引擎接口取“下一行”，重复相同的判断逻辑，直到取到这个表的最后一行。</li><li>执行器将上述遍历过程中所有满足条件的行组成的记录集作为结果集返回给客户端。</li></ol><p>至此，这个语句就执行完成了。对于有索引的表，执行的逻辑也差不多。第一次调用的是“取满足条件的第一行”这个接口，之后循环取“满足条件的下一行”这个接口，这些接口都是引擎中已经定义好的。</p><p>数据库的慢查询日志中看到一个 <code>rows_examined</code> 的字段，表示这个语句执行过程中扫描了多少行。这个值就是在执行器每次调用引擎获取数据行的时候累加的。在有些场景下，执行器调用一次，在引擎内部则扫描了多行，因此引擎扫描行数跟 rows_examined 并不是完全相同的。</p>]]></content>
      
      
      <categories>
          
          <category> MySQL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数据库 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hadoop 序列化机制</title>
      <link href="2019/10/17/Hadoop%E5%BA%8F%E5%88%97%E5%8C%96%E6%9C%BA%E5%88%B6/"/>
      <url>2019/10/17/Hadoop%E5%BA%8F%E5%88%97%E5%8C%96%E6%9C%BA%E5%88%B6/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p>对象序列化**是一个用于将内存中对象状态转换为字节流的过程，可以将其保存到磁盘文件中或通过网络发送到任何其他程序</p><p>从字节流创建对象的相反的过程称为<strong>反序列化</strong>。而创建的字节流是与平台无关的，在一个平台上序列化的对象可以在不同的平台上反序列化。</p><a id="more"></a><h1 id="1-概述"><a href="#1-概述" class="headerlink" title="1. 概述"></a>1. 概述</h1><h2 id="1-1-什么是序列化-？"><a href="#1-1-什么是序列化-？" class="headerlink" title="1.1. 什么是序列化 ？"></a>1.1. 什么是序列化 ？</h2><blockquote><p><strong>对象序列化</strong>是一个用于将内存中对象状态转换为字节流的过程，可以将其保存到磁盘文件中或通过网络发送到任何其他程序</p></blockquote><blockquote><p>从字节流创建对象的相反的过程称为<strong>反序列化</strong>。而创建的字节流是与平台无关的，在一个平台上序列化的对象可以在不同的平台上反序列化。</p></blockquote><h2 id="1-2-为什么要序列化-？"><a href="#1-2-为什么要序列化-？" class="headerlink" title="1.2. 为什么要序列化 ？"></a>1.2. 为什么要序列化 ？</h2><h2 id="1-3-Java-序列化实现"><a href="#1-3-Java-序列化实现" class="headerlink" title="1.3. Java 序列化实现"></a>1.3. <code>Java</code> 序列化实现</h2><h2 id="1-4-Hadoop-序列化"><a href="#1-4-Hadoop-序列化" class="headerlink" title="1.4. Hadoop 序列化"></a>1.4. <code>Hadoop</code> 序列化</h2><blockquote><p><code>Java</code> 的序列化机制的缺点就是计算量开销大，且序列化的结果体积大，有时能达到对象大小的数倍乃至十倍。</p><p>它的引用机制也会导致大文件不能分割的问题。这些缺点使得 <code>Java</code> 的序列化机制对<code>Hadoop</code>来说是不合适的。于是<code>Hadoop</code>设计了自己的序列化机制。</p></blockquote><blockquote><p><code>hadoop</code> 提供了 <code>Writable</code> 接口实现序列化。<code>Hadoop2.x</code> 里面包含了<code>avro</code> 和<code>protocol buffer</code>。</p><p><code>Hadoop</code> 序列化没有提供比较功能，所以和 <code>Java</code> 中的<code>Comparable</code>接口合并，提供一个接口<code>WritableComparable</code>。[自定义比较]</p></blockquote><h2 id="1-5-Hadoop-序列化特点"><a href="#1-5-Hadoop-序列化特点" class="headerlink" title="1.5. Hadoop 序列化特点"></a>1.5. <code>Hadoop</code> 序列化特点</h2><h1 id="2-自定义序列化"><a href="#2-自定义序列化" class="headerlink" title="2. 自定义序列化"></a>2. 自定义序列化</h1><h2 id="2-1-概述"><a href="#2-1-概述" class="headerlink" title="2.1. 概述"></a>2.1. 概述</h2><h2 id="2-2-步骤"><a href="#2-2-步骤" class="headerlink" title="2.2. 步骤"></a>2.2. 步骤</h2><ul><li><p>必须实现 **<code>Writable</code> ** 接口</p></li><li><p>反序列化的时候，需要反射调用空参构造函数，必须有空参构造函数</p></li><li><p>重写序列化方法</p></li><li><p>重写反序列化方法</p><blockquote><p> 注意: 反序列化的顺序和序列化的顺序必须一致</p></blockquote></li><li><p>如果要想把结果显示在文件中，需要重写 <code>toString()</code>,可用<code>\t</code>分开，方便后续使用</p></li><li><p>如果需要自定义的 <code>bean</code> 放在 <code>key</code> 中传输，则还需要实现 <code>Comparable</code> 接口，因为 <code>MapReduce</code> 框架中的 <code>shuffle</code> 过程要求对 <code>key</code> 必须能排序 </p></li></ul><h2 id="2-3-案例"><a href="#2-3-案例" class="headerlink" title="2.3. 案例"></a>2.3. 案例</h2><h1 id="3-序列化框架"><a href="#3-序列化框架" class="headerlink" title="3. 序列化框架"></a>3. 序列化框架</h1>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
            <tag> Hadoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hadoop Federation</title>
      <link href="2019/10/15/Federation%20/"/>
      <url>2019/10/15/Federation%20/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p>HDFS Federation 是解决 NameNode 内存瓶颈问题的水平横向扩展方案。</p><a id="more"></a><h1 id="1-概述"><a href="#1-概述" class="headerlink" title="1. 概述"></a>1. 概述</h1><p>HDFS 主要有两大模块：</p><blockquote><ul><li><strong>Namespace</strong></li></ul><blockquote><p>包括目录、文件和块。</p></blockquote><blockquote><p>它支持所有和命名空间相关的文件操作，如创建、删除、修改，查看所有文件和目录。</p></blockquote><ul><li><p>**Block Storage Service ** **[块存储服务]**包括两部分</p><ul><li><p>在 namenode 中的块的管理</p><ul><li>提供 datanode 集群的注册、心跳检测等功能。</li><li>处理块的报告信息和维护块的位置信息。</li><li>支持块相关的操作，如创建、删除、修改、获取块的位置信息。</li><li>管理块的冗余信息、创建副本、删除多余的副本等。</li></ul></li><li><p>存储</p><blockquote><p>datanode 提供本地文件系统上块的存储、读写、访问等。</p></blockquote></li></ul></li></ul><p>以前的 HDFS 框架整个集群只允许有一个namenode，一个 namenode管理所有的命名空间，HDFS 联邦通过增加多个 namenode 来打破这种限制。</p><p>单 NameNode 的架构使得 HDFS 在集群扩展性和性能上都有潜在的问题。当集群大到一定程度后，NameNode 进程使用的内存可能会达到上百 G，NameNode 成为了性能的瓶颈。因而提出了 namenode 水平扩展方案– Federation</p><p>hdfs federation 即 hdfs 的联邦，可以简单理解为多个 hdfs 集群聚合到一起，更准确的理解是有多个namenode节点的 hdfs 集群</p></blockquote><h1 id="2-架构"><a href="#2-架构" class="headerlink" title="2. 架构"></a>2. 架构</h1><h2 id="2-1-概述"><a href="#2-1-概述" class="headerlink" title="2.1. 概述"></a>2.1. 概述</h2><p>HDFS Federation 是解决 NameNode 内存瓶颈问题的水平横向扩展方案。</p><p>NameNode 之间相互独立，各自管理自己的区域，且不需要互相协调，一个 NameNode 挂掉了不会影响其他的 NameNode</p><p>DataNode 被用作通用的数据存储设备，每个 DataNode 要向集群中所有的 NameNode注册，且周期性的向所有NameNode 发送心跳和报告，并执行来自所有 NameNode 的命令.</p><p>一个Block Pool由属于同一个 NameSpace 的数据块组成，每个 DataNode 可能会存储集群中所有 Block Pool 数据块，每个Block Pool内部自治，各自管理各自的Block，不会与其他 Block Pool交流<br>NameNode 和 Block Pool 一起被称作 Namespace Volume，它是管理的基本单位，当一个 namespace 被删除后，所有 datanode 上与其对应的 block pool 也会被删除。当集群升级时，每个 namespace volume 作为一个基本单元进行升级</p><h2 id="2-2-Block-Pool"><a href="#2-2-Block-Pool" class="headerlink" title="2.2.  Block Pool"></a>2.2.  Block Pool</h2><p>一个块池就是属于一个namespace的一组块。datanodes存储集群中所有的块池，它独立于其它块池进行管理。这允许namespace在不与其它namespace交互的情况下生成块的ID，有故障的namenode不影响datanode继续为集群中的其它namenode服务。一个namespace和它的blockpool一起叫做namespace volume，这是一个自己的管理单位，当一个namenode被删除，那么在datanode上的相应的block pool也会被删除。在集群进行升级的时候，每一个namespace volume独立的进行升级。</p><h2 id="2-3-ClusterID"><a href="#2-3-ClusterID" class="headerlink" title="2.3. ClusterID"></a>2.3. ClusterID</h2><p>增加一个新的ClusterID标识来在集群中所有的节点。当一个namenode被格式化的时候，这个标识被指定或自动生成，这个ID会用于格式化集群中的其它namenode。</p><h2 id="2-5-不足"><a href="#2-5-不足" class="headerlink" title="2.5. 不足"></a>2.5. 不足</h2><p>HDFS Federation 并没有完全解决单点故障问题。虽然 NameNode 存在多个，但是从单个NameNode 看，仍然存在单点故障：<br>如果某个 NameNode 挂掉了，其管理的相应的文件便不可以访问。 Federation 中每个NameNode 仍然像之前 HDFS 上实现一样，配有一个 Secondary NameNode，以便主 NameNode 挂掉，用于还原元数据信息。<br>所以一般集群规模很大的时，会采用 HA+Federation 的部署方案</p><h1 id="3-配置"><a href="#3-配置" class="headerlink" title="3. 配置"></a>3. 配置</h1>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Spark 架构设计</title>
      <link href="2019/10/10/Spark%E6%9E%B6%E6%9E%84/"/>
      <url>2019/10/10/Spark%E6%9E%B6%E6%9E%84/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><h1 id="Spark-核心组件"><a href="#Spark-核心组件" class="headerlink" title="Spark 核心组件"></a>Spark 核心组件</h1><h3 id="Driver"><a href="#Driver" class="headerlink" title="Driver"></a>Driver</h3><p>Spark 驱动器节点，用于执行 Spark 任务中的 main 方法， 负责实际代码的执行工作。Driver 在 Spark 作业执行时主要负责：</p><ol><li><p>将用户程序转化为任务[job]</p></li><li><p>Executor 之间调度任务task</p></li><li><p>跟踪 Executor 的执行情况；</p></li><li><p>通过 UI 展示查询运行情况</p></li></ol><h3 id="Executor"><a href="#Executor" class="headerlink" title="Executor"></a>Executor</h3><p>Spark Executor 节点是一个 JVM 进程，负责在 Spark 作业中运行 具体 任务，任 务 彼此之 间相互独立。 Spark 应用启动时， Executor 节点被同时启动， 并且 始终伴 随着整个 Spark 应用的生命周期而存在。 如果有 Executor 节点发生了故障或崩溃 ， Spark 应用也可以继续执行， 会将出错节点上的任务调度到其他 Executor 节点上继 续运行。</p><p>Executor 有两个核心功能：</p><ol><li><p>负责运行组成 Spark 应用的任务，并将结果返回给 Driver 进程；</p></li><li><p>它们通过自身的块管理器（ Block Manager ）为用户程序中要求缓存的 RDD</p></li></ol><h2 id="运行流程概述"><a href="#运行流程概述" class="headerlink" title="运行流程概述"></a>运行流程概述</h2><p>![image-20200131155431461](/Users/zxc/Library/Application Support/typora-user-images/image-20200131155431461.png)</p><h2 id="Spark-部署模式"><a href="#Spark-部署模式" class="headerlink" title="Spark 部署模式"></a>Spark 部署模式</h2><h3 id="本地模式"><a href="#本地模式" class="headerlink" title="本地模式"></a>本地模式</h3><p>Local[N] 模式，用单机的多个线程来模拟 Spark 分布式计算，直接运行在本地，便于调试，通常用来验证开发出来的应用程序逻辑上有没有问题。</p><p>其中 N 代表可以使用 N 个线程，每个线程拥有一个 core。如果不指定 N，则默认是1个线程，该线程有1个core。</p><ul><li>local 只启动一个 executor</li><li>local[k] 启动 k 个executor</li><li>local[*] 启动 和 cpu 数目相同的 executor</li></ul><h3 id="Standalone-模式"><a href="#Standalone-模式" class="headerlink" title="Standalone 模式"></a>Standalone 模式</h3><p>独立模式，自带完整的服务，可单独部署到一个集群中，无需依赖任何其他资源管理系统。</p><h3 id="Spark-On-Mesos-模式"><a href="#Spark-On-Mesos-模式" class="headerlink" title="Spark On Mesos 模式"></a>Spark On Mesos 模式</h3><p>Spark 运行在 Mesos 上会比运行在 YARN 上更加灵活，更加自然。目前在 Spark On Mesos 环境中，用户可选择两种调度模式之一运行自己的应用程序。 </p><ul><li><p>粗粒度模式</p><blockquote><p>每个应用程序的运行环境由一个 Dirver 和若干个 Executor 组成，其中，每个 Executor 占用若干资源，内部可运行多个Task。应用程序的各个任务正式运行之前，需要将运行环境中的资源全部申请好，且运行过程中要一直占用这些资源，即使不用，最后程序运行结束后，回收这些资源。</p></blockquote></li><li><p>细粒度模式</p><blockquote><p>鉴于粗粒度模式会造成大量资源浪费，Spark On Mesos 还提供了另外一种调度模式：细粒度模式，这种模式类似于现在的云计算，思想是按需分配。与粗粒度模式一样，应用程序启动时，先会启动executor，但每个 executor 占用资源仅仅是自己运行所需的资源，不需要考虑将来要运行的任务，之后，mesos 会为每个 executor 动态分配资源，每分配一些，便可以运行一个新任务，单个 Task 运行完之后可以马上释放对应的资源。每个 Task 会汇报状态给 Mesos slave 和 Mesos Master ，便于更加细粒度管理和容错，这种调度模式类似于 MapReduce 调度模式，每个 Task 完全独立，优点是便于资源控制和隔离，但缺点也很明显，短作业运行延迟大。</p></blockquote></li></ul><h3 id="Spark-On-YARN-模式"><a href="#Spark-On-YARN-模式" class="headerlink" title="Spark On YARN 模式"></a>Spark On YARN 模式</h3><p>目前仅支持粗粒度模式。这是由于 YARN 上的 Container 资源是不可以动态伸缩的，一旦 Container 启动之后，可使用的资源不能再发生变化，不过这个已经在 YARN 计划中了。 </p><p>spark on yarn 的支持两种模式： </p><ol><li>yarn-cluster：适用于生产环境； </li><li>yarn-client：适用于交互、调试，希望立即看到 app 的输出 </li></ol><p>yarn-cluster 和 yarn-client 的区别在于 yarn appMaster，每个 yarn app 实例有一个 appMaster进程，是为 app 启动的第一个 container</p><p>负责从 ResourceManager 请求资源，获取到资源后，告诉 NodeManager 为其启动 container。</p><h2 id="运行机制"><a href="#运行机制" class="headerlink" title="运行机制"></a>运行机制</h2><h3 id="Standalone-模式运行机制"><a href="#Standalone-模式运行机制" class="headerlink" title="Standalone 模式运行机制"></a>Standalone 模式运行机制</h3><p>在 Standalone Client 模式下，Driver 在任务提交的本地机器上运行，Driver 启动后向 Master 注册应用程序，Master 根据 submit 脚本的资源需求找到内部资源至少可以启动一个 Executor 的所有 Worker ，然后在这些 Worker 之间分配 Executor ，Worker 上的 Executor 启动后会向 Driver 反向注册，所有的 Executor 注册完成后，Driver 开 始执行 main 函数， 之后执行到 Action 算子时 ， 开始划分 stage ， 每个 stage 生成对 应 的 taskSet ， 之后将task 分发到各个 Executor 上执行。</p><h3 id="YARN-模式运行机制"><a href="#YARN-模式运行机制" class="headerlink" title="YARN 模式运行机制"></a>YARN 模式运行机制</h3><h5 id="YARN-Client-模式"><a href="#YARN-Client-模式" class="headerlink" title="YARN  Client 模式"></a><font color='blue'>YARN  Client 模式</font></h5><p>在 YARN Client 模式下， Driver 在任务提交的本地机器上运行， Driver 启动后会和 ResourceManager 通讯申请启动 Application Master，随后 ResourceManager 分配 container ， 在合适的 NodeManager 上启动 ApplicationMaster，此时的 ApplicationMaster 的功能相当于一个 Executor Laucher，只 负责向 ResourceManager 申请 Executor 内存 。</p><p>ResourceManager 接到 ApplicationMaster 的资源申请后会分配 container，然后 ApplicationMaster 在资源分配指定的 NodeManager 上启动 Executor 进程， Executor 进程启动后会向 Driver 反向注册， Executor 全部注册完成后 Driver 开始执行 main 函数，之后执行到 Action 算子时， 触发一个job，并根据宽依赖开始划分 stage ， 每个 stage 生成对应的 taskSet，之后将 task 分发到各个 Executor 上执行 。</p><p>![屏幕快照 2020-03-23 上午12.33.41](/Users/zxc/Documents/hexo/source/_posts/Spark架构.assets/屏幕快照 2020-03-23 上午12.33.41.png)</p><p><font color='blue'><strong>YARN Cluster模式</strong></font></p><p>在 Yarn-Cluster 模式中，当用户向 Yarn 中提交一个应用程序后， Yarn 将分两个阶段运行该应用程序：第一个阶段是把 Spark 的 Driver 作为一个 ApplicationMaster 在 Yarn 集群中先启动；第二个阶段是由 ApplicationMaster 创建应用程序，然后为它向 ResourceManager 申请资源，并启动 Executor 来运行 Task，同时监控它的整个运行过程，直到运行完成。</p><p>![屏幕快照 2020-03-23 上午12.27.59](/Users/zxc/Documents/hexo/source/_posts/Spark架构.assets/屏幕快照 2020-03-23 上午12.27.59.png)</p><p>在 YARNCluster 模式下，任务提交后会和 ResourceManager 通讯申请启动 Application Master ， 随后 ResourceManager 分配 container， 在合适的 NodeManager 上启动 ApplicationMaster ， 此时的 ApplicationMaster 就是 Driver 。</p><p>Driver 启动后 向 ResourceManager 申请 Executor 内存， ResourceManager 接到 ApplicationMaster 的资源申请后会分配 container， 然后在合适的 NodeManager 上启动 Executor 进程， Executor 进程启动后会向 Driver 反向注册， Executor 全部注册完 成后 Driver 开始执行 main 函数，之后执行到 Action 算子时，触发一个 job ，并根据 宽依赖 开始划分 stage ， 每个 stage 生成对应 的 taskSet ， 之后将 task 分发到各个 Executor 上执行。</p>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>对扩展开放&amp;对修改关闭</title>
      <link href="2019/10/09/%E5%AF%B9%E6%89%A9%E5%B1%95%E5%BC%80%E6%94%BE&amp;%E4%BF%AE%E6%94%B9%E5%85%B3%E9%97%AD/"/>
      <url>2019/10/09/%E5%AF%B9%E6%89%A9%E5%B1%95%E5%BC%80%E6%94%BE&amp;%E4%BF%AE%E6%94%B9%E5%85%B3%E9%97%AD/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p>开闭原则的英文全称是 <code>Open Closed Principle</code>，简写为 OCP。英文描述是</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">software entities (modules, classes, functions, etc.) should be open for extension , but closed for modification。</span><br></pre></td></tr></table></figure><p>翻译成中文就是：软件实体（模块、类、方法等）应该 “对扩展开放、对修改关闭”。</p><a id="more"></a><p>添加一个新的功能应该是，在已有代码基础上扩展代码（新增模块、类、方法等），而非修改已有代码（修改模块、类、方法等）。</p><p>这是一段 API 接口监控告警的代码。</p><ol><li><code>AlertRule</code> 存储告警规则，可以自由设置。</li><li><code>Notification</code> 是告警通知类，支持邮件、短信、微信、手机等多种通知渠道。</li><li><code>NotificationEmergencyLevel</code> 表示通知的紧急程度，包括 SEVERE（严重）、URGENCY（紧急）、NORMAL（普通）、TRIVIAL（无关紧要），不同的紧急程度对应不同的发送渠道。</li></ol><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Alert</span> </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> AlertRule rule;</span><br><span class="line">  <span class="keyword">private</span> Notification notification;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="title">Alert</span><span class="params">(AlertRule rule, Notification notification)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">this</span>.rule = rule;</span><br><span class="line">    <span class="keyword">this</span>.notification = notification;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">check</span><span class="params">(String api, <span class="keyword">long</span> requestCount, <span class="keyword">long</span> errorCount, <span class="keyword">long</span> durationOfSeconds)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">long</span> tps = requestCount / durationOfSeconds;</span><br><span class="line">    <span class="keyword">if</span> (tps &gt; rule.getMatchedRule(api).getMaxTps()) &#123;</span><br><span class="line">      notification.notify(NotificationEmergencyLevel.URGENCY, <span class="string">&quot;...&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (errorCount &gt; rule.getMatchedRule(api).getMaxErrorCount()) &#123;</span><br><span class="line">      notification.notify(NotificationEmergencyLevel.SEVERE, <span class="string">&quot;...&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 设计模式 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 设计模式 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>单一职责原则</title>
      <link href="2019/10/09/%E5%8D%95%E4%B8%80%E8%81%8C%E8%B4%A3%E5%8E%9F%E5%88%99/"/>
      <url>2019/10/09/%E5%8D%95%E4%B8%80%E8%81%8C%E8%B4%A3%E5%8E%9F%E5%88%99/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p>单一职责原则 <code>Single Responsibility Principle</code>，缩写为 SRP。这个原则的英文描述是这样的</p><p><code>A class or module should have a single responsibility</code> 翻译成中文就是：一个类或者模块只负责完成一个职责（或者功能）。</p><a id="more"></a><p><code>A class or module should have a single responsibility</code> 原则描述的对象包含两个，一个是类（class），一个是模块（module）。有两种理解方式。</p><ol><li>一种理解是: 把模块看作比类更加抽象的概念，类也可以看作模块。</li><li>另一种理解是: 把模块看作比类更加粗粒度的代码块，模块中包含多个类，多个类组成一个模块。</li></ol><p>单一职责原则: 一个类只负责完成一个职责或者功能。也就是说，不要设计大而全的类，要设计粒度小、功能单一的类。换个角度来讲就是，一个类包含了两个或者两个以上业务不相干的功能，就说它职责不够单一，应该将它拆分成多个功能更加单一、粒度更细的类。</p><h1 id="如何判断类的职责是否足够单一？"><a href="#如何判断类的职责是否足够单一？" class="headerlink" title="如何判断类的职责是否足够单一？"></a>如何判断类的职责是否足够单一？</h1><p>在一个社交产品中，我们用下面的 UserInfo 类来记录用户的信息。UserInfo 类的设计是否满足单一职责原则呢？</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">UserInfo</span> </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">long</span> userId;</span><br><span class="line">  <span class="keyword">private</span> String username;</span><br><span class="line">  <span class="keyword">private</span> String email;</span><br><span class="line">  <span class="keyword">private</span> String telephone;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">long</span> createTime;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">long</span> lastLoginTime;</span><br><span class="line">  <span class="keyword">private</span> String avatarUrl;</span><br><span class="line">  </span><br><span class="line">  <span class="keyword">private</span> String provinceOfAddress; <span class="comment">// 省</span></span><br><span class="line">  <span class="keyword">private</span> String cityOfAddress; <span class="comment">// 市</span></span><br><span class="line">  <span class="keyword">private</span> String regionOfAddress; <span class="comment">// 区 </span></span><br><span class="line">  <span class="keyword">private</span> String detailedAddress; <span class="comment">// 详细地址</span></span><br><span class="line">  <span class="comment">// ...省略其他属性和方法...</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>有两种不同的观点。</p><ol><li>一种观点是，UserInfo 类包含的都是跟用户相关的信息，所有的属性和方法都隶属于用户这样一个业务模型，满足单一职责原则；</li><li>另一种观点是，地址信息在 UserInfo 类中，所占的比重比较高，可以继续拆分成独立的 UserAddress 类，UserInfo 只保留除 Address 之外的其他信息，拆分之后的两个类的职责更加单一。</li></ol><p><strong>哪种观点更对呢 ？</strong></p><blockquote><p>实际上，要从中做出选择，不能脱离具体的应用场景。如果在这个社交产品中，用户的地址信息跟其他信息一样，只是单纯地用来展示，那 <code>UserInfo</code> 现在的设计就是合理的。但是，如果这个社交产品发展得比较好，之后又在产品中添加了电商的模块，用户的地址信息还会用在电商物流中，那最好将地址信息从 <code>UserInfo</code> 中拆分出来，独立成用户物流信息（或者叫地址信息、收货信息等）。</p></blockquote><p>再进一步延伸一下。如果做这个社交产品的公司发展得越来越好，公司内部又开发出了很多其他产品。公司希望支持统一账号系统，也就是用户一个账号可以在公司内部的所有产品中登录。这个时候，我们就需要继续对 <code>UserInfo</code> 进行拆分，将跟身份认证相关的信息（比如，email、telephone 等）抽取成独立的类。</p><p>不同的应用场景、不同阶段的需求背景下，对同一个类的职责是否单一的判定，可能都是不一样的。在某种应用场景或者当下的需求背景下，一个类的设计可能已经满足单一职责原则了，但如果换个应用场景或着在未来的某个需求背景下，可能就不满足了，需要继续拆分成粒度更细的类。</p><p>除此之外，从不同的业务层面去看待同一个类的设计，对类是否职责单一，也会有不同的认识。比如，例子中的 <code>UserInfo</code> 类。如果从 “用户” 这个业务层面来看，<code>UserInfo</code> 包含的信息都属于用户，满足职责单一原则。如果我们从更加细分的 “用户展示信息” “地址信息” “登录认证信息” 等等这些更细粒度的业务层面来看，那 <code>UserInfo</code> 就应该继续拆分。</p><h1 id="类的职责是否设计得越单一越好-？"><a href="#类的职责是否设计得越单一越好-？" class="headerlink" title="类的职责是否设计得越单一越好 ？"></a>类的职责是否设计得越单一越好 ？</h1><p>为了满足单一职责原则，是不是把类拆得越细就越好呢？答案是否定的。通过一个例子来解释一下。Serialization 类实现了一个简单协议的序列化和反序列功能，具体代码如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"> * Protocol format: identifier-string;&#123;gson string&#125;</span></span><br><span class="line"><span class="comment"> * For example: UEUEUE;&#123;&quot;a&quot;:&quot;A&quot;,&quot;b&quot;:&quot;B&quot;&#125;</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">   <span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Serialization</span> </span>&#123;</span><br><span class="line">     <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String IDENTIFIER_STRING = <span class="string">&quot;UEUEUE;&quot;</span>;</span><br><span class="line">     <span class="keyword">private</span> Gson gson;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="title">Serialization</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">this</span>.gson = <span class="keyword">new</span> Gson();</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> String <span class="title">serialize</span><span class="params">(Map&lt;String, String&gt; object)</span> </span>&#123;</span><br><span class="line">    StringBuilder textBuilder = <span class="keyword">new</span> StringBuilder();</span><br><span class="line">    textBuilder.append(IDENTIFIER_STRING);</span><br><span class="line">    textBuilder.append(gson.toJson(object));</span><br><span class="line">    <span class="keyword">return</span> textBuilder.toString();</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> Map&lt;String, String&gt; <span class="title">deserialize</span><span class="params">(String text)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (!text.startsWith(IDENTIFIER_STRING)) &#123;</span><br><span class="line">        <span class="keyword">return</span> Collections.emptyMap();</span><br><span class="line">    &#125;</span><br><span class="line">    String gsonStr = text.substring(IDENTIFIER_STRING.length());</span><br><span class="line">    <span class="keyword">return</span> gson.fromJson(gsonStr, Map.class);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>如果想让类的职责更加单一，对 <code>Serialization</code> 类进一步拆分，拆分成一个只负责序列化工作的 <code>Serializer</code> 类和另一个只负责反序列化工作的 <code>Deserializer</code> 类。拆分后的具体代码如下所示：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Serializer</span> </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String IDENTIFIER_STRING = <span class="string">&quot;UEUEUE;&quot;</span>;</span><br><span class="line">  <span class="keyword">private</span> Gson gson;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="title">Serializer</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">this</span>.gson = <span class="keyword">new</span> Gson();</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> String <span class="title">serialize</span><span class="params">(Map&lt;String, String&gt; object)</span> </span>&#123;</span><br><span class="line">    StringBuilder textBuilder = <span class="keyword">new</span> StringBuilder();</span><br><span class="line">    textBuilder.append(IDENTIFIER_STRING);</span><br><span class="line">    textBuilder.append(gson.toJson(object));</span><br><span class="line">    <span class="keyword">return</span> textBuilder.toString();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Deserializer</span> </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String IDENTIFIER_STRING = <span class="string">&quot;UEUEUE;&quot;</span>;</span><br><span class="line">  <span class="keyword">private</span> Gson gson;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="title">Deserializer</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">this</span>.gson = <span class="keyword">new</span> Gson();</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> Map&lt;String, String&gt; <span class="title">deserialize</span><span class="params">(String text)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (!text.startsWith(IDENTIFIER_STRING)) &#123;</span><br><span class="line">        <span class="keyword">return</span> Collections.emptyMap();</span><br><span class="line">    &#125;</span><br><span class="line">    String gsonStr = text.substring(IDENTIFIER_STRING.length());</span><br><span class="line">    <span class="keyword">return</span> gson.fromJson(gsonStr, Map.class);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>虽然经过拆分之后，<code>Serializer</code> 类和 <code>Deserializer</code> 类的职责更加单一了，但也随之带来了新的问题。如果修改了协议的格式，数据标识从 <code>UEUEUE</code> 改为 <code>DFDFDF</code>，或者序列化方式从 JSON 改为了 XML，那 Serializer 类和 Deserializer 类都需要做相应的修改，代码的内聚性显然没有原来 Serialization 高了。而且，如果仅仅对 Serializer 类做了协议修改，而忘记了修改 Deserializer 类的代码，那就会导致序列化、反序列化不匹配，程序运行出错，也就是说，拆分之后，代码的可维护性变差了。</p><p>不管是应用设计原则还是设计模式，最终的目的还是提高代码的可读性、可扩展性、复用性、可维护性等。在考虑应用某一个设计原则是否合理的时候，也可以以此作为最终的考量标准。</p>]]></content>
      
      
      <categories>
          
          <category> 设计模式 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 设计模式 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello 设计模式</title>
      <link href="2019/10/08/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"/>
      <url>2019/10/08/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p>设计模式是一套被反复使用的、多数人知晓的、经过分类编目的、代码设计经验的总结。使用设计模式是为了重用代码、让代码更容易被他人理解、保证代码可靠性。</p><a id="more"></a><h2 id="原则"><a href="#原则" class="headerlink" title="原则"></a>原则</h2><ol><li><p><strong>单一职责原则</strong>（Single Responsibility Principle）</p><p>单一职责原则指的是应该有且仅有一个原因引起类的变更</p><p>分清职责，接口一定要做到单一职责，方法也要做到，类尽量做到</p><p>对于一个类/接口/方式而言之负责一个职责或职能。比如说A类负责两个不同的职责，职责1和职责2，当职责1发生需求变更而修改时，有可能会造成职责2执行错误，这是后需要将A类拆分为A1和A2两个。这样做的有点：1.降低了类的复杂性。2.提高了类的可读性，因为一个类只负责一个职责，看起来比较有目的性。3.提高系统的可维护性，降低了当需求变更时修改程序带来的风险。但是，如果一位的最求单一职责原则，有时候可能会造成类爆炸的问题，所以使用时需要谨慎的看待这一点，不过，接口和方法必须要遵守这一原则。</p></li><li><p><strong>开闭原则</strong></p><p>开闭原则的意思是：<strong>对扩展开放，对修改关闭</strong>。在程序需要进行拓展的时候，不能去修改原有的代码，实现一个热插拔的效果。简言之，是为了使程序的扩展性好，易于维护和升级。想要达到这样的效果，我们需要使用接口和抽象类</p><p>一个软件实体如类、模块和函数应该对扩展开放，对修改关闭</p></li><li><p><strong>依赖倒转原则</strong>（Dependence Inversion Principle）</p><p>针对接口编程，依赖与抽象而不是具体类</p></li><li><p><strong>里氏代换原则</strong>（Liskov Substitution Principle）</p><p>里氏代换原则是面向对象设计的基本原则之一。 里氏代换原则中说，任何基类可以出现的地方，子类一定可以出现。LSP 是继承复用的基石，只有当派生类可以替换掉基类，且软件单位的功能不受到影响时，基类才能真正被复用，而派生类也能够在基类的基础上增加新的行为。里氏代换原则是对开闭原则的补充。实现开闭原则的关键步骤就是抽象化，而基类与子类的继承关系就是抽象化的具体实现，所以里氏代换原则是对实现抽象化的具体步骤的规范。</p></li><li><p><strong>接口隔离原则</strong></p><p>把一个臃肿的接口变更为两个独立的接口所依赖的原则就是接口隔离原则</p></li><li><p><strong>迪米特法则</strong></p><p>一个实体应当尽量少地与其他实体之间发生相互作用，使得系统功能模块相对独立</p></li><li><p><strong>合成复用原则</strong></p><p>尽量使用合成/聚合的方式，而不是使用继承</p></li></ol><h2 id="设计模式的类型"><a href="#设计模式的类型" class="headerlink" title="设计模式的类型"></a>设计模式的类型</h2><h3 id="创建型"><a href="#创建型" class="headerlink" title="创建型"></a>创建型</h3><p>创建型模式的主要关注点是”怎样创建对象？”，它的主要特点是“将对象的创建与使用分离”。这样可以降低系统的耦合度，使用者不需要关注对象的创建细节，对象的创建由相关的工厂来完成。</p><ol><li>单例模式</li><li>工厂方法模式（工厂模式）</li><li>抽象工厂模式</li><li>建造者模式（生成器模式）</li><li>原型模式</li></ol><p>以上 5 种创建型模式，除了工厂方法模式属于类创建型模式，其他的全部属于对象创建型模式，</p><h3 id="结构型模式："><a href="#结构型模式：" class="headerlink" title="结构型模式："></a>结构型模式：</h3><ol><li>适配器模式（变压器模式/包装模式）</li><li>桥接模式（桥梁模式）</li><li>组合模式（合成模式/部分-整体模式）</li><li>装饰模式（装饰器模式）</li><li>外观模式（门面模式）</li><li>享元模式</li><li>代理模式（委托模式）</li></ol><h3 id="行为型模式："><a href="#行为型模式：" class="headerlink" title="行为型模式："></a>行为型模式：</h3><ol><li>观察者模式（发布订阅模式）</li><li>模板方法模式</li><li>命令模式</li><li>状态模式</li><li>职责链模式（责任链模式）</li><li>解释器模式</li><li>中介者模式</li><li>访问者模式</li><li>策略模式</li><li>备忘录模式</li><li>迭代器模式</li></ol>]]></content>
      
      
      <categories>
          
          <category> 设计模式 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 设计模式 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hadoop 数据压缩机制</title>
      <link href="2019/08/09/Hadoop%E6%95%B0%E6%8D%AE%E5%8E%8B%E7%BC%A9/"/>
      <url>2019/08/09/Hadoop%E6%95%B0%E6%8D%AE%E5%8E%8B%E7%BC%A9/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p>压缩技术能够有效减少底层存储系统 [HDFS] 读写字节数。压缩提高了网络带宽和磁盘空间的效率。在 Hadoop 下，尤其是数据规模很大和工作负载密集的情况下，使用数据压缩显得非常重要。在这种情况下， I/O 操作和网络数据传输要花大量的时间。还有， Shuffle与 Merge 过程同样也面临着巨大的 I/O 压力。</p><a id="more"></a><h1 id="1-概述"><a href="#1-概述" class="headerlink" title="1. 概述"></a>1. 概述</h1><blockquote><p>压缩技术能够有效减少底层存储系统 [HDFS] 读写字节数。压缩提高了网络带宽和磁盘空间的效率。在 Hadoop 下，尤其是数据规模很大和工作负载密集的情况下，使用数据压缩显得非常重要。在这种情况下， I/O 操作和网络数据传输要花大量的时间。还有， Shuffle与 Merge 过程同样也面临着巨大的 I/O 压力。</p><p>鉴于磁盘 I/O 和网络带宽是 Hadoop 的宝贵资源，数据压缩对于节省资源、最小化磁盘I/O 和网络传输非常有帮助。不过， 尽管压缩与解压操作的 CPU 开销不高，其性能的提升和资源的节省并非没有代价。</p><p>如果磁盘 I/O 和网络带宽影响了 MapReduce 作业性能，在任意 MapReduce 阶段启用压缩都可以改善端到端处理时间并减少 I/O 和网络流量。</p><p>压缩 Mapreduce 的一种优化策略：通过压缩编码对 Mapper 或者 Reducer 的输出进行压缩，以减少磁盘 IO， 提高 MR 程序运行速度（但相应增加了 cpu 运算负担） 。</p><p>注意： 压缩特性运用得当能提高性能，但运用不当也可能降低性能。<br>基本原则：<br>（1）运算密集型的 job，少用压缩<br>（2） IO 密集型的 job，多用压缩</p></blockquote><h1 id="2-MR-支持的压缩类型"><a href="#2-MR-支持的压缩类型" class="headerlink" title="2.MR 支持的压缩类型"></a>2.<code>MR</code> 支持的压缩类型</h1><h2 id="2-1-Gzip-压缩"><a href="#2-1-Gzip-压缩" class="headerlink" title="2.1. Gzip 压缩"></a>2.1. <code>Gzip</code> 压缩</h2><h3 id="2-1-1-优点"><a href="#2-1-1-优点" class="headerlink" title="2.1.1. 优点"></a>2.1.1. 优点</h3><blockquote><ul><li><p><strong>压缩率比较高，而且压缩/解压速度也比较快</strong></p></li><li><p><code>hadoop</code> 本身支持，在应用中处理 <code>gzip</code>  格式的文件就和直接处理文本一样</p></li><li><p>大部分 <code>linux</code> 系统都自带 <code>gzip</code> 命令，使用方便。</p></li></ul></blockquote><h3 id="2-1-2-缺点"><a href="#2-1-2-缺点" class="headerlink" title="2.1.2. 缺点"></a>2.1.2. 缺点</h3><blockquote><p> 不支持 <strong><code>split</code></strong></p></blockquote><h3 id="2-1-3-应用场景"><a href="#2-1-3-应用场景" class="headerlink" title="2.1.3. 应用场景"></a>2.1.3. 应用场景</h3><blockquote><p> 当每个文件压缩之后在 [**<code>1 * 1.1</code>**个块大小内]，都可以考虑用 <code>gzip</code> 压缩格式。</p><ul><li><p>例如说一天或者一个小时的日志压缩成一个 <code>gzip</code> 文件，运行 <code>mapreduce</code> 程序的时候通过多个 <code>gzip</code> 文件达到并发。 </p></li><li><p><code>hive</code> 程序， <code>streaming</code> 程序，和 <code>java</code> 写的 <code>mapreduce</code> 程序完全和文本处理一样，压缩之后原来的程序不需要做任何修改。</p></li></ul></blockquote><h2 id="2-2-Bzip2-压缩"><a href="#2-2-Bzip2-压缩" class="headerlink" title="2.2. Bzip2 压缩"></a>2.2. <code>Bzip2</code> 压缩</h2><h3 id="2-2-1-优点"><a href="#2-2-1-优点" class="headerlink" title="2.2.1. 优点"></a>2.2.1. 优点</h3><blockquote><ul><li><p>支持 <code>split</code></p></li><li><p>具有很高的压缩率，比 <code>gzip</code> 压缩率都高</p></li><li><p><code>hadoop</code> 本身支持</p></li><li><p>在 <code>linux</code> 系统下自带 <code>bzip2</code> 命令，使用方便。</p></li></ul></blockquote><h3 id="2-2-2-缺点"><a href="#2-2-2-缺点" class="headerlink" title="2.2.2. 缺点"></a>2.2.2. 缺点</h3><blockquote><p>压缩/解压速度慢</p></blockquote><h3 id="2-2-3-应用场景"><a href="#2-2-3-应用场景" class="headerlink" title="2.2.3. 应用场景"></a>2.2.3. 应用场景</h3><blockquote><ul><li>适合对速度要求不高，但需要较高的压缩率的时候，可以作为 <code>mapreduce</code> 作业的输出格式 </li><li>输出之后的数据比较大，处理之后的数据需要压缩存档减少磁盘空间并且以后数据用得比较少的情况</li><li>对单个很大的文本文件想压缩减少存储空间，同时又需要支持 <code>split</code>，而且兼容之前的应用程序的情况。</li></ul></blockquote><h2 id="2-3-Lzo-压缩"><a href="#2-3-Lzo-压缩" class="headerlink" title="2.3. Lzo 压缩"></a>2.3. <code>Lzo</code> 压缩</h2><h3 id="2-3-1-优点"><a href="#2-3-1-优点" class="headerlink" title="2.3.1. 优点"></a>2.3.1. 优点</h3><blockquote><ul><li>压缩/解压速度也比较快，合理的压缩率</li><li>支持 <code>split</code>，是 hadoop 中最流行的压缩格式</li><li>可以在 <code>linux</code>系统下安装 <code>lzop</code> 命令，使用方便。</li></ul></blockquote><h3 id="2-3-2-缺点"><a href="#2-3-2-缺点" class="headerlink" title="2.3.2. 缺点"></a>2.3.2. 缺点</h3><blockquote><ul><li>压缩率比 <code>gzip</code> 要低一些</li><li><code>hadoop</code> 本身不支持，需要安装</li><li>在应用中对 <code>lzo</code> 格式的文件需要做一些特殊处理[为了支持 <code>split</code> 需要建索引，还需要指定 <code>inputformat</code> 为 <code>lzo</code> 格式]。</li></ul></blockquote><h3 id="2-3-3-应用场景"><a href="#2-3-3-应用场景" class="headerlink" title="2.3.3. 应用场景"></a>2.3.3. 应用场景</h3><blockquote><p> 一个很大的文本文件，压缩之后还大于 <code>200M</code> 以上的可以考虑，而且单个文件越大， <code>lzo</code> 优点越越明显。</p></blockquote><h2 id="2-4-Snappy-压缩"><a href="#2-4-Snappy-压缩" class="headerlink" title="2.4. Snappy 压缩"></a>2.4. <code>Snappy</code> 压缩</h2><h3 id="2-4-1-优点"><a href="#2-4-1-优点" class="headerlink" title="2.4.1. 优点"></a>2.4.1. 优点</h3><blockquote><p> 高速压缩速度和合理的压缩率。</p></blockquote><h3 id="2-4-2-缺点"><a href="#2-4-2-缺点" class="headerlink" title="2.4.2. 缺点"></a>2.4.2. 缺点</h3><blockquote><ul><li><p>不支持 <code>split</code></p></li><li><p>压缩率比 <code>gzip</code> 要低</p></li><li><p><code>hadoop</code> 本身不支持，需要安装</p></li></ul></blockquote><h3 id="2-4-3-应用场景"><a href="#2-4-3-应用场景" class="headerlink" title="2.4.3. 应用场景"></a>2.4.3. 应用场景</h3><blockquote><p> 当 <code>Mapreduce</code> 作业的 <code>Map</code> 输出的数据比较大的时候，作为 <code>Map</code>  到 <code>Reduce</code> 的中间数据的压缩格式</p><p>或者作为一个 Mapreduce 作业的输出和另外一个 Mapreduce 作业的输入。</p></blockquote><h1 id="3-位置选择"><a href="#3-位置选择" class="headerlink" title="3. 位置选择"></a>3. 位置选择</h1><h1 id="4-配置参数"><a href="#4-配置参数" class="headerlink" title="4. 配置参数"></a>4. 配置参数</h1><h1 id="5-压缩案例"><a href="#5-压缩案例" class="headerlink" title="5. 压缩案例"></a>5. 压缩案例</h1><h2 id="5-1-数据流的压缩和解压缩"><a href="#5-1-数据流的压缩和解压缩" class="headerlink" title="5.1. 数据流的压缩和解压缩"></a>5.1. 数据流的压缩和解压缩</h2><h2 id="5-2-Map-输出端压缩"><a href="#5-2-Map-输出端压缩" class="headerlink" title="5.2. Map 输出端压缩"></a>5.2. <code>Map</code> 输出端压缩</h2><h2 id="5-3-Reduce-输出端压缩"><a href="#5-3-Reduce-输出端压缩" class="headerlink" title="5.3. Reduce 输出端压缩"></a>5.3. <code>Reduce</code> 输出端压缩</h2>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
            <tag> Hadoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>JVM 字节码[1]: 数据类型</title>
      <link href="2019/07/27/JVM-%E5%AD%97%E8%8A%82%E7%A0%81-1-%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B/"/>
      <url>2019/07/27/JVM-%E5%AD%97%E8%8A%82%E7%A0%81-1-%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p>Class字节码中有两种数据类型：</p><p>（1）字节数据直接量：这是基本的数据类型。共细分为u1、u2、u4、u8四种，分别代表连续的1个字节、2个字节、4个字节、8个字节组成的整体数据。<br>（2）表/数组：表是由多个基本数据或其他表，按照既定顺序组成的大的数据集合。表是有结构的，它的结构体：组成表的成分所在的位置和顺序都是已经严格定义好的。</p><p>Access Falgs：<br>访问标志信息包括了该class文件是类还是接口，是否被定义成public，是否是abstract，如果是类，是否被定义成final。</p><p><img src="./images/re.png"></p><p><img src="./images/hff.png"></p><ul><li><p>0x0021是0x0020和0x0001的并集，表示ACC_PUBLIC和ACC_SUPER<br>0x0002:private</p></li><li><p>字段表（Fields）：<br>字段表用于描述类和接口中声明的变量。这里的字段包含了类级别变量和实例变量，但是不包括方法内部声明的局部变量。</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> 深入理解 JVM </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>消息系统-Kafka(2)：事务机制</title>
      <link href="2019/07/12/%E6%B6%88%E6%81%AF%E7%B3%BB%E7%BB%9F-Kafka(4)%EF%BC%9A%E4%BA%8B%E5%8A%A1/"/>
      <url>2019/07/12/%E6%B6%88%E6%81%AF%E7%B3%BB%E7%BB%9F-Kafka(4)%EF%BC%9A%E4%BA%8B%E5%8A%A1/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p>Kafka 在 0.11 版本中除了引入了 <code>Exactly Once</code> 语义，还引入了事务特性。<strong>Kafka 事务特性是指一系列的生产者生产消息和消费者提交偏移量的操作在一个事务中，或者说是一个原子操作，生产消息和提交偏移量同时成功或者失败。</strong></p>]]></content>
      
      
      <categories>
          
          <category> Kafka </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kafka 消息系统源码深度剖析 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>JVM 指令集</title>
      <link href="2019/07/03/JVM-%E6%8C%87%E4%BB%A4%E9%9B%86/"/>
      <url>2019/07/03/JVM-%E6%8C%87%E4%BB%A4%E9%9B%86/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><table><thead><tr><th align="center"></th><th></th><th></th></tr></thead><tbody><tr><td align="center"><strong>指令码</strong></td><td><strong>助记符</strong></td><td><strong>功能描述</strong></td></tr><tr><td align="center">0x00</td><td>nop</td><td>无操作</td></tr><tr><td align="center">0x01</td><td>aconst_null</td><td>aconst_null 功能描述 null进栈指令执行前指令执行后栈底…… null栈顶     注意：JVM并没有为null指派一个具体的值。</td></tr><tr><td align="center">0x02</td><td>iconst_m1</td><td>int型常量值-1进栈</td></tr><tr><td align="center">0x03</td><td>iconst_0</td><td>int型常量值0进栈</td></tr><tr><td align="center">0x04</td><td>iconst_1</td><td>int型常量值1进栈</td></tr><tr><td align="center">0x05</td><td>iconst_2</td><td>int型常量值2进栈</td></tr><tr><td align="center">0x06</td><td>iconst_3</td><td>int型常量值3进栈</td></tr><tr><td align="center">0x07</td><td>iconst_4</td><td>int型常量值4进栈</td></tr><tr><td align="center">0x08</td><td>iconst_5</td><td>int型常量值5进栈</td></tr><tr><td align="center"></td><td></td><td></td></tr><tr><td align="center">0x09</td><td>lconst_0</td><td>long型常量值0进栈</td></tr><tr><td align="center">0x0A</td><td>lconst_1</td><td>long型常量值1进栈</td></tr><tr><td align="center"></td><td></td><td></td></tr><tr><td align="center">0x0B</td><td>fconst_0</td><td>float型常量值0进栈</td></tr><tr><td align="center">0x0C</td><td>fconst_1</td><td>float型常量值1进栈</td></tr><tr><td align="center">0x0D</td><td>fconst_2</td><td>float型常量值2进栈</td></tr><tr><td align="center"></td><td></td><td></td></tr><tr><td align="center">0x0E</td><td>dconst_0</td><td>double型常量值0进栈</td></tr><tr><td align="center">0x0F</td><td>dconst_1</td><td>double型常量值1进栈</td></tr><tr><td align="center"></td><td></td><td></td></tr><tr><td align="center">0x10</td><td>bipush</td><td>将一个byte型常量值推送至栈顶</td></tr><tr><td align="center">0x11</td><td>sipush</td><td>将一个short型常量值推送至栈顶</td></tr><tr><td align="center"></td><td></td><td></td></tr><tr><td align="center">0x12</td><td>ldc</td><td>将int、float或String型常量值从常量池中推送至栈顶</td></tr><tr><td align="center">0x13</td><td>ldc_w</td><td>将int、float或String型常量值从常量池中推送至栈顶（宽索引）</td></tr><tr><td align="center">0x14</td><td>ldc2_w</td><td>将long或double型常量值从常量池中推送至栈顶（宽索引）</td></tr><tr><td align="center"></td><td></td><td></td></tr><tr><td align="center">0x15</td><td>iload</td><td>指定的int型局部变量进栈</td></tr><tr><td align="center">0x16</td><td>lload</td><td>指定的long型局部变量进栈</td></tr><tr><td align="center">0x17</td><td>fload</td><td>指定的float型局部变量进栈</td></tr><tr><td align="center">0x18</td><td>dload</td><td>指定的double型局部变量进栈</td></tr><tr><td align="center">0x19</td><td>aload</td><td>指令格式： aload index功能描述： 当前frame的局部变量数组中下标为index的引用型局部变量进栈指令执行前指令执行后栈底…… objectref栈顶     index ： 无符号一byte整型。和wide指令联用， 可以使index为两byte</td></tr><tr><td align="center"></td><td></td><td></td></tr><tr><td align="center">0x1A</td><td>iload_0</td><td>第一个int型局部变量进栈</td></tr><tr><td align="center">0x1B</td><td>iload_1</td><td>第二个int型局部变量进栈</td></tr><tr><td align="center">0x1C</td><td>iload_2</td><td>第三个int型局部变量进栈</td></tr><tr><td align="center">0x1D</td><td>iload_3</td><td>第四个int型局部变量进栈</td></tr><tr><td align="center"></td><td></td><td></td></tr><tr><td align="center">0x1E</td><td>lload_0</td><td>第一个long型局部变量进栈</td></tr><tr><td align="center">0x1F</td><td>lload_1</td><td>第二个long型局部变量进栈</td></tr><tr><td align="center">0x20</td><td>lload_2</td><td>第三个long型局部变量进栈</td></tr><tr><td align="center">0x21</td><td>lload_3</td><td>第四个long型局部变量进栈</td></tr><tr><td align="center"></td><td></td><td></td></tr><tr><td align="center">0x22</td><td>fload_0</td><td>第一个float型局部变量进栈</td></tr><tr><td align="center">0x23</td><td>fload_1</td><td>第二个float型局部变量进栈</td></tr><tr><td align="center">0x24</td><td>fload_2</td><td>第三个float型局部变量进栈</td></tr><tr><td align="center">0x25</td><td>fload_3</td><td>第四个float型局部变量进栈</td></tr><tr><td align="center"></td><td></td><td></td></tr><tr><td align="center">0x26</td><td>dload_0</td><td>第一个double型局部变量进栈</td></tr><tr><td align="center">0x27</td><td>dload_1</td><td>第二个double型局部变量进栈</td></tr><tr><td align="center">0x28</td><td>dload_2</td><td>第三个double型局部变量进栈</td></tr><tr><td align="center">0x29</td><td>dload_3</td><td>第四个double型局部变量进栈</td></tr><tr><td align="center"></td><td></td><td></td></tr><tr><td align="center">0x2A</td><td>aload_0</td><td>指令格式：aload_0该指令的行为类似于aload指令index为0的情况。</td></tr><tr><td align="center">0x2B</td><td>aload_1</td><td>同上</td></tr><tr><td align="center">0x2C</td><td>aload_2</td><td>同上</td></tr><tr><td align="center">0x2D</td><td>aload_3</td><td>同上</td></tr><tr><td align="center"></td><td></td><td></td></tr><tr><td align="center">0x2E</td><td>iaload</td><td>指定的int型数组的指定下标处的值进栈</td></tr><tr><td align="center">0x2F</td><td>laload</td><td>指定的long型数组的指定下标处的值进栈</td></tr><tr><td align="center">0x30</td><td>faload</td><td>指定的float型数组的指定下标处的值进栈</td></tr><tr><td align="center">0x31</td><td>daload</td><td>指定的double型数组的指定下标处的值进栈</td></tr><tr><td align="center">0x32</td><td>aaload</td><td>指令格式： aaload 功能描述： 栈顶的数组下标（index）、数组引用（arrayref）出栈，并根据这两个数值取出对应的数组元素值（value）进栈。 抛出异常： 如果arrayref的值为null，会抛出NullPointerException。如果index造成数组越界，会抛出ArrayIndexOutOfBoundsException。指令执行前指令执行后栈底……arrayrefvalueindex 栈顶      index： int类型arrayref： 数组的引用</td></tr><tr><td align="center">0x33</td><td>baload</td><td>指定的boolean或byte型数组的指定下标处的值进栈</td></tr><tr><td align="center">0x34</td><td>caload</td><td>指定的char型数组的指定下标处的值进栈</td></tr><tr><td align="center">0x35</td><td>saload</td><td>指定的short型数组的指定下标处的值进栈</td></tr><tr><td align="center"></td><td></td><td></td></tr><tr><td align="center">0x36</td><td>istore</td><td>将栈顶int型数值存入指定的局部变量</td></tr><tr><td align="center">0x37</td><td>lstore</td><td>将栈顶long型数值存入指定的局部变量</td></tr><tr><td align="center">0x38</td><td>fstore</td><td>将栈顶float型数值存入指定的局部变量</td></tr><tr><td align="center">0x39</td><td>dstore</td><td>将栈顶double型数值存入指定的局部变量</td></tr><tr><td align="center">0x3A</td><td>astore</td><td>astore index 功能描述： 将栈顶数值（objectref）存入当前frame的局部变量数组中指定下标（index）处的变量中，栈顶数值出栈。指令执行前指令执行后栈底……objectref 栈顶     index ： 无符号一byte整数。该指令和wide联用，index可以为无符号两byte整数</td></tr><tr><td align="center"></td><td></td><td></td></tr><tr><td align="center">0x3B</td><td>istore_0</td><td>将栈顶int型数值存入第一个局部变量</td></tr><tr><td align="center">0x3C</td><td>istore_1</td><td>将栈顶int型数值存入第二个局部变量</td></tr><tr><td align="center">0x3D</td><td>istore_2</td><td>将栈顶int型数值存入第三个局部变量</td></tr><tr><td align="center">0x3E</td><td>istore_3</td><td>将栈顶int型数值存入第四个局部变量</td></tr><tr><td align="center"></td><td></td><td></td></tr><tr><td align="center">0x3F</td><td>lstore_0</td><td>将栈顶long型数值存入第一个局部变量</td></tr><tr><td align="center">0x40</td><td>lstore_1</td><td>将栈顶long型数值存入第二个局部变量</td></tr><tr><td align="center">0x41</td><td>lstore_2</td><td>将栈顶long型数值存入第三个局部变量</td></tr><tr><td align="center">0x42</td><td>lstore_3</td><td>将栈顶long型数值存入第四个局部变量</td></tr><tr><td align="center"></td><td></td><td></td></tr><tr><td align="center">0x43</td><td>fstore_0</td><td>将栈顶float型数值存入第一个局部变量</td></tr><tr><td align="center">0x44</td><td>fstore_1</td><td>将栈顶float型数值存入第二个局部变量</td></tr><tr><td align="center">0x45</td><td>fstore_2</td><td>将栈顶float型数值存入第三个局部变量</td></tr><tr><td align="center">0x46</td><td>fstore_3</td><td>将栈顶float型数值存入第四个局部变量</td></tr><tr><td align="center"></td><td></td><td></td></tr><tr><td align="center">0x47</td><td>dstore_0</td><td>将栈顶double型数值存入第一个局部变量</td></tr><tr><td align="center">0x48</td><td>dstore_1</td><td>将栈顶double型数值存入第二个局部变量</td></tr><tr><td align="center">0x49</td><td>dstore_2</td><td>将栈顶double型数值存入第三个局部变量</td></tr><tr><td align="center">0x4A</td><td>dstore_3</td><td>将栈顶double型数值存入第四个局部变量</td></tr><tr><td align="center"></td><td></td><td></td></tr><tr><td align="center">0x4B</td><td>astore_0</td><td>指令格式： astore_0功能描述： 该指令的行为类似于astore指令index为0的情况。</td></tr><tr><td align="center">0x4C</td><td>astore_1</td><td>同上</td></tr><tr><td align="center">0x4D</td><td>astore_2</td><td>同上</td></tr><tr><td align="center">0x4E</td><td>astore_3</td><td>同上</td></tr><tr><td align="center"></td><td></td><td></td></tr><tr><td align="center">0x4F</td><td>iastore</td><td>将栈顶int型数值存入指定数组的指定下标处</td></tr><tr><td align="center">0x50</td><td>lastore</td><td>将栈顶long型数值存入指定数组的指定下标处</td></tr><tr><td align="center">0x51</td><td>fastore</td><td>将栈顶float型数值存入指定数组的指定下标处</td></tr><tr><td align="center">0x52</td><td>dastore</td><td>将栈顶double型数值存入指定数组的指定下标处</td></tr><tr><td align="center">0x53</td><td>aastore</td><td>指令格式： aastore 功能描述： 根据栈顶的引用型数值（value）、数组下标（index）、数组引用（arrayref）出栈，将数值存入对应的数组元素中抛出异常： 如果value的类型和arrayref所引用的数组的元素类型不兼容，会抛出抛出ArrayStoreException。如果index造成数组越界，会抛出ArrayIndexOutOfBoundsException。 如果arrayref值为null，会抛出NullPointerException。指令执行前指令执行后栈底……arrayref index value 栈顶       arrayref ： 必须是对数组的引用index ： int类型value ： 引用类型</td></tr><tr><td align="center">0x54</td><td>bastore</td><td>将栈顶boolean或byte型数值存入指定数组的指定下标处</td></tr><tr><td align="center">0x55</td><td>castore</td><td>将栈顶char型数值存入指定数组的指定下标处</td></tr><tr><td align="center">0x56</td><td>sastore</td><td>将栈顶short型数值存入指定数组的指定下标处</td></tr><tr><td align="center"></td><td></td><td></td></tr><tr><td align="center">0x57</td><td>pop</td><td>栈顶数值出栈 (该栈顶数值不能是long或double型)</td></tr><tr><td align="center">0x58</td><td>pop2</td><td>栈顶的一个（如果是long、double型的)或两个（其它类型的）数值出栈</td></tr><tr><td align="center"></td><td></td><td></td></tr><tr><td align="center">0x59</td><td>dup</td><td>复制栈顶数值，并且复制值进栈</td></tr><tr><td align="center">0x5A</td><td>dup_x1</td><td>复制栈顶数值，并且复制值进栈2次</td></tr><tr><td align="center">0x5B</td><td>dup_x2</td><td>复制栈顶数值，并且复制值进栈2次或3次</td></tr><tr><td align="center">0x5C</td><td>dup2</td><td>复制栈顶一个（long、double型的)或两个（其它类型的）数值，并且复制值进栈</td></tr><tr><td align="center">0x5D</td><td>dup2_x1</td><td></td></tr><tr><td align="center">0x5E</td><td>dup2_x2</td><td></td></tr><tr><td align="center"></td><td></td><td></td></tr><tr><td align="center">0x5F</td><td>swap</td><td>栈顶的两个数值互换(要求栈顶的两个数值不能是long或double型的)</td></tr><tr><td align="center"></td><td></td><td></td></tr><tr><td align="center">0x60</td><td>iadd</td><td>栈顶两int型数值相加，并且结果进栈</td></tr><tr><td align="center">0x61</td><td>ladd</td><td>栈顶两long型数值相加，并且结果进栈</td></tr><tr><td align="center">0x62</td><td>fadd</td><td>栈顶两float型数值相加，并且结果进栈</td></tr><tr><td align="center">0x63</td><td>dadd</td><td>栈顶两double型数值相加，并且结果进栈</td></tr><tr><td align="center"></td><td></td><td></td></tr><tr><td align="center">0x64</td><td>isub</td><td>栈顶两int型数值相减，并且结果进栈</td></tr><tr><td align="center">0x65</td><td>lsub</td><td>栈顶两long型数值相减，并且结果进栈</td></tr><tr><td align="center">0x66</td><td>fsub</td><td>栈顶两float型数值相减，并且结果进栈</td></tr><tr><td align="center">0x67</td><td>dsub</td><td>栈顶两double型数值相减，并且结果进栈</td></tr><tr><td align="center"></td><td></td><td></td></tr><tr><td align="center">0x68</td><td>imul</td><td>栈顶两int型数值相乘，并且结果进栈</td></tr><tr><td align="center">0x69</td><td>lmul</td><td>栈顶两long型数值相乘，并且结果进栈</td></tr><tr><td align="center">0x6A</td><td>fmul</td><td>栈顶两float型数值相乘，并且结果进栈</td></tr><tr><td align="center">0x6B</td><td>dmul</td><td>栈顶两double型数值相乘，并且结果进栈</td></tr><tr><td align="center"></td><td></td><td></td></tr><tr><td align="center">0x6C</td><td>idiv</td><td>栈顶两int型数值相除，并且结果进栈</td></tr><tr><td align="center">0x6D</td><td>ldiv</td><td>栈顶两long型数值相除，并且结果进栈</td></tr><tr><td align="center">0x6E</td><td>fdiv</td><td>栈顶两float型数值相除，并且结果进栈</td></tr><tr><td align="center">0x6F</td><td>ddiv</td><td>栈顶两double型数值相除，并且结果进栈</td></tr><tr><td align="center"></td><td></td><td></td></tr><tr><td align="center">0x70</td><td>irem</td><td>栈顶两int型数值作取模运算，并且结果进栈</td></tr><tr><td align="center">0x71</td><td>lrem</td><td>栈顶两long型数值作取模运算，并且结果进栈</td></tr><tr><td align="center">0x72</td><td>frem</td><td>栈顶两float型数值作取模运算，并且结果进栈</td></tr><tr><td align="center">0x73</td><td>drem</td><td>栈顶两double型数值作取模运算，并且结果进栈</td></tr><tr><td align="center"></td><td></td><td></td></tr><tr><td align="center">0x74</td><td>ineg</td><td>栈顶int型数值取负，并且结果进栈</td></tr><tr><td align="center">0x75</td><td>lneg</td><td>栈顶long型数值取负，并且结果进栈</td></tr><tr><td align="center">0x76</td><td>fneg</td><td>栈顶float型数值取负，并且结果进栈</td></tr><tr><td align="center">0x77</td><td>dneg</td><td>栈顶double型数值取负，并且结果进栈</td></tr><tr><td align="center"></td><td></td><td></td></tr><tr><td align="center">0x78</td><td>ishl</td><td>int型数值左移指定位数，并且结果进栈</td></tr><tr><td align="center">0x79</td><td>lshl</td><td>long型数值左移指定位数，并且结果进栈</td></tr><tr><td align="center"></td><td></td><td></td></tr><tr><td align="center">0x7A</td><td>ishr</td><td>int型数值带符号右移指定位数，并且结果进栈</td></tr><tr><td align="center">0x7B</td><td>lshr</td><td>long型数值带符号右移指定位数，并且结果进栈</td></tr><tr><td align="center">0x7C</td><td>iushr</td><td>int型数值无符号右移指定位数，并且结果进栈</td></tr><tr><td align="center">0x7D</td><td>lushr</td><td>long型数值无符号右移指定位数，并且结果进栈</td></tr><tr><td align="center"></td><td></td><td></td></tr><tr><td align="center">0x7E</td><td>iand</td><td>栈顶两int型数值按位与，并且结果进栈</td></tr><tr><td align="center">0x7F</td><td>land</td><td>栈顶两long型数值按位与，并且结果进栈</td></tr><tr><td align="center"></td><td></td><td></td></tr><tr><td align="center">0x80</td><td>ior</td><td>栈顶两int型数值按位或，并且结果进栈</td></tr><tr><td align="center">0x81</td><td>lor</td><td>栈顶两long型数值按位或，并且结果进栈</td></tr><tr><td align="center"></td><td></td><td></td></tr><tr><td align="center">0x82</td><td>ixor</td><td>栈顶两int型数值按位异或，并且结果进栈</td></tr><tr><td align="center">0x83</td><td>lxor</td><td>栈顶两long型数值按位异或，并且结果进栈</td></tr><tr><td align="center"></td><td></td><td></td></tr><tr><td align="center">0x84</td><td>iinc</td><td>指定int型变量增加指定值</td></tr><tr><td align="center"></td><td></td><td></td></tr><tr><td align="center">0x85</td><td>i2l</td><td>栈顶int值强转long值，并且结果进栈</td></tr><tr><td align="center">0x86</td><td>i2f</td><td>栈顶int值强转float值，并且结果进栈</td></tr><tr><td align="center">0x87</td><td>i2d</td><td>栈顶int值强转double值，并且结果进栈</td></tr><tr><td align="center">0x88</td><td>l2i</td><td>栈顶long值强转int值，并且结果进栈</td></tr><tr><td align="center">0x89</td><td>l2f</td><td>栈顶long值强转float值，并且结果进栈</td></tr><tr><td align="center">0x8A</td><td>l2d</td><td>栈顶long值强转double值，并且结果进栈</td></tr><tr><td align="center">0x8B</td><td>f2i</td><td>栈顶float值强转int值，并且结果进栈</td></tr><tr><td align="center">0x8C</td><td>f2l</td><td>栈顶float值强转long值，并且结果进栈</td></tr><tr><td align="center">0x8D</td><td>f2d</td><td>栈顶float值强转double值，并且结果进栈</td></tr><tr><td align="center">0x8E</td><td>d2i</td><td>栈顶double值强转int值，并且结果进栈</td></tr><tr><td align="center">0x8F</td><td>d2l</td><td>栈顶double值强转long值，并且结果进栈</td></tr><tr><td align="center">0x90</td><td>d2f</td><td>栈顶double值强转float值，并且结果进栈</td></tr><tr><td align="center">0x91</td><td>i2b</td><td>栈顶int值强转byte值，并且结果进栈</td></tr><tr><td align="center">0x92</td><td>i2c</td><td>栈顶int值强转char值，并且结果进栈</td></tr><tr><td align="center">0x93</td><td>i2s</td><td>栈顶int值强转short值，并且结果进栈</td></tr><tr><td align="center"></td><td></td><td></td></tr><tr><td align="center">0x94</td><td>lcmp</td><td>比较栈顶两long型数值大小，并且结果（1，0，-1）进栈</td></tr><tr><td align="center">0x95</td><td>fcmpl</td><td>比较栈顶两float型数值大小，并且结果（1，0，-1）进栈；当其中一个数值为NaN时， -1进栈</td></tr><tr><td align="center">0x96</td><td>fcmpg</td><td>比较栈顶两float型数值大小，并且结果（1，0，-1）进栈；当其中一个数值为NaN时，1进栈</td></tr><tr><td align="center">0x97</td><td>dcmpl</td><td>比较栈顶两double型数值大小，并且结果（1，0，-1）进栈；当其中一个数值为NaN时，-1进栈</td></tr><tr><td align="center">0x98</td><td>dcmpg</td><td>比较栈顶两double型数值大小，并且结果（1，0，-1）进栈；当其中一个数值为NaN时，1进栈</td></tr><tr><td align="center"></td><td></td><td></td></tr><tr><td align="center">0x99</td><td>ifeq</td><td>当栈顶int型数值等于0时跳转</td></tr><tr><td align="center">0x9A</td><td>ifne</td><td>当栈顶int型数值不等于0时跳转</td></tr><tr><td align="center">0x9B</td><td>iflt</td><td>当栈顶int型数值小于0时跳转</td></tr><tr><td align="center">0x9C</td><td>ifge</td><td>当栈顶int型数值大于等于0时跳转</td></tr><tr><td align="center">0x9D</td><td>ifgt</td><td>当栈顶int型数值大于0时跳转</td></tr><tr><td align="center">0x9E</td><td>ifle</td><td>当栈顶int型数值小于等于0时跳转</td></tr><tr><td align="center">0x9F</td><td>if_icmpeq</td><td>比较栈顶两int型数值大小，当结果等于0时跳转</td></tr><tr><td align="center">0xA0</td><td>if_icmpne</td><td>比较栈顶两int型数值大小，当结果不等于0时跳转</td></tr><tr><td align="center">0xA1</td><td>if_icmplt</td><td>比较栈顶两int型数值大小，当结果小于0时跳转</td></tr><tr><td align="center">0xA2</td><td>if_icmpge</td><td>比较栈顶两int型数值大小，当结果大于等于0时跳转</td></tr><tr><td align="center">0xA3</td><td>if_icmpgt</td><td>比较栈顶两int型数值大小，当结果大于0时跳转</td></tr><tr><td align="center">0xA4</td><td>if_icmple</td><td>比较栈顶两int型数值大小，当结果小于等于0时跳转</td></tr><tr><td align="center">0xA5</td><td>if_acmpeq</td><td>比较栈顶两引用型数值，当结果相等时跳转</td></tr><tr><td align="center">0xA6</td><td>if_acmpne</td><td>比较栈顶两引用型数值，当结果不相等时跳转</td></tr><tr><td align="center"></td><td></td><td></td></tr><tr><td align="center">0xA7</td><td>goto</td><td>无条件跳转</td></tr><tr><td align="center"></td><td></td><td></td></tr><tr><td align="center">0xA8</td><td>jsr</td><td>跳转至指定16位offset位置，并将jsr下一条指令地址压入栈顶</td></tr><tr><td align="center">0xA9</td><td>ret</td><td>返回至局部变量指定的index的指令位置（通常与jsr、jsr_w联合使用）</td></tr><tr><td align="center">0xAA</td><td>tableswitch</td><td>用于switch条件跳转，case值连续（可变长度指令）</td></tr><tr><td align="center">0xAB</td><td>lookupswitch</td><td>用于switch条件跳转，case值不连续（可变长度指令）</td></tr><tr><td align="center"></td><td></td><td></td></tr><tr><td align="center">0xAC</td><td>ireturn</td><td>当前方法返回int</td></tr><tr><td align="center">0xAD</td><td>lreturn</td><td>当前方法返回long</td></tr><tr><td align="center">0xAE</td><td>freturn</td><td>当前方法返回float</td></tr><tr><td align="center">0xAF</td><td>dreturn</td><td>当前方法返回double</td></tr><tr><td align="center">0xB0</td><td>areturn</td><td>指令格式： areturn功能描述： 从方法中返回一个对象的引用。抛出异常： 如果当前方法是<code>synchronized</code>方法，并且当前线程不是改方法的锁的拥有者，会抛出IllegalMonitorStateException指令执行前指令执行后栈底… objectref 栈顶     objectref ： 被返回的对象引用</td></tr><tr><td align="center">0xB1</td><td>return</td><td>当前方法返回void</td></tr><tr><td align="center"></td><td></td><td></td></tr><tr><td align="center">0xB2</td><td>getstatic</td><td>获取指定类的静态域，并将其值压入栈顶</td></tr><tr><td align="center">0xB3</td><td>putstatic</td><td>为指定的类的静态域赋值</td></tr><tr><td align="center">0xB4</td><td>getfield</td><td>获取指定类的实例域，并将其值压入栈顶</td></tr><tr><td align="center">0xB5</td><td>putfield</td><td>为指定的类的实例域赋值</td></tr><tr><td align="center"></td><td></td><td></td></tr><tr><td align="center">0xB6</td><td>invokevirtual</td><td>调用实例方法</td></tr><tr><td align="center">0xB7</td><td>invokespecial</td><td>调用超类构造方法、实例初始化方法、私有方法</td></tr><tr><td align="center">0xB8</td><td>invokestatic</td><td>调用静态方法</td></tr><tr><td align="center">0xb9</td><td>invokeinterface</td><td>调用接口方法</td></tr><tr><td align="center"></td><td></td><td></td></tr><tr><td align="center">0xBA</td><td>—</td><td>因为历史原因，该码点为未使用的保留码点</td></tr><tr><td align="center"></td><td></td><td></td></tr><tr><td align="center">0xBB</td><td>new</td><td>创建一个对象，并且其引用进栈</td></tr><tr><td align="center">0xBC</td><td>newarray</td><td>创建一个基本类型数组，并且其引用进栈</td></tr><tr><td align="center">0xBD</td><td>anewarray</td><td>指令格式： anewarray index1 index2 功能描述： 栈顶数值（count）作为数组长度，创建 一个引用 型数组。栈顶数值出栈，数组引用进栈。 抛出异常： 如果count小于0，会抛出 NegativeArraySizeException指令执行前指令执行后栈底……countarrayref栈顶     count： int类型。arrayref： 对所创建的数组的引用。</td></tr><tr><td align="center">0xBE</td><td>arraylength</td><td>指令格式： arraylength功能描述： 栈顶的数组引用（arrayref）出栈，该 数组的长度进栈。抛出异常： 如果arrayref的值为null，会抛出NullPointerException。指令执行前指令执行后栈底……arrayreflength栈顶     arrayref： 数组引用length： 数组长度</td></tr><tr><td align="center"></td><td></td><td></td></tr><tr><td align="center">0xBF</td><td>athrow</td><td>指令格式： athrow功能描述： 将栈顶的数值作为异常或错误抛出抛出异常： 如果栈顶数值为null，则使用NullPointerException代替栈顶数值抛出。如果方法是的，则有可能抛出 IllegalMonitorStateException。指令执行前指令执行后栈底…objectrefobjectref 栈顶     objectref： Throwable或其子类的实例的引用。</td></tr><tr><td align="center">0xC0</td><td>checkcast</td><td>类型转换检查，如果该检查未通过将会抛出ClassCastException异常</td></tr><tr><td align="center">0xc1</td><td>instanceof</td><td>检查对象是否是指定的类的实例。如果是，1进栈；否则，0进栈</td></tr><tr><td align="center"></td><td></td><td></td></tr><tr><td align="center">0xC2</td><td>monitorenter</td><td>获得对象锁</td></tr><tr><td align="center">0xC3</td><td>monitorexit</td><td>释放对象锁</td></tr><tr><td align="center"></td><td></td><td></td></tr><tr><td align="center">0xC4</td><td>wide</td><td>用于修改其他指令的行为</td></tr><tr><td align="center"></td><td></td><td></td></tr><tr><td align="center">0xC5</td><td>multianewarray</td><td>创建指定类型和维度的多维数组（执行该指令时，栈中必须包含各维度的长度值），并且其引用值进栈</td></tr><tr><td align="center"></td><td></td><td></td></tr><tr><td align="center">0xC6</td><td>ifnull</td><td>为null时跳转</td></tr><tr><td align="center">0xC7</td><td>ifnonnull</td><td>不为null时跳转</td></tr><tr><td align="center">0xC8</td><td>goto_w</td><td>无条件跳转（宽索引）</td></tr><tr><td align="center">0xC9</td><td>jsr_w</td><td>跳转至指定32位offset位置，并且jsr_w下一条指令地址进栈</td></tr><tr><td align="center"></td><td></td><td></td></tr><tr><td align="center">0xCA</td><td>breakpoint</td><td></td></tr><tr><td align="center"></td><td></td><td></td></tr><tr><td align="center">0xFE</td><td>impdep1</td><td></td></tr><tr><td align="center">0xFF</td><td>impdep2</td><td></td></tr></tbody></table>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>JVM 垃圾收集器[1]:垃圾回收算法</title>
      <link href="2019/07/01/JVM-%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E5%99%A8-1-%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6%E7%AE%97%E6%B3%95/"/>
      <url>2019/07/01/JVM-%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E5%99%A8-1-%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6%E7%AE%97%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script>]]></content>
      
      
      <categories>
          
          <category> 深入理解 JVM </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>JVM 类加载器</title>
      <link href="2019/06/25/JVM%20%E7%B1%BB%E5%8A%A0%E8%BD%BD%E6%9C%BA%E5%88%B6%E5%AD%A6%E4%B9%A0/"/>
      <url>2019/06/25/JVM%20%E7%B1%BB%E5%8A%A0%E8%BD%BD%E6%9C%BA%E5%88%B6%E5%AD%A6%E4%B9%A0/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p>类的加载指的是将类的 .class 文件中的二进制数据读入到内存中，将其放在运行时数据区的**[方法区]<strong>内，然后在</strong>[堆区]**创建一个 <code>java.lang.Class</code> 对象，用来封装类在方法区内的数据结构。类的加载的最终产品是位于堆区中的 Class 对象，Class对象封装了类在方法区内的数据结构，并且向Java程序员提供了访问方法区内的数据结构的接口。 </p><a id="more"></a><h1 id="1-简介"><a href="#1-简介" class="headerlink" title="1.简介"></a>1.简介</h1><p>Sun 公司开发了 Java 语言，但任何人都可以在遵循 JVM 规范的前提下开发和提供 JVM 实现。所以目前业界有多种不同的JVM 实现，包括 Oracle Hostpot JVM、IBM JVM 和 Taobao JVM。<br>JRE 由 Java API 和 JVM 组成，JVM 通过类加载器(Class Loader)加类 Java 应用，并通过 Java API 进行执行。<br>最初 Java 语言被设计为基于虚拟机器在而非物理机器，重而实现 WORA(一次编写，到处运行)的目的，尽管这个目标几乎被世人所遗忘。所以，JVM 可以在所有的硬件环境上执行 Java 字节码而无须调整 Java 的执行模式。<img src="/Users/joker/Documents/Learnning/JVM/resources/images/3.png" alt="3" style="zoom:50%;" /></p><h1 id="2-基本特性"><a href="#2-基本特性" class="headerlink" title="2.基本特性"></a>2.基本特性</h1><ul><li><strong>基于栈 (Stack-based) 的虚拟机</strong></li><li><strong>符号引用 (Symbolic reference)</strong><br>  除基本类型外的所有 Java 类型 (类和接口) 都是通过符号引用取得关联的，而非显式的基于内存地址的引用。</li><li><strong>垃圾回收机制</strong><br>  类的实例通过用户代码进行显式创建，但却通过垃圾回收机制自动销毁。</li><li><strong>平台无关性</strong><br>  JVM 却通过明确的定义基本类型的字节长度来维持代码的平台兼容性，从而做到平台无关。</li><li><strong>网络字节序(Network byte order)</strong><br>  Java class文件的二进制表示使用的是基于网络的字节序(network byte order)。为了在使用小端(little endian)的Intel x86平台和在使用了大端(big endian)的RISC系列平台之间保持平台无关，必须要定义一个固定的字节序。JVM选择了网络传输协议中使用的网络字节序，即基于大端(big endian)的字节序。</li></ul><h1 id="3-Java-虚拟机与程序的生命周期"><a href="#3-Java-虚拟机与程序的生命周期" class="headerlink" title="3.Java 虚拟机与程序的生命周期"></a>3.Java 虚拟机与程序的生命周期</h1><p>在如下几种情况下。Java 虚拟机将结束生命周期</p><ol><li>执行了 System.exit()方法</li><li>程序正常执行结束</li><li>程序在执行过程中遇到了异常或错误而异常结束</li><li>由于操作系统出现错误而导致Java虚拟机进程终止<h1 id="4-类加载机制"><a href="#4-类加载机制" class="headerlink" title="4.类加载机制"></a>4.类加载机制</h1></li></ol><p>类的加载指的是将类的 <code>.class</code> 文件中的二进制数据读入到内存中，将其放在运行时数据区的方法区内，然后在内存区创建一个 <code>java.lang.Class</code> 对象，用来封装类在方法区内的数据结构<br><font color = 'red'> <strong>注意：</strong></font>规范并未说明 <em>Class</em> 对象位于哪里，<em>HotSpot</em> 虚拟机将其放在了方法区内。</p><img src="/Users/joker/Library/Application Support/typora-user-images/截屏2020-09-29 下午2.24.15.png" alt="截屏2020-09-29 下午2.24.15" style="zoom:50%;" /><h2 id="4-1-类加载时机"><a href="#4-1-类加载时机" class="headerlink" title="4.1. 类加载时机"></a>4.1. 类加载时机</h2><p>JVM 规范允许类加载器在预料某个类将要被使用时就预先加载它，如果在预先加载的过程中遇到了.class文件缺失或存在错误，类加载器必须在<strong>程序首次主动</strong>使用该类才报告错误（LinkageError错误），如果这个类没有被程序主动使用，那么类加载器就不会报告错误。</p><h2 id="4-2-类加载过程"><a href="#4-2-类加载过程" class="headerlink" title="4.2. 类加载过程"></a>4.2. 类加载过程</h2><img src="/Users/joker/Documents/Learnning/JVM/resources/images/2.png" alt="2" style="zoom:50%;" /><h3 id="4-2-1-加载"><a href="#4-2-1-加载" class="headerlink" title="4.2.1. 加载"></a>4.2.1. 加载</h3><p>加载是类加载过程的第一个阶段，在加载阶段，虚拟机需要完成以下三件事情：</p><ul><li>通过一个类的全限定名来获取其定义的二进制字节流。</li><li>将这个字节流所代表的静态存储结构转化为方法区的运行时数据结构。</li><li>在 Java 堆中生成一个代表这个类的 java.lang.Class 对象，作为对方法区中这些数据的访问入口。</li></ul><p>相对于类加载的其他阶段而言，加载阶段（准确地说，是加载阶段获取类的二进制字节流的动作）是可控性最强的阶段，因为开发人员既可以使用系统提供的类加载器来完成加载，也可以自定义自己的类加载器来完成加载。</p><p>加载阶段完成后，虚拟机外部的 二进制字节流就按照虚拟机所需的格式存储在方法区之中，而且在 Java 堆中也创建一个java.lang.Class 类的对象，这样便可以通过该对象访问方法区中的这些数据。</p><h4 id="gt-加载-class-文件的方式"><a href="#gt-加载-class-文件的方式" class="headerlink" title="-&gt; 加载 .class 文件的方式"></a><strong>-&gt; 加载 .class 文件的方式</strong></h4><ul><li>从本地系统中直接加载</li><li>通过网络下载 .class 文件</li><li>从 zip, jar 等归档文件中加载 .class 文件</li><li>从专有数据库中提取 .class 文件，比较少见</li><li>运行时计算生成: 动态代理</li><li>将 Java 源文件动态编译为 .class 文件</li></ul><h3 id="4-2-2-连接"><a href="#4-2-2-连接" class="headerlink" title="4.2.2. 连接"></a>4.2.2. 连接</h3><p><strong>类被加载之后，进入连接阶段。连接阶段就是将已经读入到内存的类的二进制数据合并到虚拟机的运行时环境中去。</strong></p><h4 id="gt-验证"><a href="#gt-验证" class="headerlink" title="-&gt; 验证"></a><strong>-&gt; 验证</strong></h4><p>验证是连接阶段的第一步，这一阶段的目的是为了确保 Class 文件的字节流中包含的信息符合当前虚拟机的要求，并且不会危害虚拟机自身的安全。验证阶段大致会完成4个阶段的检验动作：</p><ol><li><p><strong>文件格式验证</strong></p><p>验证字节流是否符合 Class 文件格式的规范；例如：是否以 <code>0xCAFEBABE</code> 开头、主次版本号是否在当前虚拟机的处理范围之内、常量池中的常量是否有不被支持的类型。</p></li><li><p><strong>元数据验证</strong></p><p>对字节码描述的信息进行语义分析（注意：对比javac编译阶段的语义分析），以保证其描述的信息符合Java语言规范的要求；例如：这个类是否有父类，除了java.lang.Object之外。</p></li><li><p><strong>字节码验证</strong></p><p>通过数据流和控制流分析，确定程序语义是合法的、符合逻辑的。</p></li><li><p><strong>符号引用验证</strong></p><p>确保解析动作能正确执行。</p></li></ol><p>验证阶段是非常重要的，但不是必须的，它对程序运行期没有影响，如果所引用的类经过反复验证，那么可以考虑采用-Xverifynone 参数来关闭大部分的类验证措施，以缩短虚拟机类加载的时间。</p><h4 id="gt-准备"><a href="#gt-准备" class="headerlink" title="-&gt; 准备"></a>-&gt; 准备</h4><p><strong>在准备阶段，Java 虚拟机为类的静态变量分配内存，并设置默认的初始值。</strong></p><blockquote><p>准备阶段是正式为类变量分配内存并设置类变量初始值的阶段，这些内存都将在方法区中分配。对于该阶段有以下几点需要注意：</p></blockquote><ol><li><p>这时候进行内存分配的仅包括类变量（static），而不包括实例变量，实例变量会在对象实例化时随着对象一块分配在Java堆中。</p></li><li><p>这里所设置的初始值通常情况下是数据类型默认的零值（如0、0L、null、false等），而不是被在Java代码中被显式地赋予的值。</p><p>![截屏2020-09-29 下午4.25.03](/Users/joker/Library/Application Support/typora-user-images/截屏2020-09-29 下午4.25.03.png)</p></li></ol><p>假设一个类变量的定义为：public static int value = 3；</p><p>那么变量 value 在准备阶段过后的初始值为0，而不是3，因为这时候尚未开始执行任何 Java 方法，而把 value 赋值为3的 putstatic 指令是在程序编译后，存放于类构造器 <clinit>() 方法之中的，所以把 value 赋值为 3 的动作将在初始化阶段才会执行。</p><p>例如对于以下 Sample 类，在准备阶段，将为 int 类型的静态变量 a 分配4个字节的内存空间，并且赋予默认值0，为long类型的静态变量b分配8个字节的内存空间，并且赋予默认值0</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Sample</span></span>&#123;</span><br><span class="line">       <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">int</span> a=<span class="number">1</span>;</span><br><span class="line">       <span class="keyword">public</span>  <span class="keyword">static</span> <span class="keyword">long</span> b;</span><br><span class="line">       <span class="keyword">public</span>  <span class="keyword">static</span> <span class="keyword">long</span> c;</span><br><span class="line">       <span class="keyword">static</span> &#123;</span><br><span class="line">           b=<span class="number">2</span>;</span><br><span class="line">       &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="gt-解析"><a href="#gt-解析" class="headerlink" title="-&gt; 解析"></a>-&gt; 解析</h4><p>解析阶段是虚拟机将常量池内的符号引用替换为直接引用的过程，解析动作主要针对类或接口、字段、类方法、接口方法、方法类型、方法句柄和调用点限定符7类符号引用进行。<strong>符号引用</strong> 就是一组符号来描述目标，可以是任何字面量。</p><p><strong>直接引用</strong>就是直接指向目标的指针、相对偏移量或一个间接定位到目标的句柄。</p><h3 id="4-2-3-初始化"><a href="#4-2-3-初始化" class="headerlink" title="4.2.3. 初始化"></a>4.2.3. 初始化</h3><p><strong>在初始化阶段，Java 虚拟机执行类的初始化语句，为类的静态变量赋予初始值。</strong>JVM 负责对类进行初始化，主要对类变量进行初始化在程序中，静态变量的初始化有两种途径</p><p>初始化阶段就是执行类构造器方法<clinit>() 的过程</p><p>![截屏2020-09-29 下午4.30.01](/Users/joker/Library/Application Support/typora-user-images/截屏2020-09-29 下午4.30.01.png)</p><ol><li>在静态变量的声明处进行初始化</li><li>在静态代码块中进行初始化。</li></ol><p><strong><font color='blue'>准备阶段即使我们为静态变量赋值为任意的数值，但是该静态变量还是会被初始化为他的默认值，最后的初始化时才会把我们赋予的值设为该静态变量的值</font></strong></p><hr><h4 id="gt-初始化时机"><a href="#gt-初始化时机" class="headerlink" title="-&gt; 初始化时机"></a>-&gt; 初始化时机</h4><p><strong>Java 虚拟机实现必须在每个类或接口被 Java 程序 “首次主动使用” 时初始化他们</strong></p><ol><li>主动使用<ul><li>创建类的实例</li><li>访问某个类或接口的静态变量，或者对该静态变量赋值</li><li>调用该类的静态方法</li><li>反射</li><li>初始化一个类的子类</li><li>Java虚拟机启动时被标为启动类的类（Java Test）</li></ul></li><li>被动使用</li></ol><hr><h4 id="gt-类的初始化步骤"><a href="#gt-类的初始化步骤" class="headerlink" title="-&gt; 类的初始化步骤"></a>-&gt; <strong>类的初始化步骤</strong></h4><ol><li>假如这个类还没有被加载和连接，那就先进行加载和连接</li><li>假如类存在直接父类，并且这个父类还没有被初始化，那就先初始化直接父类</li><li>假如类中存在初始化语句，那就依次执行这些初始化语句</li></ol><p>当 Java 虚拟机初始化一个类时，要求它的所有父类都已经被初始化，<strong>但是这条规则不适用于接口</strong>。因此，一个父接口并不会因为它的子接口或者实现类的初始化而初始化。只有当程序首次使用特定的接口的静态变量时，才会导致该接口的初始化。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ClassLoadTest9</span> </span>&#123;</span><br><span class="line">    <span class="keyword">static</span> &#123;</span><br><span class="line">        System.out.println(<span class="string">&quot;ClassLoadTest9&quot;</span>);</span><br><span class="line">    &#125; </span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        System.out.println(Child1.a);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Parent1</span> </span>&#123;</span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">int</span> a = <span class="number">9</span>;</span><br><span class="line">    <span class="keyword">static</span> &#123;</span><br><span class="line">        System.out.println(<span class="string">&quot;Parent1&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Child1</span> <span class="keyword">extends</span> <span class="title">Parent1</span> </span>&#123;</span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">int</span> b = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">static</span> &#123;</span><br><span class="line">        System.out.println(<span class="string">&quot;Child1&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 最后输出顺序</span></span><br><span class="line"><span class="comment">// ClassLoadTest9</span></span><br><span class="line"><span class="comment">// Parent1</span></span><br><span class="line"><span class="comment">// 9</span></span><br></pre></td></tr></table></figure><hr><p><strong>接口初始化</strong></p><ol><li>当一个接口在初始化时，并不要求其父接口都完成了初始化只有在真正使用到父接口的时候（如引用接口中定义的常量），才会初始化</li></ol><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * 当一个接口在初始化时，并不要求其父接口都完成了初始化</span></span><br><span class="line"><span class="comment"> * 只有在真正使用到父接口的时候（如引用接口中定义的常量），才会初始化</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ClassLoadTest5</span> </span>&#123;</span><br><span class="line">   <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">       System.out.println(MyChild.b);</span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">interface</span> <span class="title">Student5</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> a = <span class="number">9</span>; <span class="comment">//前面省了public static final</span></span><br><span class="line">    Thread thread = <span class="keyword">new</span> Thread() &#123;</span><br><span class="line">        &#123;</span><br><span class="line">            System.out.println(<span class="string">&quot;thread 初始化了&quot;</span>);<span class="comment">//如果父接口初始化了这句应该输出</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"><span class="class"><span class="keyword">interface</span> <span class="title">MyChild</span> <span class="keyword">extends</span> <span class="title">Student5</span> </span>&#123;     <span class="comment">//接口属性默认是 public static final</span></span><br><span class="line">    String b = LocalDateTime.now().toString();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><hr><h2 id="类加载器"><a href="#类加载器" class="headerlink" title="类加载器"></a>类加载器</h2><p>类的加载由类加载器完成，JVM 提供的类加载器叫做系统类加载器，此外还可以通过继承 ClassLoader 基类来自定义类加载器。</p><h3 id="系统类加载器"><a href="#系统类加载器" class="headerlink" title="系统类加载器"></a>系统类加载器</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.poplar.classload;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ClassLoadTest18</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        System.out.println(System.getProperty(<span class="string">&quot;sun.boot.class.path&quot;</span>));<span class="comment">//根加载器路径</span></span><br><span class="line">        System.out.println(System.getProperty(<span class="string">&quot;java.ext.dirs&quot;</span>));<span class="comment">//扩展类加载器路径</span></span><br><span class="line">        System.out.println(System.getProperty(<span class="string">&quot;java.class.path&quot;</span>));<span class="comment">//应用类加载器路径</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><ul><li><p><strong>根类加载器</strong>（Bootstrap）</p><blockquote><p>该加载器没有父加载器，它负责加载虚拟机中的核心类库。根类加载器从系统属性 <code>sun.boot.class.path</code> 所指定的目录中加载类库。类加载器的实现依赖于底层操作系统，属于虚拟机的实现的一部分，它并没有集成<code>java.lang.ClassLoader</code> 类。</p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.poplar.classload;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ClassLoadTest7</span> </span>&#123;</span><br><span class="line">   <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">     <span class="comment">//null 由于String是由根加载器加载，在rt.jar包下</span></span><br><span class="line">     System.out.println(String.class.getClassLoader());</span><br><span class="line">     <span class="comment">//sun.misc.Launcher$AppClassLoader@73d16e93 </span></span><br><span class="line">     System.out.println(C.class.getClassLoader());</span><br><span class="line">   &#125;</span><br><span class="line"> &#125;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">C</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul><ul><li><p><strong>扩展类加载器</strong>（Extension）</p><blockquote><p>它的父加载器为根类加载器。它从 <code>java.ext.dirs</code> 系统属性所指定的目录中加载类库，或者从JDK的安装目录的jre\lib\ext子目录（扩展目录）下加载类库，如果把用户创建的jar文件放在这个目录下，也会自动由扩展类加载器加载，扩展类加载器是纯java类，是java.lang.ClassLoader的子类。</p></blockquote></li><li><p><strong>系统应用类加载器</strong>（AppClassLoader/System）</p><blockquote><p>也称为应用类加载器，它的父加载器为扩展类加载器，它从环境变量classpath或者系统属性java.class.path所指定的目录中加载类，他是用户自定义的类加载器的默认父加载器。系统类加载器时纯java类，是java.lang.ClassLoader的子类。</p></blockquote></li></ul><h3 id="用户自定义的类加载器"><a href="#用户自定义的类加载器" class="headerlink" title="用户自定义的类加载器"></a><strong>用户自定义的类加载器</strong></h3><ul><li>java.lang.ClassLoader的子类</li><li>用户可以定制类的加载方式</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.poplar.classload;</span><br><span class="line"><span class="keyword">import</span> java.io.*;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**  </span></span><br><span class="line"><span class="comment"> * 自定义类加载器</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CustomClassLoader</span> <span class="keyword">extends</span> <span class="title">ClassLoader</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> String classLoaderName;</span><br><span class="line">  <span class="keyword">private</span> String path;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setPath</span><span class="params">(String path)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">this</span>.path = path;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String filePost = <span class="string">&quot;.class&quot;</span>;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="title">CustomClassLoader</span><span class="params">(ClassLoader parent, String classLoaderName)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">super</span>(parent);<span class="comment">//显示指定该类的父类加载器</span></span><br><span class="line">    <span class="keyword">this</span>.classLoaderName = classLoaderName;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="title">CustomClassLoader</span><span class="params">(String classLoaderName)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">super</span>();<span class="comment">//将系统类加载器当作该类的父类加载器</span></span><br><span class="line">    <span class="keyword">this</span>.classLoaderName = classLoaderName;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> Class <span class="title">findClass</span><span class="params">(String name)</span> </span>&#123;</span><br><span class="line">    System.out.println(<span class="string">&quot;findClass,输出这句话说明我们自己的类加载器加载了指定的类&quot;</span>);</span><br><span class="line">    <span class="keyword">byte</span>[] b = loadClassData(name);</span><br><span class="line">    <span class="keyword">return</span> defineClass(name, b, <span class="number">0</span>, b.length);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">byte</span>[] loadClassData(String name) &#123;</span><br><span class="line">    InputStream is = <span class="keyword">null</span>;</span><br><span class="line">    <span class="keyword">byte</span>[] data = <span class="keyword">null</span>;</span><br><span class="line">    ByteArrayOutputStream byteArrayOutputStream = <span class="keyword">null</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      name = name.replace(<span class="string">&quot;.&quot;</span>, File.separator);<span class="comment">//File.separator根据操作系统而变化</span></span><br><span class="line">      is = <span class="keyword">new</span> FileInputStream(<span class="keyword">new</span> File(path + name + filePost));</span><br><span class="line">      byteArrayOutputStream = <span class="keyword">new</span> ByteArrayOutputStream();</span><br><span class="line">      <span class="keyword">int</span> len = <span class="number">0</span>;</span><br><span class="line">      <span class="keyword">while</span> (-<span class="number">1</span> != (len = is.read())) &#123;</span><br><span class="line">        byteArrayOutputStream.write(len);</span><br><span class="line">      &#125;</span><br><span class="line">      data = byteArrayOutputStream.toByteArray();</span><br><span class="line"></span><br><span class="line">    &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">      e.printStackTrace();</span><br><span class="line">    &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">      <span class="keyword">try</span> &#123;</span><br><span class="line">        is.close();</span><br><span class="line">        byteArrayOutputStream.close();</span><br><span class="line">      &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">        e.printStackTrace();</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">return</span> data;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/*</span></span><br><span class="line"><span class="comment">   执行结果： </span></span><br><span class="line"><span class="comment">   findClass,输出这句话说明我们自己的类加载器加载了指定的类</span></span><br><span class="line"><span class="comment">   com.poplar.classload.CustomClassLoader2@15db97422018699554</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">    CustomClassLoader2 Loader2 = <span class="keyword">new</span> CustomClassLoader2(<span class="string">&quot;load2&quot;</span>);</span><br><span class="line">    test1(loader2) </span><br><span class="line">    CustomClassLoader2 Loader3 = <span class="keyword">new</span> CustomClassLoader2(<span class="string">&quot;load3&quot;</span>);</span><br><span class="line">    test1(loader3)    </span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">test1</span><span class="params">(CustomClassLoader2 loader2)</span> <span class="keyword">throws</span> ClassNotFoundException, InstantiationException, IllegalAccessException </span>&#123;</span><br><span class="line">          loader2.setPath(<span class="string">&quot;C:\\Users\\poplar\\Desktop\\&quot;</span>);</span><br><span class="line">          Class&lt;?&gt; clazz = loader2.loadClass(<span class="string">&quot;com.poplar.classload.ClassLoadTest&quot;</span>);</span><br><span class="line">          Object instance = clazz.newInstance();</span><br><span class="line">          System.out.println(instance.getClass().getClassLoader());</span><br><span class="line">          System.out.println(instance.hashCode());</span><br><span class="line">          System.out.println(<span class="string">&quot;-------------------------------------&quot;</span>);</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><hr><p><strong>命名空间</strong></p><ul><li>每个类加载器都有自己的命名空间，<strong>命名空间由该加载器及所有父加载器所加载的类构成</strong>；</li><li>在同一个命名空间中，不会出现类的完整名字（包括类的包名）相同的两个类；</li><li>在不同的命名空间中，有可能会出现类的完整名字（包括类的包名）相同的两个类；</li><li>同一命名空间内的类是互相可见的，<strong>非同一命名空间内的类是不可见的</strong>；</li><li>子加载器可以见到父加载器加载的类，<strong>父加载器也不能见到子加载器加载的类</strong>。</li></ul><hr><p><strong>注：</strong></p><ul><li>类加载器本身也是类加载器，类加载器又是谁加载的呢？？（先有鸡还是现有蛋）<br>类加载器是由启动类加载器去加载的，启动类加载器是C++写的，内嵌在JVM中。</li><li>内嵌于JVM中的启动类加载器会加载java.lang.ClassLoader以及其他的Java平台类。当JVM启动时，一块特殊的机器码会运行，它会加载扩展类加载器以及系统类加载器，这块特殊的机器码叫做启动类加载器。</li><li>启动类加载器并不是java类，其他的加载器都是java类。</li><li>启动类加载器是特定于平台的机器指令，它负责开启整个加载过程。</li></ul><hr><h3 id="双亲委派机制"><a href="#双亲委派机制" class="headerlink" title="双亲委派机制"></a>双亲委派机制</h3><p>类加载器用来把类加载到 Java 虚拟机中。从 JDK1.2 版本开始，类的加载过程采用双亲委托机制，这种机制能更好地保证 Java 平台的安全。在此委托机制中，除了 Java 虚拟机自带的根类加载器以外，其余的类加载器都有且只有一个父加载器。当 Java 程序请求加载器 loader1 加载 Sample 类时，loader1 首先委托自己的父加载器去加载 Sample 类，若父加载器能加载，则有父加载器完成加载任务，否则才由加载器loader1 本身加载 Sample 类。</p><p>在双亲委托机制中，各个加载器按照父子关系形成了树形结构，除了根加载器之外，其余的类加载器都有一个父加载器若有一个类能够成功加载 Test 类，那么这个类加载器被称为<strong>定义类加载器</strong>，所有能成功返回Class对象引用的类加载器（包括定义类加载器）称为<strong>初始类加载器</strong>。</p><h4 id="gt-工作过程"><a href="#gt-工作过程" class="headerlink" title="-&gt; 工作过程"></a>-&gt; 工作过程</h4><p>如果一个类加载器收到了类加载的请求，他首先不会自己去尝试加载这个类，而是把这个请求委派父类加载器去完成。每一个层次的类加载器都是如此，因此所有的加载请求最终都传送到顶层的启动类加载器中，只有当父加载器反馈自己无法完成这个请求[加载器的搜索范围中没有找到所需的类]时，子加载器才会尝试自己去加载。</p><hr><h4 id="gt-优点"><a href="#gt-优点" class="headerlink" title="-&gt; 优点"></a>-&gt; 优点</h4><ol><li><p>可以确保 Java 和核心库的安全</p><blockquote><p>所有的 Java 应用都会引用 java.lang 中的类，也就是说在运行期java.lang中的类会被加载到虚拟机中，如果这个加载过程如果是由自己的类加载器所加载，那么很可能就会在JVM中存在多个版本的java.lang中的类，而且这些类是相互不可见的（命名空间的作用）。借助于双亲委托机制，Java核心类库中的类的加载工作都是由启动根加载器去加载，从而确保了Java应用所使用的的都是同一个版本的Java核心类库，他们之间是相互兼容的</p></blockquote></li><li><p>确保 Java 核心类库中的类不会被自定义的类所替代</p></li><li><p>不同的类加载器可以为相同名称的类（binary name）创建额外的命名空间。相同名称的类可以并存在Java虚拟机中，只需要用不同的类加载器去加载即可。相当于在 Java 虚拟机内部建立了一个又一个相互隔离的 Java 类空间。</p></li><li><p><strong>避免重复加载</strong>，父类已经加载过的类，子类无需重新加载。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.poplar.classload;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ClassLoadTest10</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">static</span> &#123;</span><br><span class="line">    System.out.println(<span class="string">&quot;ClassLoadTest10&quot;</span>);</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  <span class="comment">/*执行结果：由于父类已经初始化过了所以Parent2只输出一次</span></span><br><span class="line"><span class="comment">   * ClassLoadTest10</span></span><br><span class="line"><span class="comment">   * Parent2</span></span><br><span class="line"><span class="comment">   * 2</span></span><br><span class="line"><span class="comment">   * Child2</span></span><br><span class="line"><span class="comment">   * 3</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">    Parent2 parent2;</span><br><span class="line">    parent2 = <span class="keyword">new</span> Parent2();</span><br><span class="line">    System.out.println(Parent2.a);</span><br><span class="line">    System.out.println(Child2.b);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Parent2</span> </span>&#123;</span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">int</span> a = <span class="number">2</span>;</span><br><span class="line">    <span class="keyword">static</span> &#123;</span><br><span class="line">        System.out.println(<span class="string">&quot;Parent2&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Child2</span> <span class="keyword">extends</span> <span class="title">Parent2</span> </span>&#123;</span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">int</span> b = <span class="number">3</span>;</span><br><span class="line">    <span class="keyword">static</span> &#123;</span><br><span class="line">        System.out.println(<span class="string">&quot;Child2&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ol><hr><h4 id="gt-双亲委派模型破坏"><a href="#gt-双亲委派模型破坏" class="headerlink" title="-&gt; 双亲委派模型破坏"></a>-&gt; <strong>双亲委派模型破坏</strong></h4><p>在实际的应用中双亲委派解决了java 基础类统一加载的问题，但是却着实存在着一定的缺席。jdk 中的基础类作为用户典型的api 被调用，但是也存在被 api 调用用户的代码的情况，典型的如 SPI 代码。</p><h5 id="SPI-机制简介"><a href="#SPI-机制简介" class="headerlink" title="SPI 机制简介"></a>SPI 机制简介</h5><hr><h3 id="类的卸载"><a href="#类的卸载" class="headerlink" title="类的卸载"></a>类的卸载</h3><ul><li>当一个类被加载、连接和初始化之后，它的生命周期就开始了。当此类的Class对象不再被引用，即不可触及时，Class对象就会结束生命周期，类在方法区内的数据也会被卸载。</li><li>一个类何时结束生命周期，取决于代表它的Class对象何时结束生命周期。</li><li>由Java虚拟机自带的类加载器所加载的类，在虚拟机的生命周期中，始终不会被卸载。Java虚拟机本身会始终引用这些加载器，而这些类加载器则会始终引用他们所加载的类的Class对象，因此这些Class对象是可触及的。</li><li>由用户自定义的类加载器所加载的类是可以被卸载的。（<strong>jvisualvm 查看当前java进程 -XX:+TraceClassUnloading这个用于追</strong>）</li></ul><img src="/Users/joker/Documents/Learnning/JVM/resources/images/截屏2020-09-29 下午2.23.49.png" alt="截屏2020-09-29 下午2.23.49" style="zoom:50%;" />]]></content>
      
      
      <categories>
          
          <category> 深入理解 JVM </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>MySQL 数据查询优化(1):理论基础</title>
      <link href="2019/06/05/MySQL-%E6%95%B0%E6%8D%AE%E6%9F%A5%E8%AF%A2%E4%BC%98%E5%8C%96-1-%EF%BC%9A%E7%90%86%E8%AE%BA%E5%9F%BA%E7%A1%80/"/>
      <url>2019/06/05/MySQL-%E6%95%B0%E6%8D%AE%E6%9F%A5%E8%AF%A2%E4%BC%98%E5%8C%96-1-%EF%BC%9A%E7%90%86%E8%AE%BA%E5%9F%BA%E7%A1%80/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><h2 id="SQL-执行时间长的原因"><a href="#SQL-执行时间长的原因" class="headerlink" title="SQL 执行时间长的原因"></a>SQL 执行时间长的原因</h2><ol><li>查询语句写的烂</li><li>索引失效</li><li>关联查询有太多 Join</li><li>服务器调优及各个参数设置，如缓冲、线程数等</li></ol><h2 id="一条-SQL-执行过长的时间，你如何优化，从哪些方面？"><a href="#一条-SQL-执行过长的时间，你如何优化，从哪些方面？" class="headerlink" title="一条 SQL 执行过长的时间，你如何优化，从哪些方面？"></a>一条 SQL 执行过长的时间，你如何优化，从哪些方面？</h2><ol><li>数据库主从分离，读写分离，降低读写针对同一表同时的压力，至于主从同步，MySQL 有自带的 binlog 实现主从同步</li><li>explain 分析 SQL 语句，查看执行计划，分析索引是否用上，分析扫描行数等等</li><li>查看 MySQL 执行日志，看看是否有其他方面的问题</li></ol><hr><h2 id="SQL-执行顺序"><a href="#SQL-执行顺序" class="headerlink" title="SQL 执行顺序"></a>SQL 执行顺序</h2><ol><li><p><strong>FROM</strong> <left_table></p><blockquote><p>sql 首先执行 from 将数据从硬盘加载到数据缓冲区中，以便对这些数据进行操作。如果有多个表则求所有表的笛卡尔积。<br>[ table1的记录条数 * table2的记录条数 * tableN的记录条数）这时第一个虚拟表产生VT1]</p></blockquote></li><li><p><strong>ON</strong> <join_condition> </p><blockquote><p>其次执行on，从 VT1 中取出匹配 on 条件的行, 产生 VT2。</p></blockquote></li><li><p><strong>JOIN</strong> </p><blockquote><p>left join，right join 外部连接就是在这个时候执行的，在 VT2 的基础上添加符合条件的外部的行，产生 VT3。</p></blockquote></li></ol><hr><ol start="4"><li><p><strong>WHERE</strong> <where_condition></p><blockquote><p>执行 where 过滤，产生 VT4<br>由于这个时候 select 没有执行，所以 select 中的别名不可以用于 where</p></blockquote></li><li><p><strong>GROUP BY</strong> <group_by_list></p><blockquote><p>在 VT4 的基础上执行 group by 分组数据，产生 VT5。</p></blockquote></li><li><p><strong>HAVING</strong>  <having_condition></p><blockquote><p>在 VT5 的基础上执行 having 过滤，产生 VT6。</p></blockquote></li></ol><hr><ol start="7"><li><p><strong>SELECT</strong> </p><blockquote><p>查出我们要的字段，产生 VT7</p></blockquote></li><li><p><strong>DISTINCT</strong> <select_list></p></li><li><p><strong>ORDER BY</strong> <order_by_condition></p><blockquote><p>排序 产生 VT8</p></blockquote></li><li><p><strong>LIMIT</strong> <limit_number></p><blockquote><p>限制返回行数</p></blockquote></li></ol><hr><h1 id="SQL-性能分析"><a href="#SQL-性能分析" class="headerlink" title="SQL 性能分析"></a>SQL 性能分析</h1><h1 id="Explain"><a href="#Explain" class="headerlink" title="Explain"></a>Explain</h1><p>Explain 命令，可以用显示 MySQL 如何使用索引来处理 select 语句以及连接表</p><h2 id="字段"><a href="#字段" class="headerlink" title="字段"></a>字段</h2><h3 id="id"><a href="#id" class="headerlink" title="id"></a><code>id</code></h3><ul><li><p>概述</p><blockquote><p>查询序号，表示查询中 select 语句执行顺序</p></blockquote></li><li><p>说明</p><ol><li>id 相同时，执行顺序由上至下</li><li>子查询 id 的序号会递增，id 值越大优先级越高，越先被执行</li><li>id 相同，可以视为一组，从上往下顺序执行</li></ol></li></ul><h3 id="select-type"><a href="#select-type" class="headerlink" title="select_type"></a><code>select_type</code></h3><ol><li><p>概述</p><p>查询类型，主要是用于区别普通查询、联合查询、子查询等复杂查询</p></li><li><p>说明</p><p><strong>[SIMPLE]</strong> 表示简单的 select，没有 union 和子查询</p><p><strong>[PRIMARY]</strong>  子查询的语句中，最外面的 select 查询就是 primary</p><p><strong>[UNION]</strong> union 语句的第二个或者说是后面那一个</p><p><strong>[DERIVED]</strong> 在 from 列表中的子查询被标记为 DERIVED[衍生]，MySQL会递归执行这些子查询，并把结果放在临时表中。</p></li></ol><h3 id="table"><a href="#table" class="headerlink" title="table"></a><code>table</code></h3><p>显示这一步所访问数据库中表名称[显示这一行的数据是关于哪张表的]，有时不是真实的表名字，可能是简称</p><h3 id="type"><a href="#type" class="headerlink" title="type"></a><code>type</code></h3><p>表示 MySQL 在表中找到所需行的方式，又称**”访问类型”**</p><p>常用的类型有:<strong>ALL, index, range, ref, eq_ref, const, system</strong>   <strong>[从左到右，性能从差到好]</strong></p><ol><li><p><strong>ALL</strong></p><blockquote><p>Full Table Scan， MySQL 将遍历全表以找到匹配的行</p></blockquote></li><li><p><strong>index</strong></p><blockquote><p>Full Index Scan，遍历所有的索引树，比 ALL 要快的多，因为索引文件要比数据文件小的多</p></blockquote></li><li><p><strong>range</strong></p><blockquote><p>查找某个索引的部分索引，一般在 where 子句中使用 &lt; 、&gt;、in、between 等关键词。只检索给定范围的行，属于范围查找</p></blockquote></li><li><p>ref</p><blockquote><p><strong><u>查找非唯一性索引</u>**，</strong>返回匹配某一条件的多条数据**。属于精确查找、数据返回可能是多条</p></blockquote></li><li><p>eq_ref</p><blockquote><p><u><strong>查找唯一性索引</strong></u>，<strong>返回的数据至多一条</strong>。属于精确查找</p></blockquote></li><li><p>const</p><blockquote><p><u><strong>查找主键索引</strong></u>，<strong>返回的数据至多一条（0或者1条）</strong>。属于精确查找（id为主键）</p></blockquote></li><li><p>system</p><blockquote><p>查找主键索引，<strong>返回的数据至多一条（0或者1条）</strong>。属于精确查找</p></blockquote></li></ol><h3 id="possible-keys"><a href="#possible-keys" class="headerlink" title="possible_keys"></a><code>possible_keys</code></h3><p>显示可能应用在这张表中的索引，一个或多个</p><p>查询涉及到的字段上若存在索引，则该索引将被列出，但不一定被查询实际使用。</p><h3 id="key"><a href="#key" class="headerlink" title="key"></a><code>key</code></h3><p>显示 MySQL 实际决定使用的索引</p><p>如果没有选择索引，键是 NULL。</p><p>要想强制 MySQL 使用或忽视 possible_keys 列中的索引，在查询中使用 FORCE INDEX、USE INDEX 或者 IGNORE INDEX</p><h3 id="key-len"><a href="#key-len" class="headerlink" title="key_len"></a><code>key_len</code></h3><p>表示索引中使用的字节数，可通过该列计算查询中使用的索引的长度[<code>key_len</code> 显示的值为索引字段的最大可能长度，并非实际使用长度，即 <code>key_len</code> 是根据表定义计算而得，不是通过表内检索出的]</p><ol><li><p>varchar(10) 变长字段并且允许 null </p><blockquote><p>key_len = 10 * (<strong>character set</strong>: utf8=3, gbk=2, latin=1)  + 1 [null] + 2 [变长字段]</p></blockquote></li><li><p>varchar(10) 变长字段不允许 null </p><blockquote><p>key_len = 10 * (<strong>character set</strong>: utf8=3, gbk=2, latin=1)  + 2 [变长字段]</p></blockquote></li><li><p>char(10) 固定字段并且允许 null </p><blockquote><p>key_len = 10 * (<strong>character set</strong>: utf8=3, gbk=2, latin=1)  + 1 [null]</p></blockquote></li><li><p>char(10) 固定字段不允许 null </p><blockquote><p>key_len = 10 * (<strong>character set</strong>: utf8=3, gbk=2, latin=1)</p></blockquote></li></ol><h3 id="ref"><a href="#ref" class="headerlink" title="ref"></a><code>ref</code></h3><h3 id="rows"><a href="#rows" class="headerlink" title="rows"></a><code>rows</code></h3><p>MySQL 估计为了找到所需的行而要读取的行，rows 值很重要，它决定采用哪个索引以及是否放弃索引改为全表。它是一个平均数，用来估算查找需要行而必须读取的平均值。</p><h3 id="extra"><a href="#extra" class="headerlink" title="extra"></a><code>extra</code></h3><p><strong><code>Using filesort</code></strong></p><p>得到所需结果集，需要对所有记录进行文件排序</p><p>这类 SQL 语句性能极差，需要进行优化。</p><p>在一个没有建立索引的列上进行 order by，就会触发 filesort，常见的优化方案是，在order by 的列上添加索引，避免每次查询都全量排序。</p><p><strong><code>Using temporary</code></strong></p><p>需要建立临时表(temporary table)来暂存中间结果。</p><p>这类 SQL 语句性能较低，往往也需要进行优化。</p><p>典型的，group by 和 order by 同时存在，且作用于不同的字段时，就会建立临时表，以便计算出最终的结果集。</p><p><strong><code>Using index</code></strong></p><p>SQL 所需要返回的所有列数据均在一棵索引树上，而无需访问实际的行记录。</p><p>出现这个说明 MySQL 使用了覆盖索引，避免访问了表的数据行，效率不错！通俗的说也就是查询的列不需要回表，在索引树上就能拿到结果</p><p><strong><code>Using index condition</code></strong></p><p>确实命中了索引，但不是所有的列数据都在索引树上，还需要访问实际的行记录。</p><p><strong><code>Using where</code></strong></p><p>说明服务器在存储引擎收到行后将进行过滤。有些where中的条件会有属于索引的列，当它读取使用索引的时候，就会被过滤，所以会出现有些where语句并没有在extra列中出现using where这么一个说明。</p><p>本例虽然 Extra 字段说明使用了 where 条件过滤，但 type 属性是 ALL，表示需要扫描全部数据，仍有优化空间。</p><p>常见的优化方法为，在where过滤属性上添加索引。</p><p><strong><code>Using join buffer</code></strong></p><p>需要进行嵌套循环计算。</p><p>这类 SQL 语句性能往往也较低，需要进行优化。</p><p>典型的，两个关联表join，关联字段均未建立索引，就会出现这种情况。常见的优化方案是，在关联字段上添加索引，避免每次嵌套循环计算。</p>]]></content>
      
      
      <categories>
          
          <category> MySQL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数据库 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MySQL 存储引擎</title>
      <link href="2019/06/03/MySQL-%E5%AD%98%E5%82%A8%E5%BC%95%E6%93%8E%E7%9A%84%E5%89%AF%E6%9C%AC/"/>
      <url>2019/06/03/MySQL-%E5%AD%98%E5%82%A8%E5%BC%95%E6%93%8E%E7%9A%84%E5%89%AF%E6%9C%AC/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p>数据库引擎简单来说就是一个”数据库发动机”。当你访问数据库时，不管是手工访问，还是程序访问，都不是直接读写数据库文件，而是通过数据库引擎去访问数据库文件。以关系型数据库为例，你发 SQL 语句给数据库引擎，数据库引擎解释 SQL 语句，提取出你需要的数据返回给你。因此，对访问者来说，数据库引擎就是SQL语句的解释器。</p><a id="more"></a><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>正式来说，数据库引擎是用于存储、处理和保护数据的核心服务。利用数据库引擎可以控制访问权限并快速处理事务，从而满足企业内大多数需要处理大量数据的应用程序的要求，这包括创建用于存储数据的表和用于查看、管理和保护数据安全的数据库对象(如索引、视图和存储过程)。存储数据的文件有不同的类型，每种文件类型对应各自不同的处理机制：比如处理文本用 txt 类型，处理表格用 excel，处理图片用 png 等数据库中的表也应该有不同的类型，表的类型不同，会对应 MySQL 不同的存取机制。</p><p>在 Oracle 和 SQL Server 等数据库中只有一种存储引擎，所有数据存储管理机制都是一样的。而 MySQL 数据库支持很多存储引擎，包括MyISAM、InnoDB 等，其中 InnoDB 和 BDB 支持事务安全。它还支持一些第三方的存储引擎。用户可以根据不同的需求为数据表选择不同的存储引擎，用户也可以根据自己的需要编写自己的存储引擎。用户可以根据应用的需求选择如何来存储数据、索引、是否使用事务等。选择合适的存储引擎往往能够有效的提高数据库的性能和数据的访问效率，另外一个数据库中的多个表可以使用不同引擎的组合以满足各种性能和实际需求。</p><p>利用 <strong>show engines</strong>; 可以查看当前版本数据库支持的存储引擎</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; show engines;</span><br><span class="line">+----------+---------+----------------------------------+--------------+------+------------+</span><br><span class="line">| Engine             | Support | Comment                                                     | Transactions | XA   | Savepoints |</span><br><span class="line">+----------+---------+----------------------------------+--------------+------+------------+</span><br><span class="line">| InnoDB             | DEFAULT | Supports transactions, row-level locking, and foreign keys     | YES          | YES  | YES        |</span><br><span class="line">| MRG_MYISAM         | YES     | Collection of identical MyISAM tables                          | NO           | NO   | NO         |</span><br><span class="line">| MEMORY             | YES     | Hash based, stored <span class="keyword">in</span> memory, useful <span class="keyword">for</span> temporary tables      | NO           | NO   | NO         |</span><br><span class="line">| BLACKHOLE          | YES     | /dev/null storage engine (anything you write to it disappears) | NO           | NO   | NO         |</span><br><span class="line">| MyISAM             | YES     | MyISAM storage engine                                          | NO           | NO   | NO         |</span><br><span class="line">| CSV                | YES     | CSV storage engine                                             | NO           | NO   | NO         |</span><br><span class="line">| ARCHIVE            | YES     | Archive storage engine                                         | NO           | NO   | NO         |</span><br><span class="line">| PERFORMANCE_SCHEMA | YES     | Performance Schema                                             | NO           | NO   | NO         |</span><br><span class="line">| FEDERATED          | NO      | Federated MySQL storage engine                                 | NULL         | NULL | NULL       |</span><br><span class="line">+--------------------+---------+----------------------------------------------------------------+--------------+------+------------+</span><br><span class="line">9 rows <span class="keyword">in</span> <span class="built_in">set</span> (0.00 sec)</span><br></pre></td></tr></table></figure><h1 id="MyISAM"><a href="#MyISAM" class="headerlink" title="MyISAM"></a>MyISAM</h1><h3 id="MyISAM-数据存储"><a href="#MyISAM-数据存储" class="headerlink" title="MyISAM 数据存储"></a>MyISAM 数据存储</h3><h4 id="存储结构"><a href="#存储结构" class="headerlink" title="存储结构"></a>存储结构</h4><p>MyISAM 会将表数据存储在两个文件中：数据文件和索引文件，分别会以 .MYD 和 .MYI 为扩展名。MyISAM 表可以包含动态或者静态行。 MySQL 会根据表的定义来决定采用何种格式。 MyISAM 表可以存储的行记录数，一般受限于可用的磁盘空间，或者操作系统中单个文件的最大尺寸。<br>.frm是描述了数据表的结构</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">root@0440a971febb:/var/lib/mysql/test<span class="comment"># ls</span></span><br><span class="line">book.frm  book.ibddb.optperson.MYD  person.MYIperson.frm  reader.frmreader.ibd</span><br></pre></td></tr></table></figure><h3 id="MyISAM-特性"><a href="#MyISAM-特性" class="headerlink" title="MyISAM 特性"></a>MyISAM 特性</h3><h4 id="并发性和锁级别"><a href="#并发性和锁级别" class="headerlink" title="并发性和锁级别"></a>并发性和锁级别</h4><p>使用表级锁，当对表中数据做修改时，会对整个表的数据进行加锁，在读取数据时候，也需要对表中的数据加共享锁，所以对读写混合的操作并发性并不是太好。MyISAM 的读写锁调度是写优先，这也是 MyISAM 不适合做以数据写入为主的数据表引擎，因为在写锁后，其他线程不能做任何操作，大量的更新操作会使查询很难得到锁，从而造成永远阻塞。</p><h4 id="表的损坏和修复"><a href="#表的损坏和修复" class="headerlink" title="表的损坏和修复"></a>表的损坏和修复</h4><p>MyISAM 支持对任意意外关闭所损坏的表的检查和修复，这里的修复并不是通过事物务来修复的，有可能造成数据丢失</p><ul><li>如果mysqld已经宕掉，且无法启动，那么可以通过 mysiamchk 工具来进行修复。此工具在mysqld 服务没有启动时才可以使用。该工具可以检查并分析修复MyISAM表。</li><li>如果 mysqld 仍在运行，或者可以重新启动，那么可以通过 mysqlcheck 工具来进行修复。或者直接通过 mysql 的内置修复SQL 语句来修复：CHECK TABLE，REPAIR TABLE ，ANALYSE TABLE，OPTIMIZE TABLE。这两种方法可以同样达到对表的修复作用。 以上两种方式各有应用场景。</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; show create table person;</span><br><span class="line">+<span class="comment">--------+---------------------------------------------------------------------------+</span></span><br><span class="line">| Table  | <span class="keyword">Create</span> <span class="keyword">Table</span>                                                              |</span><br><span class="line">+<span class="comment">--------+---------------------------------------------------------------------------+</span></span><br><span class="line">| person | <span class="keyword">CREATE</span> <span class="keyword">TABLE</span> person (</span><br><span class="line">  <span class="keyword">id</span> <span class="built_in">int</span>(<span class="number">11</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line">  <span class="keyword">name</span> <span class="built_in">varchar</span>(<span class="number">30</span>) <span class="keyword">DEFAULT</span> <span class="literal">NULL</span>,</span><br><span class="line">  Age <span class="built_in">int</span>(<span class="number">11</span>) <span class="keyword">DEFAULT</span> <span class="literal">NULL</span>,</span><br><span class="line">  PRIMARY <span class="keyword">KEY</span> (<span class="keyword">id</span>)</span><br><span class="line">) <span class="keyword">ENGINE</span>=MyISAM <span class="keyword">DEFAULT</span> <span class="keyword">CHARSET</span>=utf8 |</span><br><span class="line">+<span class="comment">--------+---------------------------------------------------------------------------+</span></span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; check table person;</span><br><span class="line">+<span class="comment">-------------+-------+----------+----------+</span></span><br><span class="line">| Table       | Op    | Msg_type | Msg_text |</span><br><span class="line">+<span class="comment">-------------+-------+----------+----------+</span></span><br><span class="line">| test.person | <span class="keyword">check</span> | <span class="keyword">status</span>   | OK       |</span><br><span class="line">+<span class="comment">-------------+-------+----------+----------+</span></span><br><span class="line"><span class="number">1</span> <span class="keyword">row</span> <span class="keyword">in</span> <span class="keyword">set</span> (<span class="number">0.01</span> sec)</span><br><span class="line"></span><br><span class="line">mysql&gt; <span class="keyword">repair</span> <span class="keyword">table</span> person;</span><br><span class="line">+<span class="comment">-------------+--------+----------+----------+</span></span><br><span class="line">| Table       | Op     | Msg_type | Msg_text |</span><br><span class="line">+<span class="comment">-------------+--------+----------+----------+</span></span><br><span class="line">| test.person | <span class="keyword">repair</span> | <span class="keyword">status</span>   | OK       |</span><br><span class="line">+<span class="comment">-------------+--------+----------+----------+</span></span><br><span class="line"><span class="number">1</span> <span class="keyword">row</span> <span class="keyword">in</span> <span class="keyword">set</span> (<span class="number">0.01</span> sec) </span><br></pre></td></tr></table></figure><h3 id="MyISAM-适用场景"><a href="#MyISAM-适用场景" class="headerlink" title="MyISAM 适用场景"></a>MyISAM 适用场景</h3><ul><li>如果应用中需要执行大量的SELECT查询，可以考虑 MyISAM。</li></ul><h1 id="InnoDB"><a href="#InnoDB" class="headerlink" title="InnoDB"></a>InnoDB</h1><p>InnoDB 存储引擎: 主要面向 OLTP(Online Transaction Processing，在线事务处理) 方面的应用，是第一个完整支持 ACID 事务的存储引擎。</p><h3 id="数据存储"><a href="#数据存储" class="headerlink" title="数据存储"></a>数据存储</h3><h4 id="存储结构-1"><a href="#存储结构-1" class="headerlink" title="存储结构"></a>存储结构</h4><p>所有的数据表都保存在同一个数据文件中（也可能是多个文件，或者是独立的表空间文件）MySQL 使用 InnoDB 存储表时，会将表的定义和数据索引等信息分开存储，其中前者存储在 .frm 文件中，后者存储在 .ibd 文件中</p><ol><li>.frm<br> 无论在 MySQL 中选择了哪个存储引擎，所有的 MySQL 表都会在硬盘上创建一个 .frm 文件用来描述表的格式或者说定义； .frm 文件的格式在不同的平台上都是相同的。</li><li>.ibd 文件<br> .ibd 文件就是每一个表独有的表空间，文件存储了当前表的数据和相关的索引数据。</li></ol><h3 id="适用场景"><a href="#适用场景" class="headerlink" title="适用场景"></a>适用场景</h3><ul><li>适用于更新密集型，支持事务，自动灾难恢复，行锁，外键</li><li>InnoDB 为 MySQL 表提供了ACID事务支持、系统崩溃修复能力和多版本并发控制（即MVCC Multi-Version Concurrency Control）的行锁</li><li>支持自增长列（auto_increment）,自增长列的值不能为空，如果在使用的时候为空则自动从现有值开始增值。</li><li>支持外键</li></ul><h2 id="区别"><a href="#区别" class="headerlink" title="区别"></a>区别</h2><p><strong>MyISAM 是 MySQL 5.5 版本之前的默认数据库引擎。 虽然性能极佳，而且提供了大量的特性，包括全文索引、压缩、空间函数等，但 MyISAM 不支持事务和行级锁，而且最大的缺陷就是崩溃后无法安全恢复。不过，5.5版本之后，MySQL引入了InnoDB（事务性数据库引擎），MySQL 5.5版本后默认的存储引擎为InnoDB。</strong></p><ol><li><p>InnoDB 是第一个完整支持 ACID 事务的存储引擎, MyISAM 不支持</p></li><li><p>InnoDB 支持外键，MyISAM 不支持</p></li><li><p>MyISAM 只有表级锁，不会死锁，并发性能差；而 InnoDB 支持行级锁和表级锁,默认为行级锁，会死锁，并发性能好。</p></li><li><p>InnoDB 必须有主键，没有指定就为每一行数据生成不可见的 ROWID 列作为主键，MyISAM 可以没有主键</p></li><li><p>MyISAM 内置了一个计数器来存储表的行数。执行 select count(*) 时直接从计数器中读取，速度非常快。而 InnoDB 不保存这些信息。</p><p><strong>InnoDB：</strong> 没有保存表的总行数，如果使用select count(*) from table；就会遍历整个表，消耗相当大，但是在加了 wehre 条件后，MyISAM 和 innodb 处理的方式都一样。</p></li><li><p>InnoDB 是聚集索引，使用 B+Tree 作为索引结构，数据文件是和（主键）索引绑在一起的（表数据文件本身就是按 B+Tree 组织的一个索引结构），必须要有主键，通过主键索引效率很高。但是辅助索引需要两次查询，先查询到主键，然后再通过主键查询到数据。因此，主键不应该过大，因为主键太大，其他索引也都会很大。</p></li><li><p>如果执行大量的 SELECT，MyISAM 是更好的选择。如果你的数据执行大量的 INSERT 或 UPDATE，出于性能方面的考虑，应该使用 InnoDB 表</p></li><li><p>MyISAM：可被压缩，存储空间小。支持三种不同的存储格式：静态表、动态表、压缩表。</p><p>Innodb：需要更多的内存和存储，它会在主内存中建立专用的缓冲池用于高速缓冲数据和索引。</p></li><li><p>MyISAM ：每个表在磁盘上存储成三个文件。</p><ul><li><p><code>.frm</code> 文件存储表定义</p></li><li><p><code>.MYD</code> 是数据文件的扩展名</p></li><li><p><code>.MYI</code> 是索引文件的扩展名。</p></li></ul><p>Innodb 有自己的表空间的概念，表中的数据是存储在表空间之中的，具体存储在什么样的表空间中呢？是由 innodb_file_per_table 这个参数决定的，</p><p>如果这个参数为ON:独立表空间，存储的表名为(表名.ibd),</p><p>如果参数为OFF:系统表空间(系统的共享表空间),存储的表名为(ibdataX(X为数字))) </p><pre><code>命令：show variables like &#39;innodb_file_per_table&#39;;       查看 mysql 数据库的存放位置: show global variables like &quot;%datadir%&quot;;接下来我们创建一个表来看一下create table myinnodb(id int,c1 varchar(100)) engine=&#39;innodb&#39;;            看一下文件系统是如何存储的，进入到数据库存放的位置，ls -lh myinnodb*            可以看到有 myinnodb.frm 和 myinnodb.ibd 两个文件，frm文件时记录表结构的，ibd 就是 innodb 表实际存储的位置       接着把 innodb_file_per_table 参数设置为 off,命令为 set global innodb_file_per_table=off;            show variables like &#39;innodb_file_per_table&#39;; 用这个命令检查是否关闭了            再创建一个表：create table myinnodb_g(id int,c1 varchar(100)) engine=&#39;innodb&#39;;            查看存储的位置，可以看到只有一个myinnodb_g.frm的文件，不存在ibd文件，也就是说它的数据存储在系统共享表的空间 存储在ibdata1中</code></pre></li></ol><ol><li><p><strong>全文索引</strong></p><p><strong>MyISAM：</strong>支持 FULLTEXT类型的全文索引</p><p><strong>InnoDB：</strong>不支持FULLTEXT类型的全文索引，但是innodb可以使用sphinx插件支持全文索引，并且效果更好。</p></li><li><p><strong>事务支持</strong></p><p><strong>MyISAM：</strong>强调的是性能，每次查询具有原子性,其执行数度比 InnoDB 类型更快，但是不提供事务支持。</p><p><strong>InnoDB：</strong>提供事务支持事务，外部键等高级数据库功能。 具有事务(commit)、回滚(rollback)和崩溃修复能力(crash recovery capabilities)的事务安全(transaction-safe (ACID compliant))型表。</p></li></ol><p><strong>可移植性、备份及恢复</strong></p><p><strong>MyISAM：</strong>数据是以文件的形式存储，所以在跨平台的数据转移中会很方便。在备份和恢复时可单独针对某个表进行操作。</p><p><strong>InnoDB：</strong>免费的方案可以是拷贝数据文件、备份 binlog，或者用 mysqldump，在数据量达到几十G的时候就相对痛苦了。</p><p><strong>数据和索引的组织方式不同。</strong></p><p>MyISAM 将索引和数据分开进行存储。索引存放在 <code>.MYI</code> 文件中，数据存放在 <code>.MYD</code> 文件中。索引中保存了相应数据的地址。以 <code>表名+.MYI</code> 文件分别保存。 InnoDB 的主键索引树的叶子节点保存主键和相应的数据。其它的索引树的叶子节点保存的是主键。也正是因为采取了这种存储方式，InnoDB 才强制要求每张表都要有主键。</p><p><strong>对 AUTO_INCREMENT 的处理方式不一样。</strong></p><p>如果将某个字段设置为 INCREMENT，InnoDB 中规定必须包含只有该字段的索引。但是在 MyISAM 中，也可以将该字段和其他字段一起建立联合索引。</p><p><strong>CURD 操作 MyISAM</strong></p><p>如果执行大量的SELECT，MyISAM 是更好的选择。 InnoDB：如果你的数据执行大量的 INSERT 或 UPDATE，出于性能方面的考虑，应该使用 InnoDB 表。DELETE 从性能上InnoDB更优，但DELETE FROM table时，InnoDB不会重新建立表，而是一行一行的删除，在innodb上如果要清空保存有大量数据的表，最好使用truncate table这个命令。</p><p><strong>锁的粒度不同。</strong></p><p>MyISAM 仅支持表锁。每次操作锁住整张表。这种处理方式一方面加锁的开销比较小，且不会出现死锁，但另一方面并发性能较差。InnoDB支持行锁。每次操作锁住一行数据，一方面行级锁在每次获取锁和释放锁的操作需要消耗比表锁更多的资源，速度较慢，且可能发生死锁，但是另一方面由于锁的粒度较小，发生锁冲突的概率也比较低，并发性较好。此外，即使是使用了InnoDB存储引擎，但如果MySQL执行一条sql语句时不能确定要扫描的范围，也会锁住整张表。</p>]]></content>
      
      
      <categories>
          
          <category> MySQL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数据库 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MySQL 存储引擎</title>
      <link href="2019/06/03/MySQL-%E5%AD%98%E5%82%A8%E5%BC%95%E6%93%8E/"/>
      <url>2019/06/03/MySQL-%E5%AD%98%E5%82%A8%E5%BC%95%E6%93%8E/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p>数据库引擎简单来说就是一个”数据库发动机”。当你访问数据库时，不管是手工访问，还是程序访问，都不是直接读写数据库文件，而是通过数据库引擎去访问数据库文件。以关系型数据库为例，你发 SQL 语句给数据库引擎，数据库引擎解释 SQL 语句，提取出你需要的数据返回给你。因此，对访问者来说，数据库引擎就是SQL语句的解释器。</p><a id="more"></a><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>正式来说，数据库引擎是用于存储、处理和保护数据的核心服务。利用数据库引擎可以控制访问权限并快速处理事务，从而满足企业内大多数需要处理大量数据的应用程序的要求，这包括创建用于存储数据的表和用于查看、管理和保护数据安全的数据库对象(如索引、视图和存储过程)。存储数据的文件有不同的类型，每种文件类型对应各自不同的处理机制：比如处理文本用 txt 类型，处理表格用 excel，处理图片用 png 等数据库中的表也应该有不同的类型，表的类型不同，会对应 MySQL 不同的存取机制。</p><p>在 Oracle 和 SQL Server 等数据库中只有一种存储引擎，所有数据存储管理机制都是一样的。而 MySQL 数据库支持很多存储引擎，包括MyISAM、InnoDB 等，其中 InnoDB 和 BDB 支持事务安全。它还支持一些第三方的存储引擎。用户可以根据不同的需求为数据表选择不同的存储引擎，用户也可以根据自己的需要编写自己的存储引擎。用户可以根据应用的需求选择如何来存储数据、索引、是否使用事务等。选择合适的存储引擎往往能够有效的提高数据库的性能和数据的访问效率，另外一个数据库中的多个表可以使用不同引擎的组合以满足各种性能和实际需求。</p><p>利用 <strong>show engines</strong>; 可以查看当前版本数据库支持的存储引擎</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; show engines;</span><br><span class="line">+----------+---------+----------------------------------+--------------+------+------------+</span><br><span class="line">| Engine             | Support | Comment                                                     | Transactions | XA   | Savepoints |</span><br><span class="line">+----------+---------+----------------------------------+--------------+------+------------+</span><br><span class="line">| InnoDB             | DEFAULT | Supports transactions, row-level locking, and foreign keys     | YES          | YES  | YES        |</span><br><span class="line">| MRG_MYISAM         | YES     | Collection of identical MyISAM tables                          | NO           | NO   | NO         |</span><br><span class="line">| MEMORY             | YES     | Hash based, stored <span class="keyword">in</span> memory, useful <span class="keyword">for</span> temporary tables      | NO           | NO   | NO         |</span><br><span class="line">| BLACKHOLE          | YES     | /dev/null storage engine (anything you write to it disappears) | NO           | NO   | NO         |</span><br><span class="line">| MyISAM             | YES     | MyISAM storage engine                                          | NO           | NO   | NO         |</span><br><span class="line">| CSV                | YES     | CSV storage engine                                             | NO           | NO   | NO         |</span><br><span class="line">| ARCHIVE            | YES     | Archive storage engine                                         | NO           | NO   | NO         |</span><br><span class="line">| PERFORMANCE_SCHEMA | YES     | Performance Schema                                             | NO           | NO   | NO         |</span><br><span class="line">| FEDERATED          | NO      | Federated MySQL storage engine                                 | NULL         | NULL | NULL       |</span><br><span class="line">+--------------------+---------+----------------------------------------------------------------+--------------+------+------------+</span><br><span class="line">9 rows <span class="keyword">in</span> <span class="built_in">set</span> (0.00 sec)</span><br></pre></td></tr></table></figure><h1 id="MyISAM"><a href="#MyISAM" class="headerlink" title="MyISAM"></a>MyISAM</h1><h3 id="MyISAM-数据存储"><a href="#MyISAM-数据存储" class="headerlink" title="MyISAM 数据存储"></a>MyISAM 数据存储</h3><h4 id="存储结构"><a href="#存储结构" class="headerlink" title="存储结构"></a>存储结构</h4><p>MyISAM 会将表数据存储在两个文件中：数据文件和索引文件，分别会以 .MYD 和 .MYI 为扩展名。MyISAM 表可以包含动态或者静态行。 MySQL 会根据表的定义来决定采用何种格式。 MyISAM 表可以存储的行记录数，一般受限于可用的磁盘空间，或者操作系统中单个文件的最大尺寸。<br>.frm是描述了数据表的结构</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">root@0440a971febb:/var/lib/mysql/test<span class="comment"># ls</span></span><br><span class="line">book.frm  book.ibddb.optperson.MYD  person.MYIperson.frm  reader.frmreader.ibd</span><br></pre></td></tr></table></figure><h3 id="MyISAM-特性"><a href="#MyISAM-特性" class="headerlink" title="MyISAM 特性"></a>MyISAM 特性</h3><h4 id="并发性和锁级别"><a href="#并发性和锁级别" class="headerlink" title="并发性和锁级别"></a>并发性和锁级别</h4><p>使用表级锁，当对表中数据做修改时，会对整个表的数据进行加锁，在读取数据时候，也需要对表中的数据加共享锁，所以对读写混合的操作并发性并不是太好。MyISAM 的读写锁调度是写优先，这也是 MyISAM 不适合做以数据写入为主的数据表引擎，因为在写锁后，其他线程不能做任何操作，大量的更新操作会使查询很难得到锁，从而造成永远阻塞。</p><h4 id="表的损坏和修复"><a href="#表的损坏和修复" class="headerlink" title="表的损坏和修复"></a>表的损坏和修复</h4><p>MyISAM 支持对任意意外关闭所损坏的表的检查和修复，这里的修复并不是通过事物务来修复的，有可能造成数据丢失</p><ul><li>如果mysqld已经宕掉，且无法启动，那么可以通过 mysiamchk 工具来进行修复。此工具在mysqld 服务没有启动时才可以使用。该工具可以检查并分析修复MyISAM表。</li><li>如果 mysqld 仍在运行，或者可以重新启动，那么可以通过 mysqlcheck 工具来进行修复。或者直接通过 mysql 的内置修复SQL 语句来修复：CHECK TABLE，REPAIR TABLE ，ANALYSE TABLE，OPTIMIZE TABLE。这两种方法可以同样达到对表的修复作用。 以上两种方式各有应用场景。</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; show create table person;</span><br><span class="line">+<span class="comment">--------+---------------------------------------------------------------------------+</span></span><br><span class="line">| Table  | <span class="keyword">Create</span> <span class="keyword">Table</span>                                                              |</span><br><span class="line">+<span class="comment">--------+---------------------------------------------------------------------------+</span></span><br><span class="line">| person | <span class="keyword">CREATE</span> <span class="keyword">TABLE</span> person (</span><br><span class="line">  <span class="keyword">id</span> <span class="built_in">int</span>(<span class="number">11</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line">  <span class="keyword">name</span> <span class="built_in">varchar</span>(<span class="number">30</span>) <span class="keyword">DEFAULT</span> <span class="literal">NULL</span>,</span><br><span class="line">  Age <span class="built_in">int</span>(<span class="number">11</span>) <span class="keyword">DEFAULT</span> <span class="literal">NULL</span>,</span><br><span class="line">  PRIMARY <span class="keyword">KEY</span> (<span class="keyword">id</span>)</span><br><span class="line">) <span class="keyword">ENGINE</span>=MyISAM <span class="keyword">DEFAULT</span> <span class="keyword">CHARSET</span>=utf8 |</span><br><span class="line">+<span class="comment">--------+---------------------------------------------------------------------------+</span></span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; check table person;</span><br><span class="line">+<span class="comment">-------------+-------+----------+----------+</span></span><br><span class="line">| Table       | Op    | Msg_type | Msg_text |</span><br><span class="line">+<span class="comment">-------------+-------+----------+----------+</span></span><br><span class="line">| test.person | <span class="keyword">check</span> | <span class="keyword">status</span>   | OK       |</span><br><span class="line">+<span class="comment">-------------+-------+----------+----------+</span></span><br><span class="line"><span class="number">1</span> <span class="keyword">row</span> <span class="keyword">in</span> <span class="keyword">set</span> (<span class="number">0.01</span> sec)</span><br><span class="line"></span><br><span class="line">mysql&gt; <span class="keyword">repair</span> <span class="keyword">table</span> person;</span><br><span class="line">+<span class="comment">-------------+--------+----------+----------+</span></span><br><span class="line">| Table       | Op     | Msg_type | Msg_text |</span><br><span class="line">+<span class="comment">-------------+--------+----------+----------+</span></span><br><span class="line">| test.person | <span class="keyword">repair</span> | <span class="keyword">status</span>   | OK       |</span><br><span class="line">+<span class="comment">-------------+--------+----------+----------+</span></span><br><span class="line"><span class="number">1</span> <span class="keyword">row</span> <span class="keyword">in</span> <span class="keyword">set</span> (<span class="number">0.01</span> sec) </span><br></pre></td></tr></table></figure><h3 id="MyISAM-适用场景"><a href="#MyISAM-适用场景" class="headerlink" title="MyISAM 适用场景"></a>MyISAM 适用场景</h3><ul><li>如果应用中需要执行大量的SELECT查询，可以考虑 MyISAM。</li></ul><h1 id="InnoDB"><a href="#InnoDB" class="headerlink" title="InnoDB"></a>InnoDB</h1><p>InnoDB 存储引擎: 主要面向 OLTP(Online Transaction Processing，在线事务处理) 方面的应用，是第一个完整支持 ACID 事务的存储引擎。</p><h3 id="数据存储"><a href="#数据存储" class="headerlink" title="数据存储"></a>数据存储</h3><h4 id="存储结构-1"><a href="#存储结构-1" class="headerlink" title="存储结构"></a>存储结构</h4><p>所有的数据表都保存在同一个数据文件中（也可能是多个文件，或者是独立的表空间文件）MySQL 使用 InnoDB 存储表时，会将表的定义和数据索引等信息分开存储，其中前者存储在 .frm 文件中，后者存储在 .ibd 文件中</p><ol><li>.frm<br> 无论在 MySQL 中选择了哪个存储引擎，所有的 MySQL 表都会在硬盘上创建一个 .frm 文件用来描述表的格式或者说定义； .frm 文件的格式在不同的平台上都是相同的。</li><li>.ibd 文件<br> .ibd 文件就是每一个表独有的表空间，文件存储了当前表的数据和相关的索引数据。</li></ol><h3 id="适用场景"><a href="#适用场景" class="headerlink" title="适用场景"></a>适用场景</h3><ul><li>适用于更新密集型，支持事务，自动灾难恢复，行锁，外键</li><li>InnoDB 为 MySQL 表提供了ACID事务支持、系统崩溃修复能力和多版本并发控制（即MVCC Multi-Version Concurrency Control）的行锁</li><li>支持自增长列（auto_increment）,自增长列的值不能为空，如果在使用的时候为空则自动从现有值开始增值。</li><li>支持外键</li></ul><h2 id="区别"><a href="#区别" class="headerlink" title="区别"></a>区别</h2><p><strong>MyISAM 是 MySQL 5.5 版本之前的默认数据库引擎。 虽然性能极佳，而且提供了大量的特性，包括全文索引、压缩、空间函数等，但 MyISAM 不支持事务和行级锁，而且最大的缺陷就是崩溃后无法安全恢复。不过，5.5版本之后，MySQL引入了InnoDB（事务性数据库引擎），MySQL 5.5版本后默认的存储引擎为InnoDB。</strong></p><ol><li><p>InnoDB 是第一个完整支持 ACID 事务的存储引擎, MyISAM 不支持</p></li><li><p>InnoDB 支持外键，MyISAM 不支持</p></li><li><p>MyISAM 只有表级锁，不会死锁，并发性能差；而 InnoDB 支持行级锁和表级锁,默认为行级锁，会死锁，并发性能好。</p></li><li><p>InnoDB 必须有主键，没有指定就为每一行数据生成不可见的 ROWID 列作为主键，MyISAM 可以没有主键</p></li><li><p>MyISAM 内置了一个计数器来存储表的行数。执行 select count(*) 时直接从计数器中读取，速度非常快。而 InnoDB 不保存这些信息。</p><p><strong>InnoDB：</strong> 没有保存表的总行数，如果使用select count(*) from table；就会遍历整个表，消耗相当大，但是在加了 wehre 条件后，MyISAM 和 innodb 处理的方式都一样。</p></li><li><p>InnoDB 是聚集索引，使用 B+Tree 作为索引结构，数据文件是和（主键）索引绑在一起的（表数据文件本身就是按 B+Tree 组织的一个索引结构），必须要有主键，通过主键索引效率很高。但是辅助索引需要两次查询，先查询到主键，然后再通过主键查询到数据。因此，主键不应该过大，因为主键太大，其他索引也都会很大。</p></li><li><p>如果执行大量的 SELECT，MyISAM 是更好的选择。如果你的数据执行大量的 INSERT 或 UPDATE，出于性能方面的考虑，应该使用 InnoDB 表</p></li><li><p>MyISAM：可被压缩，存储空间小。支持三种不同的存储格式：静态表、动态表、压缩表。</p><p>Innodb：需要更多的内存和存储，它会在主内存中建立专用的缓冲池用于高速缓冲数据和索引。</p></li><li><p>MyISAM ：每个表在磁盘上存储成三个文件。</p><ul><li><p><code>.frm</code> 文件存储表定义</p></li><li><p><code>.MYD</code> 是数据文件的扩展名</p></li><li><p><code>.MYI</code> 是索引文件的扩展名。</p></li></ul><p>Innodb 有自己的表空间的概念，表中的数据是存储在表空间之中的，具体存储在什么样的表空间中呢？是由 innodb_file_per_table 这个参数决定的，</p><p>如果这个参数为ON:独立表空间，存储的表名为(表名.ibd),</p><p>如果参数为OFF:系统表空间(系统的共享表空间),存储的表名为(ibdataX(X为数字))) </p><pre><code>命令：show variables like &#39;innodb_file_per_table&#39;;       查看 mysql 数据库的存放位置: show global variables like &quot;%datadir%&quot;;接下来我们创建一个表来看一下create table myinnodb(id int,c1 varchar(100)) engine=&#39;innodb&#39;;            看一下文件系统是如何存储的，进入到数据库存放的位置，ls -lh myinnodb*            可以看到有 myinnodb.frm 和 myinnodb.ibd 两个文件，frm文件时记录表结构的，ibd 就是 innodb 表实际存储的位置       接着把 innodb_file_per_table 参数设置为 off,命令为 set global innodb_file_per_table=off;            show variables like &#39;innodb_file_per_table&#39;; 用这个命令检查是否关闭了            再创建一个表：create table myinnodb_g(id int,c1 varchar(100)) engine=&#39;innodb&#39;;            查看存储的位置，可以看到只有一个myinnodb_g.frm的文件，不存在ibd文件，也就是说它的数据存储在系统共享表的空间 存储在ibdata1中</code></pre></li></ol><ol><li><p><strong>全文索引</strong></p><p><strong>MyISAM：</strong>支持 FULLTEXT类型的全文索引</p><p><strong>InnoDB：</strong>不支持FULLTEXT类型的全文索引，但是innodb可以使用sphinx插件支持全文索引，并且效果更好。</p></li><li><p><strong>事务支持</strong></p><p><strong>MyISAM：</strong>强调的是性能，每次查询具有原子性,其执行数度比 InnoDB 类型更快，但是不提供事务支持。</p><p><strong>InnoDB：</strong>提供事务支持事务，外部键等高级数据库功能。 具有事务(commit)、回滚(rollback)和崩溃修复能力(crash recovery capabilities)的事务安全(transaction-safe (ACID compliant))型表。</p></li></ol><p><strong>可移植性、备份及恢复</strong></p><p><strong>MyISAM：</strong>数据是以文件的形式存储，所以在跨平台的数据转移中会很方便。在备份和恢复时可单独针对某个表进行操作。</p><p><strong>InnoDB：</strong>免费的方案可以是拷贝数据文件、备份 binlog，或者用 mysqldump，在数据量达到几十G的时候就相对痛苦了。</p><p><strong>数据和索引的组织方式不同。</strong></p><p>MyISAM 将索引和数据分开进行存储。索引存放在 <code>.MYI</code> 文件中，数据存放在 <code>.MYD</code> 文件中。索引中保存了相应数据的地址。以 <code>表名+.MYI</code> 文件分别保存。 InnoDB 的主键索引树的叶子节点保存主键和相应的数据。其它的索引树的叶子节点保存的是主键。也正是因为采取了这种存储方式，InnoDB 才强制要求每张表都要有主键。</p><p><strong>对 AUTO_INCREMENT 的处理方式不一样。</strong></p><p>如果将某个字段设置为 INCREMENT，InnoDB 中规定必须包含只有该字段的索引。但是在 MyISAM 中，也可以将该字段和其他字段一起建立联合索引。</p><p><strong>CURD 操作 MyISAM</strong></p><p>如果执行大量的SELECT，MyISAM 是更好的选择。 InnoDB：如果你的数据执行大量的 INSERT 或 UPDATE，出于性能方面的考虑，应该使用 InnoDB 表。DELETE 从性能上InnoDB更优，但DELETE FROM table时，InnoDB不会重新建立表，而是一行一行的删除，在innodb上如果要清空保存有大量数据的表，最好使用truncate table这个命令。</p><p><strong>锁的粒度不同。</strong></p><p>MyISAM 仅支持表锁。每次操作锁住整张表。这种处理方式一方面加锁的开销比较小，且不会出现死锁，但另一方面并发性能较差。InnoDB支持行锁。每次操作锁住一行数据，一方面行级锁在每次获取锁和释放锁的操作需要消耗比表锁更多的资源，速度较慢，且可能发生死锁，但是另一方面由于锁的粒度较小，发生锁冲突的概率也比较低，并发性较好。此外，即使是使用了InnoDB存储引擎，但如果MySQL执行一条sql语句时不能确定要扫描的范围，也会锁住整张表。</p>]]></content>
      
      
      <categories>
          
          <category> MySQL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数据库 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MySQL 索引</title>
      <link href="2019/06/03/MySQL-%E7%B4%A2%E5%BC%95/"/>
      <url>2019/06/03/MySQL-%E7%B4%A2%E5%BC%95/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p>除数据本身之外，数据库还维护着一个满足特定查找算法的数据结构，这些数据结构一某种方式指向数据，这样就可以在这些数据结构基础上实现高级查找算法，这种数据结构就是索引。</p><a id="more"></a><h1 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h1><p>除数据本身之外，数据库还维护着一个满足特定查找算法的数据结构，这些数据结构一某种方式指向数据，这样就可以在这些数据结构基础上实现高级查找算法，这种数据结构就是索引。</p><h2 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h2><p>MySQL 官方定义为：<strong>索引是帮助 MySQL 高效获取数据的 <font color='red'>数据结构</font></strong></p><p><strong>本质：索引是一种数据结构</strong></p><h2 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h2><ol><li><p>通过创建索引，可以提高数据检索的效率，降低数据库的 IO 成本</p></li><li><p>通过创建索引，降低数据排序的成本，降低了 CPU 的消耗</p></li></ol><h2 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h2><ol><li><p>索引本质也是一张表，保存了主键与索引字段，并指向实体表的记录，所以索引列也占用存储空间</p></li><li><p>虽然索引大大提高了查询速度，同时却会降低数据更新效率</p></li></ol><h2 id="索引分类"><a href="#索引分类" class="headerlink" title="索引分类"></a>索引分类</h2><h3 id="实现方式"><a href="#实现方式" class="headerlink" title="实现方式"></a>实现方式</h3><ol><li><p>非聚集索引</p><p><strong>非聚集索引的叶子节点为索引节点，但是有一个指针指向数据节点。</strong></p><p>MyISAM 是非聚集索引。</p></li><li><p>聚集索引</p><p><strong>聚集索引叶子节点就是数据节点。</strong></p><p><strong>关于聚集索引，innodb 会按照如下规则进行处理：</strong> </p><p>如果定义主键，则主键作为聚集索引 </p><p>如果没有主键被定义，则该表的第一个唯一非空索引被作为聚集索引 </p><p>如果没有主键也没有合适的唯一索引，那么 innodb 内部会生成一个隐藏的主键作为聚集索引，这个隐藏的主键是一个6个字节的列，改列的值会随着数据的插入自增。</p></li></ol><p><strong><font color='red'>注意事项</font></strong></p><blockquote><p>innodb 的普通索引，唯一索引，联合索引都是辅助索引，采用非聚集索引结构。InnoDB 的所有辅助索引都引用主键作为 data 域。</p></blockquote><p>聚集索引这种实现方式使得按主键的搜索十分高效，但是辅助索引搜索需要检索两遍索引：首先检索辅助索引获得主键，然后用主键到主索引中检索获得记录。 </p><h3 id="结构"><a href="#结构" class="headerlink" title="结构"></a>结构</h3><ol><li>Hash索引</li></ol><p>Hash 索引结构的特殊性，其检索效率非常高，索引的检索可以一次定位，不像 B+Tree 索引需要从根节点到枝节点，最后才能访问到页节点这样多次的 IO 访问，所以 Hash 索引的查询效率要远高于 B+Tree 索引。</p><p>虽然 Hash 索引效率高，但是 Hash 索引本身由于其特殊性也带来了 很多限制和弊端，主要有以下这些。</p><p><strong>Hash 索引仅仅能满足 “=”, “IN” 和 “&lt;=&gt;” 查询，不能使用范围查询。</strong></p><blockquote><p>由于 Hash 索引比较的是进行 Hash 运算之后的 Hash 值，所以它只能用于等值的过滤，不能用于基于范围的过滤，因为经过相应的 Hash 算法处理之后的 Hash 值的大小关系，并不能保证和 Hash 运算前完全一样</p></blockquote><p><strong>Hash 索引无法被用来避免数据的排序操作</strong></p><blockquote><p>由于 Hash 索引中存放的是经过 Hash 计算之后的 Hash 值，而且 Hash 值的大小关系并不一定和 Hash 运算前的键值完全一样，所以数据库无法利用索引的数据来避免任何排序运</p></blockquote><p><strong>Hash 索引不能利用部分索引键查询</strong></p><p><strong>哈希索引也没办法利用索引完成排序，以及like ‘xxx%’ 这样的部分模糊查询</strong></p><p><strong>哈希索引也不支持多列联合索引的最左匹配规则</strong></p><blockquote><p>对于组合索引，Hash 索引在计算 Hash 值的时候是组合索引键合并后再一起计算 Hash 值，而不是单独计算 Hash 值，所以通过组合索引的前面一个或几个索引键进行查询的时候，Hash 索引也无法被利用。</p></blockquote><p><strong>Hash 索引在任何时候都不能避免表扫描</strong></p><blockquote><p>Hash 索引是将索引键通过 Hash 运算之后，将 Hash 运算结果的 Hash 值和所对应的行指针信息存放于一个 Hash 表中，由于不同索引键存在相同 Hash 值，所以即使取满足某个 Hash 键值的数据的记录条数，也无法从 Hash 索引中直接完成查询，还是要通过访问表中的实际数据进行相应的比较，并得到相应的结果。</p></blockquote><p><strong>Hash 索引遇到大量 Hash 值相等的情况后性能并不一定就会比 B+Tree 索引高</strong></p><blockquote><p>对于选择性比较低的索引键，如果创建 Hash 索引，那么将会存在大量记录指针信息存于同一个 Hash 值相关联。这样要定位某一条记录时就会非常麻烦，会浪费多次表数据的访问，而造成整体性能低下</p></blockquote><ol start="2"><li><p>B+Tree</p><blockquote><p>B+Tree 索引是 MySQL 数据库中使用最为频繁的索引类型，除了 Archive 存储引擎之外的其他所有的存储引擎都支持 B+Tree 索引。不仅仅在 MySQL 中是如此，实际上在其他的很多数据库管理系统中 B+Tree  索引也同样是作为最主要的索引类型，这主要是因为 B+Tree 索引的存储结构在数据库的数据检索中有非常优异的表现。</p></blockquote></li></ol><h2 id="其他分类"><a href="#其他分类" class="headerlink" title="其他分类"></a>其他分类</h2><ol><li><p><strong>普通索引</strong></p><p>最基本的索引，它没有任何限制，用于加速查询。</p></li><li><p><strong>唯一索引</strong></p><p>索引列的值必须唯一，但允许有空值。如果是组合索引，则列值的组合必须唯一。</p></li><li><p><strong>主键索引</strong></p><p>是一种特殊的唯一索引，一个表只能有一个主键，不允许有空值。一般是在建表的时候同时创建主键索引。</p></li><li><p><strong>组合索引</strong></p><p>指多个字段上创建的索引，只有在查询条件中使用了创建索引时的第一个字段，索引才会被使用。使用组合索引时遵循最左前缀集合。</p></li><li><p><strong>全文索引</strong></p><p>主要用来查找文本中的关键字，而不是直接与索引中的值相比较。</p><p>fulltext 索引跟其它索引大不相同，它更像是一个搜索引擎，而不是简单的 where 语句的参数匹配。</p></li><li><p><strong>覆盖索引</strong><br>覆盖索引（covering index）指一个查询语句的执行只用从索引中就能够取得，不必从数据表中读取。也可以称之为实现了索引覆盖。<br>如果一个索引包含了（或覆盖了）满足查询语句中字段与条件的数据就叫做覆盖索引。</p></li></ol><h2 id="适用场景"><a href="#适用场景" class="headerlink" title="适用场景"></a>适用场景</h2><ol><li>频繁作为查询条件的字段应该创建索引</li><li>主键列</li><li>连接列</li><li>范围搜索列，因为索引进行了排序</li><li>经常与其他表进行连接的表，在连接字段上应该建立索引； </li><li>经常出现在 Where 子句中的字段，特别是大表的字段，应该建立索引； </li><li>索引应该建在选择性高的字段上，例如性别选择性很低，不适合建索引； </li><li>索引应该建在小字段上，对于大的文本字段甚至超长字段，不要建索引； </li><li>复合索引的建立需要进行仔细分析；尽量考虑用单字段索引代替：</li><li>删除无用的索引，避免对执行计划造成负面影响；</li></ol><h2 id="注意"><a href="#注意" class="headerlink" title="注意"></a>注意</h2><ol><li><p>查询很少的列不创建索引</p></li><li><p>全值匹配(查询条件的顺序和索引的顺序一致，这种方式最好，能够充分发挥索引的作用，使用and连接的查询提交也可以不与索引的顺序一致，MySQL 会自动优化</p></li><li><p>列值较少（性别）的列不创建索引，并不是所有索引对查询都有效</p><blockquote><p>SQL 是根据表中数据来进行查询优化的，当索引列有大量数据重复时，SQL 查询可能不会去利用索引，如一表中有字段sex，male、female 几乎各一半，那么即使在 sex 上建了索引也对查询效率起不了作用。</p></blockquote></li><li><p>image，bit 数据类型的列不创建索引</p></li><li><p>修改性能远远大于索引性能的列</p><blockquote><p>索引会提高检索性能但会降低修改性能。索引并不是越多越好，索引固然可以提高相应的 select 的效率，但同时也降低了 insert 及 update 的效率，因为 insert 或 update 时有可能会重建索引，所以怎样建索引需要慎重考虑，视具体情况而定。一个表的索引数最好不要超过6个，若太多则应考虑一些不常使用到的列上建的索引是否有必要。</p></blockquote></li><li><p>只要列中包含有 NULL 值都将不会被包含在索引中，所以我们在数据库设计时不要让字段的默认值为 NULL。</p></li><li><p>对字符串列进行索引，如果可能应该指定一个前缀长度。</p><blockquote><p>例如，如果有一个CHAR(255)的列，如果在前10个或20个字符内，多数值是惟一的，那么就不要对整个列进行索引。短索引不仅可以提高查询速度而且可以节省磁盘空间和I/O操作。</p></blockquote></li><li><p>like “%aaa%” 不会使用索引而 like”aaa%” 可以使用索引, 以 % 开头不会利用到索引，结尾可以。</p></li><li><p>不要在索引列上进行运算</p><blockquote><p> 不要在索引列上做任何操作（计算、函数、(自动or手动)类型转换），会导致索引失效而转向全表扫描，在查询语句中不要使用表达式，会严重影响性能</p><p> 索引列上不要使用表达式，如 where substr(a, 1, 3) = ‘hhh’、where a = a + 1，表达式是一大忌讳，再简单 mysql 也不认。有时数据量不是大到严重影响速度时，一般可以先查出来，比如先查所有订单记录的数据，再在程序中去筛选。</p><p> 我们可以吧 id - 2 = 1改成 id = 1 + 2 的形式</p></blockquote></li><li><p>不使用 NOT IN 和 &lt;&gt; 操作</p></li><li><p>MySQL 在使用不等于(!= 或者&lt;&gt;)的时候无法使用索引，会导致全表扫描。is null 、is not null 也无法使用索引</p></li><li><p><strong>数据类型出现隐式转换的时候不会命中索引，特别是当列类型是字符串，字符串不加单引号索引失效</strong></p></li><li><p>用 or 分割开的条件，如果 or 前的条件中列有索引，而后面的列中没有索引，那么涉及到的索引都不会被用到。</p><p>因为 or 后面的条件列中没有索引，那么后面的查询肯定要走全表扫描，在存在全表扫描的情况下，就没有必要多一次索引扫描增加 IO 访问。</p></li><li><p>符合最左前缀原则</p><blockquote><p>最佳左前缀法则：如果索引了多列，要遵守最左前缀法则，指的是查询从索引的最左前列开始并且不跳过索引中的列，尤其是索引的头。如果查询条件中没有索引的头，会导致索引失效，也就是说使用索引的时候是从头开始，且索引键之间不能断</p></blockquote></li></ol><p>利用覆盖索引进行查询，避免回表。</p><p>被查询的列，数据能从索引中取得，而不用通过行定位符row-locator再到row上获取，即“被查询列要被所建的索引覆盖”，这能够加速查询速度。</p><h1 id="结构-1"><a href="#结构-1" class="headerlink" title="结构"></a>结构</h1><h2 id="概述-1"><a href="#概述-1" class="headerlink" title="概述"></a>概述</h2><p>一般来说， MySQL 中的 B-Tree 索引的物理文件大多都是以 Balance Tree 的结构来存储的，也就是所有实际需要的数据都存放于 Tree 的 Leaf Node ，而且到任何一个 Leaf Node 的最短路径的长度都是完全相同的，所以我们大家都称之为 B-Tree 索引。</p><p>当然，可能各种数据库（或 MySQL 的各种存储引擎）在存放自己的 B-Tree 索引的时候会对存储结构稍作改造。如 Innodb 存储引擎的 B-Tree 索引实际使用的存储结构实际上是 B+Tree ，也就是在 B-Tree 数据结构的基础上做了很小的改造，在每一个 Leaf Node 上面出了存放索引键的相关信息之外，还存储了指向与该 Leaf Node 相邻的后一个 LeafNode 的指针信息，这主要是为了加快检索多个相邻 Leaf Node 的效率考虑。</p><h2 id="Innodb"><a href="#Innodb" class="headerlink" title="Innodb"></a>Innodb</h2><p>在 Innodb 存储引擎中，存在两种不同形式的索引，一种是聚集形式的主键索引（Primary Key ），另外一种则是和其他存储引擎存放形式基本相同的普通 B-Tree 索引，这种索引在 Innodb 存储引擎中被称为 Secondary Index 。</p><p>在主键索引中， 叶子结点存放的是表的实际数据，不仅仅包括主键字段的数据，还包括其他字段的数据据以主键值有序的排列。而 Secondary Index 则和其他普通的 B-Tree 索引没有太大的差异，Leaf Nodes 除了存放索引键 的相关信息外，还存放了 Innodb 的主键值。</p><p>所以，在 Innodb 中如果通过主键来访问数据效率是非常高的，而如果是通过 Secondary Index 来访问数据的话， Innodb 首先通过 Secondary Index 的相关信息，通过相应的索引键检索到 Leaf Node之后，需要再通过 Leaf Node 中存放的主键值再通过主键索引来获取相应的数据行。</p><h2 id="MyISAM"><a href="#MyISAM" class="headerlink" title="MyISAM"></a>MyISAM</h2><p>MyISAM 存储引擎的主键索引和非主键索引差别很小，只不过是主键索引的索引键是一个唯一且非空的键而已。而且 MyISAM 存储引擎的索引和 Innodb 的 Secondary Index 的存储结构也基本相同，主要的区别只是 MyISAM 存储引擎在 Leaf Nodes 上面除了存放索引键信息之外，再存放能直接定位到 MyISAM 数据文件中相应的数据行的信息（如 Row Number ），但并不会存放主键的键值信息。</p><h2 id="B"><a href="#B" class="headerlink" title="B+"></a>B+</h2><p>一般使用磁盘I/O次数评价索引结构的优劣。先从B-Tree分析，根据B-Tree的定义，可知检索一次最多需要访问h个节点。数据库系统的设计者巧妙利用了磁盘预读原理，将一个节点的大小设为等于一个页，这样每个节点只需要一次I/O就可以完全载入。为了达到这个目的，在实际实现B-Tree还需要使用如下技巧：</p><p>B+ 树是应文件系统所需而产生的一种 B 树的变形树, 类似文件的目录一级一级索引，只有最底层的叶子节点[文件]保存数据，非叶子节点只保存索引，不保存实际的数据，数据都保存在叶子节点中,所有的非叶子节点都可以看成索引的一部分。</p><ol><li><p><strong>B+ 树的磁盘读写代价更低</strong></p><blockquote><p>B+ 树的内部节点并没有指向关键字具体信息的指针，因此其内部节点相对 B 树更小，如果把所有同一内部节点的关键字存放在同一盘块中，那么盘块所能容纳的关键字数量也越多，一次性读入内存的需要查找的关键字也就越多，相对IO读写次数就降低了。</p></blockquote></li><li><p><strong>B+树的查询效率更加稳定</strong></p><blockquote><p>由于非终结点并不是最终指向文件内容的结点，而只是叶子结点中关键字的索引。所以任何关键字的查找必须走一条从根结点到叶子结点的路。所有关键字查询的路径长度相同，导致每一个数据的查询效率相当。</p></blockquote></li><li><p><strong>方便</strong></p><blockquote><p>由于B+树的数据都存储在叶子结点中，分支结点均为索引，方便扫库，只需要扫一遍叶子结点即可，但是B树因为其分支结点同样存储着数据，我们要找到具体的数据，需要进行一次中序遍历按序来扫，所以B+ 树更加适合在区间查询的情况，所以通常 B+ 树用于数据库索引。</p></blockquote></li></ol><p>而红黑树这种结构，h 明显要深的多。由于逻辑上很近的节点（父子）物理上可能很远，无法利用局部性，所以红黑树的I/O渐进复杂度也为O(h)，效率明显比B-Tree差很多。</p>]]></content>
      
      
      <categories>
          
          <category> MySQL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数据库 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MySQL 事务</title>
      <link href="2019/06/02/MySQL-%E4%BA%8B%E5%8A%A1/"/>
      <url>2019/06/02/MySQL-%E4%BA%8B%E5%8A%A1/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p>一个最小的不可再分的工作单元。通常一个事务对应一个完整的业务 (例如：银行账户转账业务，该业务就是一个最小的工作单元)。一个完整的业务需要批量的 DML (insert、update、delete) 语句共同联合完成。事务只和 DML 语句有关，或者说 DML 语句才有事务。这个和业务逻辑有关，业务逻辑不同，DML 语句的个数不同。</p><a id="more"></a><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p><strong>一个最小的不可再分的工作单元。通常一个事务对应一个完整的业务</strong></p><p>一个完整的业务需要批量的 DML (insert、update、delete) 语句共同联合完成。事务只和 DML 语句有关，或者说 DML 语句才有事务。这个和业务逻辑有关，业务逻辑不同，DML 语句的个数不同。</p><h2 id="事务并发问题"><a href="#事务并发问题" class="headerlink" title="事务并发问题"></a>事务并发问题</h2><ol><li><h5 id="更新丢失"><a href="#更新丢失" class="headerlink" title="更新丢失"></a>更新丢失</h5><p>当两个事务选择同一行，然后更新数据，由于每个事务都不知道其他事务的存在，就会发生丢失更新的问题。（你我同时读取同一行数据，进行修改，你 commit 之后我也 commit，那么我的结果将会覆盖掉你的结果）</p></li><li><h5 id="脏读"><a href="#脏读" class="headerlink" title="脏读"></a>脏读</h5><p>一个事务读取到了其他事务还没有提交的数据，就叫做脏读。[一个事务正在对一条记录做修改，在这个事务提交之前，别的事务读取到了这个事务修改之后的数据]</p></li><li><h5 id="不可重复读"><a href="#不可重复读" class="headerlink" title="不可重复读"></a>不可重复读</h5><p>一个事务读某条数据读两遍，读到的是不一样的数据，也就是说，一个事务在进行中读取到了其他事务对旧数据的修改结果。（比如说 我开一个事务 修改某条数据 先查后改执行修改动作的时候发现这条数据已经被别的事务删掉了）</p></li><li><h5 id="幻读"><a href="#幻读" class="headerlink" title="幻读"></a>幻读</h5><p>一个事务中，读取到了其他事务新增的数据，仿佛出现了幻象。幻读与不可重复读类似，不可重复读是读到了其他事务 <code>update/delete</code> 的结果，幻读是读到了其他事务 <code>insert</code> 的结果</p><blockquote><p>MySQL 中默认采用的是自动提交 autocommit 模式，在自动提交模式下，如果没有start transaction 显式地开始一个事务，那么每个 SQL 语句都会被当做一个事务执行提交操作。<br>通过如下方式，可以关闭 autocommit；需要注意的是，autocommit 参数是针对连接的，在一个连接中修改了参数，不会对其他连接产生影响。</p></blockquote></li></ol><hr><hr><h1 id="特征-ACID"><a href="#特征-ACID" class="headerlink" title="特征 [ACID]"></a>特征 [ACID]</h1><p>按照严格的标准，只有同时满足 ACID 特性才是事务；但是在各大数据库厂商的实现中，真正满足 ACID 的事务少之又少。例如 MySQL 的 NDB Cluster 事务不满足持久性和隔离性；InnoDB 默认事务隔离级别是可重复读，不满足隔离性；Oracle默认的事务隔离级别为 READ COMMITTED，不满足隔离性，因此与其说 ACID 是事务必须满足的条件，不如说它们是衡量事务的四个维度。</p><h3 id="原子性-A"><a href="#原子性-A" class="headerlink" title="原子性 [A]"></a>原子性 [A]</h3><p><strong>简介</strong></p><p><strong>整个事务中的所有操作，要么全部完成，要么全部不完成，不可能停滞在中间某个环节。</strong><br><strong>事务在执行过程中发生错误，会被回滚 (Rollback) 到事务开始前的状态，就像这个事务从来没有执行过一样。</strong></p><hr><p><strong>实现原理</strong></p><p>实现原子性的关键，是当事务回滚时能够撤销所有已经成功执行的 SQL 语句。</p><p>undo log 回滚日志，是实现原子性的关键，当事务回滚时能够撤销所有已经成功执行的 SQL 语句</p><p>undo log 是逻辑日志。<strong>可以认为当 delete 一条记录时，undo log 中会记录一条对应的 insert 记录，反之亦然，当 update 一条记录时，它记录一条对应相反的 update 记录。</strong></p><p>例如</p><ul><li>当你 delete 一条数据的时候，就需要记录这条数据的信息，回滚的时候，insert 这条旧数据</li><li>当你 update 一条数据的时候，就需要记录之前的旧值，回滚的时候，根据旧值执行 update 操作</li><li>当年 insert 一条数据的时候，就需要这条记录的主键，回滚的时候，根据主键执行 delete 操作</li></ul><p>undo log 记录了这些回滚需要的信息，当事务执行失败或调用了 rollback，导致事务需要回滚，便可以利用undo log 中的信息将数据回滚到修改之前的样子</p><p>当事务提交的时候，innodb 不会立即删除 undo log，因为后续还可能会用到 undo log，如隔离级别为repeatable read 时，事务读取的都是开启事务时的最新提交行版本，只要该事务不结束，该行版本就不能删除，即 undo log 不能删除。</p><p>但是在事务提交的时候，会将该事务对应的 undo log 放入到删除列表中，未来通过 purge 删除。并且提交事务时，还会判断 undo log 分配的页是否可以重用，如果可以重用，则会分配给后面来的事务，避免为每个独立的事务分配独立的 undo log 页而浪费存储空间和性能。</p><p>通过 undo log 记录 delete 和 update 操作的结果发现：(insert操作无需分析，就是插入行而已)</p><ul><li>delete操作实际上不会直接删除，而是将 delete 对象打上delete flag，标记为删除，最终的删除操作是purge线程完成的。</li><li>update分为两种情况：update的列是否是主键列。<ul><li>如果不是主键列，在undo log中直接反向记录是如何update的。即update是直接进行的。</li><li>如果是主键列，update分两部执行：先删除该行，再插入一行目标行。</li></ul></li></ul><hr><h3 id="隔离性-I"><a href="#隔离性-I" class="headerlink" title="隔离性 [I]"></a>隔离性 [I]</h3><p><strong>简介</strong></p><p>隔离状态执行事务，使它们好像是系统在给定时间内执行的唯一操作。如果有两个事务，运行在相同的时间内，执行相同的功能，事务的隔离性将确保每一事务在系统中认为只有该事务在使用系统。这种属性有时称为串行化，为了防止事务操作间的混淆，必须串行化或序列化请求，使得在同一时间仅有一个请求用于同一数据。</p><hr><p><strong>隔离级别</strong></p><p>为了解决多个事务并发会引发的问题，进行并发控制。SQL 标准中定义了四种隔离级别，并规定了每种隔离级别下上述几个问题是否存在。一般来说，隔离级别越低，系统开销越低，可支持的并发越高，但隔离性也越差</p><p><strong>读未提交： read uncommitted</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">set session transaction isolation level READ UNCOMMITTED;</span><br></pre></td></tr></table></figure><p>在一个事务中，可以读取到其他事务未提交的数据变化，叫做脏读现象，在生产环境中切勿使用。这种隔离级别最低，这种级别一般是在理论上存在，数据库隔离级别一般都高于该级别。</p><p><strong>读已提交：read committed</strong></p><ul><li><p>概述</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">set session transaction isolation level READ COMMITTED;</span><br></pre></td></tr></table></figure><p>在一个事务中，可以读取到其他事务已经提交的数据变化，这种读取也就叫做<strong>不可重复读</strong>，因为两次同样的查询可能会得到不一样的结果。</p></li><li><p>实现</p><ul><li><p>版本链 </p><p>对于使用 InnoDB 存储引擎的表来说，它的聚簇索引记录中都包含两个必要的隐藏列（row_id 并不是必要的，我们创建的表中有主键或者非 NULL 唯一键时都不会包含 row_id 列）：</p><ul><li>trx_id：每次对某条记录进行改动时，都会把对应的事务 id 赋值给 trx_id 隐藏列。</li><li>roll_pointer：每次对某条记录进行改动时，这个隐藏列会存一个指针，可以通过这个指针找到该记录修改前的信息 </li></ul></li><li><p>ReadView</p><p>版本链中的哪个版本是当前事务可见的。ReadView 中主要包含4个比较重要的内容：</p></li><li><p>m_ids：表示在生成 ReadView 时当前系统中活跃的读写事务的事务 id 列表。</p><ul><li>min_trx_id：表示在生成 ReadView 时当前系统中活跃的读写事务中最小的事务id，也就是 m_ids 中的最小值。</li></ul></li><li><p>max_trx_id：表示生成 ReadView 时系统中应该分配给下一个事务的 id 值。</p><ul><li>creator_trx_id：表示生成该 ReadView 的事务的事务 id。</li></ul></li></ul></li></ul><p><strong>可重复读：repeatable read</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">set session transaction isolation level REPEATABLE READ;</span><br></pre></td></tr></table></figure><p>MySQL 默认隔离级别，在一个事务中，直到事务结束前，都可以反复读取到事务刚开始时看到的数据，并一直不会发生变化，避免了脏读、不可重复读现象，但是它还是无法解决幻读问题。</p><p><strong>串行化：serializable</strong></p><p>这是最高的隔离级别，它强制事务串行执行，避免了前面说的幻读现象，简单来说，它会在读取的每一行数据上都加锁，所以可能会导致大量的超时和锁争用问题。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p><strong>(1) 隔离级别</strong></p><ol><li>READ UNCOMMITTED 隔离级别下，可能发生**”脏读”、”不可重复读”、”幻读”** 等问题。</li><li>READ COMMITTED 隔离级别下，可能发生 <strong>“不可重复读”</strong> 和 <strong>“幻读问题”**，但是不会发生 **”脏读”</strong> 问题。</li><li>REPEATABLE READ 隔离级别下, 可能发生 <strong>“幻读”</strong> 问题,不会发生 <strong>“脏读”</strong> 和 <strong>“不可重复读”</strong> 问题。</li><li>SERIALIZABLE 隔离级别下，各种问题都不会发生。</li></ol><p><font color='red'><strong>[注意]</strong>   这四种隔离级别是 SQL 的标准定义，不同的数据库会有不同的表现，特别需要注意的是 MySQL 在 REPEATABLE READ 隔离级别下，是可以禁止幻读问题的发生的。</font></p><p><strong>(2) MVCC</strong></p><p><strong>MVCC</strong>，全称 Multi-Version Concurrency Control，即多版本并发控制。MVCC 是一种并发控制的方法，一般在数据库管理系统中，实现对数据库的并发访问，在编程语言中实现事务内存。</p><hr><h3 id="持久性-D"><a href="#持久性-D" class="headerlink" title="持久性 [D]"></a>持久性 [D]</h3><p><strong>简介</strong></p><p>在事务完成以后，该事务所对数据库所作的更改便持久的保存在数据库之中，并不会被回滚。</p><hr><p><strong>实现原理</strong></p><p>MySQL 是先把磁盘上的数据加载到内存中，在内存中对数据进行修改，再刷回磁盘上。如果此时突然宕机，内存中的数据就会丢失</p><p>在执行上面这条 sql 语句时，MySQL 会判断内存中有没有 user_id = 345981 的数据，没有，则去磁盘找到这条数据所在的「页」，把整页数据都加载到内存，然后找到 user_id = 345981 的 row 数据，把内存中这行数据的 age 设置为 18。</p><p>这时，内存的数据是新的、正确的，而磁盘数据是旧的、过时的，所以我们称这时的磁盘对应的页数据，为「脏页」。</p><p>这里补充一个知识点：<strong>MySQL 是按页为单位来读取数据的</strong>，一个页里面有很多行记录，从内存刷数据到磁盘，也是以页为单位来刷。</p><p>决定采用redo log解决上面的问题。当做数据修改的时候，不仅在内存中操作，还会在redo log中记录这次操作。当事务提交的时候，会将 redo log 日志进行刷盘(redo log一部分在内存中，一部分在磁盘上)。当数据库宕机重启的时候，会将 redo log 中的内容恢复到数据库中，再根据 undo log 和 binlog 内容决定回滚数据还是提交数据。</p><p>事务的持久性是由 redo log 和 undo log 一起实现，undo log 保存旧数据，redo log保存新数据，在事务提交时只需将redo log 落盘，持久化到磁盘上就可以。</p><blockquote><p>redo log 和 undo log 都属于 InnoDB 的事务日志。<br><code>InnoDB</code> 作为 MySQL 的存储引擎，数据是存放在磁盘中的，但如果每次读写数据都需要磁盘IO，效率会很低。为此，<code>InnoDB</code> 提供了缓存(Buffer Pool)，Buffer Pool 中包含了磁盘中部分数据页的映射，作为访问数据库的缓冲：当从数据库读取数据时，会首先从 Buffer Pool 中读取，如果 Buffer Pool 中没有，则从磁盘读取后放入 Buffer Pool；当向数据库写入数据时，会首先写入 Buffer Pool，Buffer Pool 中修改的数据会定期刷新到磁盘中（这一过程称为刷脏）。</p></blockquote><p>Buffer Pool 的使用大大提高了读写数据的效率，但是也带了新的问题：如果 MySQL 宕机，而此时 Buffer Pool 中修改的数据还没有刷新到磁盘，就会导致数据的丢失，事务的持久性无法保证。</p><p>redo log 被引入来解决这个问题：当数据修改时，除了修改 Buffer Pool 中的数据，还会在redo log 记录这次操作；当事务提交时，会对 redo log 进行刷盘。如果 MySQL 宕机，重启时可以读取 redo log 中的数据，对数据库进行恢复。redo log 采用的是 WAL（Write-ahead logging，预写式日志），所有修改先写入日志，再更新到 Buffer Pool，保证了数据不会因 MySQL 宕机而丢失，从而满足了持久性要求。</p><p>既然 <code>redo log</code> 也需要在事务提交时将日志写入磁盘，为什么它比直接将 <code>Buffer Pool</code> 中修改的数据写入磁盘(即刷脏)要快呢？主要有以下两方面的原因：</p><ol><li><p>刷脏是随机IO，因为每次修改的数据位置随机，但写 redo log 是追加操作，属于顺序IO。</p></li><li><p>刷脏是以数据页（Page）为单位的，MySQL 默认页大小是16KB，一个 Page 上一个小修改都要整页写入；而 redo log 中只包含真正需要写入的部分，无效IO大大减少。</p></li></ol><hr><h3 id="一致性-C"><a href="#一致性-C" class="headerlink" title="一致性 [C]"></a>一致性 [C]</h3><p><strong>简介</strong></p><p><strong>一致性是指事务执行结束后，数据库的完整性约束没有被破坏，事务执行的前后都是合法的数据状态。数据库的完整性约束包括但不限于：实体完整性（如行的主键存在且唯一）、列完整性（如字段的类型、大小、长度要符合要求）、外键约束、用户自定义完整性（如转账前后，两个账户余额的和应该不变）</strong><br><em>———————————————————————————————————————————————————————-</em></p><p>从数据库层面，数据库通过原子性、隔离性、持久性来保证一致性。也就是说 ACID 四大特性之中，C(一致性)是目的，A(原子性)、I(隔离性)、D(持久性)是手段，是为了保证一致性，数据库提供的手段。数据库必须要实现 AID 三大特性，才有可能实现一致性。例如，原子性无法保证，显然一致性也无法保证。</p><h6 id="补充"><a href="#补充" class="headerlink" title=" [ 补充 ]"></a><font color = 'blue'> [ 补充 ]</font></h6><p>完整性约束：规定了什么样的数据能够存储到数据库系统当中。当写入的数据不满足当前的约束的时候，就不允许写入。防止错误数据的输入和输出造成错误结果和无效操作。<br>为了维护数据的完整性，数据库必须提供定义完整性约束条件的机制</p><ol><li>实体完整性约束 （行的完整性）：<br> 将一个实体区分，其中必须要有一个主键，能够唯一地标识对应的记录，其值不能为空。除了primary key约束之外，还可以通过索引、unique 约束或者 identity 属性等实现数据的实体完整性</li><li>域完整性约束（列完整性约束）<br> 指的是数据输入的有效性。通过数据类型来限制，格式是通过 check 约束和规则来限制可能的取值范围（通过check约束、default定义，not null）等。</li><li>参照完整性约束<br> 约束可以保证一个实体找到另外一个相关联的实体。通过定义外键（外码）与主键（主码）之间或外键与唯一键之间的对应关系实现</li><li>用户自定义完整性约束<br> 如转账前后，两个账户余额的和应该不变</li></ol><h4 id="实现原理"><a href="#实现原理" class="headerlink" title="实现原理"></a>实现原理</h4><p>一致性是事务追求的最终目标，原子性、持久性和隔离性，都是为了保证数据库状态的一致性。此外，除了数据库层面的保障，一致性的实现也需要应用层面进行保障。</p><p>实现一致性的措施包括：</p><ol><li>保证原子性、持久性和隔离性，如果这些特性无法保证，事务的一致性也无法保证</li><li>数据库本身提供保障，例如不允许向整形列插入字符串值、字符串长度不能超过列的限制等</li><li>应用层面进行保障，例如如果转账操作只扣除转账者的余额，而没有增加接收者的余额，无论数据库实现的多么完美，也无法保证状态的一致</li></ol><h2 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a>总结</h2><p>在 RR 的隔离级别下，<code>Innodb</code> 使用 <code>MVVC</code> 和 <code>next-key locks</code> 解决幻读，<code>MVVC</code> 解决的是普通读（快照读）的幻读，<code>next-key locks</code> 解决的是当前读情况下的幻读。</p><p>所谓当前读，指的是加锁的 select, update, delete等语句。在 RR 的事务隔离级别下，数据库会使用next-key locks 来锁住本条记录以及索引区间。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> <span class="keyword">table</span> <span class="keyword">where</span> <span class="keyword">id</span> &gt;<span class="number">3</span> </span><br></pre></td></tr></table></figure><p>锁住的就是 id=3 这条记录以及 id&gt;3 这个区间范围，锁住索引记录之间的范围，避免范围间插入记录，以避免产生幻影行记录。</p>]]></content>
      
      
      <categories>
          
          <category> MySQL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数据库 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>频繁项集挖掘（二）FP-Growth算法</title>
      <link href="2019/03/17/14_FP-Growth%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B0/"/>
      <url>2019/03/17/14_FP-Growth%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B0/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><h2 id="频繁项集挖掘（二）FP-Growth算法"><a href="#频繁项集挖掘（二）FP-Growth算法" class="headerlink" title="频繁项集挖掘（二）FP-Growth算法"></a>频繁项集挖掘（二）FP-Growth算法</h2><p>FP-Growth（Frequent Patterns）相比于Apriori是一种更加有效的频繁项集挖掘算法，FP-Growth算法只需要对数据库进行两次扫描，而Apriori算法对于每次产生的候选项集都会扫描一次数据集来判断是否频繁，因此当数据量特别巨大，且扫描数据库的成本比较高时，FP-Growth的速度要比Apriori快。</p><p>但是FP-Growth只能用于发现频繁项集，不能用于发现关联规则。</p><h4 id="FP-Growth原理分析"><a href="#FP-Growth原理分析" class="headerlink" title="FP-Growth原理分析"></a>FP-Growth原理分析</h4><p>FP-Growth算法实现步骤</p><ul><li>构建FP树</li><li>从FP树中挖掘频繁项集</li></ul><p>FP-Growth算法将数据存储在一种被称为FP树的紧凑数据结构中。</p><p><img src="/img/fp-growth2.png"></p><p>下图就是利用上面的数据构建的一棵FP树（最小支持度为3）：</p><p><img src="/img/fp-growth1.png"></p><ul><li>FP树中最小支持度指项集总共出现的次数</li><li>一个元素项可以在一棵FP树中出现多次</li><li>FP树存储项集的出现频率，且每个项集会以路径的方式存储在树中</li><li>存在相似元素的集合会共享树的一部分</li><li>只有当集合之间完全不同时，树才会分叉</li><li>树节点上给出集合中的单个元素及其在序列中的出现次数，路径会给出该序列的出现次数</li></ul><p>FP-Growth算法工作流程：</p><ul><li>扫描数据集两遍</li><li>第一遍对所有元素项的出现次数进行计数</li><li>根据前面的结论，如果某元素是不频繁的，那么包含该元素的超集也是不频繁的</li><li>第二遍扫描，只考虑那些频繁元素，并且第二遍扫描开始构建FP树</li></ul><h4 id="算法实现"><a href="#算法实现" class="headerlink" title="算法实现"></a>算法实现</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">treeNode</span>(<span class="params">object</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, nameValue, numOccur, parentNode</span>):</span></span><br><span class="line">        <span class="comment"># 节点名称</span></span><br><span class="line">        self.name = nameValue</span><br><span class="line">        <span class="comment"># 节点计数</span></span><br><span class="line">        self.count = numOccur</span><br><span class="line">        <span class="comment"># 记录相似的元素项</span></span><br><span class="line">        self.nodeLink = <span class="literal">None</span></span><br><span class="line">        <span class="comment"># 父节点对象</span></span><br><span class="line">        self.parent = parentNode</span><br><span class="line">        <span class="comment"># 子节点</span></span><br><span class="line">        self.children = &#123;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">inc</span>(<span class="params">self, numOccur</span>):</span></span><br><span class="line">        self.count += numOccur</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">disp</span>(<span class="params">self, ind=<span class="number">1</span></span>):</span></span><br><span class="line">        print(<span class="string">&#x27;--&#x27;</span>*ind, self.name, <span class="string">&#x27; &#x27;</span>, self.count)</span><br><span class="line">        <span class="keyword">for</span> child <span class="keyword">in</span> self.children.values():</span><br><span class="line">            child.disp(ind+<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createTree</span>(<span class="params">dataSet, minSup=<span class="number">1</span></span>):</span>  <span class="comment"># create FP-tree from dataset but don&#x27;t mine</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;遍历数据集两遍&#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="comment"># 第一遍对元素计数</span></span><br><span class="line">    originHeaderTable = &#123;&#125;    <span class="comment"># headerTable用于记录树的结构情况</span></span><br><span class="line">    <span class="keyword">for</span> trans <span class="keyword">in</span> dataSet:</span><br><span class="line">        <span class="keyword">for</span> item <span class="keyword">in</span> trans:</span><br><span class="line">            originHeaderTable[item] = originHeaderTable.get(item, <span class="number">0</span>) + dataSet[trans]</span><br><span class="line"></span><br><span class="line">    popKeys = []</span><br><span class="line">    <span class="comment"># 过滤掉非频繁项集</span></span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> originHeaderTable.keys():</span><br><span class="line">        <span class="comment"># 记录非频繁项</span></span><br><span class="line">        <span class="keyword">if</span> originHeaderTable[k] &lt; minSup:</span><br><span class="line">            popKeys.append(k)</span><br><span class="line"></span><br><span class="line">    freqItemSet = set(originHeaderTable.keys()) - set(popKeys)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># headerTable用于记录树的结构情况</span></span><br><span class="line">    headerTable = &#123;&#125;</span><br><span class="line">    <span class="keyword">if</span> len(freqItemSet) == <span class="number">0</span>:   <span class="comment"># 如果初选没有频繁项集，那么直接退出</span></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 重新构建headerTable</span></span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> freqItemSet:</span><br><span class="line">        headerTable[k] = [originHeaderTable[k], <span class="literal">None</span>]  <span class="comment"># reformat headerTable to use Node link</span></span><br><span class="line">    <span class="keyword">del</span> originHeaderTable</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 构建空树，根节点为空集</span></span><br><span class="line">    root_node = treeNode(<span class="string">&#x27;Null Set&#x27;</span>, <span class="number">1</span>, <span class="literal">None</span>)</span><br><span class="line">    <span class="comment"># 第二遍扫描，开始构建FP树</span></span><br><span class="line">    <span class="keyword">for</span> tranSet, count <span class="keyword">in</span> dataSet.items():  <span class="comment"># go through dataset 2nd time</span></span><br><span class="line">        localD = &#123;&#125;</span><br><span class="line">        <span class="keyword">for</span> item <span class="keyword">in</span> tranSet:  <span class="comment"># put transaction items in order</span></span><br><span class="line">            <span class="keyword">if</span> item <span class="keyword">in</span> freqItemSet:</span><br><span class="line">                localD[item] = headerTable[item][<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">if</span> len(localD) &gt; <span class="number">0</span>:</span><br><span class="line">            orderedItems = [v[<span class="number">0</span>] <span class="keyword">for</span> v <span class="keyword">in</span> sorted(localD.items(), key=<span class="keyword">lambda</span> p: p[<span class="number">1</span>], reverse=<span class="literal">True</span>)]</span><br><span class="line">            updateTree(orderedItems, root_node, headerTable, count)  <span class="comment"># populate tree with ordered freq itemset</span></span><br><span class="line">    <span class="keyword">return</span> root_node, headerTable  <span class="comment"># return tree and header table</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">updateTree</span>(<span class="params">items, parentNode, headerTable, count</span>):</span></span><br><span class="line">    <span class="comment"># 判断第一个项集是已经是当前节点的子节点</span></span><br><span class="line">    <span class="keyword">if</span> items[<span class="number">0</span>] <span class="keyword">in</span> parentNode.children:  <span class="comment"># check if orderedItems[0] in retTree.children</span></span><br><span class="line">        <span class="comment"># 如果是，那么直接count + 1</span></span><br><span class="line">        parentNode.children[items[<span class="number">0</span>]].inc(count)  <span class="comment"># incrament count</span></span><br><span class="line">    <span class="keyword">else</span>:  <span class="comment"># add items[0] to inTree.children</span></span><br><span class="line">        <span class="comment"># 如果不是，那么新建节点，并存储为当前节点的子节点</span></span><br><span class="line">        parentNode.children[items[<span class="number">0</span>]] = treeNode(items[<span class="number">0</span>], count, parentNode)</span><br><span class="line">        <span class="comment"># 更新headerTable</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 判断当前item是否是第一次记录</span></span><br><span class="line">        <span class="keyword">if</span> headerTable[items[<span class="number">0</span>]][<span class="number">1</span>] == <span class="literal">None</span>:</span><br><span class="line">            <span class="comment"># 如果是第一次，那么把新建的节点直接记录到头表中</span></span><br><span class="line">            headerTable[items[<span class="number">0</span>]][<span class="number">1</span>] = parentNode.children[items[<span class="number">0</span>]]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 如果不是第一次，那么说明新节点是当前item的节点的子节点，因此将它记录到当前分支的末位去，即设置为当前分支的叶子节点</span></span><br><span class="line">            updateHeader(headerTable[items[<span class="number">0</span>]][<span class="number">1</span>], parentNode.children[items[<span class="number">0</span>]])</span><br><span class="line">    <span class="comment"># 如果还有第二个元素，那么递归执行以上操作</span></span><br><span class="line">    <span class="keyword">if</span> len(items) &gt; <span class="number">1</span>:</span><br><span class="line">        updateTree(items[<span class="number">1</span>::], parentNode.children[items[<span class="number">0</span>]], headerTable, count)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">updateHeader</span>(<span class="params">lastNode, newLeafNode</span>):</span></span><br><span class="line">    <span class="comment"># 判断上一节点是否有连接节点，如果没有，那么说明上一节点就是叶子节点，那么直接将新节点设为叶子节点</span></span><br><span class="line">    <span class="keyword">while</span> (lastNode.nodeLink != <span class="literal">None</span>):</span><br><span class="line">        <span class="comment"># 如果上一节点已经有连接节点，那么循环知道遍历到叶子节点，再设置新叶子节点</span></span><br><span class="line">        lastNode = lastNode.nodeLink</span><br><span class="line">    <span class="comment"># 将新的叶子节点设置为旧叶子节点的连接节点</span></span><br><span class="line">    lastNode.nodeLink = newLeafNode</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loadTestDataset</span>():</span></span><br><span class="line">    dataset = [[<span class="string">&#x27;r&#x27;</span>, <span class="string">&#x27;z&#x27;</span>, <span class="string">&#x27;h&#x27;</span>, <span class="string">&#x27;j&#x27;</span>, <span class="string">&#x27;p&#x27;</span>],</span><br><span class="line">               [<span class="string">&#x27;z&#x27;</span>, <span class="string">&#x27;y&#x27;</span>, <span class="string">&#x27;x&#x27;</span>, <span class="string">&#x27;w&#x27;</span>, <span class="string">&#x27;v&#x27;</span>, <span class="string">&#x27;u&#x27;</span>, <span class="string">&#x27;t&#x27;</span>, <span class="string">&#x27;s&#x27;</span>],</span><br><span class="line">               [<span class="string">&#x27;z&#x27;</span>],</span><br><span class="line">               [<span class="string">&#x27;r&#x27;</span>, <span class="string">&#x27;x&#x27;</span>, <span class="string">&#x27;n&#x27;</span>, <span class="string">&#x27;o&#x27;</span>, <span class="string">&#x27;s&#x27;</span>],</span><br><span class="line">               [<span class="string">&#x27;y&#x27;</span>, <span class="string">&#x27;r&#x27;</span>, <span class="string">&#x27;x&#x27;</span>, <span class="string">&#x27;z&#x27;</span>, <span class="string">&#x27;q&#x27;</span>, <span class="string">&#x27;t&#x27;</span>, <span class="string">&#x27;p&#x27;</span>],</span><br><span class="line">               [<span class="string">&#x27;y&#x27;</span>, <span class="string">&#x27;z&#x27;</span>, <span class="string">&#x27;x&#x27;</span>, <span class="string">&#x27;e&#x27;</span>, <span class="string">&#x27;q&#x27;</span>, <span class="string">&#x27;s&#x27;</span>, <span class="string">&#x27;t&#x27;</span>, <span class="string">&#x27;m&#x27;</span>]]</span><br><span class="line">    <span class="keyword">return</span> dataset</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createInitDataset</span>(<span class="params">dataSet</span>):</span></span><br><span class="line">    dictDataset = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> trans <span class="keyword">in</span> dataSet:</span><br><span class="line">        dictDataset[frozenset(trans)] = <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> dictDataset</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">buildCombinedItems</span>(<span class="params">leafNode, combinedItems</span>):</span></span><br><span class="line">    <span class="keyword">if</span> leafNode.parent != <span class="literal">None</span>:</span><br><span class="line">        combinedItems.append(leafNode.name)</span><br><span class="line">        buildCombinedItems(leafNode.parent, combinedItems)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">buildCombinedDataset</span>(<span class="params">nodeObject</span>):</span></span><br><span class="line">    <span class="comment"># 根据节点名称，组合出新的项集节点</span></span><br><span class="line">    combinedDataset = &#123;&#125;</span><br><span class="line">    <span class="keyword">while</span> nodeObject != <span class="literal">None</span>:</span><br><span class="line">        combinedItems = []</span><br><span class="line">        buildCombinedItems(nodeObject, combinedItems)</span><br><span class="line">        <span class="keyword">if</span> len(combinedItems) &gt; <span class="number">1</span>:</span><br><span class="line">            combinedDataset[frozenset(combinedItems[<span class="number">1</span>:])] = nodeObject.count</span><br><span class="line">        nodeObject = nodeObject.nodeLink</span><br><span class="line">    <span class="keyword">return</span> combinedDataset</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">scanFPTree</span>(<span class="params">headerTable, minSup, parentNodeNames, freqItemList</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 遍历排序后的headerTable，(节点名称，节点信息）</span></span><br><span class="line">    <span class="keyword">for</span> baseNode, nodeInfo <span class="keyword">in</span> headerTable.items():</span><br><span class="line">        <span class="comment"># 根据prefix</span></span><br><span class="line">        newFreqSet = parentNodeNames.copy()</span><br><span class="line">        newFreqSet.add(baseNode)</span><br><span class="line">        <span class="comment"># 节点计数值</span></span><br><span class="line">        nodeCount = nodeInfo[<span class="number">0</span>]</span><br><span class="line">        <span class="comment"># 节点对象</span></span><br><span class="line">        nodeObject = nodeInfo[<span class="number">1</span>]</span><br><span class="line">        <span class="comment"># 记录下频繁项集以及计数</span></span><br><span class="line">        freqItemList.append((newFreqSet, nodeCount))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 根据当前节点的子节点，构建出新的项集组合</span></span><br><span class="line">        combinedDataset = buildCombinedDataset(nodeObject)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 根据新的项集组合，重合构建子FP树</span></span><br><span class="line">        subFPTree, subFPTreeHeaderTable = createTree(combinedDataset, minSup)</span><br><span class="line">        <span class="comment"># 如果头表不为空，那么递归新树的头表</span></span><br><span class="line">        <span class="keyword">if</span> subFPTreeHeaderTable != <span class="literal">None</span>:</span><br><span class="line">            print(<span class="string">&#x27;conditional tree for: &#x27;</span>, newFreqSet)</span><br><span class="line">            subFPTree.disp(<span class="number">1</span>)</span><br><span class="line">            <span class="comment"># 根据新的头表 扫描FP-Tree</span></span><br><span class="line">            scanFPTree(subFPTreeHeaderTable, minSup, newFreqSet, freqItemList)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line"></span><br><span class="line">    <span class="keyword">from</span> pprint <span class="keyword">import</span> pprint</span><br><span class="line">    simpDat = loadTestDataset()</span><br><span class="line">    initSet = createInitDataset(simpDat)</span><br><span class="line">    <span class="comment"># 构建初始的FP-Tree</span></span><br><span class="line">    initFPtree, initFPtreeHeaderTable = createTree(initSet, <span class="number">3</span>)</span><br><span class="line">    initFPtree.disp(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    freqItems = []    <span class="comment"># 存储频繁项集</span></span><br><span class="line">    <span class="comment"># 扫描FP树，找出所有符合条件的频繁项集</span></span><br><span class="line"></span><br><span class="line">    root_node_names = set([])    <span class="comment"># 从根路径空集开始扫描</span></span><br><span class="line">    scanFPTree(initFPtreeHeaderTable, <span class="number">3</span>, root_node_names, freqItems)</span><br><span class="line">    pprint(freqItems)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 推荐算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 电影推荐系统算法综合实战 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>关键规则挖掘算法（一）Apriori算法</title>
      <link href="2019/03/17/13_Apriori%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B0/"/>
      <url>2019/03/17/13_Apriori%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B0/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><h2 id="关键规则挖掘算法（一）Apriori算法"><a href="#关键规则挖掘算法（一）Apriori算法" class="headerlink" title="关键规则挖掘算法（一）Apriori算法"></a>关键规则挖掘算法（一）Apriori算法</h2><h4 id="Apriori算法原理"><a href="#Apriori算法原理" class="headerlink" title="Apriori算法原理"></a>Apriori算法原理</h4><p>Apriori算法是著名的关联规则挖掘算法。</p><p>假如我们在经营一家商品种类并不多的杂货店，我们对哪些经常在一起被购买的商品非常感兴趣。我们只有四种商品：商品0、商品1、商品2、商品3。那么所有可能被一起购买的商品组合都有哪些？这些商品组合可能著有一种商品，比如商品0，也可能包括两种、三种或所有四种商品。但我们不关心某人买了两件商品0以及四件商品2的情况，只关心他购买了一种或多种商品。</p><p>下图显示了物品之间所有可能的组合：</p><ul><li>图中使用物品的编号0来表示物品0本身。</li><li>图中从上往下的第一个集合是$\phi$，表示空集或不包含任何物品的集合。</li><li>物品集合之间的连线表明两个或者更多集合可以组合形成一个更大的集合。</li></ul><p><img src="/img/apriori1.png"></p><p><strong>目标：</strong>我们的目标是找到经常在一起购买的物品集合。我们使用集合的支持度来度量其出现的频率。</p><blockquote><p>一个集合的支持度是指有多少比例的交易记录包含该集合。</p></blockquote><p><strong>问题：</strong> 如何对一个给定的集合，比如<code>&#123;0，3&#125;</code>，来计算其支持度？</p><ul><li>我们可以遍历毎条记录并检查该记录包含0和3，如果记录确实同时包含这两项，那么就增加总计数值。在扫描完所有数据之后，使用统计得到的总数除以总的交易记录数，就可以得到支持度。</li></ul><p><strong>注意：</strong>上述过程和结果只是针对单个集合{0,3}。要获得每种可能集合的支持度就需要多次重复上述过程。我们可以数一下图中的集合数目，会发现即使对于仅有4种物品的集合，也需要遍历数据15次。而随着物品数目的增加遍历次数会急剧增长。对于包含N种物品的数据集共有$2^{N-1}$种项集组合。而且实际上出售10 000或更多种物品的商店并不少见。即使只出售100种商品的商店也会有$1.26 * 10^{30}$种可能的项集组合。这样的运算量，其实即使是对于现在的很多计算机而言，也需要很长的时间才能完成运算。</p><p><strong>Apriori算法的原理可以帮我们减少可能感兴趣的项集，降低所需的计算时间。</strong></p><p>Apriori算法原理：</p><ul><li><p>如果某个项集是频繁的，那么它的所有子集都是频繁的，例如，假设<code>&#123;1,2&#125;</code>是频繁的，那么<code>&#123;1&#125;</code>和<code>&#123;2&#125;</code>也一定是频繁的。</p></li><li><p>将这个原理取反会发现：如果一个项集是非频繁的，那么它的所有超集也是非频繁的</p><p>如下图中，已知项集<code>&#123;2,3&#125;</code>是非频繁的，那么可立即判断出项集<code>&#123;0,2,3&#125;</code>、<code>&#123;1,2,3&#125;</code>、<code>&#123;0,1,2,3&#125;</code>都是非频繁的，因此这些项集的支持度也就不需要再计算</p><p><img src="/img/apriori2.png"></p></li></ul><p><strong>Apriori算法的一般过程：</strong></p><ol><li>收集数据：使用任意方法。</li><li>准备数据：任何数据类型都可以，因为我们只保存集合。</li><li>分析数据：使用任意方法。</li><li>训练算法：使用Apriori算法来找到频繁项集。</li><li>测试算法：不需要测试过程。</li><li>使用算法：用于发现频繁项集以及物品之间的关联规则。</li></ol><h4 id="Apriori算法实现"><a href="#Apriori算法实现" class="headerlink" title="Apriori算法实现"></a>Apriori算法实现</h4><p><img src="/img/%E6%8C%96%E6%8E%98%E9%A2%91%E7%B9%81%E9%A1%B9%E9%9B%86.png"></p><p>实现数据集扫描方法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loadDataSet</span>():</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    加载数据集</span></span><br><span class="line"><span class="string">    :return: dataset</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="keyword">return</span> [[<span class="number">1</span>, <span class="number">3</span>, <span class="number">4</span>], [<span class="number">2</span>, <span class="number">3</span>, <span class="number">5</span>], [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">5</span>], [<span class="number">2</span>, <span class="number">5</span>]]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createC1</span>(<span class="params">dataSet</span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    创建C1候选项集，C1是所有大小为1的候选项集的列表</span></span><br><span class="line"><span class="string">    :param dataSet:</span></span><br><span class="line"><span class="string">    :return: C1</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="comment"># C1是所有大小为1的候选项集的列表</span></span><br><span class="line">    C1 = []</span><br><span class="line">    <span class="comment"># 遍历数据集，逐个添加到C1中</span></span><br><span class="line">    <span class="keyword">for</span> record <span class="keyword">in</span> dataSet:</span><br><span class="line">        <span class="keyword">for</span> item <span class="keyword">in</span> record:</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> [item] <span class="keyword">in</span> C1:</span><br><span class="line">                C1.append([item])</span><br><span class="line">    C1.sort()</span><br><span class="line">    <span class="comment"># 使用不变集合存储C1内部的每个候选项集，那么就可以将其作为字典的Key，如果是list类型不能直接作为字典的Key</span></span><br><span class="line">    <span class="keyword">return</span> list(map(frozenset, C1))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">scanDataset</span>(<span class="params">dataset, ck, minSupport</span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    扫描数据集，判断频繁项集</span></span><br><span class="line"><span class="string">    :param dataset:</span></span><br><span class="line"><span class="string">    :param ck: ck是所有大小为k的候选项集的列表</span></span><br><span class="line"><span class="string">    :param minSupport: 设置的最小支持度阈值</span></span><br><span class="line"><span class="string">    :return: 符合条件的项集、每个项集的支持度</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="comment"># 存储项集的出现次数</span></span><br><span class="line">    selectedSetCount = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> record <span class="keyword">in</span> dataset:    <span class="comment"># 遍历每一条记录</span></span><br><span class="line">        <span class="keyword">for</span> candidateSet <span class="keyword">in</span> ck:</span><br><span class="line">            <span class="comment"># 判断当前候选项集是不是当前记录的子集</span></span><br><span class="line">            <span class="keyword">if</span> candidateSet.issubset(record):    </span><br><span class="line">                <span class="keyword">if</span> candidateSet <span class="keyword">not</span> <span class="keyword">in</span> selectedSetCount:</span><br><span class="line">                    selectedSetCount[candidateSet] = <span class="number">1</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    selectedSetCount[candidateSet] += <span class="number">1</span></span><br><span class="line">    <span class="comment"># 计算总条目数</span></span><br><span class="line">    numItems = float(len(dataset))</span><br><span class="line">    <span class="comment"># 存储符合条件的项集</span></span><br><span class="line">    retList = []</span><br><span class="line">    <span class="comment"># 存储项集的支持度</span></span><br><span class="line">    supportData = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> selectedSetCount:</span><br><span class="line">        <span class="comment"># 计算支持度</span></span><br><span class="line">        support = selectedSetCount[key] / numItems</span><br><span class="line">        <span class="keyword">if</span> support &gt;= minSupport:</span><br><span class="line">            retList.insert(<span class="number">0</span>, key)</span><br><span class="line">        supportData[key] = support</span><br><span class="line">    <span class="keyword">return</span> retList, supportData</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="keyword">from</span> pprint <span class="keyword">import</span> pprint</span><br><span class="line">    dataset = loadDataSet()</span><br><span class="line">    c1 = createC1(dataset)</span><br><span class="line">    pprint(scanDataset(dataset, c1, <span class="number">0.5</span>))</span><br></pre></td></tr></table></figure><p>实现频繁项集挖掘：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line">......</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createCk</span>(<span class="params">lastFrequentItems, k</span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    根据k-1项的频繁项集列表生成k项的候选项集</span></span><br><span class="line"><span class="string">    :param lastFrequentItems: k-1项的频繁项集</span></span><br><span class="line"><span class="string">    :param k: 第k个项集</span></span><br><span class="line"><span class="string">    :return: ck项集</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    retList = []</span><br><span class="line">    lenLk = len(lastFrequentItems)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(lenLk):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(i+<span class="number">1</span>, lenLk):</span><br><span class="line">            <span class="comment"># 因为新构建的ck项集，特征是任意一个k项集其中k-1项都必须存在于lastCk中</span></span><br><span class="line">            <span class="comment"># 通过以下判断，能筛选出那些符合要求的k-1项</span></span><br><span class="line">            L1 = list(lastFrequentItems[i])[:k<span class="number">-2</span>]; L2 = list(lastFrequentItems[j])[:k<span class="number">-2</span>]</span><br><span class="line">            L1.sort(); L2.sort()</span><br><span class="line">            <span class="keyword">if</span> L1==L2:</span><br><span class="line">                retList.append(lastFrequentItems[i] | lastFrequentItems[j])</span><br><span class="line">    <span class="keyword">return</span> retList</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">apriori</span>(<span class="params">dataSet, minSupport=<span class="number">0.5</span></span>):</span></span><br><span class="line">    C1 = createC1(dataSet)</span><br><span class="line">    k1FrequentItems, supportData = scanDataset(dataSet, C1, minSupport)</span><br><span class="line">    frequentItemsList = [k1FrequentItems]</span><br><span class="line">    <span class="comment"># 应为k=1的频繁项集已经找到，因此从k=2继续</span></span><br><span class="line">    k = <span class="number">2</span></span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        <span class="comment"># 根据k-1的频繁项集，创建k候选集，</span></span><br><span class="line">        <span class="comment"># k-1-1是因为列表下表从0开始</span></span><br><span class="line">        ck = createCk(frequentItemsList[k<span class="number">-1</span><span class="number">-1</span>], k)</span><br><span class="line">        <span class="comment"># 再次扫描数据集，找出新的k项频繁项集</span></span><br><span class="line">        newFrequentItems, supK = scanDataset(dataSet, ck, minSupport)</span><br><span class="line">        <span class="comment"># 更新项集的支持度</span></span><br><span class="line">        supportData.update(supK)</span><br><span class="line">        <span class="comment"># 如果无法生成新的频繁项集，那么推出循环</span></span><br><span class="line">        <span class="keyword">if</span> len(newFrequentItems) == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        <span class="comment"># 存储所有的频繁项集</span></span><br><span class="line">        frequentItemsList.append(newFrequentItems)</span><br><span class="line">        k += <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> frequentItemsList, supportData</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="keyword">from</span> pprint <span class="keyword">import</span> pprint</span><br><span class="line">    dataset = loadDataSet()</span><br><span class="line">    c1 = createC1(dataset)</span><br><span class="line"></span><br><span class="line">    pprint(apriori(dataset, <span class="number">0.3</span>))</span><br></pre></td></tr></table></figure><p>实现关联规则挖掘：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line">......</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generateRules</span>(<span class="params">frequentItemsList, supportData, minConf=<span class="number">0.7</span></span>):</span></span><br><span class="line">    <span class="comment"># 存储关联规则</span></span><br><span class="line">    ruleList = []</span><br><span class="line">    <span class="comment"># 从含有2项item的频繁项集开始遍历，计算两两的置信度</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, len(frequentItemsList)):</span><br><span class="line">        <span class="comment"># 遍历每一阶段的频繁项集</span></span><br><span class="line">        <span class="keyword">for</span> frequentItem <span class="keyword">in</span> frequentItemsList[i]:</span><br><span class="line">            print(frequentItem)</span><br><span class="line">            subItems = [frozenset([item]) <span class="keyword">for</span> item <span class="keyword">in</span> frequentItem]</span><br><span class="line">            print(subItems)</span><br><span class="line">            <span class="keyword">if</span> (i == <span class="number">1</span>):</span><br><span class="line">                <span class="comment"># 先计算2项item的频繁项集的置信度，并将关联规则存储到ruleList</span></span><br><span class="line">                calculateConfidence(frequentItem, subItems, supportData, ruleList, minConf)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="comment"># 然后使用递归依次计算3到k项item频繁项集之间两两的置信度，并提取关联规则</span></span><br><span class="line">                rulesFromRecursive(frequentItem, subItems, supportData, ruleList, minConf)</span><br><span class="line">    <span class="keyword">return</span> ruleList</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calculateConfidence</span>(<span class="params">frequentItem, subItems, supportData, ruleList, minConf=<span class="number">0.7</span></span>):</span></span><br><span class="line">    <span class="comment"># 存储符合最小置信度阈值的item</span></span><br><span class="line">    retList = []</span><br><span class="line">    <span class="keyword">for</span> subItem <span class="keyword">in</span> subItems:</span><br><span class="line">        <span class="comment">#支持度(&#123;豆奶, 莴苣&#125;)/支持度(&#123;豆奶&#125;)</span></span><br><span class="line">        <span class="comment"># 计算置信度[frozenset(&#123;2, 3&#125;), frozenset(&#123;3, 5&#125;), frozenset(&#123;2, 5&#125;), frozenset(&#123;1, 3&#125;)],</span></span><br><span class="line">        conf = supportData[frequentItem]/supportData[frequentItem-subItem]</span><br><span class="line">        <span class="keyword">if</span> conf &gt;= minConf:</span><br><span class="line">            print(<span class="string">&quot;Rule：&quot;</span>, frequentItem-subItem, <span class="string">&#x27;--&gt;&#x27;</span>, subItem, <span class="string">&#x27;confidence:&#x27;</span>, conf)</span><br><span class="line">            ruleList.append((frequentItem-subItem, subItem, conf))</span><br><span class="line">            retList.append(subItem)</span><br><span class="line">    <span class="keyword">return</span> retList</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rulesFromRecursive</span>(<span class="params">frequentItem, subItems, supportData, ruleList, minConf=<span class="number">0.7</span></span>):</span></span><br><span class="line">    m = len(subItems[<span class="number">0</span>])    <span class="comment"># 判断当前子项集的长度</span></span><br><span class="line">    <span class="keyword">if</span> (len(frequentItem) &gt; (m + <span class="number">1</span>)): <span class="comment">#frozenset(&#123;2, 3, 5&#125;)</span></span><br><span class="line">        <span class="comment"># 根据子项集得出CK候选集</span></span><br><span class="line">        ck = createCk(subItems, m+<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 根据候选集再筛选出符合最小置信度的item集合</span></span><br><span class="line">        newItems = calculateConfidence(frequentItem, ck, supportData, ruleList, minConf)</span><br><span class="line">        <span class="comment"># 如果符合要求的item至少有2个，那么继续递归</span></span><br><span class="line">        <span class="keyword">if</span> (len(newItems) &gt; <span class="number">1</span>):</span><br><span class="line">            rulesFromRecursive(frequentItem, newItems, supportData, ruleList, minConf)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="keyword">from</span> pprint <span class="keyword">import</span> pprint</span><br><span class="line">    dataset = loadDataSet()</span><br><span class="line">    c1 = createC1(dataset)</span><br><span class="line">    <span class="comment"># pprint(scanDataset(dataset, c1, 0.5))</span></span><br><span class="line"></span><br><span class="line">    pprint(generateRules(*apriori(dataset, <span class="number">0.3</span>)))</span><br></pre></td></tr></table></figure><p>面向对象封装</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loadDataSet</span>():</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    加载数据集</span></span><br><span class="line"><span class="string">    :return: dataset</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="keyword">return</span> [[<span class="number">1</span>, <span class="number">3</span>, <span class="number">4</span>], [<span class="number">2</span>, <span class="number">3</span>, <span class="number">5</span>], [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">5</span>], [<span class="number">2</span>, <span class="number">5</span>]]</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AssociationRule</span>(<span class="params">object</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, minSupport=<span class="number">0.5</span>, minConf=<span class="number">0.7</span></span>):</span></span><br><span class="line">        self.minSupport = minSupport</span><br><span class="line">        self.minConf = minConf</span><br><span class="line">        self.dataset = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span>(<span class="params">self, dataset</span>):</span></span><br><span class="line">        self.dataset = dataset</span><br><span class="line">        self.frequentItemsList, self.supportData = self.apriori(dataset)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_createC1</span>(<span class="params">self, dataset</span>):</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        创建C1候选项集，C1是所有大小为1的候选项集的列表</span></span><br><span class="line"><span class="string">        :return: C1</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="comment"># C1是所有大小为1的候选项集的列表</span></span><br><span class="line">        C1 = []</span><br><span class="line">        <span class="comment"># 遍历数据集，逐个添加到C1中</span></span><br><span class="line">        <span class="keyword">for</span> record <span class="keyword">in</span> dataset:</span><br><span class="line">            <span class="keyword">for</span> item <span class="keyword">in</span> record:</span><br><span class="line">                <span class="keyword">if</span> <span class="keyword">not</span> [item] <span class="keyword">in</span> C1:</span><br><span class="line">                    C1.append([item])</span><br><span class="line">        C1.sort()</span><br><span class="line">        <span class="comment"># 使用不变集合存储C1内部的每个候选项集，那么就可以将其作为字典的Key，如果是list类型不能直接作为字典的Key</span></span><br><span class="line">        <span class="keyword">return</span> list(map(frozenset, C1))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_scanDataset</span>(<span class="params">self, ck</span>):</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        扫描数据集，判断频繁项集</span></span><br><span class="line"><span class="string">        :param ck: ck是所有大小为k的候选项集的列表</span></span><br><span class="line"><span class="string">        :return: 符合条件的项集、每个项集的支持度</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="comment"># 存储项集的出现次数</span></span><br><span class="line">        selectedSetCount = &#123;&#125;</span><br><span class="line">        <span class="keyword">for</span> record <span class="keyword">in</span> self.dataset:  <span class="comment"># 遍历每一条记录</span></span><br><span class="line">            <span class="keyword">for</span> candidateSet <span class="keyword">in</span> ck:</span><br><span class="line">                <span class="comment"># 判断当前候选项集是不是当前记录的子集</span></span><br><span class="line">                <span class="keyword">if</span> candidateSet.issubset(record):</span><br><span class="line">                    <span class="keyword">if</span> candidateSet <span class="keyword">not</span> <span class="keyword">in</span> selectedSetCount:</span><br><span class="line">                        selectedSetCount[candidateSet] = <span class="number">1</span></span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        selectedSetCount[candidateSet] += <span class="number">1</span></span><br><span class="line">        <span class="comment"># 计算总条目数</span></span><br><span class="line">        numItems = float(len(self.dataset))</span><br><span class="line">        <span class="comment"># 存储符合条件的项集</span></span><br><span class="line">        retList = []</span><br><span class="line">        <span class="comment"># 存储项集的支持度</span></span><br><span class="line">        supportData = &#123;&#125;</span><br><span class="line">        <span class="keyword">for</span> key <span class="keyword">in</span> selectedSetCount:</span><br><span class="line">            <span class="comment"># 计算支持度</span></span><br><span class="line">            support = selectedSetCount[key] / numItems</span><br><span class="line">            <span class="keyword">if</span> support &gt;= self.minSupport:</span><br><span class="line">                retList.insert(<span class="number">0</span>, key)</span><br><span class="line">            supportData[key] = support</span><br><span class="line">        <span class="keyword">return</span> retList, supportData</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_createCk</span>(<span class="params">self, lastFrequentItems, k</span>):</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        根据k-1项的频繁项集列表生成k项的候选项集</span></span><br><span class="line"><span class="string">        :param lastFrequentItems: k-1项的频繁项集</span></span><br><span class="line"><span class="string">        :param k: 第k个项集</span></span><br><span class="line"><span class="string">        :return: ck项集</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        retList = []</span><br><span class="line">        lenLk = len(lastFrequentItems)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(lenLk):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(i + <span class="number">1</span>, lenLk):</span><br><span class="line">                <span class="comment"># 因为新构建的ck项集，特征是任意一个k项集其中k-1项都必须存在于lastCk中</span></span><br><span class="line">                <span class="comment"># 通过以下判断，能筛选出那些符合要求的k-1项</span></span><br><span class="line">                L1 = list(lastFrequentItems[i])[:k - <span class="number">2</span>]</span><br><span class="line">                L2 = list(lastFrequentItems[j])[:k - <span class="number">2</span>]</span><br><span class="line">                L1.sort()</span><br><span class="line">                L2.sort()</span><br><span class="line">                <span class="keyword">if</span> L1 == L2:</span><br><span class="line">                    retList.append(lastFrequentItems[i] | lastFrequentItems[j])</span><br><span class="line">        <span class="keyword">return</span> retList</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">apriori</span>(<span class="params">self, dataset</span>):</span></span><br><span class="line">        C1 = self._createC1(dataset)</span><br><span class="line">        k1FrequentItems, supportData = self._scanDataset(C1)</span><br><span class="line">        frequentItemsList = [k1FrequentItems]</span><br><span class="line">        <span class="comment"># 应为k=1的频繁项集已经找到，因此从k=2继续</span></span><br><span class="line">        k = <span class="number">2</span></span><br><span class="line">        <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">            <span class="comment"># 根据k-1的频繁项集，创建k候选集，</span></span><br><span class="line">            <span class="comment"># k-1-1是因为列表下表从0开始</span></span><br><span class="line">            ck = self._createCk(frequentItemsList[k - <span class="number">1</span> - <span class="number">1</span>], k)</span><br><span class="line">            <span class="comment"># 再次扫描数据集，找出新的k项频繁项集</span></span><br><span class="line">            newFrequentItems, supK = self._scanDataset(ck)</span><br><span class="line">            <span class="comment"># 更新项集的支持度</span></span><br><span class="line">            supportData.update(supK)</span><br><span class="line">            <span class="comment"># 如果无法生成新的频繁项集，那么推出循环</span></span><br><span class="line">            <span class="keyword">if</span> len(newFrequentItems) == <span class="number">0</span>:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            <span class="comment"># 存储所有的频繁项集</span></span><br><span class="line">            frequentItemsList.append(newFrequentItems)</span><br><span class="line">            k += <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> frequentItemsList, supportData</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">generateRules</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="comment"># 存储关联规则</span></span><br><span class="line">        ruleList = []</span><br><span class="line">        <span class="comment"># 从含有2项item的频繁项集开始遍历，计算两两的置信度</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, len(self.frequentItemsList)):</span><br><span class="line">            <span class="comment"># 遍历每一阶段的频繁项集</span></span><br><span class="line">            <span class="keyword">for</span> frequentItem <span class="keyword">in</span> self.frequentItemsList[i]:</span><br><span class="line">                subItems = [frozenset([item]) <span class="keyword">for</span> item <span class="keyword">in</span> frequentItem]</span><br><span class="line">                <span class="keyword">if</span> (i == <span class="number">1</span>):</span><br><span class="line">                    <span class="comment"># 先计算2项item的频繁项集的置信度，并将关联规则存储到ruleList</span></span><br><span class="line">                    self._calculateConfidence(frequentItem, subItems, self.supportData, ruleList)</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    <span class="comment"># 然后使用递归依次计算3到k项item频繁项集之间两两的置信度，并提取关联规则</span></span><br><span class="line">                    self._rulesFromRecursive(frequentItem, subItems, self.supportData, ruleList)</span><br><span class="line">        <span class="keyword">return</span> ruleList</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_calculateConfidence</span>(<span class="params">self, frequentItem, subItems, supportData, ruleList</span>):</span></span><br><span class="line">        <span class="comment"># 存储符合最小置信度阈值的item</span></span><br><span class="line">        retList = []</span><br><span class="line">        <span class="keyword">for</span> subItem <span class="keyword">in</span> subItems:</span><br><span class="line">            <span class="comment"># 计算置信度</span></span><br><span class="line">            conf = supportData[frequentItem] / supportData[frequentItem - subItem]</span><br><span class="line">            <span class="keyword">if</span> conf &gt;= self.minConf:</span><br><span class="line">                print(<span class="string">&quot;Rule：&quot;</span>, frequentItem - subItem, <span class="string">&#x27;--&gt;&#x27;</span>, subItem, <span class="string">&#x27;confidence:&#x27;</span>, conf)</span><br><span class="line">                ruleList.append((frequentItem - subItem, subItem, conf))</span><br><span class="line">                retList.append(subItem)</span><br><span class="line">        <span class="keyword">return</span> retList</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_rulesFromRecursive</span>(<span class="params">self, frequentItem, subItems, supportData, ruleList</span>):</span></span><br><span class="line">        m = len(subItems[<span class="number">0</span>])  <span class="comment"># 判断当前子项集的长度</span></span><br><span class="line">        <span class="keyword">if</span> (len(frequentItem) &gt; (m + <span class="number">1</span>)):</span><br><span class="line">            <span class="comment"># 根据子项集得出CK候选集</span></span><br><span class="line">            ck = self._createCk(subItems, m + <span class="number">1</span>)</span><br><span class="line">            <span class="comment"># 根据候选集再筛选出符合最小置信度的item集合</span></span><br><span class="line">            newItems = self._calculateConfidence(frequentItem, ck, supportData, ruleList)</span><br><span class="line">            <span class="comment"># 如果符合要求的item至少有2个，那么继续递归</span></span><br><span class="line">            <span class="keyword">if</span> (len(newItems) &gt; <span class="number">1</span>):</span><br><span class="line">                self._rulesFromRecursive(frequentItem, newItems, supportData, ruleList)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="keyword">from</span> pprint <span class="keyword">import</span> pprint</span><br><span class="line">    dataset = loadDataSet()</span><br><span class="line">    ar = AssociationRule()</span><br><span class="line">    <span class="comment"># pprint(scanDataset(dataset, c1, 0.5))</span></span><br><span class="line">    ar.fit(dataset)</span><br><span class="line">    pprint(ar.generateRules())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># pprint(ar.generateRules(*ar.apriori(dataset, 0.3)))</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Kafka </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 电影推荐系统算法综合实战 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>推荐算法基础(12)  基于关联规则的推荐</title>
      <link href="2019/03/16/12_%E5%9F%BA%E4%BA%8E%E5%85%B3%E8%81%94%E8%A7%84%E5%88%99%E7%9A%84%E6%8E%A8%E8%8D%90/"/>
      <url>2019/03/16/12_%E5%9F%BA%E4%BA%8E%E5%85%B3%E8%81%94%E8%A7%84%E5%88%99%E7%9A%84%E6%8E%A8%E8%8D%90/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><h2 id="基于关联规则的推荐"><a href="#基于关联规则的推荐" class="headerlink" title="基于关联规则的推荐"></a>基于关联规则的推荐</h2><p>基于关联规则的推荐思想类似基于物品的协同过滤推荐</p><p><strong>“啤酒与尿布”</strong></p><p>关联分析中最有名的例子就是“啤酒与尿布”。</p><p>据报道，在美国沃尔玛超市会发现一个很有趣的现象：货架上啤酒与尿布竟然放在一起售卖，这看似两者毫不相关的东西，为什么会放在一起售卖呢？</p><p>原来，在美国，妇女们经常会嘱咐她们的丈夫下班以后给孩子买一点尿布回来，而丈夫在买完尿布后，大都会顺手买回一瓶自己爱喝的啤酒（由此看出美国人爱喝酒）。商家通过对一年多的原始交易记录进行详细的分析，发现了这对神奇的组合。于是就毫不犹豫地将尿布与啤酒摆放在一起售卖，通过它们的关联性，互相促进销售。“啤酒与尿布”的故事一度是营销界的神话。</p><p>那么问题来了，<strong>商家是如何发现啤酒与尿布两者之间的关联性呢？</strong></p><p>这里我们可以使用数据挖掘中的关联规则挖掘技术，目的就是为了找出两个对象（如X,Y）之间的关联性。一旦找出二者关联性，那么就可以根据它来进行推荐。</p><p><strong>基于关联规则的推荐</strong></p><p>一般我们可以找出用户购买的所有物品数据里频繁出现的项集活序列，来做频繁集挖掘，找到满足支持度阈值的关联物品的频繁N项集或者序列。如果用户购买了频繁N项集或者序列里的部分物品，那么我们可以将频繁项集或序列里的其他物品按一定的评分准则推荐给用户，这个评分准则可以包括支持度，置信度和提升度等。</p><p>常用的关联推荐算法有Apriori，FP-Growth</p><h4 id="关联分析"><a href="#关联分析" class="headerlink" title="关联分析"></a>关联分析</h4><p>关联分析是一种在大规模数据集中寻找有趣关系的任务。 这些关系可以有两种形式:</p><ul><li>频繁项集（frequent item sets）是指经常出现在一块的物品的集合。</li><li>关联规则（associational rules）是暗示两种物品之间可能存在很强的关系。</li></ul><p>从大规模数据集中寻找物品间的隐含关系被称作关联分析(association analysis)或者关联规则学习（association rule learning）</p><h4 id="关联性衡量指标"><a href="#关联性衡量指标" class="headerlink" title="关联性衡量指标"></a>关联性衡量指标</h4><p>假设我们下图所示的一份数据集</p><p><img src="/img/%E5%85%B3%E8%81%94%E8%A7%84%E5%88%99%E6%95%B0%E6%8D%AE%E7%A4%BA%E4%BE%8B.png"></p><p>确定X， Y的关联性，需要用两个指标来衡量：</p><ul><li><p><strong>支持度（support）</strong></p><p>支持度是针对项集而言的</p><p>项集的支持度被定义为数据集中包含该项集的记录所占的比例</p><p>那么项集<code>&#123;豆奶&#125;</code>的支持度就是4/5，那么项集<code>&#123;豆奶, 莴苣&#125;</code>的支持度就是3/5</p></li><li><p><strong>置信度（confidence）</strong></p><p>置信度也成为可信度，是针对一个关联规则而言的，如<code>&#123;豆奶&#125;</code> &gt;&gt;&gt;<code>&#123;莴苣&#125;</code>，表示<code>&#123;豆奶&#125;</code>之于<code>&#123;莴苣&#125;</code>的关联程度（注意：<code>&#123;莴苣&#125;</code> &gt;&gt;&gt;<code>&#123;豆奶&#125;</code>不等价于<code>&#123;豆奶&#125;</code> &gt;&gt;&gt;<code>&#123;莴苣&#125;</code>）</p><p><code>&#123;豆奶&#125;</code> &gt;&gt;&gt;<code>&#123;莴苣&#125;</code>的置信度 = 支持度(<code>&#123;豆奶, 莴苣&#125;</code>)/支持度(<code>&#123;豆奶&#125;</code>)，即3/4</p><p><code>&#123;莴苣&#125;</code> &gt;&gt;&gt;<code>&#123;豆奶&#125;</code>的置信度 = 支持度(<code>&#123;豆奶, 莴苣&#125;</code>)/支持度(<code>&#123;莴苣&#125;</code>)，即3/4</p><p>注意：这里他们俩的置信度相等纯属巧合</p></li></ul><p>如果不考虑关联规则的支持度和置信度，那么在数据库中会存在着无穷多的关联规则。因此我们为了提取出真正的频繁项集和关联规则，必须指定一个最小支持度阈值和最小置信度阈值，因为对于支持度和置信度太低的关联规则基本没有什么使用价值。</p><ul><li><p><strong>最小支持度</strong>：</p><p>它表示了一组物品集在统计意义上需要满足的最低程度</p></li><li><p><strong>最小可信度</strong></p><p>它反映了关联规则的最低可靠程度</p></li></ul><p><strong>同时满足最小可信度阈值和最小支持度阈值的关联规则被称为强关联规则。</strong>比如啤酒与尿布。</p><p>比如这里，如果我们假设最小支持度阈值为50%，最小可信度阈值为70%，那么这里<code>&#123;豆奶&#125;</code> &gt;&gt;&gt;<code>&#123;莴苣&#125;</code>和<code>&#123;莴苣&#125;</code> &gt;&gt;&gt;<code>&#123;豆奶&#125;</code>都属于符合条件的两条关联规则，分别表示：</p><ul><li>同时购买豆奶和莴苣的顾客占全部顾客的60%</li><li><code>&#123;豆奶&#125;</code> &gt;&gt;&gt;<code>&#123;莴苣&#125;</code>：在购买豆奶的用户中，有75%的顾客会购买莴苣</li><li><code>&#123;莴苣&#125;</code> &gt;&gt;&gt;<code>&#123;豆奶&#125;</code>：在购买莴苣的用户中，有75%的顾客会购买豆奶</li></ul>]]></content>
      
      
      <categories>
          
          <category> Kafka </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 电影推荐系统算法综合实战 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>推荐算法基础(11)  基于内容的电影推荐：物品冷启动处理</title>
      <link href="2019/03/15/11_%E7%94%B5%E5%BD%B1%E6%8E%A8%E8%8D%90(ContentBased)_%E7%89%A9%E5%93%81%E5%86%B7%E5%90%AF%E5%8A%A8%E5%A4%84%E7%90%86/"/>
      <url>2019/03/15/11_%E7%94%B5%E5%BD%B1%E6%8E%A8%E8%8D%90(ContentBased)_%E7%89%A9%E5%93%81%E5%86%B7%E5%90%AF%E5%8A%A8%E5%A4%84%E7%90%86/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><h2 id="基于内容的电影推荐：物品冷启动处理"><a href="#基于内容的电影推荐：物品冷启动处理" class="headerlink" title="基于内容的电影推荐：物品冷启动处理"></a>基于内容的电影推荐：物品冷启动处理</h2><p>利用Word2Vec可以计算电影所有标签词之间的关系程度，可用于计算电影之间的相似度</p><h4 id="word2vec原理简介"><a href="#word2vec原理简介" class="headerlink" title="word2vec原理简介"></a>word2vec原理简介</h4><ul><li><p>word2vec是google在2013年开源的一个NLP(Natural Language Processing自然语言处理) 工具，它的特点是将所有的词向量化，这样词与词之间就可以定量的去度量他们之间的关系，挖掘词之间的联系。</p></li><li><p>one-hot vector VS. word vector</p><ul><li>用向量来表示词并不是word2vec的首创</li><li>最早的词向量是很冗长的，它使用是词向量维度大小为整个词汇表的大小，对于每个具体的词汇表中的词，将对应的位置置为1。</li><li>比如下面5个词组成词汇表，词”Queen”的序号为2， 那么它的词向量就是(0,1,0,0,0)同样的道理，词”Woman”的词向量就是(0,0,0,1,0)。</li></ul><p><img src="/img/word2vec1.png"></p></li><li><p>one hot vector的问题</p><ul><li>如果词汇表非常大，如达到万级别，这样每个词都用万维的向量来表示浪费内存。这样的向量除了一个位置是1，其余位置全部为0，表达效率低(稀疏)，需要降低词向量的维度</li><li>难以发现词之间的关系，以及难以捕捉句法（结构）和语义（意思）之间的关系</li><li>Dristributed representation可以解决One hot representation的问题，它的思路是通过训练，将每个词都映射到一个较短的词向量上来。所有的这些词向量就构成了向量空间，进而可以用普通的统计学的方法来研究词与词之间的关系。这个较短的词向量维度一般需要我们在训练时指定。</li><li>比如下图我们将词汇表里的词用”Royalty(王位)”,”Masculinity(男性气质)”, “Femininity(女性气质)”和”Age”4个维度来表示，King这个词对应的词向量可能是(0.99,0.99,0.05,0.7)。当然在实际情况中，我们并不一定能对词向量的每个维度做一个很好的解释。</li></ul><p><img src="/img/word2vec2.png"></p></li><li><p>有了用Dristributed representation表示的较短的词向量，就可以较容易的分析词之间的关系，比如将词的维度降维到2维，用下图的词向量表示我们的词时，发现：$\vec{King} - \vec{Man} + \vec{Woman} = \vec{Queen}​$ </p><p><img src="/img/word2vec3.png"></p></li><li><p>什么是word vector（词向量）</p><ul><li>每个单词被表征为多维的浮点数，每一维的浮点数的数值大小表示了它与另一个单词之间的“距离”，表征的结果就是语义相近的词被映射到相近的集合空间上，好处是这样单词之间就是可以计算的：</li></ul><table>    <th>    <td> animal </td>    <td> pet </td>    </th><tr> <td> dog </td> <td> -0.4 </td> <td> 0.02 </td></tr><tr> <td> lion </td> <td> 0.2 </td> <td> 0.35 </td></tr></table><p>animal那一列表示的就是左边的词与animal这个概念的”距离“</p></li><li><h4 id="Word2Vec"><a href="#Word2Vec" class="headerlink" title="Word2Vec"></a>Word2Vec</h4><h5 id="两个重要模型：CBOW和Skip-Gram"><a href="#两个重要模型：CBOW和Skip-Gram" class="headerlink" title="两个重要模型：CBOW和Skip-Gram"></a>两个重要模型：CBOW和Skip-Gram</h5><ul><li><p>介绍：CBOW把一个词从词窗剔除。在CBOW下给定<em>n</em>词围绕着词<em>w</em>，word2vec预测一个句子中其中一个缺漏的词<em>c</em>，即以概率$p(c|w)$来表示。相反地，Skip-gram给定词窗中的文本，预测当前的词$p(w|c)​$。</p></li><li><p>原理：拥有差不多上下文的两个单词的意思往往是相近的</p></li><li><p><strong>Continuous Bag-of-Words(CBOW)</strong> 连续词袋向量</p><ul><li><p><img src="/img/CBOW.png"></p></li><li><p>功能：通过上下文预测当前词出现的概率</p></li><li><p>原理分析</p><p>假设文本如下：“the florid <u>prose of</u> <strong>the</strong> <u>nineteenth century.</u>”</p><p>想象有个滑动窗口，中间的词是关键词，两边为相等长度的文本来帮助分析。文本的长度为7，就得到了7个one-hot向量，作为神经网络的输入向量，训练目标是：最大化在给定前后文本情况下输出正确关键词的概率，比如给定(“prose”,”of”,”nineteenth”,”century”)的情况下，要最大化输出”the”的概率，用公式表示就是</p><p>P(“the”|(“prose”,”of”,”nineteenth”,”century”))</p></li><li><p>特性</p><ul><li>hidden layer只是将权重求和，传递到下一层，是线性的</li></ul></li></ul></li><li><p><strong>Continuous Skip-gram</strong></p><ul><li><img src="/img/skip-gram.png"></li><li>功能：根据当前词预测上下文</li><li>原理分析<ul><li>和CBOW相反，则我们要求的概率就变为P(Context(w)|w)</li></ul></li></ul></li></ul></li><li><p><strong>总结：</strong>word2vec算法可以计算出每个词语的一个词向量，我们可以用它来表示该词的语义层面的含义</p></li></ul><h4 id="Word2Vec使用"><a href="#Word2Vec使用" class="headerlink" title="Word2Vec使用"></a>Word2Vec使用</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> gensim.models <span class="keyword">import</span> TfidfModel</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_movie_dataset</span>():</span></span><br><span class="line">    <span class="comment"># 加载基于所有电影的标签</span></span><br><span class="line">    <span class="comment"># all-tags.csv来自ml-latest数据集中</span></span><br><span class="line">    <span class="comment"># 由于ml-latest-small中标签数据太多，因此借助其来扩充</span></span><br><span class="line">    _tags = pd.read_csv(<span class="string">&quot;datasets/ml-latest-small/all-tags.csv&quot;</span>, usecols=range(<span class="number">1</span>, <span class="number">3</span>)).dropna()</span><br><span class="line">    tags = _tags.groupby(<span class="string">&quot;movieId&quot;</span>).agg(list)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 加载电影列表数据集</span></span><br><span class="line">    movies = pd.read_csv(<span class="string">&quot;datasets/ml-latest-small/movies.csv&quot;</span>, index_col=<span class="string">&quot;movieId&quot;</span>)</span><br><span class="line">    <span class="comment"># 将类别词分开</span></span><br><span class="line">    movies[<span class="string">&quot;genres&quot;</span>] = movies[<span class="string">&quot;genres&quot;</span>].apply(<span class="keyword">lambda</span> x: x.split(<span class="string">&quot;|&quot;</span>))</span><br><span class="line">    <span class="comment"># 为每部电影匹配对应的标签数据，如果没有将会是NAN</span></span><br><span class="line">    movies_index = set(movies.index) &amp; set(tags.index)</span><br><span class="line">    new_tags = tags.loc[list(movies_index)]</span><br><span class="line">    ret = movies.join(new_tags)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 构建电影数据集，包含电影Id、电影名称、类别、标签四个字段</span></span><br><span class="line">    <span class="comment"># 如果电影没有标签数据，那么就替换为空列表</span></span><br><span class="line">    movie_dataset = pd.DataFrame(</span><br><span class="line">        map(</span><br><span class="line">            <span class="keyword">lambda</span> x: (x[<span class="number">0</span>], x[<span class="number">1</span>], x[<span class="number">2</span>], x[<span class="number">2</span>]+x[<span class="number">3</span>]) <span class="keyword">if</span> x[<span class="number">3</span>] <span class="keyword">is</span> <span class="keyword">not</span> np.nan <span class="keyword">else</span> (x[<span class="number">0</span>], x[<span class="number">1</span>], x[<span class="number">2</span>], []), ret.itertuples())</span><br><span class="line">        , columns=[<span class="string">&quot;movieId&quot;</span>, <span class="string">&quot;title&quot;</span>, <span class="string">&quot;genres&quot;</span>,<span class="string">&quot;tags&quot;</span>]</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    movie_dataset.set_index(<span class="string">&quot;movieId&quot;</span>, inplace=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> movie_dataset</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_movie_profile</span>(<span class="params">movie_dataset</span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    使用tfidf，分析提取topn关键词</span></span><br><span class="line"><span class="string">    :param movie_dataset:</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    dataset = movie_dataset[<span class="string">&quot;tags&quot;</span>].values</span><br><span class="line"></span><br><span class="line">    <span class="keyword">from</span> gensim.corpora <span class="keyword">import</span> Dictionary</span><br><span class="line">    dct = Dictionary(dataset)</span><br><span class="line">    corpus = [dct.doc2bow(line) <span class="keyword">for</span> line <span class="keyword">in</span> dataset]</span><br><span class="line"></span><br><span class="line">    model = TfidfModel(corpus)</span><br><span class="line"></span><br><span class="line">    _movie_profile = []</span><br><span class="line">    <span class="keyword">for</span> i, data <span class="keyword">in</span> enumerate(movie_dataset.itertuples()):</span><br><span class="line">        mid = data[<span class="number">0</span>]</span><br><span class="line">        title = data[<span class="number">1</span>]</span><br><span class="line">        genres = data[<span class="number">2</span>]</span><br><span class="line">        vector = model[corpus[i]]</span><br><span class="line">        movie_tags = sorted(vector, key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>], reverse=<span class="literal">True</span>)[:<span class="number">30</span>]</span><br><span class="line">        topN_tags_weights = dict(map(<span class="keyword">lambda</span> x: (dct[x[<span class="number">0</span>]], x[<span class="number">1</span>]), movie_tags))</span><br><span class="line">        <span class="comment"># 将类别词的添加进去，并设置权重值为1.0</span></span><br><span class="line">        <span class="keyword">for</span> g <span class="keyword">in</span> genres:</span><br><span class="line">            topN_tags_weights[g] = <span class="number">1.0</span></span><br><span class="line">        topN_tags = [i[<span class="number">0</span>] <span class="keyword">for</span> i <span class="keyword">in</span> topN_tags_weights.items()]</span><br><span class="line">        _movie_profile.append((mid, title, topN_tags, topN_tags_weights))</span><br><span class="line"></span><br><span class="line">    movie_profile = pd.DataFrame(_movie_profile, columns=[<span class="string">&quot;movieId&quot;</span>, <span class="string">&quot;title&quot;</span>, <span class="string">&quot;profile&quot;</span>, <span class="string">&quot;weights&quot;</span>])</span><br><span class="line">    movie_profile.set_index(<span class="string">&quot;movieId&quot;</span>, inplace=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> movie_profile</span><br><span class="line"></span><br><span class="line">movie_dataset = get_movie_dataset()</span><br><span class="line">movie_profile = create_movie_profile(movie_dataset)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> gensim, logging</span><br><span class="line"></span><br><span class="line">logging.basicConfig(format=<span class="string">&#x27;%(asctime)s : %(levelname)s : %(message)s&#x27;</span>, level=logging.INFO)</span><br><span class="line"></span><br><span class="line">sentences = list(movie_profile[<span class="string">&quot;profile&quot;</span>].values)</span><br><span class="line"></span><br><span class="line">model = gensim.models.Word2Vec(sentences, window=<span class="number">3</span>, min_count=<span class="number">1</span>, iter=<span class="number">20</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">    words = input(<span class="string">&quot;words: &quot;</span>)  <span class="comment"># action</span></span><br><span class="line">    ret = model.wv.most_similar(positive=[words], topn=<span class="number">10</span>)</span><br><span class="line">    print(ret)</span><br><span class="line">    </span><br></pre></td></tr></table></figure><p>Doc2Vec是建立在Word2Vec上的，用于直接计算以文档为单位的文档向量，这里我们将一部电影的所有标签词，作为整个文档，这样可以计算出每部电影的向量，通过计算向量之间的距离，来判断用于计算电影之间的相似程度。</p><p>这样可以解决物品冷启动问题</p><h4 id="Doc2Vec使用"><a href="#Doc2Vec使用" class="headerlink" title="Doc2Vec使用"></a>Doc2Vec使用</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> gensim.models <span class="keyword">import</span> TfidfModel</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> pprint <span class="keyword">import</span> pprint</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_movie_dataset</span>():</span></span><br><span class="line">    <span class="comment"># 加载基于所有电影的标签</span></span><br><span class="line">    <span class="comment"># all-tags.csv来自ml-latest数据集中</span></span><br><span class="line">    <span class="comment"># 由于ml-latest-small中标签数据太多，因此借助其来扩充</span></span><br><span class="line">    _tags = pd.read_csv(<span class="string">&quot;datasets/ml-latest-small/all-tags.csv&quot;</span>, usecols=range(<span class="number">1</span>, <span class="number">3</span>)).dropna()</span><br><span class="line">    tags = _tags.groupby(<span class="string">&quot;movieId&quot;</span>).agg(list)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 加载电影列表数据集</span></span><br><span class="line">    movies = pd.read_csv(<span class="string">&quot;datasets/ml-latest-small/movies.csv&quot;</span>, index_col=<span class="string">&quot;movieId&quot;</span>)</span><br><span class="line">    <span class="comment"># 将类别词分开</span></span><br><span class="line">    movies[<span class="string">&quot;genres&quot;</span>] = movies[<span class="string">&quot;genres&quot;</span>].apply(<span class="keyword">lambda</span> x: x.split(<span class="string">&quot;|&quot;</span>))</span><br><span class="line">    <span class="comment"># 为每部电影匹配对应的标签数据，如果没有将会是NAN</span></span><br><span class="line">    movies_index = set(movies.index) &amp; set(tags.index)</span><br><span class="line">    new_tags = tags.loc[list(movies_index)]</span><br><span class="line">    ret = movies.join(new_tags)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 构建电影数据集，包含电影Id、电影名称、类别、标签四个字段</span></span><br><span class="line">    <span class="comment"># 如果电影没有标签数据，那么就替换为空列表</span></span><br><span class="line">    movie_dataset = pd.DataFrame(</span><br><span class="line">        map(</span><br><span class="line">            <span class="keyword">lambda</span> x: (x[<span class="number">0</span>], x[<span class="number">1</span>], x[<span class="number">2</span>], x[<span class="number">2</span>]+x[<span class="number">3</span>]) <span class="keyword">if</span> x[<span class="number">3</span>] <span class="keyword">is</span> <span class="keyword">not</span> np.nan <span class="keyword">else</span> (x[<span class="number">0</span>], x[<span class="number">1</span>], x[<span class="number">2</span>], []), ret.itertuples())</span><br><span class="line">        , columns=[<span class="string">&quot;movieId&quot;</span>, <span class="string">&quot;title&quot;</span>, <span class="string">&quot;genres&quot;</span>,<span class="string">&quot;tags&quot;</span>]</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    movie_dataset.set_index(<span class="string">&quot;movieId&quot;</span>, inplace=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> movie_dataset</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_movie_profile</span>(<span class="params">movie_dataset</span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    使用tfidf，分析提取topn关键词</span></span><br><span class="line"><span class="string">    :param movie_dataset:</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    dataset = movie_dataset[<span class="string">&quot;tags&quot;</span>].values</span><br><span class="line"></span><br><span class="line">    <span class="keyword">from</span> gensim.corpora <span class="keyword">import</span> Dictionary</span><br><span class="line">    dct = Dictionary(dataset)</span><br><span class="line">    corpus = [dct.doc2bow(line) <span class="keyword">for</span> line <span class="keyword">in</span> dataset]</span><br><span class="line"></span><br><span class="line">    model = TfidfModel(corpus)</span><br><span class="line"></span><br><span class="line">    _movie_profile = []</span><br><span class="line">    <span class="keyword">for</span> i, data <span class="keyword">in</span> enumerate(movie_dataset.itertuples()):</span><br><span class="line">        mid = data[<span class="number">0</span>]</span><br><span class="line">        title = data[<span class="number">1</span>]</span><br><span class="line">        genres = data[<span class="number">2</span>]</span><br><span class="line">        vector = model[corpus[i]]</span><br><span class="line">        movie_tags = sorted(vector, key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>], reverse=<span class="literal">True</span>)[:<span class="number">30</span>]</span><br><span class="line">        topN_tags_weights = dict(map(<span class="keyword">lambda</span> x: (dct[x[<span class="number">0</span>]], x[<span class="number">1</span>]), movie_tags))</span><br><span class="line">        <span class="comment"># 将类别词的添加进去，并设置权重值为1.0</span></span><br><span class="line">        <span class="keyword">for</span> g <span class="keyword">in</span> genres:</span><br><span class="line">            topN_tags_weights[g] = <span class="number">1.0</span></span><br><span class="line">        topN_tags = [i[<span class="number">0</span>] <span class="keyword">for</span> i <span class="keyword">in</span> topN_tags_weights.items()]</span><br><span class="line">        _movie_profile.append((mid, title, topN_tags, topN_tags_weights))</span><br><span class="line"></span><br><span class="line">    movie_profile = pd.DataFrame(_movie_profile, columns=[<span class="string">&quot;movieId&quot;</span>, <span class="string">&quot;title&quot;</span>, <span class="string">&quot;profile&quot;</span>, <span class="string">&quot;weights&quot;</span>])</span><br><span class="line">    movie_profile.set_index(<span class="string">&quot;movieId&quot;</span>, inplace=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> movie_profile</span><br><span class="line"></span><br><span class="line">movie_dataset = get_movie_dataset()</span><br><span class="line">movie_profile = create_movie_profile(movie_dataset)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> gensim, logging</span><br><span class="line"><span class="keyword">from</span> gensim.models.doc2vec <span class="keyword">import</span> Doc2Vec, TaggedDocument</span><br><span class="line"></span><br><span class="line">logging.basicConfig(format=<span class="string">&#x27;%(asctime)s : %(levelname)s : %(message)s&#x27;</span>, level=logging.INFO)</span><br><span class="line"></span><br><span class="line">documents = [TaggedDocument(words, [movie_id]) <span class="keyword">for</span> movie_id, words <span class="keyword">in</span> movie_profile[<span class="string">&quot;profile&quot;</span>].iteritems()]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练模型并保存</span></span><br><span class="line">model = Doc2Vec(documents, vector_size=<span class="number">100</span>, window=<span class="number">3</span>, min_count=<span class="number">1</span>, workers=<span class="number">4</span>, epochs=<span class="number">20</span>)</span><br><span class="line"><span class="keyword">from</span> gensim.test.utils <span class="keyword">import</span> get_tmpfile</span><br><span class="line">fname = get_tmpfile(<span class="string">&quot;my_doc2vec_model&quot;</span>)</span><br><span class="line">model.save(fname)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">words = movie_profile[<span class="string">&quot;profile&quot;</span>].loc[<span class="number">6</span>]</span><br><span class="line">print(words)</span><br><span class="line">inferred_vector = model.infer_vector(words)</span><br><span class="line">sims = model.docvecs.most_similar([inferred_vector], topn=<span class="number">10</span>)</span><br><span class="line">print(sims)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Kafka </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 电影推荐系统算法综合实战 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>推荐算法基础(9)  基于内容的电影推荐：用户画像</title>
      <link href="2019/03/15/09_%E7%94%B5%E5%BD%B1%E6%8E%A8%E8%8D%90(ContentBased)_%E7%94%A8%E6%88%B7%E7%94%BB%E5%83%8F/"/>
      <url>2019/03/15/09_%E7%94%B5%E5%BD%B1%E6%8E%A8%E8%8D%90(ContentBased)_%E7%94%A8%E6%88%B7%E7%94%BB%E5%83%8F/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><h2 id="基于内容的电影推荐：用户画像"><a href="#基于内容的电影推荐：用户画像" class="headerlink" title="基于内容的电影推荐：用户画像"></a>基于内容的电影推荐：用户画像</h2><p>用户画像构建步骤：</p><ul><li>根据用户的评分历史，结合物品画像，将有观影记录的电影的画像标签作为初始标签反打到用户身上</li><li>通过对用户观影标签的次数进行统计，计算用户的每个初始标签的权重值，排序后选取TOP-N作为用户最终的画像标签</li></ul><h4 id="用户画像建立"><a href="#用户画像建立" class="headerlink" title="用户画像建立"></a>用户画像建立</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> gensim.models <span class="keyword">import</span> TfidfModel</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> functools <span class="keyword">import</span> reduce</span><br><span class="line"><span class="keyword">import</span> collections</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> pprint <span class="keyword">import</span> pprint</span><br><span class="line"></span><br><span class="line"><span class="comment"># ......</span></span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">user profile画像建立：</span></span><br><span class="line"><span class="string">1. 提取用户观看列表</span></span><br><span class="line"><span class="string">2. 根据观看列表和物品画像为用户匹配关键词，并统计词频</span></span><br><span class="line"><span class="string">3. 根据词频排序，最多保留TOP-k个词，这里K设为100，作为用户的标签</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_user_profile</span>():</span></span><br><span class="line">    watch_record = pd.read_csv(<span class="string">&quot;datasets/ml-latest-small/ratings.csv&quot;</span>, usecols=range(<span class="number">2</span>), dtype=&#123;<span class="string">&quot;userId&quot;</span>:np.int32, <span class="string">&quot;movieId&quot;</span>: np.int32&#125;)</span><br><span class="line"></span><br><span class="line">    watch_record = watch_record.groupby(<span class="string">&quot;userId&quot;</span>).agg(list)</span><br><span class="line">    <span class="comment"># print(watch_record)</span></span><br><span class="line"></span><br><span class="line">    movie_dataset = get_movie_dataset()</span><br><span class="line">    movie_profile = create_movie_profile(movie_dataset)</span><br><span class="line"></span><br><span class="line">    user_profile = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> uid, mids <span class="keyword">in</span> watch_record.itertuples():</span><br><span class="line">        record_movie_prifole = movie_profile.loc[list(mids)]</span><br><span class="line">        counter = collections.Counter(reduce(<span class="keyword">lambda</span> x, y: list(x)+list(y), record_movie_prifole[<span class="string">&quot;profile&quot;</span>].values))</span><br><span class="line">        <span class="comment"># 兴趣词</span></span><br><span class="line">        interest_words = counter.most_common(<span class="number">50</span>)</span><br><span class="line">        maxcount = interest_words[<span class="number">0</span>][<span class="number">1</span>]</span><br><span class="line">        interest_words = [(w,round(c/maxcount, <span class="number">4</span>)) <span class="keyword">for</span> w,c <span class="keyword">in</span> interest_words]</span><br><span class="line">        user_profile[uid] = interest_words</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> user_profile</span><br><span class="line"></span><br><span class="line">user_profile = create_user_profile()</span><br><span class="line">pprint(user_profile)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Kafka </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 电影推荐系统算法综合实战 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>推荐算法基础(10) 基于内容的电影推荐：为用户产生TOP-N推荐结果</title>
      <link href="2019/03/15/10_%E7%94%B5%E5%BD%B1%E6%8E%A8%E8%8D%90(ContentBased)_TOP-N%E7%94%A8%E6%88%B7%E6%8E%A8%E8%8D%90/"/>
      <url>2019/03/15/10_%E7%94%B5%E5%BD%B1%E6%8E%A8%E8%8D%90(ContentBased)_TOP-N%E7%94%A8%E6%88%B7%E6%8E%A8%E8%8D%90/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><h2 id="基于内容的电影推荐：为用户产生TOP-N推荐结果"><a href="#基于内容的电影推荐：为用户产生TOP-N推荐结果" class="headerlink" title="基于内容的电影推荐：为用户产生TOP-N推荐结果"></a>基于内容的电影推荐：为用户产生TOP-N推荐结果</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ......</span></span><br><span class="line"></span><br><span class="line">user_profile = create_user_profile()</span><br><span class="line"></span><br><span class="line">watch_record = pd.read_csv(<span class="string">&quot;datasets/ml-latest-small/ratings.csv&quot;</span>, usecols=range(<span class="number">2</span>),dtype=&#123;<span class="string">&quot;userId&quot;</span>: np.int32, <span class="string">&quot;movieId&quot;</span>: np.int32&#125;)</span><br><span class="line"></span><br><span class="line">watch_record = watch_record.groupby(<span class="string">&quot;userId&quot;</span>).agg(list)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> uid, interest_words <span class="keyword">in</span> user_profile.items():</span><br><span class="line">    result_table = &#123;&#125; <span class="comment"># 电影id:[0.2,0.5,0.7]</span></span><br><span class="line">    <span class="keyword">for</span> interest_word, interest_weight <span class="keyword">in</span> interest_words:</span><br><span class="line">        related_movies = inverted_table[interest_word]</span><br><span class="line">        <span class="keyword">for</span> mid, related_weight <span class="keyword">in</span> related_movies:</span><br><span class="line">            _ = result_table.get(mid, [])</span><br><span class="line">            _.append(interest_weight)    <span class="comment"># 只考虑用户的兴趣程度</span></span><br><span class="line">            <span class="comment"># _.append(related_weight)    # 只考虑兴趣词与电影的关联程度</span></span><br><span class="line">            <span class="comment"># _.append(interest_weight*related_weight)    # 二者都考虑</span></span><br><span class="line">            result_table.setdefault(mid, _)</span><br><span class="line"></span><br><span class="line">    rs_result = map(<span class="keyword">lambda</span> x: (x[<span class="number">0</span>], sum(x[<span class="number">1</span>])), result_table.items())</span><br><span class="line">    rs_result = sorted(rs_result, key=<span class="keyword">lambda</span> x:x[<span class="number">1</span>], reverse=<span class="literal">True</span>)[:<span class="number">100</span>]</span><br><span class="line">    print(uid)</span><br><span class="line">    pprint(rs_result)</span><br><span class="line">    <span class="keyword">break</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 历史数据  ==&gt;  历史兴趣程度 ==&gt;  历史推荐结果       离线推荐    离线计算</span></span><br><span class="line">    <span class="comment"># 在线推荐 ===&gt;    娱乐(王思聪)   ===&gt;   我 ==&gt;  王思聪 100%  </span></span><br><span class="line">    <span class="comment"># 近线：最近1天、3天、7天           实时计算</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Kafka </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 电影推荐系统算法综合实战 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>推荐算法基础(1)_ 基于模型的协同过滤推荐</title>
      <link href="2019/03/10/01_%E5%9F%BA%E4%BA%8E%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A4%E6%8E%A8%E8%8D%90/"/>
      <url>2019/03/10/01_%E5%9F%BA%E4%BA%8E%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A4%E6%8E%A8%E8%8D%90/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><h4 id="Model-Based-协同过滤算法"><a href="#Model-Based-协同过滤算法" class="headerlink" title="Model-Based 协同过滤算法"></a>Model-Based 协同过滤算法</h4><p>随着机器学习技术的逐渐发展与完善，推荐系统也逐渐运用机器学习的思想来进行推荐。将机器学习应用到推荐系统中的方案真是不胜枚举。以下对Model-Based CF算法做一个大致的分类：</p><ul><li>基于分类算法、回归算法、聚类算法</li><li>基于矩阵分解的推荐</li><li>基于神经网络算法</li><li>基于图模型算法</li></ul><p>接下来我们重点学习以下几种应用较多的方案：</p><ul><li><strong>基于K最近邻的协同过滤推荐</strong></li><li><strong>基于回归模型的协同过滤推荐</strong></li><li><strong>基于矩阵分解的协同过滤推荐</strong></li></ul>]]></content>
      
      
      <categories>
          
          <category> 推荐算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 电影推荐系统算法综合实战 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SparkSQL 基于 HBase 做⾃定义数据源</title>
      <link href="2019/03/02/SparkSQL-%E5%9F%BA%E4%BA%8E-HBase-%E5%81%9A%E2%BE%83%E5%AE%9A%E4%B9%89%E6%95%B0%E6%8D%AE%E6%BA%90/"/>
      <url>2019/03/02/SparkSQL-%E5%9F%BA%E4%BA%8E-HBase-%E5%81%9A%E2%BE%83%E5%AE%9A%E4%B9%89%E6%95%B0%E6%8D%AE%E6%BA%90/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>MySQL 锁</title>
      <link href="2019/02/22/MySQL-%E9%94%81/"/>
      <url>2019/02/22/MySQL-%E9%94%81/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p>数据库是一个多用户使用的共享资源。当多个用户并发地存取数据时，在数据库中就会产生多个事务同时存取同一数据的情况。若对并发操作不加控制就可能会读取和存储不正确的数据，破坏数据库的一致性。加锁是实现数据库并发控制的一个非常重要的技术。当事务在对某个数据对象进行操作前，先向系统发出请求，对其加锁。加锁后事务就对该数据对象有了一定的控制，在该事务释放锁之前，其他的事务不能对此数据对象进行更新操作。</p><a id="more"></a><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>相对其他数据库而言，MySQL 的锁机制比较简单，其最显著的特点是不同的存储引擎支持不同的锁机制。</p><h2 id="分类"><a href="#分类" class="headerlink" title="分类"></a>分类</h2><p>MySQL 大致可归纳为以下 3 种锁：</p><ol><li><p>表级锁：</p><ul><li>偏向 <code>MyISAM</code> 存储引擎，开销小，加锁快 </li><li>不会出现死锁</li><li>锁定粒度大，发生锁冲突的概率最高，并发度最低。</li></ul></li><li><p>行级锁</p><ul><li><p>开销大，加锁慢</p></li><li><p>会出现死锁 </p></li><li><p>锁定粒度最小，发生锁冲突的概率最低，并发度也最高</p></li></ul></li><li><p>页面锁</p><ul><li><p>开销和加锁时间界于表锁和行锁之间 </p></li><li><p>会出现死锁；锁定粒度界于表锁和行锁之间，并发度一般</p></li></ul></li></ol><h2 id="表锁"><a href="#表锁" class="headerlink" title="表锁"></a>表锁</h2><p>MySQL 的表级锁有两种模式</p><ol><li>表共享读锁 Table Read Lock</li><li>写独占写锁 Table Write Lock</li></ol><h2 id="相关-SQL-语句"><a href="#相关-SQL-语句" class="headerlink" title="相关 SQL 语句"></a>相关 SQL 语句</h2><ol><li><p>手动增加表锁 </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lock table 表名字 read(write)，表名字 read(write)，...;</span><br></pre></td></tr></table></figure></li><li><p>查看表上加过的锁</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">show open tables;</span><br></pre></td></tr></table></figure></li><li><p>释放表锁</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">unlock tables;</span><br></pre></td></tr></table></figure></li></ol><h2 id="实验数据"><a href="#实验数据" class="headerlink" title="实验数据"></a>实验数据</h2><p>创建 Person 数据表，其存储引擎为 MyISAM</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">CREATE TABLE person (</span><br><span class="line">  id int primary key, </span><br><span class="line">  name varchar(30), </span><br><span class="line">  Age int</span><br><span class="line">)ENGINE&#x3D;MyISAM DEFAULT CHARSET&#x3D;utf8;</span><br><span class="line"></span><br><span class="line">mysql&gt; select * from person;</span><br><span class="line">+----+-------+------+</span><br><span class="line">| id | name  | Age  |</span><br><span class="line">+----+-------+------+</span><br><span class="line">|  6 | go    |   11 |</span><br><span class="line">|  5 | hello |   11 |</span><br><span class="line">|  4 | jack  |   11 |</span><br><span class="line">|  3 | chen  |   12 |</span><br><span class="line">|  2 | tom   |   10 |</span><br><span class="line">|  1 | jerry |   16 |</span><br><span class="line">+----+-------+------+</span><br><span class="line">6 rows in set (0.00 sec)</span><br></pre></td></tr></table></figure><h3 id="读锁"><a href="#读锁" class="headerlink" title="读锁"></a>读锁</h3><h4 id="简介-1"><a href="#简介-1" class="headerlink" title="简介"></a>简介</h4><p>对 MyISAM 表加读锁，不会阻塞其他进程对同一表的读请求，但会阻塞对同一表的写请求，只有当读锁释放后，才会执行其他进程的写操作。</p><h4 id="学习示例"><a href="#学习示例" class="headerlink" title="学习示例"></a>学习示例</h4><p>在客户端 [1]中，为 person 表添加读锁</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; lock table person read;</span><br><span class="line">Query OK, 0 rows affected (0.00 sec)</span><br><span class="line">mysql&gt; show open tables;</span><br><span class="line">+--------------------+---------------------------------------------+--------+-------------+</span><br><span class="line">| Database           | Table                                       | In_use | Name_locked |</span><br><span class="line">+--------------------+---------------------------------------------+--------+-------------+</span><br><span class="line">| performance_schema | events_statements_current                   |      0 |           0 |</span><br><span class="line">| performance_schema | events_stages_current                       |      0 |           0 |</span><br><span class="line">| performance_schema | events_waits_summary_by_instance            |      0 |           0 |</span><br><span class="line">| test               | person                                      |      1 |           0 |</span><br></pre></td></tr></table></figure><p>对 MyISAM 表加读锁，不会阻塞其他进程对同一表的读请求</p><p>在客户端 [2] 中，可以正常读取数据表 person</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; select * from person;</span><br><span class="line">+<span class="comment">----+-------+------+</span></span><br><span class="line">| id | name  | Age  |</span><br><span class="line">+<span class="comment">----+-------+------+</span></span><br><span class="line">|  6 | go    |   11 |</span><br><span class="line">|  5 | hello |   11 |</span><br><span class="line">|  4 | jack  |   11 |</span><br><span class="line">|  3 | chen  |   12 |</span><br><span class="line">|  2 | tom   |   10 |</span><br><span class="line">|  1 | jerry |   16 |</span><br><span class="line">+<span class="comment">----+-------+------+</span></span><br><span class="line">6 rows in <span class="keyword">set</span> (<span class="number">0.00</span> sec)</span><br></pre></td></tr></table></figure><p>对 MyISAM 表加读锁，会阻塞其他进程对同一表的写请求</p><p>在客户端 [2] 中，无法正常修改数据表 person,会一直处于阻塞状态。直至客户端 [1] 释放表锁。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; insert into person (id, name, Age) values (7, &#x27;zxc&#x27;, 11);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><blockquote><p><font color='red'><strong>注意:</strong></font> 在客户端 [1] 中，无法正常查看其他数据表数据。直至客户端 [1] 释放表锁。</p></blockquote><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; select * from book;</span><br><span class="line">ERROR 1100 (HY000): Table &#x27;book&#x27; was not locked <span class="keyword">with</span> <span class="keyword">LOCK</span> <span class="keyword">TABLES</span></span><br></pre></td></tr></table></figure><h3 id="写锁"><a href="#写锁" class="headerlink" title="写锁"></a>写锁</h3><h4 id="简介-2"><a href="#简介-2" class="headerlink" title="简介"></a>简介</h4><p>对 MyISAM 表加写锁，会阻塞其他进程对同一表的读和写操作，只有当写锁释放后，才会执行其他进程的写操作。</p><h4 id="学习示例-1"><a href="#学习示例-1" class="headerlink" title="学习示例"></a>学习示例</h4><blockquote><p>在客户端 [1]中，为 person 表添加读锁</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; lock table person write;</span><br><span class="line">Query OK, 0 rows affected (0.00 sec)</span><br><span class="line">mysql&gt; show open tables;</span><br><span class="line">+--------------------+--------------------------------------------+--------+-------------+</span><br><span class="line">| Database           | Table                                      | In_use | Name_locked |</span><br><span class="line">+--------------------+--------------------------------------------+--------+-------------+</span><br><span class="line">| performance_schema | events_statements_current                  |      0 |           0 |</span><br><span class="line">| performance_schema | events_stages_current                      |      0 |           0 |</span><br><span class="line">| performance_schema | events_waits_summary_by_instance           |      0 |           0 |</span><br><span class="line">| test               | person                                     |      1 |           0 |</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>对 MyISAM 表加写锁，会阻塞其他进程对同一表的读请求，直至客户端 [1]释放锁</p><p>在客户端 [2] 中，读取数据表 person 出现阻塞</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; select * from person;</span><br><span class="line">mysql&gt;</span><br><span class="line">mysql&gt;</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>对 MyISAM 表加读锁，会阻塞其他进程对同一表的写请求</p><p>在客户端 [2] 中，无法正常修改数据表 person,会一直处于阻塞状态。直至客户端 [1] 释放表锁。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; insert into person (id, name, Age) values (9, &#x27;chener&#x27;, 11);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="分析表锁定"><a href="#分析表锁定" class="headerlink" title="分析表锁定"></a>分析表锁定</h3><blockquote><p> 可以通过 show status like ‘table%’ 检查 table_locks_waited 和 table_locks_immediate 状态变量来分析系统上的表锁定</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; show status like &#39;table%&#39;;</span><br><span class="line">+----------------------------+-------+</span><br><span class="line">| Variable_name              | Value |</span><br><span class="line">+----------------------------+-------+</span><br><span class="line">| Table_locks_immediate      | 142   |</span><br><span class="line">| Table_locks_waited         | 0     |</span><br><span class="line">| Table_open_cache_hits      | 4     | </span><br><span class="line">| Table_open_cache_misses    | 4     |</span><br><span class="line">| Table_open_cache_overflows | 0     |</span><br><span class="line">+----------------------------+-------+</span><br><span class="line">5 rows in set (0.00 sec)</span><br><span class="line"></span><br></pre></td></tr></table></figure><ol><li><p>Table_locks_immediate</p><blockquote><p>产生表级锁定的次数，表示可以立即获取锁的查询次数，每次立即获取锁加1</p></blockquote></li><li><p>Table_locks_waited</p><blockquote><p>出现表级锁定争用发生等待的次数(不能立即获取锁的次数，每等待一次锁值加1)，此值高则 说明存在着较严重的表级锁争用情况。</p></blockquote></li></ol><p>此外，MyISAM 的读写锁调度是写优先，这也是 MyISAM 不适合做以数据写如为主的表引擎，因为在写锁后，其他线程不能做任何操作，大量的更新操作会使查询很难得到锁，从而造成永远阻塞。</p><h2 id="行锁"><a href="#行锁" class="headerlink" title="行锁"></a>行锁</h2><h4 id="加锁的方式"><a href="#加锁的方式" class="headerlink" title="加锁的方式"></a>加锁的方式</h4><p>自动加锁。对于 UPDATE、DELETE 和 INSERT 语句，InnoDB 会自动给涉及数据集加排他锁；</p><p>对于普通 SELECT 语句，InnoDB 不会加任何锁</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">共享锁：<span class="keyword">select</span> * <span class="keyword">from</span> tableName <span class="keyword">where</span> ... + <span class="keyword">lock</span> <span class="keyword">in</span> <span class="keyword">share</span> more</span><br><span class="line">排他锁：<span class="keyword">select</span> * <span class="keyword">from</span> tableName <span class="keyword">where</span> ... + <span class="keyword">for</span> <span class="keyword">update</span> </span><br></pre></td></tr></table></figure><p>In share mode 获得共享锁，主要用于确认某行数据是否存在，并确保没有人对这个记录进行更新或者删除操作，但是如果当前事务也需要对该记录进行更新操作，就有可能造成死锁；因此对于锁定行记录之后还需要进行更新操作，应该是用 for update 获得排它锁</p><p>行锁开销大，加锁慢；会出现死锁；锁定粒度最小，发生锁冲突的概率最低，并发度也最高。最大程度的支持并发，同时也带来了最大的锁开销。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; set autocommit &#x3D; 0;</span><br><span class="line">Query OK, 0 rows affected (0.00 sec)</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; update staffs set age&#x3D;&#39;16&#39; where id&#x3D;&#39;2&#39;;</span><br><span class="line">Query OK, 1 row affected (0.12 sec)</span><br><span class="line">Rows matched: 1  Changed: 1  Warnings: 0</span><br><span class="line"></span><br><span class="line">mysql&gt; select * from staffs;</span><br><span class="line">+----+------+-----+---------+---------------------+</span><br><span class="line">| id | name | age | pos     | add_time            |</span><br><span class="line">+----+------+-----+---------+---------------------+</span><br><span class="line">|  1 | Z3   |  22 | manager | 2019-11-24 05:36:40 |</span><br><span class="line">|  2 | July |  16 | dev     | 2019-11-24 05:36:40 |</span><br><span class="line">|  3 | 2000 |  22 | dev     | 2019-11-24 05:36:40 |</span><br><span class="line">+----+------+-----+---------+---------------------+</span><br><span class="line">3 rows in set (0.00 sec)</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="行锁演示案例"><a href="#行锁演示案例" class="headerlink" title="行锁演示案例"></a>行锁演示案例</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; update staffs <span class="built_in">set</span> age=<span class="string">&#x27;3&#x27;</span> <span class="built_in">where</span> id=<span class="string">&#x27;1&#x27;</span>;</span><br><span class="line">Query OK, 1 row affected (0.01 sec)</span><br><span class="line">Rows matched: 1  Changed: 1  Warnings: 0</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 阻塞</span></span><br><span class="line">mysql&gt; update staffs <span class="built_in">set</span> age=<span class="string">&#x27;10&#x27;</span> <span class="built_in">where</span> id=<span class="string">&#x27;1&#x27;</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; commit;</span><br><span class="line">Query OK, 0 rows affected (0.13 sec)</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; update staffs <span class="built_in">set</span> age=<span class="string">&#x27;10&#x27;</span> <span class="built_in">where</span> id=<span class="string">&#x27;1&#x27;</span>;</span><br><span class="line">Query OK, 1 row affected (43.86 sec)</span><br><span class="line">Rows matched: 1  Changed: 1  Warnings: 0</span><br><span class="line"></span><br><span class="line">mysql&gt; commit ;</span><br><span class="line">Query OK, 0 rows affected (0.05 sec)</span><br><span class="line"></span><br></pre></td></tr></table></figure><blockquote><p>不同行不阻塞</p></blockquote><p>1 InnoDB 支持表锁和行锁，使用索引作为检索条件修改数据时采用行锁，否则采用表锁。</p><p>2 InnoDB 自动给修改操作加锁，给查询操作不自动加锁</p><p>3 行锁可能因为未使用索引而升级为表锁，所以除了检查索引是否创建的同时，也需要通过 explain 执行计划查询索引是否被实际使用。</p><p>4 行锁相对于表锁来说，优势在于高并发场景下表现更突出，毕竟锁的粒度小。</p><p>5 当表的大部分数据需要被修改，或者是多表复杂关联查询时，建议使用表锁优于行锁。</p><p>6 为了保证数据的一致完整性，任何一个数据库都存在锁定机制。锁定机制的优劣直接影响到一个数据库的并发处理能力和性能。</p><h3 id="2-3-行锁升级为表锁"><a href="#2-3-行锁升级为表锁" class="headerlink" title="2.3. 行锁升级为表锁"></a>2.3. 行锁升级为表锁</h3><p>varchar必须用 ‘’</p><h3 id="2-5-锁定一行"><a href="#2-5-锁定一行" class="headerlink" title="2.5. 锁定一行"></a>2.5. 锁定一行</h3><p><img src="https://img-blog.csdnimg.cn/20200111103106459.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTA4MDk4NA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200111103446655.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTA4MDk4NA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200111131714835.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTA4MDk4NA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/2020011113173370.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTA4MDk4NA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200111131817318.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTA4MDk4NA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h2 id="间隙锁"><a href="#间隙锁" class="headerlink" title="间隙锁"></a>间隙锁</h2><h3 id="简介-3"><a href="#简介-3" class="headerlink" title="简介"></a>简介</h3><p>编程的思想源于生活，生活中的例子能帮助我们更好的理解一些编程中的思想。<br>生活中排队的场景，小明，小红，小花三个人依次站成一排，此时，如何让新来的小刚不能站在小红旁边，这时候只要将小红和她前面的小明之间的空隙封锁，将小红和她后面的小花之间的空隙封锁，那么小刚就不能站到小红的旁边。这里的小红，小明，小花，小刚就是数据库的一条条记录。<br>他们之间的空隙也就是间隙，而封锁他们之间距离的锁，叫做间隙锁。</p><p><strong>间隙锁（Gap Lock）是 Innodb 在可重复读提交下为了解决幻读问题时引入的锁机制，幻读的问题存在是因为新增操作，这时如果进行范围查询的时候（加锁查询），会出现不一致的问题，这时使用不同的行锁已经没有办法满足要求，需要对一定范围内的数据进行加锁，间隙锁就是解决这类问题的。</strong></p><p><strong>在可重复读隔离级别下，数据库是通过行锁和间隙锁共同组成的[next-key lock]，来实现的</strong></p><p><strong>当我们使用范围条件而不是相等条件检索数据，并请求共享或排他锁时，InnoDB 会给符合条件的已有数据记录的索引项加锁，对于键值在条件范围内但并不存在的记录，叫做  间隙 GAP</strong></p><p>InnoDB 也会对这个 <strong>间隙 GAP</strong> 加锁，这种锁机制就是所谓的  <strong>间隙锁 Next-Key 锁</strong></p><h2 id="加锁规则"><a href="#加锁规则" class="headerlink" title="加锁规则"></a>加锁规则</h2><ol><li>加锁的基本单位是（next-key lock）,他是前开后闭原则</li><li>插叙过程中访问的对象会增加锁</li><li>索引上的等值查询–给唯一索引加锁的时候，next-key lock升级为行锁</li><li>索引上的等值查询–向右遍历时最后一个值不满足查询需求时，next-key lock 退化为间隙锁</li><li>唯一索引上的范围查询会访问到不满足条件的第一个值为止</li></ol><h2 id="间隙锁的目的是为了防止幻读，主要通过两个方面实现"><a href="#间隙锁的目的是为了防止幻读，主要通过两个方面实现" class="headerlink" title="间隙锁的目的是为了防止幻读，主要通过两个方面实现"></a><strong>间隙锁的目的是为了防止幻读，主要通过两个方面实现</strong></h2><ol><li>防止间隙内有新数据被插入</li><li>防止已存在的数据，更新成间隙内的数据（例如防止numer=3的记录通过update变成number=5）</li></ol><h2 id="innodb-使用间隙锁的条件"><a href="#innodb-使用间隙锁的条件" class="headerlink" title="innodb 使用间隙锁的条件"></a>innodb 使用间隙锁的条件</h2><ol><li>必须在 REPEATABLE READ 级别下</li><li>检索条件必须有索引，如果没有索引，MySQL 会全表扫描，那样会锁定整张表所有的记录，包括不存在的记录，此时其他事务不能修改不能删除不能添加。</li></ol><h3 id="危害"><a href="#危害" class="headerlink" title="危害"></a>危害</h3><p>执行查询过程中通过范围查找会锁定整个范围内的所有索引键值，即使这个键值并不存在，造成锁定的时候无法插入键值范围内的任何数据，在某些场景下可能对性能造成很大的危害。</p><h2 id="5-2PL"><a href="#5-2PL" class="headerlink" title="5. 2PL"></a>5. <code>2PL</code></h2><p>2PL,两阶段加锁协议:主要用于单机事务中的一致性与隔离性。</p><p>2PC,两阶段提交协议:主要用于分布式事务。</p><h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><p>并行操作对并行事务的操作的调度是随机的，不同的调度可能产生不同的结果。在这些不同的调度中，肯定有些调度的结果是正确的，究竟哪些调度是正确的呢？<br>若每个事务的基本操作都串连在一起，没有其它事务的操作与之交叉执行，这样的调度称为串行调度，多个事务的的串行调度，其执行结果一定是正确的。但串行调度限制了系统并行性的发挥，而很多并行调度又不具有串行调度的结果，所以我们必须研究具有串行调度效果的并行调度方法。</p><p><strong>当且仅当某组并发事务的交叉调度产生的结果和这些事务的某一串行调度的结果相同，则称这个交叉调度是可串行化。</strong><br>可串行化是并行事务正确性的准则，一个交叉调度，当且仅当它是可串行化的，它才是正确的。两段锁协议是保证并行事务可串行化的方法。</p><hr><p><strong>定理：</strong>若所有事务均遵守两段锁协议，则这些事务的所有交叉调度都是可串行化的。</p><hr><p>两阶段锁协议是指所有事务必须分两个阶段对数据加锁和解锁，在对任何数据进行读、写操作之前，事务首先要获得对该数据的封锁；在释放一个封锁之后，事务不再申请和获得任何其他封锁。</p><p>两段锁协议规定所有的事务应遵守的规则：</p><ol><li>在对任何数据进行读、写操作之前，首先要申请并获得对该数据的封锁。</li><li>在释放一个封锁之后，事务不再申请和获得其它任何封锁。</li></ol><p>即事务的执行分为两个阶段：</p><ol><li>第一阶段是获得封锁的阶段，称为扩展阶段。</li><li>第二阶段是释放封锁的阶段，称为收缩阶段。</li></ol><h3 id="加锁阶段"><a href="#加锁阶段" class="headerlink" title="加锁阶段"></a>加锁阶段</h3><p>在对记录更新操作或者(select for update、lock in share model)时，会对记录加锁(有共享锁、排它锁、意向锁、gap锁、nextkey锁等等)</p><p>当对记录进行更新操作或者 <code>select for update (X锁)、</code> <code>lock in share mode(S锁) </code>时，会对记录进行加锁。</p><p>加锁阶段：在该阶段可以进行加锁操作。在对任何数据进行读操作之前要申请并获得 <code>S</code> 锁[共享锁，其它事务可以继续加共享锁，但不能加排它锁]，在进行写操作之前要申请并获得 X 锁[排它锁，其它事务不能再获得任何锁]。加锁不成功，则事务进入等待状态，直到加锁成功才继续执行。</p><h3 id="解锁"><a href="#解锁" class="headerlink" title="解锁"></a>解锁</h3><p>在一个事务中，只有在 <code>commit</code> 或者 <code>rollback</code> 时，才是解锁阶段。</p><h2 id="6-2PC"><a href="#6-2PC" class="headerlink" title="6. 2PC"></a>6. <code>2PC</code></h2><p>为了性能考虑，每次提交事务的时候，只需要将 redo 和 undo 落盘就代表事务已经持久化了，而不需要等待数据落盘。这样就已经能保证事务的 crash 时的前滚或者回滚。</p><p>由于 undo 的信息也会写入 redo，所以其实我们只需要根据 redo 是否落盘而决定crash recovrey的时候是重做还是回滚。而上面提到，开启binlog后，还需要考虑binlog是否落盘（binlog牵扯到主从数据一致性，全备恢复的位点）。根据事务是否成功写binlog决定事务的重做还是回滚。</p><p>2PC 即innodb对于事务的两阶段提交机制。当mysql开启binlog的时候，会存在一个内部XA的问题：事务在存储引擎层（redo）commit的顺序和在binlog中提交的顺序不一致的问题。</p><h4 id="2PC-原理"><a href="#2PC-原理" class="headerlink" title="2PC 原理"></a>2PC 原理</h4><p>将事务的 commit 分为 prepare 和 commit 两个阶段</p><ol><li><p>prepare 阶段</p><p>redo 持久化到磁盘（redo group commit），并将回滚段置为 prepared 状态，此时 binlog 不做操作。</p></li><li><p>commit 阶段</p><p>innodb 释放锁，释放回滚段，设置提交状态，binlog 持久化到磁盘，然后存储引擎层提交</p></li></ol><h3 id="Group-Commit"><a href="#Group-Commit" class="headerlink" title="Group Commit"></a>Group Commit</h3><p>日志的写入基本上是顺序IO。WAL 用顺序的日志写入代替数据的随机 IO 实现事务持久化。但是尽管如此，每次事务提交都需要日志刷盘，仍然受限于磁盘IO。group commit 的出现就是为了将日志（redo/binlog）刷盘的动作合并，从而提升 IO 性能</p><h2 id="死锁"><a href="#死锁" class="headerlink" title="死锁"></a>死锁</h2><p>两阶段锁协议是指所有事务必须分两个阶段对数据加锁和解锁，在对任何数据进行读、写操作之前，事务首先要获得对该数据的封锁；在释放一个封锁之后，事务不再申请和获得任何其他封锁。</p><p>对应到 MySQL 上分为两个阶段：</p><ol><li>扩展阶段（事务开始后，commit 之前）：获取锁</li><li>收缩阶段（commit 之后）：释放锁</li></ol><p>就是说呢，只有遵循两段锁协议，才能实现 可串行化调度。</p><p>但是两阶段锁协议不要求事务必须一次将所有需要使用的数据加锁，并且在加锁阶段没有顺序要求，所以这种并发控制方式会形成死锁。</p><h3 id="MySQL-如何处理死锁"><a href="#MySQL-如何处理死锁" class="headerlink" title="MySQL 如何处理死锁 ?"></a><strong>MySQL 如何处理死锁</strong> ?</h3><p>MySQL 有两种死锁处理方式：</p><ol><li><p>等待，直到超时</p><p><strong><code>innodb_lock_wait_timeout=50s</code></strong></p><blockquote><p>直观方法是在两个事务相互等待时，当一个等待时间超过设置的某一阀值时，对其中一个事务进行回滚，另一个事务就能继续执行。这种方法简单有效，在 innodb 中，参数 innodb_lock_wait_timeout 用来设置超时时间。</p></blockquote></li><li><p>发起死锁检测，主动回滚一条事务，让其他事务继续执行</p><p><strong><code>innodb_deadlock_detect=on</code></strong></p><blockquote><p>innodb 还提供了 wait-for graph 算法来主动进行死锁检测，每当加锁请求无法立即满足需要并进入等待时，wait-for graph 算法都会被触发</p></blockquote><blockquote><p>innodb 将各个事务看为一个个节点，资源就是各个事务占用的锁，当事务1需要等待事务2的锁时，就生成一条有向边从1指向2，最后行成一个有向图。</p></blockquote></li></ol><p>由于性能原因，一般都是使用死锁检测来进行处理死锁。</p><h3 id="死锁成因"><a href="#死锁成因" class="headerlink" title="死锁成因"></a>死锁成因</h3><h5 id="不同表相同记录行锁冲突"><a href="#不同表相同记录行锁冲突" class="headerlink" title="不同表相同记录行锁冲突"></a><strong>不同表相同记录行锁冲突</strong></h5><p>这种情况很好理解，事务A和事务B操作两张表，但出现循环等待锁情况。</p><h5 id="同表记录行锁冲突"><a href="#同表记录行锁冲突" class="headerlink" title="同表记录行锁冲突"></a>同表记录行锁冲突</h5><h3 id="如何避免发生死锁"><a href="#如何避免发生死锁" class="headerlink" title="如何避免发生死锁 ?"></a><strong>如何避免发生死锁 ?</strong></h3><p><strong>收集死锁信息：</strong></p><ol><li>利用命令 <strong><code>SHOW ENGINE INNODB STATUS</code></strong> 查看死锁原因。</li><li>调试阶段开启 **<code>innodb_print_all_deadlocks</code>**，收集所有死锁日志。</li></ol><p><strong>减少死锁：</strong></p><ol><li>使用事务，不使用 lock tables 。</li><li>保证没有长事务。</li><li>操作完之后立即提交事务，特别是在交互式命令行中。</li><li>如果在用 (SELECT … FOR UPDATE or SELECT … LOCK IN SHARE MODE)，尝试降低隔离级别。</li><li>修改多个表或者多个行的时候，将修改的顺序保持一致。</li><li>创建索引，可以使创建的锁更少。</li><li>最好不要用 (SELECT … FOR UPDATE or SELECT … LOCK IN SHARE MODE)。</li><li>如果上述都无法解决问题，那么尝试使用 lock tables t1, t2, t3 锁多张表</li></ol><h1 id="意向锁"><a href="#意向锁" class="headerlink" title="意向锁"></a>意向锁</h1><h2 id="概述-1"><a href="#概述-1" class="headerlink" title="概述"></a>概述</h2><p>innodb 的意向锁主要用户多粒度的锁并存的情况。比如事务 A 要在一个表上加 S 锁，如果表中的一行已被事务B 加了 X 锁，那么该锁的申请也应被阻塞。如果表中的数据很多，逐行检查锁标志的开销将很大，系统的性能将会受到影响。为了解决这个问题，可以在表级上引入新的锁类型来表示其所属行的加锁情况，这就引出了 <strong>“意向锁”</strong> 的概念。</p><p>如果表中记录1亿，事务 A 把其中有几条记录上了行锁了，这时事务 B 需要给这个表加表级锁，如果没有意向锁的话，那就要去表中查找这一亿条记录是否上锁了。如果存在意向锁，那么假如事务Ａ在更新一条记录之前，先加意向锁，再加Ｘ锁，事务 B 先检查该表上是否存在意向锁，存在的意向锁是否与自己准备加的锁冲突，如果有冲突，则等待直到事务Ａ释放，而无须逐条记录去检测。事务Ｂ更新表时，其实无须知道到底哪一行被锁了，它只要知道反正有一行被锁了就行了。</p><h2 id="6-2-作用"><a href="#6-2-作用" class="headerlink" title="6.2. 作用"></a>6.2. 作用</h2><p>意向锁的主要作用是处理行锁和表锁之间的矛盾，能够显示“某个事务正在某一行上持有了锁，或者准备去持有锁”</p>]]></content>
      
      
      <categories>
          
          <category> MySQL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数据库 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>DataNode启动流程</title>
      <link href="2019/02/22/DataNode%E5%90%AF%E5%8A%A8%E6%B5%81%E7%A8%8B/"/>
      <url>2019/02/22/DataNode%E5%90%AF%E5%8A%A8%E6%B5%81%E7%A8%8B/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script>]]></content>
      
      
      <categories>
          
          <category> Hadoop 源码阅读 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>推荐算法基础(2) 基于K最近邻的协同过滤推荐</title>
      <link href="2019/02/15/02_%E5%9F%BA%E4%BA%8EK%E6%9C%80%E8%BF%91%E9%82%BB%E7%9A%84%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A4%E6%8E%A8%E8%8D%90/"/>
      <url>2019/02/15/02_%E5%9F%BA%E4%BA%8EK%E6%9C%80%E8%BF%91%E9%82%BB%E7%9A%84%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A4%E6%8E%A8%E8%8D%90/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><h2 id="基于K最近邻的协同过滤推荐"><a href="#基于K最近邻的协同过滤推荐" class="headerlink" title="基于K最近邻的协同过滤推荐"></a>基于K最近邻的协同过滤推荐</h2><p>基于K最近邻的协同过滤推荐其实本质上就是MemoryBased CF，只不过在选取近邻的时候，加上K最近邻的限制。</p><p>这里我们直接根据MemoryBased CF的代码实现</p><p>修改以下地方</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CollaborativeFiltering</span>(<span class="params">object</span>):</span></span><br><span class="line"></span><br><span class="line">    based = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, k=<span class="number">40</span>, rules=None, use_cache=False, standard=None</span>):</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        :param k: 取K个最近邻来进行预测</span></span><br><span class="line"><span class="string">        :param rules: 过滤规则，四选一，否则将抛异常：&quot;unhot&quot;, &quot;rated&quot;, [&quot;unhot&quot;,&quot;rated&quot;], None</span></span><br><span class="line"><span class="string">        :param use_cache: 相似度计算结果是否开启缓存</span></span><br><span class="line"><span class="string">        :param standard: 评分标准化方法，None表示不使用、mean表示均值中心化、zscore表示Z-Score标准化</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        self.k = <span class="number">40</span></span><br><span class="line">        self.rules = rules</span><br><span class="line">        self.use_cache = use_cache</span><br><span class="line">        self.standard = standard</span><br></pre></td></tr></table></figure><p>修改所有的选取近邻的地方的代码，根据相似度来选取K个最近邻</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">similar_users = self.similar[uid].drop([uid]).dropna().sort_values(ascending=<span class="literal">False</span>)[:self.k]</span><br><span class="line"></span><br><span class="line">similar_items = self.similar[iid].drop([iid]).dropna().sort_values(ascending=<span class="literal">False</span>)[:self.k]</span><br></pre></td></tr></table></figure><p>但由于我们的原始数据较少，这里我们的KNN方法的效果会比纯粹的MemoryBasedCF要差</p>]]></content>
      
      
      <categories>
          
          <category> Kafka </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 电影推荐系统算法综合实战 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>推荐算法基础(4)  基于矩阵分解的 CF 算法</title>
      <link href="2019/02/15/04_%E5%9F%BA%E4%BA%8E%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3%E7%9A%84%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A4%E6%8E%A8%E8%8D%90/"/>
      <url>2019/02/15/04_%E5%9F%BA%E4%BA%8E%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3%E7%9A%84%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A4%E6%8E%A8%E8%8D%90/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><h2 id="基于矩阵分解的CF算法"><a href="#基于矩阵分解的CF算法" class="headerlink" title="基于矩阵分解的CF算法"></a>基于矩阵分解的CF算法</h2><h4 id="矩阵分解发展史"><a href="#矩阵分解发展史" class="headerlink" title="矩阵分解发展史"></a>矩阵分解发展史</h4><p><strong>Traditional SVD:</strong></p><p>通常SVD矩阵分解指的是SVD（奇异值）分解技术，在这我们姑且将其命名为Traditional SVD（传统并经典着）其公式如下：</p><p><img src="/img/%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A31.jpg" alt="img"></p><p>Traditional SVD分解的形式为3个矩阵相乘，中间矩阵为奇异值矩阵。如果想运用SVD分解的话，有一个前提是要求矩阵是稠密的，即矩阵里的元素要非空，否则就不能运用SVD分解。</p><p>很显然我们的数据其实绝大多数情况下都是稀疏的，因此如果要使用Traditional SVD，一般的做法是先用均值或者其他统计学方法来填充矩阵，然后再运用Traditional SVD分解降维，但这样做明显对数据的原始性造成一定影响。</p><p><strong>FunkSVD（LFM）</strong></p><p>刚才提到的Traditional SVD首先需要填充矩阵，然后再进行分解降维，同时存在计算复杂度高的问题，因为要分解成3个矩阵，所以后来提出了Funk SVD的方法，它不在将矩阵分解为3个矩阵，而是分解为2个用户-隐含特征，项目-隐含特征的矩阵，Funk SVD也被称为最原始的LFM模型</p><p><img src="/img/%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A32.jpg" alt="img"></p><p>借鉴线性回归的思想，通过最小化观察数据的平方来寻求最优的用户和项目的隐含向量表示。同时为了避免过度拟合（Overfitting）观测数据，又提出了带有L2正则项的FunkSVD，上公式：</p><p><img src="/img/%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A33.jpg" alt="img"></p><p>以上两种最优化函数都可以通过梯度下降或者随机梯度下降法来寻求最优解。</p><p><strong>BiasSVD:</strong></p><p>在FunkSVD提出来之后，出现了很多变形版本，其中一个相对成功的方法是BiasSVD，顾名思义，即带有偏置项的SVD分解：</p><p><img src="/img/%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A34.jpg" alt="img"></p><p>它基于的假设和Baseline基准预测是一样的，但这里将Baseline的偏置引入到了矩阵分解中</p><p><strong>SVD++:</strong></p><p>人们后来又提出了改进的BiasSVD，被称为SVD++，该算法是在BiasSVD的基础上添加了用户的隐式反馈信息：</p><p><img src="/img/%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A35.jpg" alt="img"></p><p>显示反馈指的用户的评分这样的行为，隐式反馈指用户的浏览记录、购买记录、收听记录等。</p><p>SVD++是基于这样的假设：在BiasSVD基础上，认为用户对于项目的历史浏览记录、购买记录、收听记录等可以从侧面反映用户的偏好。</p>]]></content>
      
      
      <categories>
          
          <category> Kafka </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 电影推荐系统算法综合实战 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>推荐算法基础(5) 基于矩阵分解的CF算法实现（一）：LFM</title>
      <link href="2019/02/15/05_LFM%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B0/"/>
      <url>2019/02/15/05_LFM%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B0/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><h2 id="基于矩阵分解的CF算法实现（一）：LFM"><a href="#基于矩阵分解的CF算法实现（一）：LFM" class="headerlink" title="基于矩阵分解的CF算法实现（一）：LFM"></a>基于矩阵分解的CF算法实现（一）：LFM</h2><p>LFM也就是前面提到的Funk SVD矩阵分解</p><h4 id="LFM原理解析"><a href="#LFM原理解析" class="headerlink" title="LFM原理解析"></a>LFM原理解析</h4><p>LFM(latent factor model)隐语义模型核心思想是通过隐含特征联系用户和物品，如下图：</p><p><img src="/img/LFM%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3%E5%9B%BE%E8%A7%A3.png"></p><ul><li>P矩阵是User-LF矩阵，即用户和隐含特征矩阵。LF有三个，表示共总有三个隐含特征。</li><li>Q矩阵是LF-Item矩阵，即隐含特征和物品的矩阵</li><li>R矩阵是User-Item矩阵，有P*Q得来</li><li>能处理稀疏评分矩阵</li></ul><p>利用矩阵分解技术，将原始User-Item的评分矩阵（稠密/稀疏）分解为P和Q矩阵，然后利用$P*Q$还原出User-Item评分矩阵$R$。整个过程相当于降维处理，其中：</p><ul><li><p>矩阵值$P_{11}$表示用户1对隐含特征1的权重值</p></li><li><p>矩阵值$Q_{11}$表示隐含特征1在物品1上的权重值</p></li><li><p>矩阵值$R_{11}​$就表示预测的用户1对物品1的评分，且$R_{11}=\vec{P_{1,k}}\cdot \vec{Q_{k,1}}​$</p><p><img src="/img/LFM%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3%E5%9B%BE%E8%A7%A32.png"></p></li></ul><p>利用LFM预测用户对物品的评分，$k​$表示隐含特征数量：<br>$$<br>\begin{split}<br>\hat {r}<em>{ui} &amp;=\vec {p</em>{uk}}\cdot \vec {q_{ik}}<br>\&amp;={\sum_{k=1}}^k p_{uk}q_{ik}<br>\end{split}<br>$$<br>因此最终，我们的目标也就是要求出P矩阵和Q矩阵及其当中的每一个值，然后再对用户-物品的评分进行预测。</p><h4 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h4><p>同样对于评分预测我们利用平方差来构建损失函数：<br>$$<br>\begin{split}<br>Cost &amp;= \sum_{u,i\in R} (r_{ui}-\hat{r}<em>{ui})^2<br>\&amp;=\sum</em>{u,i\in R} (r_{ui}-{\sum_{k=1}}^k p_{uk}q_{ik})^2<br>\end{split}<br>$$<br>加入L2正则化：<br>$$<br>Cost = \sum_{u,i\in R} (r_{ui}-{\sum_{k=1}}^k p_{uk}q_{ik})^2 + \lambda(\sum_U{p_{uk}}^2+\sum_I{q_{ik}}^2)<br>$$<br>对损失函数求偏导：<br>$$<br>\begin{split}<br>\cfrac {\partial}{\partial p_{uk}}Cost &amp;= \cfrac {\partial}{\partial p_{uk}}[\sum_{u,i\in R} (r_{ui}-{\sum_{k=1}}^k p_{uk}q_{ik})^2 + \lambda(\sum_U{p_{uk}}^2+\sum_I{q_{ik}}^2)]<br>\&amp;=2\sum_{u,i\in R} (r_{ui}-{\sum_{k=1}}^k p_{uk}q_{ik})(-q_{ik}) + 2\lambda p_{uk}<br>\\<br>\cfrac {\partial}{\partial q_{ik}}Cost &amp;= \cfrac {\partial}{\partial q_{ik}}[\sum_{u,i\in R} (r_{ui}-{\sum_{k=1}}^k p_{uk}q_{ik})^2 + \lambda(\sum_U{p_{uk}}^2+\sum_I{q_{ik}}^2)]<br>\&amp;=2\sum_{u,i\in R} (r_{ui}-{\sum_{k=1}}^k p_{uk}q_{ik})(-p_{uk}) + 2\lambda q_{ik}<br>\end{split}<br>$$</p><h4 id="随机梯度下降法优化"><a href="#随机梯度下降法优化" class="headerlink" title="随机梯度下降法优化"></a>随机梯度下降法优化</h4><p>梯度下降更新参数$p_{uk}​$：<br>$$<br>\begin{split}<br>p_{uk}&amp;:=p_{uk} - \alpha\cfrac {\partial}{\partial p_{uk}}Cost<br>\&amp;:=p_{uk}-\alpha [2\sum_{u,i\in R} (r_{ui}-{\sum_{k=1}}^k p_{uk}q_{ik})(-q_{ik}) + 2\lambda p_{uk}]<br>\&amp;:=p_{uk}+\alpha [\sum_{u,i\in R} (r_{ui}-{\sum_{k=1}}^k p_{uk}q_{ik})q_{ik} - \lambda p_{uk}]<br>\end{split}<br>$$<br> 同理：<br>$$<br>\begin{split}<br>q_{ik}&amp;:=q_{ik} + \alpha[\sum_{u,i\in R} (r_{ui}-{\sum_{k=1}}^k p_{uk}q_{ik})p_{uk} - \lambda q_{ik}]<br>\end{split}<br>$$<br><strong>随机梯度下降：</strong> 向量乘法 每一个分量相乘 求和<br>$$<br>\begin{split}<br>&amp;p_{uk}:=p_{uk}+\alpha [(r_{ui}-{\sum_{k=1}}^k p_{uk}q_{ik})q_{ik} - \lambda_1 p_{uk}]<br>\&amp;q_{ik}:=q_{ik} + \alpha[(r_{ui}-{\sum_{k=1}}^k p_{uk}q_{ik})p_{uk} - \lambda_2 q_{ik}]<br>\end{split}<br>$$<br>由于P矩阵和Q矩阵是两个不同的矩阵，通常分别采取不同的正则参数，如$\lambda_1​$和$\lambda_2​$</p><p><strong>算法实现</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">LFM Model</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 评分预测    1-5</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LFM</span>(<span class="params">object</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, alpha, reg_p, reg_q, number_LatentFactors=<span class="number">10</span>, number_epochs=<span class="number">10</span>, columns=[<span class="string">&quot;uid&quot;</span>, <span class="string">&quot;iid&quot;</span>, <span class="string">&quot;rating&quot;</span>]</span>):</span></span><br><span class="line">        self.alpha = alpha <span class="comment"># 学习率</span></span><br><span class="line">        self.reg_p = reg_p    <span class="comment"># P矩阵正则</span></span><br><span class="line">        self.reg_q = reg_q    <span class="comment"># Q矩阵正则</span></span><br><span class="line">        self.number_LatentFactors = number_LatentFactors  <span class="comment"># 隐式类别数量</span></span><br><span class="line">        self.number_epochs = number_epochs    <span class="comment"># 最大迭代次数</span></span><br><span class="line">        self.columns = columns</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span>(<span class="params">self, dataset</span>):</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        fit dataset</span></span><br><span class="line"><span class="string">        :param dataset: uid, iid, rating</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">        self.dataset = pd.DataFrame(dataset)</span><br><span class="line"></span><br><span class="line">        self.users_ratings = dataset.groupby(self.columns[<span class="number">0</span>]).agg([list])[[self.columns[<span class="number">1</span>], self.columns[<span class="number">2</span>]]]</span><br><span class="line">        self.items_ratings = dataset.groupby(self.columns[<span class="number">1</span>]).agg([list])[[self.columns[<span class="number">0</span>], self.columns[<span class="number">2</span>]]]</span><br><span class="line"></span><br><span class="line">        self.globalMean = self.dataset[self.columns[<span class="number">2</span>]].mean()</span><br><span class="line"></span><br><span class="line">        self.P, self.Q = self.sgd()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_init_matrix</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        初始化P和Q矩阵，同时为设置0，1之间的随机值作为初始值</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="comment"># User-LF</span></span><br><span class="line">        P = dict(zip(</span><br><span class="line">            self.users_ratings.index,</span><br><span class="line">            np.random.rand(len(self.users_ratings), self.number_LatentFactors).astype(np.float32)</span><br><span class="line">        ))</span><br><span class="line">        <span class="comment"># Item-LF</span></span><br><span class="line">        Q = dict(zip(</span><br><span class="line">            self.items_ratings.index,</span><br><span class="line">            np.random.rand(len(self.items_ratings), self.number_LatentFactors).astype(np.float32)</span><br><span class="line">        ))</span><br><span class="line">        <span class="keyword">return</span> P, Q</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">sgd</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        使用随机梯度下降，优化结果</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        P, Q = self._init_matrix()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(self.number_epochs):</span><br><span class="line">            print(<span class="string">&quot;iter%d&quot;</span>%i)</span><br><span class="line">            error_list = []</span><br><span class="line">            <span class="keyword">for</span> uid, iid, r_ui <span class="keyword">in</span> self.dataset.itertuples(index=<span class="literal">False</span>):</span><br><span class="line">                <span class="comment"># User-LF P</span></span><br><span class="line">                <span class="comment">## Item-LF Q</span></span><br><span class="line">                v_pu = P[uid] <span class="comment">#用户向量</span></span><br><span class="line">                v_qi = Q[iid] <span class="comment">#物品向量</span></span><br><span class="line">                err = np.float32(r_ui - np.dot(v_pu, v_qi))</span><br><span class="line"></span><br><span class="line">                v_pu += self.alpha * (err * v_qi - self.reg_p * v_pu)</span><br><span class="line">                v_qi += self.alpha * (err * v_pu - self.reg_q * v_qi)</span><br><span class="line">                </span><br><span class="line">                P[uid] = v_pu </span><br><span class="line">                Q[iid] = v_qi</span><br><span class="line"></span><br><span class="line">                <span class="comment"># for k in range(self.number_of_LatentFactors):</span></span><br><span class="line">                <span class="comment">#     v_pu[k] += self.alpha*(err*v_qi[k] - self.reg_p*v_pu[k])</span></span><br><span class="line">                <span class="comment">#     v_qi[k] += self.alpha*(err*v_pu[k] - self.reg_q*v_qi[k])</span></span><br><span class="line"></span><br><span class="line">                error_list.append(err ** <span class="number">2</span>)</span><br><span class="line">            print(np.sqrt(np.mean(error_list)))</span><br><span class="line">        <span class="keyword">return</span> P, Q</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span>(<span class="params">self, uid, iid</span>):</span></span><br><span class="line">        <span class="comment"># 如果uid或iid不在，我们使用全剧平均分作为预测结果返回</span></span><br><span class="line">        <span class="keyword">if</span> uid <span class="keyword">not</span> <span class="keyword">in</span> self.users_ratings.index <span class="keyword">or</span> iid <span class="keyword">not</span> <span class="keyword">in</span> self.items_ratings.index:</span><br><span class="line">            <span class="keyword">return</span> self.globalMean</span><br><span class="line"></span><br><span class="line">        p_u = self.P[uid]</span><br><span class="line">        q_i = self.Q[iid]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> np.dot(p_u, q_i)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">test</span>(<span class="params">self,testset</span>):</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;预测测试集数据&#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="keyword">for</span> uid, iid, real_rating <span class="keyword">in</span> testset.itertuples(index=<span class="literal">False</span>):</span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">                pred_rating = self.predict(uid, iid)</span><br><span class="line">            <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">                print(e)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">yield</span> uid, iid, real_rating, pred_rating</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    dtype = [(<span class="string">&quot;userId&quot;</span>, np.int32), (<span class="string">&quot;movieId&quot;</span>, np.int32), (<span class="string">&quot;rating&quot;</span>, np.float32)]</span><br><span class="line">    dataset = pd.read_csv(<span class="string">&quot;datasets/ml-latest-small/ratings.csv&quot;</span>, usecols=range(<span class="number">3</span>), dtype=dict(dtype))</span><br><span class="line"></span><br><span class="line">    lfm = LFM(<span class="number">0.02</span>, <span class="number">0.01</span>, <span class="number">0.01</span>, <span class="number">10</span>, <span class="number">100</span>, [<span class="string">&quot;userId&quot;</span>, <span class="string">&quot;movieId&quot;</span>, <span class="string">&quot;rating&quot;</span>])</span><br><span class="line">    lfm.fit(dataset)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        uid = input(<span class="string">&quot;uid: &quot;</span>)</span><br><span class="line">        iid = input(<span class="string">&quot;iid: &quot;</span>)</span><br><span class="line">        print(lfm.predict(int(uid), int(iid)))</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Kafka </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 电影推荐系统算法综合实战 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>推荐算法基础(6) 基于矩阵分解的CF算法实现之BiasSvd</title>
      <link href="2019/02/15/06_BiasSVD%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B0/"/>
      <url>2019/02/15/06_BiasSVD%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B0/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><h2 id="基于矩阵分解的CF算法实现（二）：BiasSvd"><a href="#基于矩阵分解的CF算法实现（二）：BiasSvd" class="headerlink" title="基于矩阵分解的CF算法实现（二）：BiasSvd"></a>基于矩阵分解的CF算法实现（二）：BiasSvd</h2><p>BiasSvd其实就是前面提到的Funk SVD矩阵分解基础上加上了偏置项。</p><h4 id="BiasSvd"><a href="#BiasSvd" class="headerlink" title="BiasSvd"></a>BiasSvd</h4><p>利用BiasSvd预测用户对物品的评分，$k$表示隐含特征数量：<br>$$<br>\begin{split}<br>\hat {r}<em>{ui} &amp;=\mu + b_u + b_i + \vec {p</em>{uk}}\cdot \vec {q_{ki}}<br>\&amp;=\mu + b_u + b_i + {\sum_{k=1}}^k p_{uk}q_{ik}<br>\end{split}<br>$$</p><h4 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h4><p>同样对于评分预测我们利用平方差来构建损失函数：<br>$$<br>\begin{split}<br>Cost &amp;= \sum_{u,i\in R} (r_{ui}-\hat{r}<em>{ui})^2<br>\&amp;=\sum</em>{u,i\in R} (r_{ui}-\mu - b_u - b_i -{\sum_{k=1}}^k p_{uk}q_{ik})^2<br>\end{split}<br>$$<br>加入L2正则化：<br>$$<br>Cost = \sum_{u,i\in R} (r_{ui}-\mu - b_u - b_i-{\sum_{k=1}}^k p_{uk}q_{ik})^2 + \lambda(\sum_U{b_u}^2+\sum_I{b_i}^2+\sum_U{p_{uk}}^2+\sum_I{q_{ik}}^2)<br>$$<br>对损失函数求偏导：<br>$$<br>\begin{split}<br>\cfrac {\partial}{\partial p_{uk}}Cost &amp;= \cfrac {\partial}{\partial p_{uk}}[\sum_{u,i\in R} (r_{ui}-\mu - b_u - b_i-{\sum_{k=1}}^k p_{uk}q_{ik})^2 + \lambda(\sum_U{b_u}^2+\sum_I{b_i}^2+\sum_U{p_{uk}}^2+\sum_I{q_{ik}}^2)]<br>\&amp;=2\sum_{u,i\in R} (r_{ui}-\mu - b_u - b_i-{\sum_{k=1}}^k p_{uk}q_{ik})(-q_{ik}) + 2\lambda p_{uk}<br>\\<br>\cfrac {\partial}{\partial q_{ik}}Cost &amp;= \cfrac {\partial}{\partial q_{ik}}[\sum_{u,i\in R} (r_{ui}-\mu - b_u - b_i-{\sum_{k=1}}^k p_{uk}q_{ik})^2 + \lambda(\sum_U{b_u}^2+\sum_I{b_i}^2+\sum_U{p_{uk}}^2+\sum_I{q_{ik}}^2)]<br>\&amp;=2\sum_{u,i\in R} (r_{ui}-\mu - b_u - b_i-{\sum_{k=1}}^k p_{uk}q_{ik})(-p_{uk}) + 2\lambda q_{ik}<br>\end{split}<br>$$</p><p>$$<br>\begin{split}<br>\cfrac {\partial}{\partial b_u}Cost &amp;= \cfrac {\partial}{\partial b_u}[\sum_{u,i\in R} (r_{ui}-\mu - b_u - b_i-{\sum_{k=1}}^k p_{uk}q_{ik})^2 + \lambda(\sum_U{b_u}^2+\sum_I{b_i}^2+\sum_U{p_{uk}}^2+\sum_I{q_{ik}}^2)]<br>\&amp;=2\sum_{u,i\in R} (r_{ui}-\mu - b_u - b_i-{\sum_{k=1}}^k p_{uk}q_{ik})(-1) + 2\lambda b_u<br>\\<br>\cfrac {\partial}{\partial b_i}Cost &amp;= \cfrac {\partial}{\partial b_i}[\sum_{u,i\in R} (r_{ui}-\mu - b_u - b_i-{\sum_{k=1}}^k p_{uk}q_{ik})^2 + \lambda(\sum_U{b_u}^2+\sum_I{b_i}^2+\sum_U{p_{uk}}^2+\sum_I{q_{ik}}^2)]<br>\&amp;=2\sum_{u,i\in R} (r_{ui}-\mu - b_u - b_i-{\sum_{k=1}}^k p_{uk}q_{ik})(-1) + 2\lambda b_i<br>\end{split}<br>$$</p><h4 id="随机梯度下降法优化"><a href="#随机梯度下降法优化" class="headerlink" title="随机梯度下降法优化"></a>随机梯度下降法优化</h4><p>梯度下降更新参数$p_{uk}$：<br>$$<br>\begin{split}<br>p_{uk}&amp;:=p_{uk} - \alpha\cfrac {\partial}{\partial p_{uk}}Cost<br>\&amp;:=p_{uk}-\alpha [2\sum_{u,i\in R} (r_{ui}-\mu - b_u - b_i-{\sum_{k=1}}^k p_{uk}q_{ik})(-q_{ik}) + 2\lambda p_{uk}]<br>\&amp;:=p_{uk}+\alpha [\sum_{u,i\in R} (r_{ui}-\mu - b_u - b_i-{\sum_{k=1}}^k p_{uk}q_{ik})q_{ik} - \lambda p_{uk}]<br>\end{split}<br>$$<br> 同理：<br>$$<br>\begin{split}<br>q_{ik}&amp;:=q_{ik} + \alpha[\sum_{u,i\in R} (r_{ui}-\mu - b_u - b_i-{\sum_{k=1}}^k p_{uk}q_{ik})p_{uk} - \lambda q_{ik}]<br>\end{split}<br>$$</p><p>$$<br>b_u:=b_u + \alpha[\sum_{u,i\in R} (r_{ui}-\mu - b_u - b_i-{\sum_{k=1}}^k p_{uk}q_{ik}) - \lambda b_u]<br>$$</p><p>$$<br>b_i:=b_i + \alpha[\sum_{u,i\in R} (r_{ui}-\mu - b_u - b_i-{\sum_{k=1}}^k p_{uk}q_{ik}) - \lambda b_i]<br>$$</p><p><strong>随机梯度下降：</strong><br>$$<br>\begin{split}<br>&amp;p_{uk}:=p_{uk}+\alpha [(r_{ui}-\mu - b_u - b_i-{\sum_{k=1}}^k p_{uk}q_{ik})q_{ik} - \lambda_1 p_{uk}]<br>\&amp;q_{ik}:=q_{ik} + \alpha[(r_{ui}-\mu - b_u - b_i-{\sum_{k=1}}^k p_{uk}q_{ik})p_{uk} - \lambda_2 q_{ik}]<br>\end{split}<br>$$</p><p>$$<br>b_u:=b_u + \alpha[(r_{ui}-\mu - b_u - b_i-{\sum_{k=1}}^k p_{uk}q_{ik}) - \lambda_3 b_u]<br>$$</p><p>$$<br>b_i:=b_i + \alpha[(r_{ui}-\mu - b_u - b_i-{\sum_{k=1}}^k p_{uk}q_{ik}) - \lambda_4 b_i]<br>$$</p><p>由于P矩阵和Q矩阵是两个不同的矩阵，通常分别采取不同的正则参数，如$\lambda_1$和$\lambda_2$</p><p><strong>算法实现</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">BiasSvd Model</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BiasSvd</span>(<span class="params">object</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, alpha, reg_p, reg_q, reg_bu, reg_bi, number_LatentFactors=<span class="number">10</span>, number_epochs=<span class="number">10</span>, columns=[<span class="string">&quot;uid&quot;</span>, <span class="string">&quot;iid&quot;</span>, <span class="string">&quot;rating&quot;</span>]</span>):</span></span><br><span class="line">        self.alpha = alpha <span class="comment"># 学习率</span></span><br><span class="line">        self.reg_p = reg_p</span><br><span class="line">        self.reg_q = reg_q</span><br><span class="line">        self.reg_bu = reg_bu</span><br><span class="line">        self.reg_bi = reg_bi</span><br><span class="line">        self.number_LatentFactors = number_LatentFactors  <span class="comment"># 隐式类别数量</span></span><br><span class="line">        self.number_epochs = number_epochs</span><br><span class="line">        self.columns = columns</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span>(<span class="params">self, dataset</span>):</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        fit dataset</span></span><br><span class="line"><span class="string">        :param dataset: uid, iid, rating</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">        self.dataset = pd.DataFrame(dataset)</span><br><span class="line"></span><br><span class="line">        self.users_ratings = dataset.groupby(self.columns[<span class="number">0</span>]).agg([list])[[self.columns[<span class="number">1</span>], self.columns[<span class="number">2</span>]]]</span><br><span class="line">        self.items_ratings = dataset.groupby(self.columns[<span class="number">1</span>]).agg([list])[[self.columns[<span class="number">0</span>], self.columns[<span class="number">2</span>]]]</span><br><span class="line">        self.globalMean = self.dataset[self.columns[<span class="number">2</span>]].mean()</span><br><span class="line"></span><br><span class="line">        self.P, self.Q, self.bu, self.bi = self.sgd()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_init_matrix</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        初始化P和Q矩阵，同时为设置0，1之间的随机值作为初始值</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="comment"># User-LF</span></span><br><span class="line">        P = dict(zip(</span><br><span class="line">            self.users_ratings.index,</span><br><span class="line">            np.random.rand(len(self.users_ratings), self.number_LatentFactors).astype(np.float32)</span><br><span class="line">        ))</span><br><span class="line">        <span class="comment"># Item-LF</span></span><br><span class="line">        Q = dict(zip(</span><br><span class="line">            self.items_ratings.index,</span><br><span class="line">            np.random.rand(len(self.items_ratings), self.number_LatentFactors).astype(np.float32)</span><br><span class="line">        ))</span><br><span class="line">        <span class="keyword">return</span> P, Q</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">sgd</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        使用随机梯度下降，优化结果</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        P, Q = self._init_matrix()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 初始化bu、bi的值，全部设为0</span></span><br><span class="line">        bu = dict(zip(self.users_ratings.index, np.zeros(len(self.users_ratings))))</span><br><span class="line">        bi = dict(zip(self.items_ratings.index, np.zeros(len(self.items_ratings))))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(self.number_epochs):</span><br><span class="line">            print(<span class="string">&quot;iter%d&quot;</span>%i)</span><br><span class="line">            error_list = []</span><br><span class="line">            <span class="keyword">for</span> uid, iid, r_ui <span class="keyword">in</span> self.dataset.itertuples(index=<span class="literal">False</span>):</span><br><span class="line">                v_pu = P[uid]</span><br><span class="line">                v_qi = Q[iid]</span><br><span class="line">                err = np.float32(r_ui - self.globalMean - bu[uid] - bi[iid] - np.dot(v_pu, v_qi))</span><br><span class="line"></span><br><span class="line">                v_pu += self.alpha * (err * v_qi - self.reg_p * v_pu)</span><br><span class="line">                v_qi += self.alpha * (err * v_pu - self.reg_q * v_qi)</span><br><span class="line">                </span><br><span class="line">                P[uid] = v_pu </span><br><span class="line">                Q[iid] = v_qi</span><br><span class="line">                </span><br><span class="line">                bu[uid] += self.alpha * (err - self.reg_bu * bu[uid])</span><br><span class="line">                bi[iid] += self.alpha * (err - self.reg_bi * bi[iid])</span><br><span class="line"></span><br><span class="line">                error_list.append(err ** <span class="number">2</span>)</span><br><span class="line">            print(np.sqrt(np.mean(error_list)))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> P, Q, bu, bi</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span>(<span class="params">self, uid, iid</span>):</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> uid <span class="keyword">not</span> <span class="keyword">in</span> self.users_ratings.index <span class="keyword">or</span> iid <span class="keyword">not</span> <span class="keyword">in</span> self.items_ratings.index:</span><br><span class="line">            <span class="keyword">return</span> self.globalMean</span><br><span class="line"></span><br><span class="line">        p_u = self.P[uid]</span><br><span class="line">        q_i = self.Q[iid]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> self.globalMean + self.bu[uid] + self.bi[iid] + np.dot(p_u, q_i)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    dtype = [(<span class="string">&quot;userId&quot;</span>, np.int32), (<span class="string">&quot;movieId&quot;</span>, np.int32), (<span class="string">&quot;rating&quot;</span>, np.float32)]</span><br><span class="line">    dataset = pd.read_csv(<span class="string">&quot;datasets/ml-latest-small/ratings.csv&quot;</span>, usecols=range(<span class="number">3</span>), dtype=dict(dtype))</span><br><span class="line"></span><br><span class="line">    bsvd = BiasSvd(<span class="number">0.02</span>, <span class="number">0.01</span>, <span class="number">0.01</span>, <span class="number">0.01</span>, <span class="number">0.01</span>, <span class="number">10</span>, <span class="number">20</span>)</span><br><span class="line">    bsvd.fit(dataset)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        uid = input(<span class="string">&quot;uid: &quot;</span>)</span><br><span class="line">        iid = input(<span class="string">&quot;iid: &quot;</span>)</span><br><span class="line">        print(bsvd.predict(int(uid), int(iid)))</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Kafka </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 电影推荐系统算法综合实战 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>推荐算法基础(7)  基于内容的推荐算法（Content-Based）</title>
      <link href="2019/02/15/07_%E5%9F%BA%E4%BA%8E%E5%86%85%E5%AE%B9%E7%9A%84%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95/"/>
      <url>2019/02/15/07_%E5%9F%BA%E4%BA%8E%E5%86%85%E5%AE%B9%E7%9A%84%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><h2 id="基于内容的推荐算法（Content-Based）"><a href="#基于内容的推荐算法（Content-Based）" class="headerlink" title="基于内容的推荐算法（Content-Based）"></a>基于内容的推荐算法（Content-Based）</h2><h4 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h4><p>基于内容的推荐方法是非常直接的，它以物品的内容描述信息为依据来做出的推荐，本质上是基于对物品和用户自身的特征或属性的直接分析和计算。</p><p>例如，假设已知电影A是一部喜剧，而恰巧我们得知某个用户喜欢看喜剧电影，那么我们基于这样的已知信息，就可以将电影A推荐给该用户。</p><h4 id="基于内容的推荐实现步骤"><a href="#基于内容的推荐实现步骤" class="headerlink" title="基于内容的推荐实现步骤"></a>基于内容的推荐实现步骤</h4><ul><li><p><strong>画像构建</strong>。顾名思义，画像就是刻画物品或用户的特征。本质上就是给用户或物品贴标签。</p><ul><li><p><strong>物品画像</strong>：例如给电影《战狼2》贴标签，可以有哪些？</p><p><img src="/img/%E5%9F%BA%E4%BA%8E%E5%86%85%E5%AE%B9%E6%8E%A8%E8%8D%901.png"></p><p>“动作”、”吴京”、”吴刚”、”张翰”、”大陆电影”、”国产”、”爱国”、”军事”等等一系列标签是不是都可以贴上</p></li><li><p><strong>用户画像</strong>：例如已知用户的观影历史是：”《战狼1》”、”《战狼2》”、”《建党伟业》”、”《建军大业》”、”《建国大业》”、”《红海行动》”、”《速度与激情1-8》”等，我们是不是就可以分析出该用户的一些兴趣特征如：”爱国”、”战争”、”赛车”、”动作”、”军事”、”吴京”、”韩三平”等标签。</p></li></ul></li></ul><h6 id="问题：物品的标签来自哪儿？"><a href="#问题：物品的标签来自哪儿？" class="headerlink" title="问题：物品的标签来自哪儿？"></a>问题：物品的标签来自哪儿？</h6><ol><li>PGC    物品画像–冷启动<ul><li>物品自带的属性（物品一产生就具备的）：如电影的标题、导演、演员、类型等等</li><li>服务提供方设定的属性（服务提供方为物品附加的属性）：如短视频话题、微博话题（平台拟定）</li><li>其他渠道：如爬虫</li></ul></li><li>UGC    冷启动问题<ul><li>用户在享受服务过程中提供的物品的属性：如用户评论内容，微博话题（用户拟定）</li></ul></li></ol><p>根据PGC内容构建的物品画像的可以解决物品的冷启动问题</p><h6 id="基于内容推荐的算法流程："><a href="#基于内容推荐的算法流程：" class="headerlink" title="基于内容推荐的算法流程："></a>基于内容推荐的算法流程：</h6><ul><li>根据PGC/UGC内容构建物品画像</li><li>根据用户行为记录生成用户画像</li><li>根据用户画像从物品中寻找最匹配的TOP-N物品进行推荐</li></ul><h6 id="物品冷启动处理："><a href="#物品冷启动处理：" class="headerlink" title="物品冷启动处理："></a>物品冷启动处理：</h6><ul><li>根据PGC内容构建物品画像</li><li>利用物品画像计算物品间两两相似情况</li><li>为每个物品产生TOP-N最相似的物品进行相关推荐：如与该商品相似的商品有哪些？与该文章相似文章有哪些？</li></ul>]]></content>
      
      
      <categories>
          
          <category> Kafka </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 电影推荐系统算法综合实战 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>推荐算法基础(8)  基于内容的电影推荐：物品画像</title>
      <link href="2019/02/15/08_%E7%94%B5%E5%BD%B1%E6%8E%A8%E8%8D%90(ContentBased)_%E7%89%A9%E5%93%81%E7%94%BB%E5%83%8F/"/>
      <url>2019/02/15/08_%E7%94%B5%E5%BD%B1%E6%8E%A8%E8%8D%90(ContentBased)_%E7%89%A9%E5%93%81%E7%94%BB%E5%83%8F/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><h2 id="基于内容的电影推荐：物品画像"><a href="#基于内容的电影推荐：物品画像" class="headerlink" title="基于内容的电影推荐：物品画像"></a>基于内容的电影推荐：物品画像</h2><p>物品画像构建步骤：</p><ul><li>利用tags.csv中每部电影的标签作为电影的候选关键词</li><li>利用TF·IDF计算每部电影的标签的tfidf值，选取TOP-N个关键词作为电影画像标签</li><li>将电影的分类词直接作为每部电影的画像标签</li></ul><h2 id="基于TF-IDF的特征提取技术"><a href="#基于TF-IDF的特征提取技术" class="headerlink" title="基于TF-IDF的特征提取技术"></a>基于TF-IDF的特征提取技术</h2><p>前面提到，物品画像的特征标签主要都是指的如电影的导演、演员、图书的作者、出版社等结构话的数据，也就是他们的特征提取，尤其是体征向量的计算是比较简单的，如直接给作品的分类定义0或者1的状态。</p><p>但另外一些特征，比如电影的内容简介、电影的影评、图书的摘要等文本数据，这些被称为非结构化数据，首先他们本应该也属于物品的一个特征标签，但是这样的特征标签进行量化时，也就是计算它的特征向量时是很难去定义的。</p><p>因此这时就需要借助一些自然语言处理、信息检索等技术，将如用户的文本评论或其他文本内容信息的非结构化数据进行量化处理，从而实现更加完善的物品画像/用户画像。</p><p>TF-IDF算法便是其中一种在自然语言处理领域中应用比较广泛的一种算法。可用来提取目标文档中，并得到关键词用于计算对于目标文档的权重，并将这些权重组合到一起得到特征向量。</p><h4 id="算法原理"><a href="#算法原理" class="headerlink" title="算法原理"></a>算法原理</h4><p>TF-IDF自然语言处理领域中计算文档中词或短语的权值的方法，是<strong>词频</strong>（Term Frequency，TF）和逆转文档频率（Inverse Document Frequency，IDF）的乘积。TF指的是某一个给定的词语在该文件中出现的次数。这个数字通常会被正规化，以防止它偏向长的文件（同一个词语在长文件里可能会比短文件有更高的词频，而不管该词语重要与否）。IDF是一个词语普遍重要性的度量，某一特定词语的IDF，可以由总文件数目除以包含该词语之文件的数目，再将得到的商取对数得到。</p><p>TF-IDF算法基于一个这样的假设：若一个词语在目标文档中出现的频率高而在其他文档中出现的频率低，那么这个词语就可以用来区分出目标文档。这个假设需要掌握的有两点：</p><ul><li>在本文档出现的频率高；</li><li>在其他文档出现的频率低。</li></ul><p>因此，TF-IDF算法的计算可以分为词频（Term Frequency，TF）和逆转文档频率（Inverse Document Frequency，IDF）两部分，由TF和IDF的乘积来设置文档词语的权重。</p><p>TF指的是一个词语在文档中的出现频率。假设文档集包含的文档数为$$N​$$，文档集中包含关键词$$k_i​$$的文档数为$$n_i​$$，$$f_{ij}​$$表示关键词$$k_i​$$在文档$$d_j​$$中出现的次数，$$f_{dj}​$$表示文档$$d_j​$$中出现的词语总数，$$k_i​$$在文档dj中的词频$$TF_{ij}​$$定义为：$$TF_{ij}=\frac {f_{ij}}{f_{dj}}​$$。并且注意，这个数字通常会被正规化，以防止它偏向长的文件（指同一个词语在长文件里可能会比短文件有更高的词频，而不管该词语重要与否）。</p><p>IDF是一个词语普遍重要性的度量。表示某一词语在整个文档集中出现的频率，由它计算的结果取对数得到关键词$$k_i​$$的逆文档频率$$IDF_i​$$：$$IDF_i=log\frac {N}{n_i}​$$</p><p>由TF和IDF计算词语的权重为：$$w_{ij}=TF_{ij}$$<strong>·</strong>$$IDF_{i}=\frac {f_{ij}}{f_{dj}}$$<strong>·</strong>$$log\frac {N}{n_i}$$</p><p><strong>结论：TF-IDF与词语在文档中的出现次数成正比，与该词在整个文档集中的出现次数成反比。</strong></p><p><strong>用途：在目标文档中，提取关键词(特征标签)的方法就是将该文档所有词语的TF-IDF计算出来并进行对比，取其中TF-IDF值最大的k个数组成目标文档的特征向量用以表示文档。</strong></p><p>注意：文档中存在的停用词（Stop Words），如“是”、“的”之类的，对于文档的中心思想表达没有意义的词，在分词时需要先过滤掉再计算其他词语的TF-IDF值。</p><h4 id="算法举例"><a href="#算法举例" class="headerlink" title="算法举例"></a>算法举例</h4><p>对于计算影评的TF-IDF，以电影“加勒比海盗：黑珍珠号的诅咒”为例，假设它总共有1000篇影评，其中一篇影评的总词语数为200，其中出现最频繁的词语为“海盗”、“船长”、“自由”，分别是20、15、10次，并且这3个词在所有影评中被提及的次数分别为1000、500、100，就这3个词语作为关键词的顺序计算如下。</p><ol><li><p>将影评中出现的停用词过滤掉，计算其他词语的词频。以出现最多的三个词为例进行计算如下：</p><ul><li>“海盗”出现的词频为20/200＝0.1</li><li>“船长”出现的词频为15/200=0.075</li><li>“自由”出现的词频为10/200=0.05；</li></ul></li><li><p>计算词语的逆文档频率如下：</p><ul><li>“海盗”的IDF为：log(1000/1000)=0</li><li>“船长”的IDF为：log(1000/500)=0.3<br>“自由”的IDF为：log(1000/100)=1</li></ul></li><li><p>由1和2计算的结果求出词语的TF-IDF结果，“海盗”为0，“船长”为0.0225，“自由”为0.05。</p></li></ol><p>通过对比可得，该篇影评的关键词排序应为：“自由”、“船长”、“海盗”。把这些词语的TF-IDF值作为它们的权重按照对应的顺序依次排列，就得到这篇影评的特征向量，我们就用这个向量来代表这篇影评，向量中每一个维度的分量大小对应这个属性的重要性。</p><p>将总的影评集中所有的影评向量与特定的系数相乘求和，得到这部电影的综合影评向量，与电影的基本属性结合构建视频的物品画像，同理构建用户画像，可采用多种方法计算物品画像和用户画像之间的相似度，为用户做出推荐。</p><h4 id="加载数据集"><a href="#加载数据集" class="headerlink" title="加载数据集"></a>加载数据集</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">- 利用tags.csv中每部电影的标签作为电影的候选关键词</span></span><br><span class="line"><span class="string">- 利用TF·IDF计算每部电影的标签的tfidf值，选取TOP-N个关键词作为电影画像标签</span></span><br><span class="line"><span class="string">- 并将电影的分类词直接作为每部电影的画像标签</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_movie_dataset</span>():</span></span><br><span class="line">    <span class="comment"># 加载基于所有电影的标签</span></span><br><span class="line">    <span class="comment"># all-tags.csv来自ml-latest数据集中</span></span><br><span class="line">    <span class="comment"># 由于ml-latest-small中标签数据太多，因此借助其来扩充</span></span><br><span class="line">    _tags = pd.read_csv(<span class="string">&quot;datasets/ml-latest-small/all-tags.csv&quot;</span>, usecols=range(<span class="number">1</span>, <span class="number">3</span>)).dropna()</span><br><span class="line">    tags = _tags.groupby(<span class="string">&quot;movieId&quot;</span>).agg(list)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 加载电影列表数据集</span></span><br><span class="line">    movies = pd.read_csv(<span class="string">&quot;datasets/ml-latest-small/movies.csv&quot;</span>, index_col=<span class="string">&quot;movieId&quot;</span>)</span><br><span class="line">    <span class="comment"># 将类别词分开</span></span><br><span class="line">    movies[<span class="string">&quot;genres&quot;</span>] = movies[<span class="string">&quot;genres&quot;</span>].apply(<span class="keyword">lambda</span> x: x.split(<span class="string">&quot;|&quot;</span>))</span><br><span class="line">    <span class="comment"># 为每部电影匹配对应的标签数据，如果没有将会是NAN</span></span><br><span class="line">    movies_index = set(movies.index) &amp; set(tags.index)</span><br><span class="line">    new_tags = tags.loc[list(movies_index)]</span><br><span class="line">    ret = movies.join(new_tags)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 构建电影数据集，包含电影Id、电影名称、类别、标签四个字段</span></span><br><span class="line">    <span class="comment"># 如果电影没有标签数据，那么就替换为空列表</span></span><br><span class="line">    <span class="comment"># map(fun,可迭代对象)</span></span><br><span class="line">    movie_dataset = pd.DataFrame(</span><br><span class="line">        map(</span><br><span class="line">            <span class="keyword">lambda</span> x: (x[<span class="number">0</span>], x[<span class="number">1</span>], x[<span class="number">2</span>], x[<span class="number">2</span>]+x[<span class="number">3</span>]) <span class="keyword">if</span> x[<span class="number">3</span>] <span class="keyword">is</span> <span class="keyword">not</span> np.nan <span class="keyword">else</span> (x[<span class="number">0</span>], x[<span class="number">1</span>], x[<span class="number">2</span>], []), ret.itertuples())</span><br><span class="line">        , columns=[<span class="string">&quot;movieId&quot;</span>, <span class="string">&quot;title&quot;</span>, <span class="string">&quot;genres&quot;</span>,<span class="string">&quot;tags&quot;</span>]</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    movie_dataset.set_index(<span class="string">&quot;movieId&quot;</span>, inplace=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> movie_dataset</span><br><span class="line"></span><br><span class="line">movie_dataset = get_movie_dataset()</span><br><span class="line">print(movie_dataset)</span><br></pre></td></tr></table></figure><h4 id="基于TF·IDF提取TOP-N关键词，构建电影画像"><a href="#基于TF·IDF提取TOP-N关键词，构建电影画像" class="headerlink" title="基于TF·IDF提取TOP-N关键词，构建电影画像"></a>基于TF·IDF提取TOP-N关键词，构建电影画像</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> gensim.models <span class="keyword">import</span> TfidfModel</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> pprint <span class="keyword">import</span> pprint</span><br><span class="line"></span><br><span class="line"><span class="comment"># ......</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_movie_profile</span>(<span class="params">movie_dataset</span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    使用tfidf，分析提取topn关键词</span></span><br><span class="line"><span class="string">    :param movie_dataset: </span></span><br><span class="line"><span class="string">    :return: </span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    dataset = movie_dataset[<span class="string">&quot;tags&quot;</span>].values</span><br><span class="line"></span><br><span class="line">    <span class="keyword">from</span> gensim.corpora <span class="keyword">import</span> Dictionary</span><br><span class="line">    <span class="comment"># 根据数据集建立词袋，并统计词频，将所有词放入一个词典，使用索引进行获取</span></span><br><span class="line">    dct = Dictionary(dataset)</span><br><span class="line">    <span class="comment"># 根据将每条数据，返回对应的词索引和词频</span></span><br><span class="line">    corpus = [dct.doc2bow(line) <span class="keyword">for</span> line <span class="keyword">in</span> dataset]</span><br><span class="line">    <span class="comment"># 训练TF-IDF模型，即计算TF-IDF值</span></span><br><span class="line">    model = TfidfModel(corpus)</span><br><span class="line"></span><br><span class="line">    movie_profile = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> i, mid <span class="keyword">in</span> enumerate(movie_dataset.index):</span><br><span class="line">        <span class="comment"># 根据每条数据返回，向量</span></span><br><span class="line">        vector = model[corpus[i]]</span><br><span class="line">        <span class="comment"># 按照TF-IDF值得到top-n的关键词</span></span><br><span class="line">        movie_tags = sorted(vector, key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>], reverse=<span class="literal">True</span>)[:<span class="number">30</span>]</span><br><span class="line">        <span class="comment"># 根据关键词提取对应的名称</span></span><br><span class="line">        movie_profile[mid] = dict(map(<span class="keyword">lambda</span> x:(dct[x[<span class="number">0</span>]], x[<span class="number">1</span>]), movie_tags))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> movie_profile</span><br><span class="line"></span><br><span class="line">movie_dataset = get_movie_dataset()</span><br><span class="line">pprint(create_movie_profile(movie_dataset))</span><br></pre></td></tr></table></figure><h4 id="完善画像关键词"><a href="#完善画像关键词" class="headerlink" title="完善画像关键词"></a>完善画像关键词</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> gensim.models <span class="keyword">import</span> TfidfModel</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> pprint <span class="keyword">import</span> pprint</span><br><span class="line"></span><br><span class="line"><span class="comment"># ......</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_movie_profile</span>(<span class="params">movie_dataset</span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    使用tfidf，分析提取topn关键词</span></span><br><span class="line"><span class="string">    :param movie_dataset:</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    dataset = movie_dataset[<span class="string">&quot;tags&quot;</span>].values</span><br><span class="line"></span><br><span class="line">    <span class="keyword">from</span> gensim.corpora <span class="keyword">import</span> Dictionary</span><br><span class="line">    <span class="comment"># 根据数据集建立词袋，并统计词频，将所有词放入一个词典，使用索引进行获取</span></span><br><span class="line">    dct = Dictionary(dataset)</span><br><span class="line">    <span class="comment"># 根据将每条数据，返回对应的词索引和词频</span></span><br><span class="line">    corpus = [dct.doc2bow(line) <span class="keyword">for</span> line <span class="keyword">in</span> dataset]</span><br><span class="line">    <span class="comment"># 训练TF-IDF模型，即计算TF-IDF值</span></span><br><span class="line">    model = TfidfModel(corpus)</span><br><span class="line"></span><br><span class="line">    _movie_profile = []</span><br><span class="line">    <span class="keyword">for</span> i, data <span class="keyword">in</span> enumerate(movie_dataset.itertuples()):</span><br><span class="line">        mid = data[<span class="number">0</span>]</span><br><span class="line">        title = data[<span class="number">1</span>]</span><br><span class="line">        genres = data[<span class="number">2</span>]</span><br><span class="line">        vector = model[corpus[i]]</span><br><span class="line">        movie_tags = sorted(vector, key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>], reverse=<span class="literal">True</span>)[:<span class="number">30</span>]</span><br><span class="line">        topN_tags_weights = dict(map(<span class="keyword">lambda</span> x: (dct[x[<span class="number">0</span>]], x[<span class="number">1</span>]), movie_tags))</span><br><span class="line">        <span class="comment"># 将类别词的添加进去，并设置权重值为1.0</span></span><br><span class="line">        <span class="keyword">for</span> g <span class="keyword">in</span> genres:</span><br><span class="line">            topN_tags_weights[g] = <span class="number">1.0</span></span><br><span class="line">        topN_tags = [i[<span class="number">0</span>] <span class="keyword">for</span> i <span class="keyword">in</span> topN_tags_weights.items()]</span><br><span class="line">        _movie_profile.append((mid, title, topN_tags, topN_tags_weights))</span><br><span class="line"></span><br><span class="line">    movie_profile = pd.DataFrame(_movie_profile, columns=[<span class="string">&quot;movieId&quot;</span>, <span class="string">&quot;title&quot;</span>, <span class="string">&quot;profile&quot;</span>, <span class="string">&quot;weights&quot;</span>])</span><br><span class="line">    movie_profile.set_index(<span class="string">&quot;movieId&quot;</span>, inplace=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> movie_profile</span><br><span class="line"></span><br><span class="line">movie_dataset = get_movie_dataset()</span><br><span class="line">pprint(create_movie_profile(movie_dataset))</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>为了根据指定关键词迅速匹配到对应的电影，因此需要对物品画像的标签词，建立<strong>倒排索引</strong></p><p><strong>倒排索引介绍</strong></p><p>通常数据存储数据，都是以物品的ID作为索引，去提取物品的其他信息数据</p><p>而倒排索引就是用物品的其他数据作为索引，去提取它们对应的物品的ID列表</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ......</span></span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">建立tag-物品的倒排索引</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_inverted_table</span>(<span class="params">movie_profile</span>):</span></span><br><span class="line">    inverted_table = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> mid, weights <span class="keyword">in</span> movie_profile[<span class="string">&quot;weights&quot;</span>].iteritems():</span><br><span class="line">        <span class="keyword">for</span> tag, weight <span class="keyword">in</span> weights.items():</span><br><span class="line">            <span class="comment">#到inverted_table dict 用tag作为Key去取值 如果取不到就返回[]</span></span><br><span class="line">            _ = inverted_table.get(tag, [])</span><br><span class="line">            _.append((mid, weight))</span><br><span class="line">            inverted_table.setdefault(tag, _)</span><br><span class="line">    <span class="keyword">return</span> inverted_table</span><br><span class="line"></span><br><span class="line">inverted_table = create_inverted_table(movie_profile)</span><br><span class="line">pprint(inverted_table)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Kafka </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 电影推荐系统算法综合实战 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>消息系统-Kafka(5)：可靠性保障</title>
      <link href="2019/02/15/%E6%B6%88%E6%81%AF%E7%B3%BB%E7%BB%9F-Kafka(5)%EF%BC%9A%E5%8F%AF%E9%9D%A0%E6%80%A7%E4%BF%9D%E9%9A%9C/"/>
      <url>2019/02/15/%E6%B6%88%E6%81%AF%E7%B3%BB%E7%BB%9F-Kafka(5)%EF%BC%9A%E5%8F%AF%E9%9D%A0%E6%80%A7%E4%BF%9D%E9%9A%9C/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><h1 id="1-副本"><a href="#1-副本" class="headerlink" title="1. 副本"></a>1. 副本</h1><h2 id="1-1-概述"><a href="#1-1-概述" class="headerlink" title="1.1. 概述"></a>1.1. 概述</h2><h2 id="1-2-失效副本"><a href="#1-2-失效副本" class="headerlink" title="1.2. 失效副本"></a>1.2. 失效副本</h2><blockquote></blockquote><h3 id="1-2-1-失效副本判定"><a href="#1-2-1-失效副本判定" class="headerlink" title="1.2.1.失效副本判定"></a>1.2.1.失效副本判定</h3><p>从 Kafka 0.9.x 版本开始通过唯一的一个参数 replica.lag.time.max.ms[默认大小为10,000]来控制，当 ISR 中的一个 follower 副本滞后 leader 副本的时间超过参数 replica.lag.time.max.ms 指定的值时即判定为副本失效，需要将此 follower副本 剔出除 ISR 之外。</p><ul><li><p><strong><font color = 'red'>注意</font></strong></p><blockquote><p>在 Kafka 0.9.x 版本之前还有另一个 Broker 级别的参数 <code>replica.lag.max.messages</code> 也是用来判定失效副本的，当一个 follower 副本滞后 leader 副本的消息数超过replica.lag.max.messages 的大小时则判定此 follower 副本为失效副本。</p><p>它与 <code>replica.lag.time.max.ms</code> 参数判定出的失败副本去并集组成一个失效副本的集合，从而进一步剥离出ISR。不过这个 replica.lag.max.messages 参数很难给定一个合适的值，若设置的太大则这个参数本身就没有太多意义，若设置的太小则会让 follower 副本反复的处于同步、未同步、同步的死循环中，进而又会造成ISR的频繁变动。而且这个参数是 Broker 级别的，也就是说对 Broker 中的所有 topic 都生效，就以默认的值4000来说，对于消息流入速度很低的topic来说，比如TPS=10，这个参数并无用武之地；而对于消息流入速度很高的topic来说，比如TPS=20,000，这个参数的取值又会引入ISR的频繁变动，所以从0.9.x版本开始就彻底移除了这一参数</p></blockquote></li></ul><p>当follower副本将leader副本的LEO（Log End Offset，每个分区最后一条消息的位置）之前的日志全部同步时，则认为该follower副本已经追赶上leader副本，此时更新该副本的lastCaughtUpTimeMs标识。Kafka的副本管理器（ReplicaManager）启动时会启动一个副本过期检测的定时任务，而这个定时任务会定时检查当前时间与副本的lastCaughtUpTimeMs差值是否大于参数replica.lag.time.max.ms指定的值。千万不要错误的认为follower副本只要拉取leader副本的数据就会更新lastCaughtUpTimeMs，试想当leader副本的消息流入速度大于follower副本的拉取速度时，follower副本一直不断的拉取leader副本的消息也不能与leader副本同步，如果还将此follower副本置于ISR中，那么当leader副本失效，而选取此follower副本为新的leader副本，那么就会有严重的消息丢失。</p><h2 id="1-3-ISR"><a href="#1-3-ISR" class="headerlink" title="1.3. ISR"></a>1.3. ISR</h2><p>分区中所有副本统称为 AR (Assign Replicas).所有和 leader 副本保持一定程度同步的副本[包括leader 副本在内]组成 LSR</p><p>LSR 集合是 AR 集合的一个子集</p><p>消息会首先发送到 leader 副本，然后 follower 副本才能从 leader 副本中拉取消息进行同步，同步期内 follower副本相较 leader副本有一定的滞后，这个滞后在可忍受的滞后范围，这个范围可以通过参数进行配置</p><p>与 leader 副本滞后过多的副本组成 OSR </p><p>在正常情况下，所有的 follower 副本都应该与 leader副本保持一定程度的同步,即 AR=ISR, OR集合为空</p><h2 id="1-3-1-维护"><a href="#1-3-1-维护" class="headerlink" title="1.3.1. 维护"></a>1.3.1. 维护</h2><p><strong>leader 副本负责维护和跟踪</strong> ISR 集合中所有 follower 副本的滞后状态，当follower副本落后太多或失效时，leader副本会把它从 ISR 集合中剔除。如果 OSR 集合中所有follower副本“追上”了leader副本，那么leader副本会把它从 OSR 集合转移至 ISR 集合。默认情况下，当leader副本发生故障时，只有在 ISR 集合中的follower副本才有资格被选举为新的leader，而在 OSR 集合中的副本则没有任何机会（不过这个可以通过配置来改变）。</p><h2 id="1-3-2-ISR-缩减"><a href="#1-3-2-ISR-缩减" class="headerlink" title="1.3.2. ISR 缩减"></a>1.3.2. ISR 缩减</h2><ul><li><p>isr-expiration定时任务会周期性的检测每个分区是否需要缩减其ISR集合，当检测到ISR中有是失效的副本的时候，就会缩减 ISR 集合；</p></li><li><p><strong>将变更后的数据记录到 ZooKeeper 对应 /brokers/topics//partition//state 节点；</strong></p></li><li><p><strong>ISR 集合发生变更时将变更后的数据缓存到 isrChangeSet</strong></p></li><li><p>isr-change-propagation 定时任务会周期性（固定值为2500ms）地检查 isrChangeSet，在 zk 中的 /isr_change_notification 节点下创建 isr_change 开头的持久顺序节点，并保存 isrChangeSet 的数据</p></li><li><p>kafka控制器为 /isr_change_notification 添加了一个 Watcher，当这个节点中有子节点发生变化的时候会触发 Watcher 动作，以此通知控制器<strong>更新相关的元数据信息</strong>并向它管理的 broker 节点发送更新元数据信息的请求。最<strong>后删除 /isr_change_notification 的路径下已经处理过的节点</strong>。</p><ul><li><p><strong><font color='red'>注意</font></strong></p><blockquote><p>频繁的触发 Watcher 会影响 kafka 控制器，zookeeper 甚至其他的 broker 性能。为了避免这种情况，kafka 添加了指定的条件，当检测到分区 ISR 集合发生变化的时候，还需要检查一下两个条件：</p></blockquote><ul><li>上一次 ISR 集合发生变化距离现在已经超过5秒，</li><li>上一次写入zookeeper的时候距离现在已经超过60秒。</li></ul><blockquote><p>满足以上两个条件之一者可以将 ISR 写入集合的变化的目标节点。</p></blockquote></li></ul></li></ul><h3 id="1-3-3-ISR-增加"><a href="#1-3-3-ISR-增加" class="headerlink" title="1.3.3. ISR 增加"></a>1.3.3. ISR 增加</h3><blockquote><p>随着 follower 副本不断进行消息同步，follower 副本 LEO 也会逐渐后移，并且最终赶上 leader 副本，此时 follower 副本就有资格进入 ISR 集合，追赶上leader 副本的判定准侧是<strong>此副本的 LEO 是否大于等于 leader 副本 HW</strong>。更新 ZooKeeper 中的 /broker/topics//partition//state 节点和 isrChangeSet，之后的操作同 ISR  集合的缩减。</p></blockquote><h2 id="1-4-副本同步"><a href="#1-4-副本同步" class="headerlink" title="1.4. 副本同步"></a>1.4. 副本同步</h2><h3 id="1-4-1-相关概念"><a href="#1-4-1-相关概念" class="headerlink" title="1.4.1. 相关概念"></a>1.4.1. 相关概念</h3><p><strong>LEO [Log End Offset]，标识当前日志文件中下一条待写入的消息的 offset</strong>。上图中 offset 为 9 的位置即为当前日志文件的 LEO</p><p><strong>LEO 的大小相当于当前日志分区中最后一条消息的 offset 值加 1</strong></p><p>分区 ISR 集合中的每个副本都会维护自身的 LEO ，而 ISR 集合中最小的 LEO 即为分区的 HW，对消费者而言只能消费 HW 之前的消息。</p><p><strong>HW</strong> 是 High Watermark 的缩写，俗称高水位，水印，它标识了一个特定的消息偏移量[Offset],消费者只能拉取到这个 Offset 之前的消息。LSR 队列中的最小 LEO。</p><p>Leader副本 发生故障之后，从 LSR 中选出一个新的 Leader副本，为保证多个副本之间的数据一致性，其余的 Follower副本会从各自的日志文件中高于 HW 的部分截掉，然后从新的 Leader 副本同步数据。</p><img src="/Users/zxc/Documents/big_data/Kafka/resources/未命名.png" alt="未命名" style="zoom:72%;" /><h3 id="1-4-2-过程"><a href="#1-4-2-过程" class="headerlink" title="1.4.2. 过程"></a>1.4.2. 过程</h3><p>某个分区有3个副本分别位于 broker0、broker1 和 broker2 节点中，假设 broker0 上的副本1为当前分区的 leader 副本，那么副本2和副本3就是 follower 副本，整个消息追加的过程可以概括如下：</p><ul><li>生产者客户端发送消息至 leader 副本中。</li><li>消息被追加到 leader 副本的本地日志，并且会更新日志的偏移量。</li><li>follower 副本（副本2和副本3）向 leader 副本请求同步数据。</li><li>leader 副本所在的服务器读取本地日志，并更新对应拉取的 follower 副本的信息。</li><li>leader 副本所在的服务器将拉取结果返回给 follower 副本。</li><li>follower 副本收到 leader 副本返回的拉取结果，将消息追加到本地日志中，并更新日志的偏移量信息。</li></ul><p>某一时刻，leader 副本的 LEO 增加至5，并且所有副本的 HW 还都为0。</p><p><strong>之后 follower 副本 向 leader 副本 拉取消息，在拉取的请求中会带有自身的 LEO 信息</strong>[这个 LEO 信息对应的是 FetchRequest 请求中的 fetch_offset]。<strong>leader 副本返回给 follower 副本相应的消息，并且还带有自身的 HW 信息</strong></p><p><strong>follower 副本 各自拉取到了消息，并更新各自的 LEO 为3和4。与此同时，follower 副本 还会更新自己的 HW</strong>，更新 HW 的算法是比较当前 LEO 和 leader 副本 中传送过来的 HW 的值，取较小值作为自己的 HW 值。当前两个 follower 副本的 HW 都等于0</p><p><strong>接下来 follower 副本 再次请求拉取 leader 副本中的消息。</strong></p><p><strong>leader 副本 收到来自 follower 副本 的 FetchRequest 请求，其中带有 LEO 的相关信息，选取其中的最小值作为新的 HW即 min(15,3,4)=3，然后连同消息和 HW 一起返回 FetchResponse 给 follower 副本。</strong> leader 副本 的 HW 是一个很重要的东西，因为它直接影响了分区数据对消费者的可见性。</p><p>两个 follower 副本 在收到新的消息之后更新 LEO 并且更新自己的 HW 为<strong>3</strong> [min(LEO,3)=3]。</p><h2 id="1-4-Leader-Epoch"><a href="#1-4-Leader-Epoch" class="headerlink" title="1.4. Leader Epoch"></a>1.4. Leader Epoch</h2><p>leader epoch 代表 leader 的纪元信息（epoch），初始值为0。每当 leader 变更一次，leader epoch 的值就会加1，相当于为 leader 增设了一个版本号。<br>每个副本中还会增设一个矢量 &lt;LeaderEpoch =&gt; StartOffset&gt;，其中 StartOffset 表示当前 LeaderEpoch 下写入的第一条消息的偏移量。</p><h2 id="1-5-为什么不支持读写分离？"><a href="#1-5-为什么不支持读写分离？" class="headerlink" title="1.5. 为什么不支持读写分离？"></a>1.5. 为什么不支持读写分离？</h2><h2 id="1-6-日志同步"><a href="#1-6-日志同步" class="headerlink" title="1.6. 日志同步"></a>1.6. 日志同步</h2>]]></content>
      
      
      <categories>
          
          <category> Kafka </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kafka 消息系统源码深度剖析 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>推荐算法基础(3)  基于回归模型的协同过滤推荐</title>
      <link href="2019/02/15/03_%E5%9F%BA%E4%BA%8E%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A4%E6%8E%A8%E8%8D%90/"/>
      <url>2019/02/15/03_%E5%9F%BA%E4%BA%8E%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A4%E6%8E%A8%E8%8D%90/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><h2 id="基于回归模型的协同过滤推荐"><a href="#基于回归模型的协同过滤推荐" class="headerlink" title="基于回归模型的协同过滤推荐"></a>基于回归模型的协同过滤推荐</h2><p>如果我们将评分看作是一个连续的值而不是离散的值，那么就可以借助线性回归思想来预测目标用户对某物品的评分。其中一种实现策略被称为Baseline（基准预测）。</p><h4 id="Baseline：基准预测"><a href="#Baseline：基准预测" class="headerlink" title="Baseline：基准预测"></a>Baseline：基准预测</h4><p>Baseline设计思想基于以下的假设：</p><ul><li>有些用户的评分普遍高于其他用户，有些用户的评分普遍低于其他用户。比如有些用户天生愿意给别人好评，心慈手软，比较好说话，而有的人就比较苛刻，总是评分不超过3分（5分满分）</li><li>一些物品的评分普遍高于其他物品，一些物品的评分普遍低于其他物品。比如一些物品一被生产便决定了它的地位，有的比较受人们欢迎，有的则被人嫌弃。</li></ul><p>这个用户或物品普遍高于或低于平均值的差值，我们称为偏置(bias)</p><p><strong>Baseline目标：</strong></p><ul><li>找出每个用户普遍高于或低于他人的偏置值$b_u​$</li><li>找出每件物品普遍高于或低于其他物品的偏置值$b_i$</li><li>我们的目标也就转化为寻找最优的$b_u$和$b_i$</li></ul><p>使用Baseline的算法思想预测评分的步骤如下：</p><ul><li><p>计算所有电影的平均评分$\mu$（即全局平均评分）</p></li><li><p>计算每个用户评分与平均评分$\mu$的偏置值$b_u$</p></li><li><p>计算每部电影所接受的评分与平均评分$\mu$的偏置值​$b_i$</p></li><li><p>预测用户对电影的评分：<br>$$<br>\hat{r}<em>{ui} = b</em>{ui} = \mu + b_u + b_i<br>$$</p></li></ul><p>举例：</p><p>​    比如我们想通过Baseline来预测用户A对电影“阿甘正传”的评分，那么首先计算出整个评分数据集的平均评分$\mu$是3.5分；而用户A是一个比较苛刻的用户，他的评分比较严格，普遍比平均评分低0.5分，即用户A的偏置值$b_i$是-0.5；而电影“阿甘正传”是一部比较热门而且备受好评的电影，它的评分普遍比平均评分要高1.2分，那么电影“阿甘正传”的偏置值$b_i$是+1.2，因此就可以预测出用户A对电影“阿甘正传”的评分为：$3.5+(-0.5)+1.2$，也就是4.2分。</p><p>对于所有电影的平均评分$\mu$是直接能计算出的，因此问题在于要测出每个用户的$b_u$值和每部电影的$b_i$的值。对于线性回归问题，我们可以利用平方差构建损失函数如下：<br>$$<br>\begin{split}<br>Cost &amp;= \sum_{u,i\in R}(r_{ui}-\hat{r}<em>{ui})^2<br>\&amp;=\sum</em>{u,i\in R}(r_{ui}-\mu-b_u-b_i)^2<br>\end{split}<br>$$<br><img src="/img/%E5%81%8F%E7%BD%AE.png"></p><p>加入L2正则化：<br>$$<br>Cost=\sum_{u,i\in R}(r_{ui}-\mu-b_u-b_i)^2 + \lambda*(\sum_u {b_u}^2 + \sum_i {b_i}^2)<br>$$<br>公式解析：</p><ul><li>公式第一部分$ \sum_{u,i\in R}(r_{ui}-\mu-b_u-b_i)^2$是用来寻找与已知评分数据拟合最好的$b_u$和$b_i$</li><li>公式第二部分$\lambda*(\sum_u {b_u}^2 + \sum_i {b_i}^2)​$是正则化项，用于避免过拟合现象</li></ul><p>对于最小过程的求解，我们一般采用<strong>随机梯度下降法</strong>或者<strong>交替最小二乘法</strong>来优化实现。</p><h4 id="方法一：随机梯度下降法优化"><a href="#方法一：随机梯度下降法优化" class="headerlink" title="方法一：随机梯度下降法优化"></a>方法一：随机梯度下降法优化</h4><p>使用随机梯度下降优化算法预测Baseline偏置值</p><h6 id="step-1：梯度下降法推导"><a href="#step-1：梯度下降法推导" class="headerlink" title="step 1：梯度下降法推导"></a>step 1：梯度下降法推导</h6><p>损失函数：<br>$$<br>\begin{split}<br>&amp;J(\theta)=Cost=f(b_u, b_i)\<br>\<br>&amp;J(\theta)=\sum_{u,i\in R}(r_{ui}-\mu-b_u-b_i)^2 + \lambda*(\sum_u {b_u}^2 + \sum_i {b_i}^2)<br>\end{split}<br>$$<br>梯度下降参数更新原始公式：<br>$$<br>\theta_j:=\theta_j-\alpha\cfrac{\partial }{\partial \theta_j}J(\theta)<br>$$<br>梯度下降更新$b_u​$:</p><p>​    损失函数偏导推导：<br>$$<br>\begin{split}<br>\cfrac{\partial}{\partial b_u} J(\theta)&amp;=\cfrac{\partial}{\partial b_u} f(b_u, b_i)<br>\&amp;=2\sum_{u,i\in R}(r_{ui}-\mu-b_u-b_i)(-1) + 2\lambda{b_u}<br>\&amp;=-2\sum_{u,i\in R}(r_{ui}-\mu-b_u-b_i) + 2\lambda<em>b_u<br>\end{split}<br>$$<br>​    $b_u$更新(因为alpha可以人为控制，所以2可以省略掉)：<br>$$<br>\begin{split}<br>b_u&amp;:=b_u - \alpha</em>(-\sum_{u,i\in R}(r_{ui}-\mu-b_u-b_i) + \lambda * b_u)\<br>&amp;:=b_u + \alpha*(\sum_{u,i\in R}(r_{ui}-\mu-b_u-b_i) - \lambda* b_u)<br>\end{split}<br>$$<br>同理可得，梯度下降更新$b_i​$:<br>$$<br>b_i:=b_i + \alpha*(\sum_{u,i\in R}(r_{ui}-\mu-b_u-b_i) -\lambda*b_i)<br>$$</p><h6 id="step-2：随机梯度下降"><a href="#step-2：随机梯度下降" class="headerlink" title="step 2：随机梯度下降"></a>step 2：随机梯度下降</h6><p>由于<strong>随机梯度下降法</strong>本质上利用<strong>每个样本的损失</strong>来更新参数，而不用每次求出全部的损失和，因此使用SGD时：</p><p>单样本损失值：<br>$$<br>\begin{split}<br>error &amp;=r_{ui}-\hat{r}<em>{ui}<br>\&amp;= r</em>{ui}-(\mu+b_u+b_i)<br>\&amp;= r_{ui}-\mu-b_u-b_i<br>\end{split}<br>$$<br>参数更新：<br>$$<br>\begin{split}<br>b_u&amp;:=b_u + \alpha*((r_{ui}-\mu-b_u-b_i) -\lambda<em>b_u)  \<br>&amp;:=b_u + \alpha</em>(error - \lambda<em>b_u) \<br>\<br>b_i&amp;:=b_i + \alpha</em>((r_{ui}-\mu-b_u-b_i) -\lambda<em>b_i)\<br>&amp;:=b_i + \alpha</em>(error -\lambda*b_i)<br>\end{split}<br>$$</p><h6 id="step-3：算法实现"><a href="#step-3：算法实现" class="headerlink" title="step 3：算法实现"></a>step 3：算法实现</h6><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BaselineCFBySGD</span>(<span class="params">object</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, number_epochs, alpha, reg, columns=[<span class="string">&quot;uid&quot;</span>, <span class="string">&quot;iid&quot;</span>, <span class="string">&quot;rating&quot;</span>]</span>):</span></span><br><span class="line">        <span class="comment"># 梯度下降最高迭代次数</span></span><br><span class="line">        self.number_epochs = number_epochs</span><br><span class="line">        <span class="comment"># 学习率</span></span><br><span class="line">        self.alpha = alpha</span><br><span class="line">        <span class="comment"># 正则参数</span></span><br><span class="line">        self.reg = reg</span><br><span class="line">        <span class="comment"># 数据集中user-item-rating字段的名称</span></span><br><span class="line">        self.columns = columns</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span>(<span class="params">self, dataset</span>):</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        :param dataset: uid, iid, rating</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        self.dataset = dataset</span><br><span class="line">        <span class="comment"># 用户评分数据</span></span><br><span class="line">        self.users_ratings = dataset.groupby(self.columns[<span class="number">0</span>]).agg([list])[[self.columns[<span class="number">1</span>], self.columns[<span class="number">2</span>]]]</span><br><span class="line">        <span class="comment"># 物品评分数据</span></span><br><span class="line">        self.items_ratings = dataset.groupby(self.columns[<span class="number">1</span>]).agg([list])[[self.columns[<span class="number">0</span>], self.columns[<span class="number">2</span>]]]</span><br><span class="line">        <span class="comment"># 计算全局平均分</span></span><br><span class="line">        self.global_mean = self.dataset[self.columns[<span class="number">2</span>]].mean()</span><br><span class="line">        <span class="comment"># 调用sgd方法训练模型参数</span></span><br><span class="line">        self.bu, self.bi = self.sgd()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">sgd</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        利用随机梯度下降，优化bu，bi的值</span></span><br><span class="line"><span class="string">        :return: bu, bi</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="comment"># 初始化bu、bi的值，全部设为0</span></span><br><span class="line">        bu = dict(zip(self.users_ratings.index, np.zeros(len(self.users_ratings))))</span><br><span class="line">        bi = dict(zip(self.items_ratings.index, np.zeros(len(self.items_ratings))))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(self.number_epochs):</span><br><span class="line">            print(<span class="string">&quot;iter%d&quot;</span> % i)</span><br><span class="line">            <span class="keyword">for</span> uid, iid, real_rating <span class="keyword">in</span> self.dataset.itertuples(index=<span class="literal">False</span>):</span><br><span class="line">                error = real_rating - (self.global_mean + bu[uid] + bi[iid])</span><br><span class="line"></span><br><span class="line">                bu[uid] += self.alpha * (error - self.reg * bu[uid])</span><br><span class="line">                bi[iid] += self.alpha * (error - self.reg * bi[iid])</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> bu, bi</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span>(<span class="params">self, uid, iid</span>):</span></span><br><span class="line">        predict_rating = self.global_mean + self.bu[uid] + self.bi[iid]</span><br><span class="line">        <span class="keyword">return</span> predict_rating</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    dtype = [(<span class="string">&quot;userId&quot;</span>, np.int32), (<span class="string">&quot;movieId&quot;</span>, np.int32), (<span class="string">&quot;rating&quot;</span>, np.float32)]</span><br><span class="line">    dataset = pd.read_csv(<span class="string">&quot;datasets/ml-latest-small/ratings.csv&quot;</span>, usecols=range(<span class="number">3</span>), dtype=dict(dtype))</span><br><span class="line"></span><br><span class="line">    bcf = BaselineCFBySGD(<span class="number">20</span>, <span class="number">0.1</span>, <span class="number">0.1</span>, [<span class="string">&quot;userId&quot;</span>, <span class="string">&quot;movieId&quot;</span>, <span class="string">&quot;rating&quot;</span>])</span><br><span class="line">    bcf.fit(dataset)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        uid = int(input(<span class="string">&quot;uid: &quot;</span>))</span><br><span class="line">        iid = int(input(<span class="string">&quot;iid: &quot;</span>))</span><br><span class="line">        print(bcf.predict(uid, iid))</span><br></pre></td></tr></table></figure><h6 id="Step-4-准确性指标评估"><a href="#Step-4-准确性指标评估" class="headerlink" title="Step 4: 准确性指标评估"></a>Step 4: 准确性指标评估</h6><ul><li>添加test方法，然后使用之前实现accuary方法计算准确性指标</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">data_split</span>(<span class="params">data_path, x=<span class="number">0.8</span>, random=False</span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    切分数据集， 这里为了保证用户数量保持不变，将每个用户的评分数据按比例进行拆分</span></span><br><span class="line"><span class="string">    :param data_path: 数据集路径</span></span><br><span class="line"><span class="string">    :param x: 训练集的比例，如x=0.8，则0.2是测试集</span></span><br><span class="line"><span class="string">    :param random: 是否随机切分，默认False</span></span><br><span class="line"><span class="string">    :return: 用户-物品评分矩阵</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    print(<span class="string">&quot;开始切分数据集...&quot;</span>)</span><br><span class="line">    <span class="comment"># 设置要加载的数据字段的类型</span></span><br><span class="line">    dtype = &#123;<span class="string">&quot;userId&quot;</span>: np.int32, <span class="string">&quot;movieId&quot;</span>: np.int32, <span class="string">&quot;rating&quot;</span>: np.float32&#125;</span><br><span class="line">    <span class="comment"># 加载数据，我们只用前三列数据，分别是用户ID，电影ID，已经用户对电影的对应评分</span></span><br><span class="line">    ratings = pd.read_csv(data_path, dtype=dtype, usecols=range(<span class="number">3</span>))</span><br><span class="line"></span><br><span class="line">    testset_index = []</span><br><span class="line">    <span class="comment"># 为了保证每个用户在测试集和训练集都有数据，因此按userId聚合</span></span><br><span class="line">    <span class="keyword">for</span> uid <span class="keyword">in</span> ratings.groupby(<span class="string">&quot;userId&quot;</span>).any().index:</span><br><span class="line">        user_rating_data = ratings.where(ratings[<span class="string">&quot;userId&quot;</span>]==uid).dropna()</span><br><span class="line">        <span class="keyword">if</span> random:</span><br><span class="line">            <span class="comment"># 因为不可变类型不能被 shuffle方法作用，所以需要强行转换为列表</span></span><br><span class="line">            index = list(user_rating_data.index)</span><br><span class="line">            np.random.shuffle(index)    <span class="comment"># 打乱列表</span></span><br><span class="line">            _index = round(len(user_rating_data) * x)</span><br><span class="line">            testset_index += list(index[_index:])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 将每个用户的x比例的数据作为训练集，剩余的作为测试集</span></span><br><span class="line">            index = round(len(user_rating_data) * x)</span><br><span class="line">            testset_index += list(user_rating_data.index.values[index:])</span><br><span class="line"></span><br><span class="line">    testset = ratings.loc[testset_index]</span><br><span class="line">    trainset = ratings.drop(testset_index)</span><br><span class="line">    print(<span class="string">&quot;完成数据集切分...&quot;</span>)</span><br><span class="line">    <span class="keyword">return</span> trainset, testset</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">accuray</span>(<span class="params">predict_results, method=<span class="string">&quot;all&quot;</span></span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    准确性指标计算方法</span></span><br><span class="line"><span class="string">    :param predict_results: 预测结果，类型为容器，每个元素是一个包含uid,iid,real_rating,pred_rating的序列</span></span><br><span class="line"><span class="string">    :param method: 指标方法，类型为字符串，rmse或mae，否则返回两者rmse和mae</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">rmse</span>(<span class="params">predict_results</span>):</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        rmse评估指标</span></span><br><span class="line"><span class="string">        :param predict_results:</span></span><br><span class="line"><span class="string">        :return: rmse</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        length = <span class="number">0</span></span><br><span class="line">        _rmse_sum = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> uid, iid, real_rating, pred_rating <span class="keyword">in</span> predict_results:</span><br><span class="line">            length += <span class="number">1</span></span><br><span class="line">            _rmse_sum += (pred_rating - real_rating) ** <span class="number">2</span></span><br><span class="line">        <span class="keyword">return</span> round(np.sqrt(_rmse_sum / length), <span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">mae</span>(<span class="params">predict_results</span>):</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        mae评估指标</span></span><br><span class="line"><span class="string">        :param predict_results:</span></span><br><span class="line"><span class="string">        :return: mae</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        length = <span class="number">0</span></span><br><span class="line">        _mae_sum = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> uid, iid, real_rating, pred_rating <span class="keyword">in</span> predict_results:</span><br><span class="line">            length += <span class="number">1</span></span><br><span class="line">            _mae_sum += abs(pred_rating - real_rating)</span><br><span class="line">        <span class="keyword">return</span> round(_mae_sum / length, <span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">rmse_mae</span>(<span class="params">predict_results</span>):</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        rmse和mae评估指标</span></span><br><span class="line"><span class="string">        :param predict_results:</span></span><br><span class="line"><span class="string">        :return: rmse, mae</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        length = <span class="number">0</span></span><br><span class="line">        _rmse_sum = <span class="number">0</span></span><br><span class="line">        _mae_sum = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> uid, iid, real_rating, pred_rating <span class="keyword">in</span> predict_results:</span><br><span class="line">            length += <span class="number">1</span></span><br><span class="line">            _rmse_sum += (pred_rating - real_rating) ** <span class="number">2</span></span><br><span class="line">            _mae_sum += abs(pred_rating - real_rating)</span><br><span class="line">        <span class="keyword">return</span> round(np.sqrt(_rmse_sum / length), <span class="number">4</span>), round(_mae_sum / length, <span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> method.lower() == <span class="string">&quot;rmse&quot;</span>:</span><br><span class="line">        rmse(predict_results)</span><br><span class="line">    <span class="keyword">elif</span> method.lower() == <span class="string">&quot;mae&quot;</span>:</span><br><span class="line">        mae(predict_results)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> rmse_mae(predict_results)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BaselineCFBySGD</span>(<span class="params">object</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, number_epochs, alpha, reg, columns=[<span class="string">&quot;uid&quot;</span>, <span class="string">&quot;iid&quot;</span>, <span class="string">&quot;rating&quot;</span>]</span>):</span></span><br><span class="line">        <span class="comment"># 梯度下降最高迭代次数</span></span><br><span class="line">        self.number_epochs = number_epochs</span><br><span class="line">        <span class="comment"># 学习率</span></span><br><span class="line">        self.alpha = alpha</span><br><span class="line">        <span class="comment"># 正则参数</span></span><br><span class="line">        self.reg = reg</span><br><span class="line">        <span class="comment"># 数据集中user-item-rating字段的名称</span></span><br><span class="line">        self.columns = columns</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span>(<span class="params">self, dataset</span>):</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        :param dataset: uid, iid, rating</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        self.dataset = dataset</span><br><span class="line">        <span class="comment"># 用户评分数据</span></span><br><span class="line">        self.users_ratings = dataset.groupby(self.columns[<span class="number">0</span>]).agg([list])[[self.columns[<span class="number">1</span>], self.columns[<span class="number">2</span>]]]</span><br><span class="line">        <span class="comment"># 物品评分数据</span></span><br><span class="line">        self.items_ratings = dataset.groupby(self.columns[<span class="number">1</span>]).agg([list])[[self.columns[<span class="number">0</span>], self.columns[<span class="number">2</span>]]]</span><br><span class="line">        <span class="comment"># 计算全局平均分</span></span><br><span class="line">        self.global_mean = self.dataset[self.columns[<span class="number">2</span>]].mean()</span><br><span class="line">        <span class="comment"># 调用sgd方法训练模型参数</span></span><br><span class="line">        self.bu, self.bi = self.sgd()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">sgd</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        利用随机梯度下降，优化bu，bi的值</span></span><br><span class="line"><span class="string">        :return: bu, bi</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="comment"># 初始化bu、bi的值，全部设为0</span></span><br><span class="line">        bu = dict(zip(self.users_ratings.index, np.zeros(len(self.users_ratings))))</span><br><span class="line">        bi = dict(zip(self.items_ratings.index, np.zeros(len(self.items_ratings))))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(self.number_epochs):</span><br><span class="line">            print(<span class="string">&quot;iter%d&quot;</span> % i)</span><br><span class="line">            <span class="keyword">for</span> uid, iid, real_rating <span class="keyword">in</span> self.dataset.itertuples(index=<span class="literal">False</span>):</span><br><span class="line">                error = real_rating - (self.global_mean + bu[uid] + bi[iid])</span><br><span class="line"></span><br><span class="line">                bu[uid] += self.alpha * (error - self.reg * bu[uid])</span><br><span class="line">                bi[iid] += self.alpha * (error - self.reg * bi[iid])</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> bu, bi</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span>(<span class="params">self, uid, iid</span>):</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;评分预测&#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="keyword">if</span> iid <span class="keyword">not</span> <span class="keyword">in</span> self.items_ratings.index:</span><br><span class="line">            <span class="keyword">raise</span> Exception(<span class="string">&quot;无法预测用户&lt;&#123;uid&#125;&gt;对电影&lt;&#123;iid&#125;&gt;的评分，因为训练集中缺失&lt;&#123;iid&#125;&gt;的数据&quot;</span>.format(uid=uid, iid=iid))</span><br><span class="line"></span><br><span class="line">        predict_rating = self.global_mean + self.bu[uid] + self.bi[iid]</span><br><span class="line">        <span class="keyword">return</span> predict_rating</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">test</span>(<span class="params">self,testset</span>):</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;预测测试集数据&#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="keyword">for</span> uid, iid, real_rating <span class="keyword">in</span> testset.itertuples(index=<span class="literal">False</span>):</span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">                pred_rating = self.predict(uid, iid)</span><br><span class="line">            <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">                print(e)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">yield</span> uid, iid, real_rating, pred_rating</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line"></span><br><span class="line">    trainset, testset = data_split(<span class="string">&quot;datasets/ml-latest-small/ratings.csv&quot;</span>, random=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    bcf = BaselineCFBySGD(<span class="number">20</span>, <span class="number">0.1</span>, <span class="number">0.1</span>, [<span class="string">&quot;userId&quot;</span>, <span class="string">&quot;movieId&quot;</span>, <span class="string">&quot;rating&quot;</span>])</span><br><span class="line">    bcf.fit(trainset)</span><br><span class="line"></span><br><span class="line">    pred_results = bcf.test(testset)</span><br><span class="line"></span><br><span class="line">    rmse, mae = accuray(pred_results)</span><br><span class="line"></span><br><span class="line">    print(<span class="string">&quot;rmse: &quot;</span>, rmse, <span class="string">&quot;mae: &quot;</span>, mae)</span><br><span class="line"></span><br></pre></td></tr></table></figure><h4 id="方法二：交替最小二乘法优化"><a href="#方法二：交替最小二乘法优化" class="headerlink" title="方法二：交替最小二乘法优化"></a>方法二：交替最小二乘法优化</h4><p>使用交替最小二乘法优化算法预测Baseline偏置值</p><h6 id="step-1-交替最小二乘法推导"><a href="#step-1-交替最小二乘法推导" class="headerlink" title="step 1: 交替最小二乘法推导"></a>step 1: 交替最小二乘法推导</h6><p>最小二乘法和梯度下降法一样，可以用于求极值。</p><p><strong>最小二乘法思想：对损失函数求偏导，然后再使偏导为0</strong></p><p>同样，损失函数：<br>$$<br>J(\theta)=\sum_{u,i\in R}(r_{ui}-\mu-b_u-b_i)^2 + \lambda*(\sum_u {b_u}^2 + \sum_i {b_i}^2)<br>$$<br>对损失函数求偏导：<br>$$<br>\cfrac{\partial}{\partial b_u} f(b_u, b_i) =-2 \sum_{u,i\in R}(r_{ui}-\mu-b_u-b_i) + 2\lambda * b_u<br>$$<br>令偏导为0，则可得：<br>$$<br>\sum_{u,i\in R}(r_{ui}-\mu-b_u-b_i) = \lambda* b_u<br>\\sum_{u,i\in R}(r_{ui}-\mu-b_i) = \sum_{u,i\in R} b_u+\lambda * b_u<br>$$<br>为了简化公式，这里令$\sum_{u,i\in R} b_u \approx |R(u)|*b_u$，即直接假设每一项的偏置都相等，可得：<br>$$<br>b_u := \cfrac {\sum_{u,i\in R}(r_{ui}-\mu-b_i)}{\lambda_1 + |R(u)|}<br>$$<br>其中$|R(u)|$表示用户$u​$的有过评分数量</p><p>同理可得：<br>$$<br>b_i := \cfrac {\sum_{u,i\in R}(r_{ui}-\mu-b_u)}{\lambda_2 + |R(i)|}<br>$$<br>其中$|R(i)|$表示物品$i​$收到的评分数量</p><p>$b_u$和$b_i​$分别属于用户和物品的偏置，因此他们的正则参数可以分别设置两个独立的参数</p><h6 id="step-2-交替最小二乘法应用"><a href="#step-2-交替最小二乘法应用" class="headerlink" title="step 2: 交替最小二乘法应用"></a>step 2: 交替最小二乘法应用</h6><p>通过最小二乘推导，我们最终分别得到了$b_u$和$b_i$的表达式，但他们的表达式中却又各自包含对方，因此这里我们将利用一种叫交替最小二乘的方法来计算他们的值：    </p><ul><li>计算其中一项，先固定其他未知参数，即看作其他未知参数为已知</li><li>如求$b_u$时，将$b_i$看作是已知；求$b_i$时，将$b_u$看作是已知；如此反复交替，不断更新二者的值，求得最终的结果。这就是<strong>交替最小二乘法（ALS）</strong></li></ul><h6 id="step-3-算法实现"><a href="#step-3-算法实现" class="headerlink" title="step 3: 算法实现"></a>step 3: 算法实现</h6><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BaselineCFByALS</span>(<span class="params">object</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, number_epochs, reg_bu, reg_bi, columns=[<span class="string">&quot;uid&quot;</span>, <span class="string">&quot;iid&quot;</span>, <span class="string">&quot;rating&quot;</span>]</span>):</span></span><br><span class="line">        <span class="comment"># 梯度下降最高迭代次数</span></span><br><span class="line">        self.number_epochs = number_epochs</span><br><span class="line">        <span class="comment"># bu的正则参数</span></span><br><span class="line">        self.reg_bu = reg_bu</span><br><span class="line">        <span class="comment"># bi的正则参数</span></span><br><span class="line">        self.reg_bi = reg_bi</span><br><span class="line">        <span class="comment"># 数据集中user-item-rating字段的名称</span></span><br><span class="line">        self.columns = columns</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span>(<span class="params">self, dataset</span>):</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        :param dataset: uid, iid, rating</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        self.dataset = dataset</span><br><span class="line">        <span class="comment"># 用户评分数据</span></span><br><span class="line">        self.users_ratings = dataset.groupby(self.columns[<span class="number">0</span>]).agg([list])[[self.columns[<span class="number">1</span>], self.columns[<span class="number">2</span>]]]</span><br><span class="line">        <span class="comment"># 物品评分数据</span></span><br><span class="line">        self.items_ratings = dataset.groupby(self.columns[<span class="number">1</span>]).agg([list])[[self.columns[<span class="number">0</span>], self.columns[<span class="number">2</span>]]]</span><br><span class="line">        <span class="comment"># 计算全局平均分</span></span><br><span class="line">        self.global_mean = self.dataset[self.columns[<span class="number">2</span>]].mean()</span><br><span class="line">        <span class="comment"># 调用sgd方法训练模型参数</span></span><br><span class="line">        self.bu, self.bi = self.als()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">als</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        利用随机梯度下降，优化bu，bi的值</span></span><br><span class="line"><span class="string">        :return: bu, bi</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="comment"># 初始化bu、bi的值，全部设为0</span></span><br><span class="line">        bu = dict(zip(self.users_ratings.index, np.zeros(len(self.users_ratings))))</span><br><span class="line">        bi = dict(zip(self.items_ratings.index, np.zeros(len(self.items_ratings))))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(self.number_epochs):</span><br><span class="line">            print(<span class="string">&quot;iter%d&quot;</span> % i)</span><br><span class="line">            <span class="keyword">for</span> iid, uids, ratings <span class="keyword">in</span> self.items_ratings.itertuples(index=<span class="literal">True</span>):</span><br><span class="line">                _sum = <span class="number">0</span></span><br><span class="line">                <span class="keyword">for</span> uid, rating <span class="keyword">in</span> zip(uids, ratings):</span><br><span class="line">                    _sum += rating - self.global_mean - bu[uid]</span><br><span class="line">                bi[iid] = _sum / (self.reg_bi + len(uids))</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> uid, iids, ratings <span class="keyword">in</span> self.users_ratings.itertuples(index=<span class="literal">True</span>):</span><br><span class="line">                _sum = <span class="number">0</span></span><br><span class="line">                <span class="keyword">for</span> iid, rating <span class="keyword">in</span> zip(iids, ratings):</span><br><span class="line">                    _sum += rating - self.global_mean - bi[iid]</span><br><span class="line">                bu[uid] = _sum / (self.reg_bu + len(iids))</span><br><span class="line">        <span class="keyword">return</span> bu, bi</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span>(<span class="params">self, uid, iid</span>):</span></span><br><span class="line">        predict_rating = self.global_mean + self.bu[uid] + self.bi[iid]</span><br><span class="line">        <span class="keyword">return</span> predict_rating</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    dtype = [(<span class="string">&quot;userId&quot;</span>, np.int32), (<span class="string">&quot;movieId&quot;</span>, np.int32), (<span class="string">&quot;rating&quot;</span>, np.float32)]</span><br><span class="line">    dataset = pd.read_csv(<span class="string">&quot;datasets/ml-latest-small/ratings.csv&quot;</span>, usecols=range(<span class="number">3</span>), dtype=dict(dtype))</span><br><span class="line"></span><br><span class="line">    bcf = BaselineCFByALS(<span class="number">20</span>, <span class="number">25</span>, <span class="number">15</span>, [<span class="string">&quot;userId&quot;</span>, <span class="string">&quot;movieId&quot;</span>, <span class="string">&quot;rating&quot;</span>])</span><br><span class="line">    bcf.fit(dataset)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        uid = int(input(<span class="string">&quot;uid: &quot;</span>))</span><br><span class="line">        iid = int(input(<span class="string">&quot;iid: &quot;</span>))</span><br><span class="line">        print(bcf.predict(uid, iid))</span><br></pre></td></tr></table></figure><h6 id="Step-4-准确性指标评估-1"><a href="#Step-4-准确性指标评估-1" class="headerlink" title="Step 4: 准确性指标评估"></a>Step 4: 准确性指标评估</h6><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">data_split</span>(<span class="params">data_path, x=<span class="number">0.8</span>, random=False</span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    切分数据集， 这里为了保证用户数量保持不变，将每个用户的评分数据按比例进行拆分</span></span><br><span class="line"><span class="string">    :param data_path: 数据集路径</span></span><br><span class="line"><span class="string">    :param x: 训练集的比例，如x=0.8，则0.2是测试集</span></span><br><span class="line"><span class="string">    :param random: 是否随机切分，默认False</span></span><br><span class="line"><span class="string">    :return: 用户-物品评分矩阵</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    print(<span class="string">&quot;开始切分数据集...&quot;</span>)</span><br><span class="line">    <span class="comment"># 设置要加载的数据字段的类型</span></span><br><span class="line">    dtype = &#123;<span class="string">&quot;userId&quot;</span>: np.int32, <span class="string">&quot;movieId&quot;</span>: np.int32, <span class="string">&quot;rating&quot;</span>: np.float32&#125;</span><br><span class="line">    <span class="comment"># 加载数据，我们只用前三列数据，分别是用户ID，电影ID，已经用户对电影的对应评分</span></span><br><span class="line">    ratings = pd.read_csv(data_path, dtype=dtype, usecols=range(<span class="number">3</span>))</span><br><span class="line"></span><br><span class="line">    testset_index = []</span><br><span class="line">    <span class="comment"># 为了保证每个用户在测试集和训练集都有数据，因此按userId聚合</span></span><br><span class="line">    <span class="keyword">for</span> uid <span class="keyword">in</span> ratings.groupby(<span class="string">&quot;userId&quot;</span>).any().index:</span><br><span class="line">        user_rating_data = ratings.where(ratings[<span class="string">&quot;userId&quot;</span>]==uid).dropna()</span><br><span class="line">        <span class="keyword">if</span> random:</span><br><span class="line">            <span class="comment"># 因为不可变类型不能被 shuffle方法作用，所以需要强行转换为列表</span></span><br><span class="line">            index = list(user_rating_data.index)</span><br><span class="line">            np.random.shuffle(index)    <span class="comment"># 打乱列表</span></span><br><span class="line">            _index = round(len(user_rating_data) * x)</span><br><span class="line">            testset_index += list(index[_index:])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 将每个用户的x比例的数据作为训练集，剩余的作为测试集</span></span><br><span class="line">            index = round(len(user_rating_data) * x)</span><br><span class="line">            testset_index += list(user_rating_data.index.values[index:])</span><br><span class="line"></span><br><span class="line">    testset = ratings.loc[testset_index]</span><br><span class="line">    trainset = ratings.drop(testset_index)</span><br><span class="line">    print(<span class="string">&quot;完成数据集切分...&quot;</span>)</span><br><span class="line">    <span class="keyword">return</span> trainset, testset</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">accuray</span>(<span class="params">predict_results, method=<span class="string">&quot;all&quot;</span></span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    准确性指标计算方法</span></span><br><span class="line"><span class="string">    :param predict_results: 预测结果，类型为容器，每个元素是一个包含uid,iid,real_rating,pred_rating的序列</span></span><br><span class="line"><span class="string">    :param method: 指标方法，类型为字符串，rmse或mae，否则返回两者rmse和mae</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">rmse</span>(<span class="params">predict_results</span>):</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        rmse评估指标</span></span><br><span class="line"><span class="string">        :param predict_results:</span></span><br><span class="line"><span class="string">        :return: rmse</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        length = <span class="number">0</span></span><br><span class="line">        _rmse_sum = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> uid, iid, real_rating, pred_rating <span class="keyword">in</span> predict_results:</span><br><span class="line">            length += <span class="number">1</span></span><br><span class="line">            _rmse_sum += (pred_rating - real_rating) ** <span class="number">2</span></span><br><span class="line">        <span class="keyword">return</span> round(np.sqrt(_rmse_sum / length), <span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">mae</span>(<span class="params">predict_results</span>):</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        mae评估指标</span></span><br><span class="line"><span class="string">        :param predict_results:</span></span><br><span class="line"><span class="string">        :return: mae</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        length = <span class="number">0</span></span><br><span class="line">        _mae_sum = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> uid, iid, real_rating, pred_rating <span class="keyword">in</span> predict_results:</span><br><span class="line">            length += <span class="number">1</span></span><br><span class="line">            _mae_sum += abs(pred_rating - real_rating)</span><br><span class="line">        <span class="keyword">return</span> round(_mae_sum / length, <span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">rmse_mae</span>(<span class="params">predict_results</span>):</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        rmse和mae评估指标</span></span><br><span class="line"><span class="string">        :param predict_results:</span></span><br><span class="line"><span class="string">        :return: rmse, mae</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        length = <span class="number">0</span></span><br><span class="line">        _rmse_sum = <span class="number">0</span></span><br><span class="line">        _mae_sum = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> uid, iid, real_rating, pred_rating <span class="keyword">in</span> predict_results:</span><br><span class="line">            length += <span class="number">1</span></span><br><span class="line">            _rmse_sum += (pred_rating - real_rating) ** <span class="number">2</span></span><br><span class="line">            _mae_sum += abs(pred_rating - real_rating)</span><br><span class="line">        <span class="keyword">return</span> round(np.sqrt(_rmse_sum / length), <span class="number">4</span>), round(_mae_sum / length, <span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> method.lower() == <span class="string">&quot;rmse&quot;</span>:</span><br><span class="line">        rmse(predict_results)</span><br><span class="line">    <span class="keyword">elif</span> method.lower() == <span class="string">&quot;mae&quot;</span>:</span><br><span class="line">        mae(predict_results)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> rmse_mae(predict_results)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BaselineCFByALS</span>(<span class="params">object</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, number_epochs, reg_bu, reg_bi, columns=[<span class="string">&quot;uid&quot;</span>, <span class="string">&quot;iid&quot;</span>, <span class="string">&quot;rating&quot;</span>]</span>):</span></span><br><span class="line">        <span class="comment"># 梯度下降最高迭代次数</span></span><br><span class="line">        self.number_epochs = number_epochs</span><br><span class="line">        <span class="comment"># bu的正则参数</span></span><br><span class="line">        self.reg_bu = reg_bu</span><br><span class="line">        <span class="comment"># bi的正则参数</span></span><br><span class="line">        self.reg_bi = reg_bi</span><br><span class="line">        <span class="comment"># 数据集中user-item-rating字段的名称</span></span><br><span class="line">        self.columns = columns</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span>(<span class="params">self, dataset</span>):</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        :param dataset: uid, iid, rating</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        self.dataset = dataset</span><br><span class="line">        <span class="comment"># 用户评分数据</span></span><br><span class="line">        self.users_ratings = dataset.groupby(self.columns[<span class="number">0</span>]).agg([list])[[self.columns[<span class="number">1</span>], self.columns[<span class="number">2</span>]]]</span><br><span class="line">        <span class="comment"># 物品评分数据</span></span><br><span class="line">        self.items_ratings = dataset.groupby(self.columns[<span class="number">1</span>]).agg([list])[[self.columns[<span class="number">0</span>], self.columns[<span class="number">2</span>]]]</span><br><span class="line">        <span class="comment"># 计算全局平均分</span></span><br><span class="line">        self.global_mean = self.dataset[self.columns[<span class="number">2</span>]].mean()</span><br><span class="line">        <span class="comment"># 调用sgd方法训练模型参数</span></span><br><span class="line">        self.bu, self.bi = self.als()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">als</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        利用随机梯度下降，优化bu，bi的值</span></span><br><span class="line"><span class="string">        :return: bu, bi</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="comment"># 初始化bu、bi的值，全部设为0</span></span><br><span class="line">        bu = dict(zip(self.users_ratings.index, np.zeros(len(self.users_ratings))))</span><br><span class="line">        bi = dict(zip(self.items_ratings.index, np.zeros(len(self.items_ratings))))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(self.number_epochs):</span><br><span class="line">            print(<span class="string">&quot;iter%d&quot;</span> % i)</span><br><span class="line">            <span class="keyword">for</span> iid, uids, ratings <span class="keyword">in</span> self.items_ratings.itertuples(index=<span class="literal">True</span>):</span><br><span class="line">                _sum = <span class="number">0</span></span><br><span class="line">                <span class="keyword">for</span> uid, rating <span class="keyword">in</span> zip(uids, ratings):</span><br><span class="line">                    _sum += rating - self.global_mean - bu[uid]</span><br><span class="line">                bi[iid] = _sum / (self.reg_bi + len(uids))</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> uid, iids, ratings <span class="keyword">in</span> self.users_ratings.itertuples(index=<span class="literal">True</span>):</span><br><span class="line">                _sum = <span class="number">0</span></span><br><span class="line">                <span class="keyword">for</span> iid, rating <span class="keyword">in</span> zip(iids, ratings):</span><br><span class="line">                    _sum += rating - self.global_mean - bi[iid]</span><br><span class="line">                bu[uid] = _sum / (self.reg_bu + len(iids))</span><br><span class="line">        <span class="keyword">return</span> bu, bi</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span>(<span class="params">self, uid, iid</span>):</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;评分预测&#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="keyword">if</span> iid <span class="keyword">not</span> <span class="keyword">in</span> self.items_ratings.index:</span><br><span class="line">            <span class="keyword">raise</span> Exception(<span class="string">&quot;无法预测用户&lt;&#123;uid&#125;&gt;对电影&lt;&#123;iid&#125;&gt;的评分，因为训练集中缺失&lt;&#123;iid&#125;&gt;的数据&quot;</span>.format(uid=uid, iid=iid))</span><br><span class="line"></span><br><span class="line">        predict_rating = self.global_mean + self.bu[uid] + self.bi[iid]</span><br><span class="line">        <span class="keyword">return</span> predict_rating</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">test</span>(<span class="params">self,testset</span>):</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;预测测试集数据&#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="keyword">for</span> uid, iid, real_rating <span class="keyword">in</span> testset.itertuples(index=<span class="literal">False</span>):</span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">                pred_rating = self.predict(uid, iid)</span><br><span class="line">            <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">                print(e)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">yield</span> uid, iid, real_rating, pred_rating</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    trainset, testset = data_split(<span class="string">&quot;datasets/ml-latest-small/ratings.csv&quot;</span>, random=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    bcf = BaselineCFByALS(<span class="number">20</span>, <span class="number">25</span>, <span class="number">15</span>, [<span class="string">&quot;userId&quot;</span>, <span class="string">&quot;movieId&quot;</span>, <span class="string">&quot;rating&quot;</span>])</span><br><span class="line">    bcf.fit(trainset)</span><br><span class="line"></span><br><span class="line">    pred_results = bcf.test(testset)</span><br><span class="line"></span><br><span class="line">    rmse, mae = accuray(pred_results)</span><br><span class="line"></span><br><span class="line">    print(<span class="string">&quot;rmse: &quot;</span>, rmse, <span class="string">&quot;mae: &quot;</span>, mae)</span><br></pre></td></tr></table></figure><p>函数求导：</p><p><img src="/img/%E5%B8%B8%E8%A7%81%E5%87%BD%E6%95%B0%E6%B1%82%E5%AF%BC.png"></p><p><img src="/img/%E5%AF%BC%E6%95%B0%E7%9A%84%E5%9B%9B%E5%88%99%E8%BF%90%E7%AE%97.png"></p>]]></content>
      
      
      <categories>
          
          <category> Kafka </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 电影推荐系统算法综合实战 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>消息系统-Kafka(3)：消费者</title>
      <link href="2019/02/13/%E6%B6%88%E6%81%AF%E7%B3%BB%E7%BB%9F-Kafka(3)%EF%BC%9A%E6%B6%88%E8%B4%B9%E8%80%85/"/>
      <url>2019/02/13/%E6%B6%88%E6%81%AF%E7%B3%BB%E7%BB%9F-Kafka(3)%EF%BC%9A%E6%B6%88%E8%B4%B9%E8%80%85/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><h1 id="1-消费者和消费者组"><a href="#1-消费者和消费者组" class="headerlink" title="1.消费者和消费者组"></a>1.消费者和消费者组</h1><h2 id="1-1-概述"><a href="#1-1-概述" class="headerlink" title="1.1. 概述"></a>1.1. 概述</h2><p>Kafka 消费者从属于消费者组。一个消费者组里的消费者订阅的为同一主题。每个消费者接受主题一部分分区的消息。</p><p><strong>向消费者组里增加消费者是横向伸缩消费能力的主要方式</strong>。Kafka 消费者在做一些高延迟的操作，如向HDFS或数据库中写入数据，或者使用数据进行一些耗时的计算操作。在这些情况下，单个消费者无法跟上数据的生成速度，此时我们可以增加消费者，以分担负载，每个消费者只处理部分分区上的消息，这就是横向伸缩的主要手段。我们有必要为主题创建大量分区，负载增长时可以增加更多的消费者</p><p><font color = 'red'><strong>注意：</strong> 一个主题的同一个分区同时只能供同一个消费者组里的一个消费者消费数据，因此不要让消费者的数量超过主题分区的数量，多于的消费者只会被闲置。</font></p><h2 id="1-2-消费方式"><a href="#1-2-消费方式" class="headerlink" title="1.2. 消费方式"></a>1.2. 消费方式</h2><p>由于 Push 模式 很难适应消费速率不同的消费者，因为消息发送速率是由 broker 决定的。它的目标是尽可能的以最快的速度传递消息，但是这样容易造成 consumer 来不及处理消息，典型的表现就是拒绝服务以及网络拥塞。 所以 kafka 采用 pull 模式，根据消费者的能力以适当的速率消费消息。</p><p><strong>Pull 模式的不足之处是如果 kafka 中没有数据，消费者可能会陷入循环中，一直返回空数据，针对这一点，Kafka 的消费者在消费数据时会传入一个时长参数 timeout，如果当前没有数据可供消费，consumer 会等待一段时间在返回，这段时长即为 timeout。</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> ConsumerRecords&lt;K, V&gt; <span class="title">poll</span><span class="params">(<span class="keyword">final</span> Duration timeout)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> poll(timeout.toMillis(), <span class="keyword">true</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="1-3-特定偏移处消费数据"><a href="#1-3-特定偏移处消费数据" class="headerlink" title="1.3. 特定偏移处消费数据"></a>1.3. 特定偏移处消费数据</h2><p>使用 poll() 开始消费每个分区中最后一个已经发生的偏移量的消息，并继续按顺序处理所有消息。但是，有时我们肯想要以不同的偏移量开始阅读。如果你想从分区的开头读取所有消息，或者你想要一直跳到分区的末尾并开始只消费新消息，那么可以使用有一些专门的API：seekToBeginning(TopicPartition tp)和seekToEnd(TopicPartition tp)。</p><h2 id="1-4-独立消费者"><a href="#1-4-独立消费者" class="headerlink" title="1.4. 独立消费者"></a>1.4. 独立消费者</h2><p>单一的消费者总是需要从主题中的所有分区读取数据，或者从一个主题特定分区读取数据。在这种情况下没有理由需要组或负载均衡，只是订阅特定的主题或分区，偶尔使用消息和提交偏移量。</p><h2 id="1-5-多线程消费者"><a href="#1-5-多线程消费者" class="headerlink" title="1.5. 多线程消费者"></a>1.5. 多线程消费者</h2><p>KafkaProducer是线程安全的，而 KafkaConsumer 是非线程安全的，多线程需要处理好线程同步，多线程的实现方式有多种，这里介绍一种：<strong>每个线程各自实例化一个KakfaConsumer对象</strong>，这种方式的缺点是：当这些线程属于同一个消费组时，线程的数量受限于分区数，当消费者线程的数量大于分区数时，就有一部分消费线程一直处于空闲状态</p><h2 id="1-6-退出"><a href="#1-6-退出" class="headerlink" title="1.6. 退出"></a>1.6. 退出</h2><p>如果确定要退出循环，需要通过另一个线程调用 consumer.wakeup() 方法。</p><p>如果循环运行在主线程里，可以在 ShutdownHook 里调用该方法。consumer.wakeup() 是消费者唯一一个可以从其他线程里安全调用的方法。</p><p>ShutdownHook运行在单独的线程里，所以退出循环最安全的方式只能是调用 consumer.wakeup()</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Runtime.getRuntime().addShutdownHook(<span class="keyword">new</span> Thread() &#123;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    System.out.println(<span class="string">&quot;Starting exit...&quot;</span>);</span><br><span class="line">    consumer.wakeup();</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      mainThread.join();</span><br><span class="line">    &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">      e.printStackTrace();</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure><p>调用 consumer.wakeup() 可以退出 poll() ，并抛出 WakeupException异常，或者如果调用consumer.wakeup() 时线程没有等待轮询，那么异常将在下一轮调用 poll() 时抛出。不需要处理WakeupException,因为它只是用于跳出循环的一种方式。不过，在退出线程之前调用 consumer.close()是很有必要的，它会提交任何还没有提交的东西，并向群组协调器发送消息，告知自己要离开群组，接下来就会触发再均衡，而不需要等待会话超时。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">try</span>&#123;</span><br><span class="line">  <span class="keyword">while</span>(<span class="keyword">true</span>) &#123;</span><br><span class="line">    ConsumerRecords records = movingAvg.consumer.poll(<span class="number">1000</span>);</span><br><span class="line">    System.out.println(System.currentTimeMillis() + <span class="string">&quot; -- waiting for data...&quot;</span>);</span><br><span class="line">    <span class="keyword">for</span>(ConsumerRecord record : records) &#123;</span><br><span class="line">      System.out.printf(<span class="string">&quot;offset = %d, key = %s, value = %s\n&quot;</span>,</span><br><span class="line">                        record.offset(), record.key(), record.value());</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">for</span>(TopicPartition tp: consumer.assignment())&#123;</span><br><span class="line">    System.out.println(<span class="string">&quot;Committing offset at position:&quot;</span> + consumer.position(tp));</span><br><span class="line">    &#125;</span><br><span class="line">    movingAvg.consumer.commitSync();</span><br><span class="line">  &#125;</span><br><span class="line">&#125; <span class="keyword">catch</span>(WakeupException e) &#123;</span><br><span class="line">    <span class="comment">// ignore for shutdown</span></span><br><span class="line">&#125; <span class="keyword">finally</span>&#123;</span><br><span class="line">  <span class="comment">//在退出之前，确保你已经完全关闭了消费者</span></span><br><span class="line">    consumer.close();</span><br><span class="line">    System.out.println(<span class="string">&quot;Closed consumer and we are done&quot;</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="2-分区分配"><a href="#2-分区分配" class="headerlink" title="2. 分区分配"></a>2. 分区分配</h1><p>群组里的消费者共同读取主题的分区，一个新的消费者加入群组时，它读取的是原本由其他消费者读取的消息。</p><p>当一个消费者被关闭或发生崩溃时，它就离开群组，原本由它读取的分区将由群组里的其他消费者来读取。在主题发生变化时，比如管理员添加了新的分区，会发生分区重分配。</p><h2 id="3-1-再均衡"><a href="#3-1-再均衡" class="headerlink" title="3.1. 再均衡"></a>3.1. 再均衡</h2><h3 id="3-1-1-概述"><a href="#3-1-1-概述" class="headerlink" title="3.1.1. 概述"></a>3.1.1. 概述</h3><p><font color = 'blue'><strong>分区的所有权从一个消费者转移到另一个消费者，这样的行为被称为再均衡</strong></font></p><p>再均衡为消费者群组带来了<strong>高可用</strong>和<strong>伸缩性。</strong></p><p>再均衡期间，消费者无法读取消息，造成整个群组一小段时间内的不可用。此外，当分区被重新分配给另一个消费者时，消费者当前的读取状态会丢失，它还有可能还需要去刷新缓存，在它重新恢复状态之前会拖慢应用程序。<br>当消费者要加入群组时，会向群组协调器发送一个JoinGroup请求。第一个加入群组的消费者将成为群主。群主从协调器那里获取群组的成员列表，列表包含了最近发送过心跳的消费者，并负责给每一个消费者分配分区。<br>PartitionAssignor 接口的类决定哪些分区被分配给哪些消费者</p><p>在Kafka中，当有新消费者加入或者订阅的topic数发生变化时，会触发Rebalance(再均衡：在同一个消费者组当中，分区的所有权从一个消费者转移到另外一个消费者)机制，Rebalance顾名思义就是重新均衡消费者消费。Rebalance的过程如下：</p><p>第一步：所有成员都向coordinator发送请求，请求入组。一旦所有成员都发送了请求，coordinator会从中选择一个consumer担任leader的角色，并把组成员信息以及订阅信息发给leader。<br>第二步：leader开始分配消费方案，指明具体哪个consumer负责消费哪些topic的哪些partition。一旦完成分配，leader会将这个方案发给coordinator。coordinator接收到分配方案之后会把方案发给各个consumer，这样组内的所有成员就都知道自己应该消费哪些分区了。</p><h3 id="3-1-2-条件"><a href="#3-1-2-条件" class="headerlink" title="3.1.2. 条件"></a>3.1.2. 条件</h3><ol><li><p>消费者组中新<strong>添加消费者</strong>读取到原本是其他消费者读取的消息</p></li><li><p><strong>消费者关闭或崩溃</strong>之后离开群组，原本由他读取的  partition 将由群组里其他消费者读取</p></li><li><p>当向一个 Topic <strong>添加新的 partition</strong>，会发生 partition 在消费者中的重新分配</p></li></ol><h3 id="3-1-3-再均衡监听器"><a href="#3-1-3-再均衡监听器" class="headerlink" title="3.1.3.  再均衡监听器"></a>3.1.3.  再均衡监听器</h3><p>在为消费者分配新的partition或者移除旧的partition时，可以通过消费者API执行一些应用程序代码，在使用<strong>subscribe</strong>()方法时传入一个<strong>ConsumerRebalanceListener</strong>实例。</p><p><strong>ConsumerRebalanceListener</strong>需要实现的两个方法</p><ol><li><p><strong>public void onPartitionRevoked(Collection<TopicPartition> partitions)</strong></p><p>该方法会在<strong>再均衡开始之前</strong>和<strong>消费者停止读取消息之后</strong>被调用。如果在这里提交偏移量，下一个接管partition的消费者就知道该从哪里开始读取了。</p></li><li><p><strong>public void onPartitionAssigned(Collection<TopicPartition> partitions)</strong></p><p>该方法会在<strong>重新分配partition之后</strong>和<strong>消费者开始读取消息之前</strong>被调用。</p></li></ol><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> Map&lt;TopicPartition, OffsetAndMetadata&gt; currentOffsets = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line"><span class="keyword">private</span> <span class="class"><span class="keyword">class</span> <span class="title">HandleRebalance</span> <span class="keyword">implements</span> <span class="title">ConsumerRebalanceListener</span> </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onPartitionsAssigned</span><span class="params">(Collection&lt;TopicPartition&gt; partitions)</span> </span>&#123;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onPartitionsRevoked</span><span class="params">(Collection&lt;TopicPartition&gt; partitions)</span> </span>&#123;</span><br><span class="line">       <span class="comment">//如果发生再均衡，要在即将失去partition所有权时提交偏移量。</span></span><br><span class="line">       <span class="comment">//调用commitSync方法，确保在再均衡发生之前提交偏移量</span></span><br><span class="line">        consumer.commitSync(currentOffsets);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="keyword">try</span>&#123;</span><br><span class="line">      consumer.subscribe(topics, <span class="keyword">new</span> HandleRebalance());</span><br><span class="line">      <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">          ConsumerRecords&lt;String, String&gt; records = consumer.poll(<span class="number">100</span>);</span><br><span class="line">          <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; record : records) &#123;</span><br><span class="line">              currentOffsets.put(<span class="keyword">new</span> TopicPartition(record.topic(), record.partition()), </span><br><span class="line">                                 <span class="keyword">new</span> OffsetAndMetadata(record.offset() + <span class="number">1</span>, “no matadata”));</span><br><span class="line">          &#125;</span><br><span class="line">          consumer.commitAsync(currentOffsets, <span class="keyword">null</span>);</span><br><span class="line">  &#125; <span class="keyword">catch</span>(WakeupException e) &#123;</span><br><span class="line">      <span class="comment">//忽略异常，正在关闭消费者</span></span><br><span class="line">  &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">      log.error(<span class="string">&quot;unexpected error&quot;</span>, e);</span><br><span class="line">  &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">      <span class="keyword">try</span>&#123;</span><br><span class="line">          consumer.commitSync(currentOffsets);</span><br><span class="line">      &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">          consumer.close();</span><br><span class="line">      &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><h2 id="3-2-分区分配"><a href="#3-2-分区分配" class="headerlink" title="3.2. 分区分配"></a>3.2. 分区分配</h2><h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><p>在 Kafka 中，存在着三种分区分配策略。一种是 RangeAssignor 分配策略(范围分区)，另一种是 RoundRobinAssignor 分配策略(轮询分区)。默认采用 Range 范围分区。</p><p>Kafka 提供了消费者客户端参数 partition.assignment.strategy 用来设置消费者与订阅主题之间的分区分配策略。默认情况下，此参数的值为：org.apache.kafka.clients.consumer.RangeAssignor，即采用 RangeAssignor 分配策略。</p><p><img src="https://img-blog.csdnimg.cn/20200105211408449.png" alt="在这里插入图片描述"></p><ol><li><p><strong>RangeAssignor</strong></p><p><strong>RangeAssignor 策略的原理是按照消费者总数和分区总数进行整除运算来获得一个跨度，然后将分区按照跨度进行平均分配，以保证分区尽可能均匀地分配给所有的消费者。对于每一个topic，RangeAssignor 策略会将消费组内所有订阅这个 topic 的消费者按照名称的字典序排序，然后为每个消费者划分固定的分区范围，如果不够平均分配，那么字典序靠前的消费者会被多分配一个分区。</strong></p><ul><li>topic下的所有有效分区平铺</li><li>消费者按照字典排序</li><li>分区数除以消费者数，得到 n</li><li>分区数对消费者数取余，得到 m</li><li>消费者集合中，前 m 个消费者能够分配到 n+1 个分区，而剩余的消费者只能分配到 n 个分区</li></ul></li><li><p><strong>RoundRobinAssignor</strong></p><p><strong>RoundRobin 轮询分区策略，是把所有的 partition 和所有的 consumer 都列出来，然后按照 hashcode 进行排序，最后通过轮询算法来分配 partition 给到各个消费者。</strong></p><ul><li>消费者按照字典排序，例如C0, C1, C2… …，并构造环形迭代器。</li><li>topic 名称按照字典排序，并得到每个 topic 的所有分区，从而得到所有分区集合。</li><li>遍历第2步所有分区集合，同时轮询消费者。</li><li>如果轮询到的消费者订阅的topic不包括当前遍历的分区所属topic，则跳过；否则分配给当前消费者，并继续第3步。</li></ul></li><li><p><strong>StickyAssignor</strong></p><p>Kafka 从 0.11.x 版本开始引入 StickyAssignor 分配策略，它主要有两个目的：分区的分配要尽可能的均匀和分区的分配尽可能的与上次分配的保持相同。当两者发生冲突时，第一个目标优先于第二个目标。鉴于这两个目标，StickyAssignor 策略的具体实现要比 RangeAssignor 和RoundRobinAssignor 这两种分配策略要复杂很多。</p></li></ol><h2 id="4-提交和偏移量-Offset"><a href="#4-提交和偏移量-Offset" class="headerlink" title="4. 提交和偏移量 Offset"></a>4. 提交和偏移量 Offset</h2><p>调用 poll() 时，返回由生产者写入 Kafka 但还没有被消费者读取过的记录</p><h4 id="Offset-的维护"><a href="#Offset-的维护" class="headerlink" title="Offset 的维护"></a>Offset 的维护</h4><p>由于 consumer 在消费过程中可能会出现断电等故障，consumer恢复之后，需要从故障前的位置继续消费，所以 consumer 需要记录自己消费位置，以便故障恢复后继续消费。 </p><p>Kafka 0.9 版本之前， comsumer 默认将 offset 保存在 Zookeeper中</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 3] ls /</span><br><span class="line">[cluster, controller, brokers, zookeeper, admin, isr_change_notification, dubbo, log_dir_event_notification, controller_epoch, kafka-manager, consumers, hive_zookeeper_namespace_hive, latest_producer_id_block, config, hbase]</span><br><span class="line"></span><br><span class="line">[zk: localhost:2181(CONNECTED) 16] ls /consumers/console-consumer-37662/offsets/test_kafka</span><br><span class="line">[44, 45, 46, 47, 48, 49, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43]</span><br><span class="line"></span><br><span class="line">[zk: localhost:2181(CONNECTED) 19] ls /consumers/console-consumer-37662/offsets/test_kafka/0</span><br><span class="line">[]</span><br></pre></td></tr></table></figure><p>从0.9 版本开始， consumer 默认将 offset 保存在 kafka __consumer_offsets主题中。</p><img src="https://img-blog.csdnimg.cn/20200105165739847.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTA4MDk4NA==,size_13,color_FFFFFF,t_70" alt="在这里插入图片描述" style="zoom:50%;" /><h4 id="2-2-自动提交偏移量"><a href="#2-2-自动提交偏移量" class="headerlink" title="2.2. 自动提交偏移量"></a>2.2. 自动提交偏移量</h4><p>将enable.auto.commit 设为true，则每过5秒，消费者会自动把从 poll() 方法接受到的最大偏移量提交上去。提交时间由auto.commit.interval.ms控制，默认值为5秒</p><ul><li><p>自动提交偏移量不足</p><p>假设我们使用默认的5秒提交时间间隔，在最近一次提交之后的3秒发生了再均衡，再均衡之后，消费者从最后一次提交的偏移量位置开始读取消息。这个时候偏移量已经落后了3秒，所以这3秒内到达的消息将会被重复处理。<br>**<font color='red'>注：可以通过修改提交时间间隔来频繁提交偏移量，减少可能出现重复消息的时间窗，不过这种情况无法完全避免。</font>**</p><h4 id="2-3-手动提交偏移量"><a href="#2-3-手动提交偏移量" class="headerlink" title="2.3. 手动提交偏移量"></a>2.3. 手动提交偏移量</h4><p>把 enable.auto.commit 设为 false，让应用程序决定何时提交偏移量。使用 commitSync() 提交偏移量最简单也最可靠。</p></li></ul><h2 id="5-总结"><a href="#5-总结" class="headerlink" title="5.总结"></a>5.总结</h2><h4 id="Kafka-如何保证消息有序"><a href="#Kafka-如何保证消息有序" class="headerlink" title="Kafka 如何保证消息有序 ?"></a><font color='blue'>Kafka 如何保证消息有序 ?</font></h4><p>kafka 中的每个 partition 中的消息在写入时都是有序的，而且单独一个 partition 只能由一个消费者去消费，可以在里面保证消息的顺序性。但是分区之间的消息是不保证有序的。</p><p>两种方案</p><ol><li><p>kafka topic 只设置一个 partition 分区 </p><p>kafka 默认保证同一个 partition 分区内的消息是有序的，则可以设置topic只使用一个分区，这样消息就是全局有序，缺点是只能被consumer group里的一个消费者消费，降低了性能，不适用高并发的情况</p></li><li><p>producer 将消息发送到指定 partition 分区</p><p>既然 kafka 默认保证同一个 partition 分区内的消息是有序的，则 producer 可以在发送消息时可以指定需要保证顺序的几条消息发送到同一个分区，这样消费者消费时，消息就是有序。</p></li></ol><h5 id="Kafka-缺点-？"><a href="#Kafka-缺点-？" class="headerlink" title="Kafka 缺点 ？"></a><font color='blue'>Kafka 缺点 ？</font></h5><ol><li>由于是批量发送，数据并非真正的实时；</li><li>对于mqtt协议不支持；</li><li>不支持物联网传感数据直接接入；</li><li>仅支持统一分区内消息有序，无法实现全局消息有序；</li><li>监控不完善，需要安装插件；</li><li>依赖zookeeper进行元数据管理；</li></ol>]]></content>
      
      
      <categories>
          
          <category> Kafka </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kafka 消息系统源码深度剖析 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>消息系统-Kafka(1)：初识Kafka</title>
      <link href="2019/02/12/%E6%B6%88%E6%81%AF%E7%B3%BB%E7%BB%9F-Kafka(1)%EF%BC%9A%E5%88%9D%E8%AF%86Kafka/"/>
      <url>2019/02/12/%E6%B6%88%E6%81%AF%E7%B3%BB%E7%BB%9F-Kafka(1)%EF%BC%9A%E5%88%9D%E8%AF%86Kafka/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p>一个消息系统负责将数据从一个应用传递到另外一个应用，应用只需关注于数据，无需关注数据在两个或多个应用间是如何传递的。</p><a id="more"></a><h1 id="1-消息系统"><a href="#1-消息系统" class="headerlink" title="1.消息系统"></a>1.消息系统</h1><p>一个消息系统负责将数据从一个应用传递到另外一个应用，应用只需关注于数据，无需关注数据在两个或多个应用间是如何传递的。</p><ol><li><h4 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h4><p>分布式消息传递基于可靠的消息队列，在客户端应用和消息系统之间异步传递消息。有两种主要的消息传递模式：<strong>点对点传递模式、发布-订阅模式</strong>。大部分的消息系统选用发布-订阅模式。<strong>Kafka就是一种发布-订阅模式</strong>。</p></li><li><h4 id="点对点传递模式"><a href="#点对点传递模式" class="headerlink" title="点对点传递模式"></a>点对点传递模式</h4><p>在点对点消息系统中，消息持久化到一个队列中。此时，将有一个或多个消费者消费队列中的数据。但是一条消息只能被消费一次。当一个消费者消费了队列中的某条数据之后，该条数据则从消息队列中删除。该模式即使有多个消费者同时消费数据，也能保证数据处理的顺序。</p></li><li><h4 id="发布-订阅模式"><a href="#发布-订阅模式" class="headerlink" title="发布-订阅模式"></a>发布-订阅模式</h4><p>在发布-订阅消息系统中，消息被持久化到一个 topic 中。与点对点消息系统不同的是，消费者可以订阅一个或多个 topic，消费者可以消费该topic中所有的数据，同一条数据可以被多个消费者消费，数据被消费后不会立马删除。在发布-订阅消息系统中，消息的生产者称为发布者，消费者称为订阅者。</p></li></ol><h1 id="2-Kafka"><a href="#2-Kafka" class="headerlink" title="2.Kafka"></a>2.Kafka</h1><h2 id="2-1-概述"><a href="#2-1-概述" class="headerlink" title="2.1.概述"></a>2.1.概述</h2><p>Kafka 是<strong>分布式发布-订阅消息系统</strong>，它最初是由 LinkedIn 公司开发的，之后成为 Apache 项目的一部分，<strong>Kafka是一个分布式，可划分的，冗余备份的持久性的日志服务，它主要用于处理流式数据</strong>。</p><h2 id="2-2-特征"><a href="#2-2-特征" class="headerlink" title="2.2. 特征"></a>2.2. 特征</h2><ol><li><p><strong>支持多个生产者</strong></p><p>Kafka 可以无缝地支持多个生产者，不管客户端在使用单个主题还是多个主题。所以它很适合用来从多个前端系统收集数据，并以统一的格式对外提供数据。例如，一个包含了多个微服务的网站，可以为页面视图创建一个单独的主题，所有服务都以相同的消息格式向该主题写入数据。消费者应用程序会获得统一的页面视图，而无需协调来自不同生产者的数据流。</p></li><li><p><strong>支持多个消费者</strong></p><p>Kafka 也支持多个消费者从一个单独的消息流上读取数据，而且消费者之间直不影响。这与其他队列系统不同，其他队列系统的消息一旦被一个客户端读取，其他客户端就无法再读取它。另外，多个消费者可以组成一个群组，它们共享一个悄息流，并保证整个群组对每个给定的消息只处理一次。</p></li><li><p><strong>基于磁盘存储</strong></p><p>消费者可能会因为处理速度慢或突发的流量高峰导致无陆及时读取消息，而持久化数据可以保证数据不会丢失。消费者可以在进行应用程序维护时离线一小段时间，而无需担心消息丢失或堵塞在生产者端。消费者可以被关闭，但消息会继续保留在Kafka 里。消费者可以从上次中断的地方继续处理消息。</p></li><li><p><strong>可伸缩，高性能</strong></p><p>通过横向扩展生产者、消费者和 broker, Kafka 可以轻松处理巨大的消息流。在处理大量数据的同时，它还能保证亚秒级的消息延迟。</p></li></ol><h2 id="2-3-应用场景"><a href="#2-3-应用场景" class="headerlink" title="2.3.应用场景"></a>2.3.应用场景</h2><ul><li><p><strong>日志收集</strong></p><blockquote><p>一个公司可以用 Kafka 可以收集各种服务的 log，通过 Kafka 以统一接口服务的方式开放给各种consumer，例如 Hadoop、Hbase 等</p></blockquote></li><li><p><strong>消息系统</strong></p><blockquote><p>解耦和生产者和消费者、缓存消息等</p></blockquote></li><li><p><strong>用户活动跟踪</strong></p><blockquote><p>Kafka 经常被用来记录 web 用户或者 app 用户的各种活动，如浏览网页、搜索、点击等活动，这些活动信息被各个服务器发布到 Kafka 的 topic 中，然后订阅者通过订阅这些 topic 来做实时的监控分析，或者装载到 Hadoop、数据仓库中做离线分析和挖掘</p></blockquote></li><li><p><strong>运营指标</strong></p><blockquote><p>Kafka 也经常用来记录运营监控数据。包括收集各种分布式应用的数据，生产各种操作的集中反馈，比如报警和报告</p></blockquote></li><li><p><strong>流式处理</strong></p><blockquote><p>比如 spark streaming 和 storm</p></blockquote></li></ul><h1 id="3-架构"><a href="#3-架构" class="headerlink" title="3. 架构"></a>3. 架构</h1><p>一个典型的 Kafka 体系架构包括若干Producer [可以是服务器日志，业务数据，页面前端产生的page view等等]，若干broker [Kafka支持水平扩展，一般broker数量越多，集群吞吐率越高]，若干Consumer (Group)，以及一个Zookeeper集群。Kafka通过Zookeeper管理集群配置，选举leader，以及在consumer group发生变化时进行rebalance。</p><p>Producer 使用 <strong>push[推]</strong> 模式将消息发布到broker</p><p>Consumer 使用 **pull[拉] **模式从broker订阅并消费消息。</p><p>Pull 模式下，consumer 可以自主决定是否批量的从 broker 拉取数据。Push 模式必须在不知道下游consumer 消费能力和消费策略的情况下决定是立即推送每条消息还是缓存之后批量推送。如果为了避免consumer 崩溃而采用较低的推送速率，将可能导致一次只推送较少的消息而造成浪费。</p><p>Pull 模式下，consumer 就可以根据自己的消费能力去决定这些策略</p><p>Pull 模式有个缺点是，如果 broker 没有可供消费的消息，将导致 consumer 不断在循环中轮询，直到新消息到达。为了避免这点，Kafka 有个参数可以让 consumer 阻塞知道新消息到达[当然也可以阻塞知道消息的数量达到某个特定的量这样就可以批量发]</p><h2 id="3-1-Broker"><a href="#3-1-Broker" class="headerlink" title="3.1. Broker"></a>3.1. Broker</h2><p><strong>Kafka 集群包含一个或多个服务器，服务器节点称为 broker。</strong></p><p><strong>broker 存储 topic 的数据。如果某 topic 有 N 个partition，集群有 N 个broker，那么每个broker 存储该 topic 的一个 partition。</strong></p><ol><li><p><strong>Topic</strong></p><p>每条发布到 Kafka 集群的消息都有一个类别，这个类别被称为 Topic。物理上不同 Topic 的消息分开存储，逻辑上一个 Topic 的消息虽然保存于一个或多个 broker 上但用户只需指定消息的Topic即可生产或消费数据而不必关心数据存于何处类似于数据库的表名</p></li><li><p><strong>Partition</strong></p></li></ol><p>topic 中的数据分割为一个或多个 partition。每个 topic 至少有一个 partition。每个partition 中的数据使用多个 segment 文件存储。partition 中的数据是有序的，不同partition 间的数据丢失了数据的顺序。如果 topic 有多个 partition，消费数据时就不能保证数据的顺序。在需要严格保证消息的消费顺序的场景下，需要将 partition 数目设为1。</p><p>如果 Topic 不进行分区，而将 Topic 内的消息存储于一个 broker，那么关于该 Topic 的所有读写请求都将由这一个 broker 处理，吞吐量很容易陷入瓶颈，这显然是不符合高吞吐量应用场景的。</p><p>有了 Partition 概念以后，假设一个 Topic 被分为 10 个 Partitions，Kafka 会根据一定的算法将 10 个 Partition 尽可能均匀的分布到不同的 broker（服务器）上，当 producer 发布消息时，producer 客户端可以采用 random、key-hash 及 轮询 等算法选定目标 partition，若不指定，Kafka 也将根据一定算法将其置于某一分区上。Partiton 机制可以极大的提高吞吐量，并且使得系统具备良好的水平扩展能力。</p><p>如果某topic有N个partition，集群有(N+M)个broker，那么其中有N个broker存储该topic的一个partition，剩下的M个broker不存储该topic的partition数据。</p><p>如果某topic有N个partition，集群中broker数目少于N个，那么一个broker存储该topic的一个或多个partition。在实际生产环境中，尽量避免这种情况的发生，这种情况容易导致Kafka集群数据不均衡。</p><h2 id="3-2-Producer"><a href="#3-2-Producer" class="headerlink" title="3.2.Producer"></a>3.2.Producer</h2><p>生产者即数据的发布者，该角色将消息发布到 Kafka 的 topic 中。broker 接收到生产者发送的消息后，broker 将该消息<strong>追加</strong>到当前用于追加数据的 segment 文件中。生产者发送的消息，存储到一个partition 中，生产者也可以指定数据存储的 partition。</p><h2 id="3-3-Consumer-amp-Consumer-Group"><a href="#3-3-Consumer-amp-Consumer-Group" class="headerlink" title="3.3.Consumer &amp; Consumer Group"></a>3.3.Consumer &amp; Consumer Group</h2><p>消费者可以从 Broker 中读取数据。消费者可以消费多个 topic 中的数据。</p><p>每个 Consumer 属于一个特定的 Consumer Group（可为每个Consumer指定group name，若不指定group name则属于默认的group）。这是 Kafka 用来实现一个 topic 消息的广播（发给所有的consumer）和单播（发给任意一个consumer）的手段。一个 topic 可以有多个 CG。topic 的消息会复制给 consumer。如果需要实现广播，只要每个 consumer 有一个独立的 CG 就可以了。要实现单播只要所有的 consumer 在同一个 CG。用CG还可以将 consumer 进行自由的分组而不需要多次发送消息到不同的topic。</p><h2 id="3-4-Zookeeper"><a href="#3-4-Zookeeper" class="headerlink" title="3.4.Zookeeper"></a>3.4.Zookeeper</h2><h3 id="3-4-1-概述"><a href="#3-4-1-概述" class="headerlink" title="3.4.1. 概述"></a>3.4.1. 概述</h3><p><strong>Zookeeper 是一个开放源码的、高性能的分布式协调服务，它用于 Kafka 的分布式应用。Zookeeper 主要用来跟踪 Kafka 集群中的节点状态, 以及 Kafka Topic, message 等等其他信息. 同时, Kafka 依赖于Zookeeper, 没有Zookeeper 是不能运行起来 Kafka 的.</strong></p><p>Zookeeper 存储了一些关于 consumer 和 broker 的信息，那么就从这两方面说明 zookeeper 的作用。</p><ol><li><p><strong>Broker 注册</strong></p><p>zookeeper 记录了所有 broker 的存活状态，broker 会向 zookeeper 发送心跳请求来上报自己的状态。</p><p>zookeeper 维护了一个正在运行并且属于集群的 broker 列表。</p></li><li><p><strong>Topic 注册</strong></p><p>在 Kafka 中，同一个 <strong>Topic 的消息会被分成多个分区</strong>并将其分布在多个 Broker 上，<strong>这些分区信息及与 Broker 的对应关系</strong>也都是由 Zookeeper 在维护，由专门的节点来记录</p></li><li><p>控制器选举</p><p>Kafka 集群中有多个 Broker，其中有一个会被选举为控制器。</p><p>控制器负责管理整个集群所有分区和副本的状态，例如某个分区的 leader 故障了，控制器会选举新的 leader。</p><p>从多个 broker 中选出控制器，这个工作就是 zookeeper 负责的。</p></li><li><p>记录 ISR 信息</p><p> zookeeper 记录着 ISR 的信息，而且是实时更新的，只要发现其中有成员不正常，马上移除。</p></li><li><p>topic 配置</p><p>zookeeper 保存了 topic 相关配置，例如 topic 列表、每个 topic 的 partition 数量、副本的位置等等。</p></li><li><p>consumer</p><ul><li><p>offset</p><blockquote><p>kafka 老版本中，consumer 的消费偏移量是默认存储在 zookeeper 中的。</p></blockquote><blockquote><p>新版本中，逐渐弱化了 zookeeper 的作用。新的 consumer 使用了 kafka 内部的group coordination 协议，也减少了对zookeeper的依赖，工作由 kafka 自己做了，kafka 专门做了一个 offset manager。</p></blockquote></li><li><p>注册</p><blockquote><p>和 broker 一样，consumer 也需要注册。</p><p>consumer 会自动注册，注册的方式也是创建一个临时节点，consumer down 了之后就会自动销毁。</p></blockquote></li></ul></li><li><p><strong>分区注册</strong></p><blockquote><p>kafka 的每个 partition 只能被消费组中的一个 consumer 消费，kafka 必须知道所有 partition 与 consumer 的关系。</p></blockquote></li></ol><h2 id="4-总结"><a href="#4-总结" class="headerlink" title="4.总结"></a>4.总结</h2><h5 id="Kafka-分区数可以增加或减少吗？为什么？"><a href="#Kafka-分区数可以增加或减少吗？为什么？" class="headerlink" title="Kafka 分区数可以增加或减少吗？为什么？"></a><font color='blue'>Kafka 分区数可以增加或减少吗？为什么？</font></h5><p>我们可以使用 bin/kafka-topics.sh 命令对 Kafka 增加 Kafka 的分区数据，但是 Kafka 不支持减少分区数。<br>Kafka 分区数据不支持减少是由很多原因的，比如减少的分区其数据放到哪里去？是删除，还是保留？删除的话，那么这些没消费的消息不就丢了。如果保留这些消息如何放到其他分区里面？追加到其他分区后面的话那么就破坏了 Kafka 单个分区的有序性。如果要保证删除分区数据插入到其他分区保证有序性，那么实现起来逻辑就会非常复杂。</p>]]></content>
      
      
      <categories>
          
          <category> Kafka </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kafka 消息系统源码深度剖析 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>消息系统-Kafka(2)：生产者</title>
      <link href="2019/02/12/%E6%B6%88%E6%81%AF%E7%B3%BB%E7%BB%9F-Kafka(2)%EF%BC%9A%E7%94%9F%E4%BA%A7%E8%80%85/"/>
      <url>2019/02/12/%E6%B6%88%E6%81%AF%E7%B3%BB%E7%BB%9F-Kafka(2)%EF%BC%9A%E7%94%9F%E4%BA%A7%E8%80%85/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p>生产者即数据的发布者，该角色将消息发布到 Kafka 的 topic 中。broker 接收到生产者发送的消息后，broker 将该消息<strong>追加</strong>到当前用于追加数据的 segment 文件中。生产者发送的消息，存储到一个partition 中，生产者也可以指定数据存储的 partition。</p><a id="more"></a><h1 id="1-生产者概览"><a href="#1-生产者概览" class="headerlink" title="1.生产者概览"></a>1.生产者概览</h1><p>生产者即数据的发布者，该角色将消息发布到 Kafka 的 topic 中。broker 接收到生产者发送的消息后，将该消息<strong>追加</strong>到当前用于追加数据的 segment 文件中。生产者发送的消息，存储到一个partition 中，生产者也可以指定数据存储的 partition。</p><h2 id="1-1-创建生产者"><a href="#1-1-创建生产者" class="headerlink" title="1.1.创建生产者"></a>1.1.创建生产者</h2><p>ProductRecord 对象还可以指定键或分区。</p><p>在发送 ProductRecord 对象时，生产者要把键和值对象序列化为字节数组，这样才可以在网络上进行传输。接下来，数据被传给分区器。如果在 ProductRecord 对象里指定了分区，分区器直接将指定的分区返回。如果没有指定分区，分区器会根据 ProductRecord 对象的键选择一个分区。如果选定分区以后，生产者就知道向哪个主题和分区发送这条记录。<br>紧接着，这条记录被添加到一个记录批次里，这个批次里的所有消息被发送到相同主题和分区。(有一个独立的线程负责把这些记录批次发送到相应的 broker 上)</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Create a record with no key</span></span><br><span class="line"><span class="comment"> * </span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> topic The topic this record should be sent to</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> value The record contents</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">ProducerRecord</span><span class="params">(String topic, V value)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">this</span>(topic, <span class="keyword">null</span>, <span class="keyword">null</span>, <span class="keyword">null</span>, value, <span class="keyword">null</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>服务器在收到这些消息时会返回一个响应。如果消息成功写入 Kafka ，返回一个 RecordMetaData对象，包含了主题和分区信息，以及记录在分区的偏移量。如果写入失败，则会返回一个错误。生产者收到错误之后会尝试重新发送消息，几次之后如果还是失败，就返回错误信息。</p><img src="../images/kafka2.png" alt="" style="zoom:50%;" /><h2 id="2-创建生产者"><a href="#2-创建生产者" class="headerlink" title="2.创建生产者"></a>2.创建生产者</h2><p>向 Kafka 写入消息，首先要创建一个生产者对象，并设置一些属性。Kafka 生产者有3个必选的属性</p><ul><li><p>bootstrap.servers</p><p> 指定 broker 的地址清单</p> <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">properties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">&quot;master:9092,data1:9092,data2:9092,data3:9092,data4:9092,data5:9092&quot;</span>);</span><br></pre></td></tr></table></figure></li><li><p>key.serializer</p><p>broker 希望接受到的消息的键和值都是字节数组。生产者接口允许使用参数化类型。因此可以把 Java 对象作为键和值发送给 broker(这样代码具有良好的可读性)</p></li><li><p>value.serializer </p></li></ul><h1 id="3-拦截器"><a href="#3-拦截器" class="headerlink" title="3.拦截器"></a>3.拦截器</h1><h2 id="3-1-概述"><a href="#3-1-概述" class="headerlink" title="3.1.概述"></a>3.1.概述</h2><p>对于生产者而言，拦截器使用户在消息发送前以及 Producer 回调逻辑前有机会对消息做一些定制化需求，比如修改消息等。同时，生产者允许用户指定多个拦截器按序作用于同一条消息从而形成一个拦截链(interceptor chain)。<br>Intercetpor 的实现接口是org.apache.kafka.clients.producer.ProducerInterceptor，其定义的方法包括：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * A plugin interface that allows you to intercept (and possibly mutate) the records received by the producer before</span></span><br><span class="line"><span class="comment"> * they are published to the Kafka cluster.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">ProducerInterceptor</span>&lt;<span class="title">K</span>, <span class="title">V</span>&gt; <span class="keyword">extends</span> <span class="title">Configurable</span></span></span><br></pre></td></tr></table></figure><ul><li><p><strong>onSend(ProducerRecord)</strong></p><p>生产者确保消息被序列化以及计算分区前调用该方法。用户可以在该方法中对消息做任何操作，但最好保证不要修改消息所属的topic和分区，否则会影响目标分区的计算。</p></li><li><p><strong>onAcknowledgement(RecordMetadata, Exception)</strong></p><p>该方法会在消息被应答之前或消息发送失败时调用，并且通常都是在生产者回调逻辑触发之前。onAcknowledgement运行在生产者的IO线程中，因此不要在该方法中放入很重的逻辑，否则会拖慢生产者的消息发送效率</p></li></ul><ul><li><p><strong>close</strong></p><p>关闭interceptor，主要用于执行一些资源清理工作</p></li></ul><h2 id="3-2-拦截器实现"><a href="#3-2-拦截器实现" class="headerlink" title="3.2. 拦截器实现"></a>3.2. 拦截器实现</h2><p>实现一个简单的双 interceptor组成的拦截链。</p><ul><li>第一个interceptor会在消息发送前将时间戳信息加到消息前面；</li><li>第二个interceptor会在消息发送后更新成功发送消息数或失败发送消息数。</li></ul><h3 id="3-2-1-时间拦截器实现"><a href="#3-2-1-时间拦截器实现" class="headerlink" title="3.2.1.  时间拦截器实现"></a>3.2.1.  时间拦截器实现</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TimerInterceptor</span> <span class="keyword">implements</span> <span class="title">ProducerInterceptor</span>&lt;<span class="title">String</span>, <span class="title">String</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">configure</span><span class="params">(Map&lt;String, ?&gt; configs)</span> </span>&#123;&#125;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> ProducerRecord&lt;String, String&gt; <span class="title">onSend</span><span class="params">(ProducerRecord&lt;String, String&gt; record)</span> </span>&#123;</span><br><span class="line">        String value = System.currentTimeMillis() + <span class="string">&quot;---&quot;</span> + record.value();</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> ProducerRecord&lt;&gt;(record.topic(), record.partition(), record.key(), value);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onAcknowledgement</span><span class="params">(RecordMetadata metadata, Exception exception)</span> </span>&#123;&#125;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> </span>&#123;&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="3-2-2-计数拦截器实现"><a href="#3-2-2-计数拦截器实现" class="headerlink" title="3.2.2.  计数拦截器实现"></a>3.2.2.  计数拦截器实现</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.Map;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CountInterceptor</span> <span class="keyword">implements</span> <span class="title">ProducerInterceptor</span>&lt;<span class="title">String</span>, <span class="title">String</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">int</span> success;</span><br><span class="line">    <span class="keyword">int</span> error;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">configure</span><span class="params">(Map&lt;String, ?&gt; configs)</span> </span>&#123;&#125;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> ProducerRecord&lt;String, String&gt; <span class="title">onSend</span><span class="params">(ProducerRecord&lt;String, String&gt; record)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> record;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onAcknowledgement</span><span class="params">(RecordMetadata metadata, Exception exception)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(metadata != <span class="keyword">null</span>)&#123;</span><br><span class="line">            success ++;</span><br><span class="line">        &#125;<span class="keyword">else</span> &#123;</span><br><span class="line">            error++;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        System.out.println(<span class="string">&quot;success:    &quot;</span> + success);</span><br><span class="line">        System.out.println(<span class="string">&quot;error:    &quot;</span> + error);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="3-2-3-配置拦截器"><a href="#3-2-3-配置拦截器" class="headerlink" title="3.2.3.  配置拦截器"></a>3.2.3.  配置拦截器</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">ArrayList&lt;String&gt; interceptors = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">interceptors.add(<span class="string">&quot;api.interceptor.TimerInterceptor&quot;</span>);</span><br><span class="line">interceptors.add(<span class="string">&quot;api.interceptor.CountInterceptor&quot;</span>);</span><br><span class="line"></span><br><span class="line">properties.put(ProducerConfig.INTERCEPTOR_CLASSES_CONFIG, interceptors);</span><br></pre></td></tr></table></figure><h2 id="4-序列化器"><a href="#4-序列化器" class="headerlink" title="4. 序列化器"></a>4. 序列化器</h2><p>创建Kafka 生产者时必须指定序列化器。Kafka 除提供默认的字符串序列化器 org.apache.kafka.common.serialization.StringSerializer，还提供了整形和字节数组序列化器等。</p><h2 id="5-分区器"><a href="#5-分区器" class="headerlink" title="5.分区器"></a>5.分区器</h2><h3 id="5-1-分区原因"><a href="#5-1-分区原因" class="headerlink" title="5.1.分区原因"></a>5.1.分区原因</h3><ul><li>方便在集群中扩展，每个 Partition 可以通过调整以适应它所在的机器，而一个 topic 又可以存在多个Partition</li><li>提高并发。</li></ul><h3 id="5-2-分区原则"><a href="#5-2-分区原则" class="headerlink" title="5.2.分区原则"></a>5.2.分区原则</h3><p>kafka 中默认分区器为 <strong>org.apache.kafka.clients.producer.internals.DefaultPartitioner，</strong>其实现了 org.apache.kafka.clients.producer.Partitioner 接口。默认分区原则为:</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">* The <span class="keyword">default</span> partitioning strategy:</span><br><span class="line">* &lt;ul&gt;</span><br><span class="line">* &lt;li&gt;If a partition is specified in the record, use it</span><br><span class="line">* &lt;li&gt;If no partition is specified but a key is present choose a partition based on a hash of the key</span><br><span class="line">* &lt;li&gt;If no partition or key is present choose a partition in a round-robin fashion</span><br></pre></td></tr></table></figure><ol><li><p><strong>指明 partition 的情况下</strong></p><p>直接将指明的值直接作为 partition 的值</p></li><li><p><strong>没有指明 partition 值但是有 key</strong> </p><p>将 key 的 hash 值 与 topic 的 partition 数 进行取余 得到 partition 值</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// hash the keyBytes to choose a partition</span></span><br><span class="line"><span class="keyword">return</span> Utils.toPositive(Utils.murmur2(keyBytes)) % numPartitions;</span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">// 将 number 转换为正数。 </span></span><br><span class="line"><span class="comment">// 当 number 为正时，返回原始值。 </span></span><br><span class="line"><span class="comment">// 当 number 为负数时，返回原始值位与0x7fffffff的绝对值之和。 0x7FFFFFFF 的二进制表示就是除了首位是 0，其余都是1, </span></span><br><span class="line"><span class="comment">// 即最大的整型数 int</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">int</span> <span class="title">toPositive</span><span class="params">(<span class="keyword">int</span> number)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> number &amp; <span class="number">0x7fffffff</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p><strong>没有指定 partition 值又没有 key 值</strong></p><p><strong>round-robin</strong> 既第一次调用时随即生成一个整数(后面每次调用在这个整数上进行自增)，将这个值与 topic 可用的 partition 总数取余，得到 partition 值。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"> <span class="keyword">int</span> nextValue = nextValue(topic);</span><br><span class="line"><span class="comment">// broker 集群中 topic 主题可以利用的分区数</span></span><br><span class="line">List&lt;PartitionInfo&gt; availablePartitions = cluster.availablePartitionsForTopic(topic);</span><br><span class="line"><span class="keyword">if</span> (availablePartitions.size() &gt; <span class="number">0</span>) &#123;</span><br><span class="line">    <span class="keyword">int</span> part = Utils.toPositive(nextValue) % availablePartitions.size();</span><br><span class="line">    <span class="keyword">return</span> availablePartitions.get(part).partition();</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="comment">// no partitions are available, give a non-available partition</span></span><br><span class="line">    <span class="keyword">return</span> Utils.toPositive(nextValue) % numPartitions;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> ConcurrentMap&lt;String, AtomicInteger&gt; topicCounterMap = <span class="keyword">new</span> ConcurrentHashMap&lt;&gt;();</span><br><span class="line">  </span><br><span class="line"> <span class="function"><span class="keyword">private</span> <span class="keyword">int</span> <span class="title">nextValue</span><span class="params">(String topic)</span> </span>&#123;</span><br><span class="line">     AtomicInteger counter = topicCounterMap.get(topic);</span><br><span class="line">     <span class="comment">// 如果是第一次分区，随机生成一个数</span></span><br><span class="line">     <span class="keyword">if</span> (<span class="keyword">null</span> == counter) &#123;</span><br><span class="line">         counter = <span class="keyword">new</span> AtomicInteger(ThreadLocalRandom.current().nextInt());</span><br><span class="line">         AtomicInteger currentCounter = topicCounterMap.putIfAbsent(topic, counter);</span><br><span class="line">         <span class="keyword">if</span> (currentCounter != <span class="keyword">null</span>) &#123;</span><br><span class="line">             counter = currentCounter;</span><br><span class="line">         &#125;</span><br><span class="line">     &#125;</span><br><span class="line">     <span class="comment">// 不是第一次分区， 则++</span></span><br><span class="line">     <span class="keyword">return</span> counter.getAndIncrement();</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure></li></ol><h1 id="6-原理"><a href="#6-原理" class="headerlink" title="6.原理"></a>6.原理</h1><h2 id="6-1-整体架构"><a href="#6-1-整体架构" class="headerlink" title="6.1.整体架构"></a>6.1.整体架构</h2><p>在生产者将消息发往 Kafka之前，有可能需要经历拦截器、序列化器和分区器等一系列的作用，随后才真正进入消息发送流程。</p><img src="../images/kafka1.png" alt="" style="zoom:50%;" /><h3 id="6-1-1-消息累加器-RecordAccumulator"><a href="#6-1-1-消息累加器-RecordAccumulator" class="headerlink" title="6.1.1.消息累加器 RecordAccumulator"></a>6.1.1.消息累加器 RecordAccumulator</h3><p>整个生产者客户端由两个线程协调运行，这两个线程分别为 <strong>主线程</strong> 和 <strong>Sender线程</strong>[发送线程]。</p><p>在主线程中由 KafkaProducer 创建消息，然后通过可能的拦截器、序列化器和分区器的作用之后缓存到消息累加器 [RecordAccumulator，也称为消息收集器]。</p><p>Sender 线程负责从 RecordAccumulator 中获取消息并将其发送到 Kafka 中。</p><ol><li><p>RecordAccumulator 主要用来缓存消息，以便 Sender 线程可以批量发送，进而减少网络传输的资源消耗提升性能。</p></li><li><p>RecordAccumulator 缓存的大小可以通过生产者客户端参数 buffer.memory 的配置，默认值为32MB。如果生产者发送消息的速度超过发送到服务器的速度，则会导致生产者空间不足，这个时候KafkaProducer的send()方法调用要么被阻塞，要么抛出异常，这个取决于参数max.block.ms的配置，此参数的默认值为60000，即60秒。</p></li></ol><h3 id="6-1-2-ProducerBatch"><a href="#6-1-2-ProducerBatch" class="headerlink" title="6.1.2.ProducerBatch"></a>6.1.2.ProducerBatch</h3><p><strong>在 RecordAccumulator 的内部为每个分区都维护了一个双端队列</strong></p><p>主线程中发送过来的消息都会被追回到 RecordAccumulator 的某个双端队列 [Deque]中，</p><p>队列中的内容就是 ProducerBatch,即 Deque<ProducerBatch>。</p><p>消息写入缓存时，追回到双端队列的尾部；Sender 读取消息时，从双端队列的头部读取。</p><ul><li><p><strong><font color='red'>注意</font></strong></p><ol><li><p><strong>ProducerBatch 不是 ProducerRecord</strong></p><blockquote><p>ProducerBatch 是指一个消息批次，ProducerRecord 会被包含在 ProducerBatch 中，这样可以使字节的使用更加紧凑。与此同时，将较小的 ProducerRecord 拼凑成一个较大的ProducerBatch 可以减少网络请求的次数以提升整体的吞量。如果生产者客户端需要向很多分区发送消息，则可以将 buffer.memory 参数适当调大以增加整体的吞吐量。</p></blockquote></li><li><p><strong>ProducerBatch 的大小和 batch.size 参数有着密切的关系。</strong></p><blockquote><p>当一条消息 [ProducerRecord] 流入 RecordAccumulator 时，会先寻找与消息分区所对应的双端队列,如果没有则创建，再从这个双端队列的尾部获取一个 ProducerBatch[如果没有则创建]，查看 ProducerBatch 中是否还可以写入这个 ProdcucerRecord，如果可以则写入，如果不可以则需要创建一个新的 ProducerBatch。</p></blockquote><blockquote><p>在新建 ProducerBatch 时评估这条消息的大小是否超过 batch.size 参数的大小，如果不超过，那么就以 batch.size 参数的大小来创建 ProducerBatch，这样在使用完这段内存区域后，可以通过 BufferPool 的管理来进行复用；如果超过，那么就以评估的大小创建ProducerBatch，这段内存区域不会被复用。</p></blockquote></li></ol></li></ul><h3 id="6-1-3-BufferPool"><a href="#6-1-3-BufferPool" class="headerlink" title="6.1.3. BufferPool"></a>6.1.3. BufferPool</h3><p>消息在网络上都是以字节[Byte]的形式传输的，在发送之前需要创建一块内存区域来保存对应的消息。在Kafka生产者客户端中，通过 java.io.ByteBuffer 实现消息内存的创建和释放。不过频繁的创建和释放是比较耗费资源的，在RecordAccumulator的内部还有一个BufferPool，它主要是用来实现ByteBuffer的复用，以实现缓存的高效利用。</p><p>不过 BufferPool 只针对特定大小的 ByteBuffer 进行管理，而其它大小的 ByteBuffer 不会缓存进BufferPool 中，这个特定的大小由 batch.size 参数指定，默认值为64KB，可以适当地调大batch.size 参数以便多缓存一些消息。</p><p>Sender 从 RecordAccumulator 中获取缓存的消息之后，会进一步将原本<code>&lt;分区，Deque&lt;ProducerBatch&gt;&gt;</code>的保存形式转变成&lt;Node, List<ProducerBatch>&gt;的形式，其中Node表示Kafka集群的broker节点。对于网络连接来说，生产者客户端是与具体的broker节点建立的连接，就是向具体的broker节点发送消息，而并不关心消息属于哪一个分区；而对于KafkaProducer的应用逻辑而言，我们只关注向哪个分区中发送哪些消息，所以这里需要做一个应用逻辑层面到网络I/O层面的转换。</p><p>在转换成&lt;Node, List<ProducerBatch>&gt;的形式之后，Sender还会进一步封装成&lt;Node, Request&gt;的形式，这样就可以将Request请求发往各个Node了，这里的Request是指Kafka的各种协议请求，对于消息发送而言就是指具体的ProducerRequest。</p><h3 id="6-1-4-InFlightRequest"><a href="#6-1-4-InFlightRequest" class="headerlink" title="6.1.4. InFlightRequest"></a>6.1.4. InFlightRequest</h3><p>请求在从 Sender 线程发往 Kafka 之前还会保存到 InFlightRequests 中，InFlightRequest 保存对象的具体形式为Map&lt;NodeId, Deque<Request>&gt;</p><p>它的主要作用是缓存已经发出去但还没有收到响应的请求[NodeId是一个 String 类型，表示节点的id编号]。</p><h1 id="7-总结"><a href="#7-总结" class="headerlink" title="7.总结"></a>7.总结</h1><h5 id="Kafka-生产者如何保证不丢失，不重复？"><a href="#Kafka-生产者如何保证不丢失，不重复？" class="headerlink" title="Kafka 生产者如何保证不丢失，不重复？"></a><font color='blue'>Kafka 生产者如何保证不丢失，不重复？</font></h5><p>生产者丢数据，即发送的数据根本没有保存到 Broker 端。出现这个情况的原因可能是，网络抖动，导致消息压根就没有发送到 Broker 端；也可能是消息本身不合格导致 Broker 拒绝接收（比如消息太大了，超过了 Broker 的承受能力）等等。</p><p>上面所说比如网络原因导致消息没有成功发送到 broker 端，常见，也并不可怕。可怕的不是没发送成功，而是发送失败了你不做任何处理。</p><p>很简单的一个<strong>重试配置</strong>，基本就可以解决这种网络瞬时抖动问题。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">props.put(<span class="string">&quot;retries&quot;</span>, <span class="number">10</span>);</span><br></pre></td></tr></table></figure><p>当然还有很多其他原因导致的，不能只依靠 kafka 的配置来做处理，我们看一下 kafka 发送端的源码，其实人家是提供了两个方法的，通常会出问题的方法是那个简单的 send，没有 callback（回调）。简单的 send发送后不会去管它的结果是否成功，而 callback 能准确地告诉你消息是否真的提交成功了。一旦出现消息提交失败的情况，你就可以有针对性地进行处理。</p><p><font color='red'><strong>因此，一定要使用带有回调通知的 send 方法。</strong></font></p><p>通过多 Broker 达到高可用的效果，所以对于生产者程序来说，也不能简单的认为发送到一台就算成功，如果只满足于一台，机器如果损坏了，那消息必然会丢失。设置 <strong>acks = all</strong>，表明所有副本 Broker 都要接收到消息，该消息才算是“已提交”，这样可以达到高可用的效果。</p><p>考虑到 producer,broker,consumer 之间都有可能造成消息重复，所以我们要求接收端需要支持消息去重的功能，最好借助业务消息本身的幂等性来做。<br>举例：</p><blockquote><p>在华泰证券中Kafka的幂等性是如何保证的？在接收端，启动专门的消费者拉取 kafka 数据存入 hbase。hbase 的 rowkey 的设计主要包括 SecurityId（股票id）和 timestamp（行情数据时间）。消费线程从 kafka 拉取数据后反序列化，然后批量插入 hbase，只有插入成功后才往 kafka 中持久化 offset。这样的好处是，如果在中间任意一个阶段发生报错，程序恢复后都会从上一次持久化 offset 的位置开始消费数据，而不会造成数据丢失。如果中途有重复消费的数据，则插入 hbase 的 rowkey 是相同的，数据只会覆盖不会重复，最终达到数据一致。</p></blockquote><p><strong>在0.11之前主要是通过下游系统具有幂等性来保证 Exactly Once。但是这样有几个缺陷：</strong></p><blockquote><p>要求下游系统支持幂等操作，限制了Kafka的适用场景</p></blockquote><blockquote><p>实现门槛相对较高，需要用户对Kafka的工作机制非常了解</p></blockquote><blockquote><p>对于Kafka Stream而言，Kafka Producer本身就是“下游”系统，能让Producer具有幂等处理特性，那就可以让Kafka Stream在一定程度上支持Exactly once语义。</p></blockquote><p><strong>0.11之后的版本，引入了 <code>Producer ID（PID）</code>和 <code>Sequence Number</code> 实现 <code>Producer </code>的幂等语义。</strong></p><blockquote><p>Producer ID：每个新的 Producer 在初始化的时候会被分配一个唯一的PID</p></blockquote><blockquote><p>Sequence Number：对于每个PID，该Producer发送数据的每个&lt;Topic, Partition&gt;都对应一个从0开始单调递增的Sequence Number。</p></blockquote><p>Broker端也会为每个&lt;PID, Topic, Partition&gt;维护一个序号，并且每次Commit一条消息时将其对应序号递增。对于接收的每条消息，如果其序号比Broker维护的序号（即最后一次Commit的消息的序号）大一，则Broker会接受它，否则将其丢弃：</p><blockquote><p>如果消息序号比Broker维护的序号大一以上，说明中间有数据尚未写入，也即乱序，此时Broker拒绝该消息，Producer 抛出InvalidSequenceNumber</p></blockquote><blockquote><p>如果消息序号小于等于Broker维护的序号，说明该消息已被保存，即为重复消息，Broker直接丢弃该消息，Producer 抛出 DuplicateSequenceNumber</p></blockquote><p>这种机制很好的解决了数据重复和数据乱序的问题。</p><h1 id="8-数据可靠性保障"><a href="#8-数据可靠性保障" class="headerlink" title="8. 数据可靠性保障"></a>8. 数据可靠性保障</h1>]]></content>
      
      
      <categories>
          
          <category> Kafka </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kafka 消息系统源码深度剖析 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Scilab 云化改造(3)_Xcos仿真流程分析</title>
      <link href="2018/04/05/Xcos%E4%BB%BF%E7%9C%9F%E6%B5%81%E7%A8%8B%E5%88%86%E6%9E%90/"/>
      <url>2018/04/05/Xcos%E4%BB%BF%E7%9C%9F%E6%B5%81%E7%A8%8B%E5%88%86%E6%9E%90/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p>Xcos 是基于 Java 图形界面程序的可视化模拟仿真功能组件。Xcos 的运行必须依赖于 Java 的虚拟机环境。Xcos 的所有可视化的组件和相应控件都是由 Java 代码所编写(包括 Scilab 的可视化界面也都是由 Java 代码编写)。但是 Scilab 的核心代码是由 Fortran 和 C/C++ 编写，所以存在 Java 与其他编程语言的交互问题，开发者利用了 JNI 的方式在 Java 和其他代码之间进行数据和信息交互的。</p><a id="more"></a><p>Xcos 是基于图形界面程序的可视化模拟仿真功能组件</p><p>Scilab 一共有4种工作模式</p><ol><li>API: Scilab 作为API接口供外部程序调用。</li><li>STD: 标准的Scilab模式（包括图形界面和绘制图表功能）。</li><li>NW:以命令行的模式运行Scilab，没有标准的图形界面，但可以进行图表绘制。</li><li>NWNI: 完全以命令行的模式运行Scilab，没有任何图形界面。</li></ol><p>在 Scilab 以 NWNI 和 API 模式运行时，是不能加载 Xcos 模块，即不能运行模拟仿真的功能。</p><h2 id="Xcos-模型的模拟仿真流程"><a href="#Xcos-模型的模拟仿真流程" class="headerlink" title="Xcos 模型的模拟仿真流程"></a>Xcos 模型的模拟仿真流程</h2><p>一个仿真模型的模拟仿真分为以下几个步骤如图所示：</p><img src="/Users/joker/Documents/chen_blog/source/images/截屏2021-01-19 下午1.27.48.png" alt="截屏2021-01-19 下午1.27.48" style="zoom:70%;" /><p>在分析过后，我们发现，Xcos 对于仿真模型的基本操作是一致的，即编译，仿真，输出结果的操作流程是一样的。</p><p>下面简单介绍建模过程。在 Xcos 下建模，就是根据已有的模型，在 Xcos 的组件盘中选取合适的模块，在工作区中进行正确的端口连接。</p><p>Xcos是一个可视化的仿真组件，所以，Xcos有自己的文件格式：.xcos/.zcos。.xcos文件可以当做文本文件打开，如图5.4打开其实可以发现，其实.xcos文件就是一个类xml文件，其中包含了当前模型的所有的信息。</p><img src="/Users/joker/Documents/chen_blog/source/images/截屏2021-01-19 下午1.29.09.png" alt="截屏2021-01-19 下午1.29.09" style="zoom:70%;" /><p>.zcos 文件是为了 Xcos 的批处理而做出的特殊文件格式，即，zcos 就是多个 xcos 文件的集合，即相当于一个压缩文件夹的作用。</p><p>xcos文件中已经包含了一个仿真的所有信息。建模过程：从组件盘中向仿真工作区间进行拖拽仿真模块的操作、模块端口连接操作，实质上就是在编写这样的一个xml文件。建模的过程的实质是建立仿真信息的xml文件。</p><p>另外，可以在仿真的建模过程中通过打开菜单栏中的 “查看” 按钮下拉中的“图表浏览器”选项，来实时查看当前建模的组织关系树状图。</p><img src="/Users/joker/Documents/chen_blog/source/images/截屏2021-01-19 下午1.29.57.png" alt="截屏2021-01-19 下午1.29.57" style="zoom:70%;" /><img src="/Users/joker/Documents/chen_blog/source/images/截屏2021-01-19 下午1.30.37.png" alt="截屏2021-01-19 下午1.30.37" style="zoom:50%;" /><p>Xcos 是可视化的 Java 代码界面，所以存在如何从 Java 代码与 Scilab 核心层的交互以及在核心层如何实现模拟仿真的。接下来，分两部分说明一个模型是如何成为机器代码去运行的。为简要说明，假设 Scilab 是在 NW 模式下运行的。</p><h2 id="Xcos-的仿真模型的解析"><a href="#Xcos-的仿真模型的解析" class="headerlink" title="Xcos 的仿真模型的解析"></a>Xcos 的仿真模型的解析</h2><p>Xcos 的模型都是一些可视化的控件块，对于这些控件块的操作(比如移动动作，按下编译，运行操作等操作)都在Java代码层进行执行。现单纯的就“编译按钮”进行说明。</p><p>Java 代码将 Xcos 的行为分成了很多类包，比如动作，日志，事件监听等。很明显，按钮是一个控件，而按下按钮是一个事件，这个事件触发后会链接到该控件所绑定的动作上。所以，在xcos的动作类包中找到 “编译” 动作。</p><img src="/Users/joker/Documents/chen_blog/source/images/截屏2021-01-19 下午1.34.46.png" alt="截屏2021-01-19 下午1.34.46" style="zoom:50%;" /><p>打开所找到的 Java 源文件，找到动作执行的方法 actionPerformed 中，Java 通过 JNI 的方式向 Scilab 发送了一条Scilab 语言类型的命令。</p><img src="/Users/joker/Documents/chen_blog/source/images/截屏2021-01-19 下午1.35.28.png" alt="截屏2021-01-19 下午1.35.28" style="zoom:50%;" /><p>现在，Scilab 收到了这条消息，现在回到 Scilab 的 C/C++，Fortran 源代码进行分析。</p><p>Scilab 收到这条发送到控制台的指令后要进行解析，将它进行分解，然后在 Scilab 的动态库当中寻找合适的代码去解析它，执行它。大致流程如图5.9所示。</p><img src="/Users/joker/Documents/chen_blog/source/images/截屏2021-01-19 下午1.36.51.png" alt="截屏2021-01-19 下午1.36.51" style="zoom:50%;" /><p>针对这条”编译指令” 首先控制台收到了 Java 程序发来的</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cpr = xcos_compile(scs_m)；</span><br></pre></td></tr></table></figure><p>这是一条在控制台的Sci语言的命令。</p><p>在控制台会有事件监听程序在运行，收到这条指令后，Scilab会在控制台运行相应的事件响应函数GetCommandLine()调用getCmdLine()获取在控制台的字符流，然后通过bexec()将Scilab语言解析成所需参数，然后调用相应的函数进行运算执行。实际上，最后这条指令的结果是将scs_m中所含的编译信息储存到了变量cpr中。</p><blockquote><p>注：在 Scilab 中是通过每个模块函数的 ID 并利用 callFunctionFromGateway() 获取所需具体函数的位置并执行调用，函数具体 ID 在每个模块的 sci_gateway 文件夹下的 xxx_gateway.xml 文件下可查。</p></blockquote><p>至此，已经完整分析了从 Xcos 中是怎么与控制台交互信息以及如何让控制台进行对 Sci 语言指令响应的过程。下面，我们针对仿真过程在 Sci 中是如何运作的。</p><h2 id="Xcos-编译器"><a href="#Xcos-编译器" class="headerlink" title="Xcos 编译器"></a>Xcos 编译器</h2><p>Xcos 的编译器其实就是将 .xcos 文件解析成需要的仿真信息，就目前来看，其实转化成 Sci 语言就是通过如下 3 条指令进行的。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">importXcosDiagram()</span><br><span class="line">cpr = xcos_compile(scs_m)</span><br><span class="line">xcos_simulate(scs_m, <span class="number">4</span>)</span><br></pre></td></tr></table></figure><p>importXcosDiagram() 函数是将.xcos文件中的所有模型的信息储存到变量scs_m中，以供编译和模拟仿真执行。</p><p>xcos_compile(scs_m) 函数是检查模型链接中的错误以及将变量 scs_m 中所包含的信息转化成仿真执行的所需要的信息，包括仿真调度表，解算器以及各种模型仿真信息，并将信息储存入 <code>%cpr</code> 变量中。关于 <code>%cpr</code> 中调度表的具体信息可以查看《Scilab/Scicos在建模与仿真中的应用》一书的附录 A.2 章节详细介绍。</p><p>xcos_simulate(scs_m, 4) 函数是将模型进行仿真的的起始指令，其中隐含调用了 <code>xcos_compile()</code> 函数，在遵循 <code>%cpr</code> 的调度表的情况下进行对模拟仿真的执行并求解结果。</p><h2 id="Xcos-仿真过程分析"><a href="#Xcos-仿真过程分析" class="headerlink" title="Xcos 仿真过程分析"></a>Xcos 仿真过程分析</h2><img src="/Users/joker/Documents/chen_blog/source/images/截屏2021-01-19 下午1.40.44.png" alt="截屏2021-01-19 下午1.40.44" style="zoom:50%;" />]]></content>
      
      
      <categories>
          
          <category> Scilab </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Scilab 仿真云化改造 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Scilab 云化改造(4)_Xcos工具箱功能包挂载</title>
      <link href="2018/03/30/Xcos%E5%B7%A5%E5%85%B7%E7%AE%B1%E5%8A%9F%E8%83%BD%E5%8C%85%E6%8C%82%E8%BD%BD/"/>
      <url>2018/03/30/Xcos%E5%B7%A5%E5%85%B7%E7%AE%B1%E5%8A%9F%E8%83%BD%E5%8C%85%E6%8C%82%E8%BD%BD/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p>为实现 Scilab 扩展功能的增加和挂载，将 C/C++/Fortran 语言实现的功能添加到 Scilab 平台，并在 Scilab 平台中对该类函数实现的功能进行调用。Scilab 提供了工具箱的模式对此类功能进行管理。</p><a id="more"></a><h1 id="工具箱概述"><a href="#工具箱概述" class="headerlink" title="工具箱概述"></a>工具箱概述</h1><h2 id="工具箱结构"><a href="#工具箱结构" class="headerlink" title="工具箱结构"></a>工具箱结构</h2><p>scilab 工具箱有专门的层次结构和概念，包括以下几部分：</p><ol><li><p><strong>script</strong></p><p>script 是一个扩展名为.sce的脚本文件，用于实现Scilab批处理代码。</p></li><li><p><strong>macro</strong></p><p>macro是用Scilab代码编写的Scilab函数，是一些具体功能的实现，其文件扩展名为.sci。</p></li><li><p><strong>primitive</strong></p><p>primitive是Scilab可调用的外部函数，该类函数的原型可以是C/C ++/Fortran语言编写的。该类型的函数需要通过sci_gateway的方式实现调用。</p></li><li><p><strong>sci_gateway</strong></p><p>sci_gateway是调用外部函数的模式方法，其方法使用C或C ++编写，遵循一定的原则，调用外部的功能实现函数，使其可以从Scilab中直接调用。一旦在builder脚本中声明了网关函数，构建并加载了其所对应的功能函数，就可以直接从Scilab中使用Scilab的方法使用这些外部实现函数。</p></li><li><p><strong>builder</strong></p><p>builder文件是用于构建工具箱的Scilab脚本，即对工具箱源码进行编译并生成对应的库文件。sci文件生成对应的bin文件，C/C++/Fortran文件生成对应的dll/so等动态库文件。以及符合Scilab加载的loader脚本文件、卸载的unloader脚本，以及清理生成文件的cleaner脚本。</p></li><li><p><strong>loader</strong></p><p>loader是Scilab脚本，它在Scilab中加载工具箱组件（或整个工具箱）。加载程序名称包含加载程序* .sce（示例：loader.sce，loadhelp.sce，…）。该文件由对应的builder文件生成。</p></li><li><p><strong>Toolbox start &amp; quit</strong></p><p>每个通过ATOMS安装的工具箱都在Scilab启动时自动加载启动脚本，在Scilab退出时自动执行工具箱卸载脚本。</p></li></ol><p>Scilab 的工具箱构建时一般遵循以下结构：</p><table><thead><tr><th>目录</th><th>内容</th><th>备注</th></tr></thead><tbody><tr><td>macros</td><td>macros(.sci)，builder(buildmacros.sce)</td><td>必需</td></tr><tr><td>src</td><td>源代码文件（.c，.cpp，.f，…）和builder脚本（builder*.sce）和加载脚本（loader * .sce，由builder脚本产生）</td><td>必需</td></tr><tr><td>sci_gateway</td><td>网关（.c或.cpp）以及网关builder脚本（.sce）的源代码文件</td><td>可选，如果没有使用其他语言实现的功能时，可忽略(如celestlab)</td></tr><tr><td>jar</td><td>java包和帮助文件（.jar）</td><td>不需构建，由builder运行时自动生成帮助文档的jar</td></tr><tr><td>help</td><td>XML帮助文件（.xml）</td><td>可选，但建议添加必要的帮助说明</td></tr><tr><td>etc</td><td>启动脚本（.start）和退出（.quit）脚本</td><td>必需</td></tr><tr><td>tests</td><td>模块测试脚本（.tst）</td><td>可选</td></tr><tr><td>demos</td><td>脚本实例</td><td>可选，但建议添加必要的示例</td></tr><tr><td>includes</td><td>要与模块一起发布的头文件（.h）</td><td>可选，如果想让别的程序调用本工具箱的功能，可以通过该方法进行分享</td></tr></tbody></table><h2 id="工具箱加载方式"><a href="#工具箱加载方式" class="headerlink" title="工具箱加载方式"></a>工具箱加载方式</h2><p>工具箱加载时需要两类脚本：一个是编译脚本(builder.sce)，用于编译工具箱; 另一个是加载脚本(loader.sce)，在Scilab 中加载该工具箱时运行。两者都是 Scilab 的批处理脚本文件（.sce文件）。</p><p>编译工具箱时，运行 builder.sce 文件，在编译过程中，该脚本将去各个源码目录下执行各自的builder脚本，完成各个源码的编译工作。编译完成后，可以使用加载脚本（loader.sce），在Scilab中加载模块。加载过程中，加载脚本会去工具箱/etc/目录下执行工具箱的启动脚本.start。</p><h2 id="工具箱示例"><a href="#工具箱示例" class="headerlink" title="工具箱示例"></a>工具箱示例</h2><p>本小节将以 Scilab 自带的一个骨架式工具包为示例。</p><p><strong>最外层builder.sce</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line"></span><br><span class="line">mode(-1);</span><br><span class="line"></span><br><span class="line">lines(0);</span><br><span class="line"></span><br><span class="line">try getversion(&#39;scilab&#39;);</span><br><span class="line"></span><br><span class="line">catch error(gettext(&#39;Scilab 5.0 or more is required.&#39;));</span><br><span class="line"></span><br><span class="line">end;</span><br><span class="line"></span><br><span class="line">if ~with_module(&#39;development_tools&#39;) then </span><br><span class="line"></span><br><span class="line">error(msprintf(gettext(&#39;%s module not installed.&#39;),&#39;development_tools&#39;));</span><br><span class="line"></span><br><span class="line">end</span><br><span class="line"></span><br><span class="line">TOOLBOX_NAME &#x3D; &#39;toolbox_skeleton&#39;;TOOLBOX_TITLE &#x3D; &#39;Toolbox Skeleton&#39;;</span><br><span class="line"></span><br><span class="line">toolbox_dir&#x3D;get_absolute_file_path(&#39;builder.sce&#39;);</span><br><span class="line"></span><br><span class="line">tbx_builder_macros(toolbox_dir);</span><br><span class="line"></span><br><span class="line">tbx_builder_src(toolbox_dir);</span><br><span class="line"></span><br><span class="line">tbx_builder_gateway(toolbox_dir);</span><br><span class="line"></span><br><span class="line">tbx_builder_help(toolbox_dir);</span><br><span class="line"></span><br><span class="line">tbx_build_loader(TOOLBOX_NAME, toolbox_dir);</span><br><span class="line"></span><br><span class="line">clear toolbox_dir TOOLBOX_NAME TOOLBOX_TITLE;</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br></pre></td></tr></table></figure><p>很显然，builder的作用就是去个级目录下执行各自的builder.sce脚本，关于以上使用的函数可以参考scilab的帮助页面中关于”Modules manager”一节中进行查看。在执行完脚本后，将会生成工具箱的加载脚本(各个需要加载的源码目录下)。</p><p><strong>生成的最外层loader.sce**</strong>内容如下：**</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line"></span><br><span class="line">oldmode &#x3D; mode(); </span><br><span class="line"></span><br><span class="line">mode(-1);</span><br><span class="line"></span><br><span class="line">oldlines &#x3D; lines()(2); lines(0);</span><br><span class="line"></span><br><span class="line">try</span><br><span class="line"></span><br><span class="line">exec(get_absolute_file_path(&quot;loader.sce&quot;)+&quot;etc&#x2F;&quot;+&quot;xcos_toolbox_skeleton.start&quot;);</span><br><span class="line"></span><br><span class="line">catch</span><br><span class="line"></span><br><span class="line">  [errmsg, tmp, nline, func] &#x3D; lasterror()</span><br><span class="line"></span><br><span class="line">  msg &#x3D; &quot;%s: error on line #%d: &quot;&quot;%s&quot;&quot;\n&quot;</span><br><span class="line"></span><br><span class="line">  msg &#x3D; msprintf(msg, func, nline, errmsg)</span><br><span class="line"></span><br><span class="line">  lines(oldlines)</span><br><span class="line"></span><br><span class="line">  mode(oldmode);</span><br><span class="line"></span><br><span class="line">  clear oldlines oldmode tmp nline func</span><br><span class="line"></span><br><span class="line">  error(msg);</span><br><span class="line"></span><br><span class="line">end</span><br><span class="line"></span><br><span class="line">lines(oldlines);</span><br><span class="line"></span><br><span class="line">mode(oldmode);</span><br><span class="line"></span><br><span class="line">clear oldlines oldmode;</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br></pre></td></tr></table></figure><p>显然生成的loader脚本是去调用了工具箱目录下的/etc/目录下的启动脚本(.start)</p><p><strong>/etc/toolbox_skeleton.start</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line"></span><br><span class="line">function toolbox_skeletonlib &#x3D; startModule()</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  TOOLBOX_NAME &#x3D; &quot;toolbox_skeleton&quot;;</span><br><span class="line"></span><br><span class="line">  TOOLBOX_TITLE &#x3D; &quot;Toolbox Skeleton&quot;;</span><br><span class="line"></span><br><span class="line"> mprintf(&quot;Start &quot; + TOOLBOX_TITLE + &quot;\n&quot;);</span><br><span class="line"></span><br><span class="line"> if isdef(&quot;toolbox_skeletonlib&quot;) then</span><br><span class="line"></span><br><span class="line">  warning(&quot;Toolbox skeleton library is already loaded&quot;);</span><br><span class="line"></span><br><span class="line">  return;</span><br><span class="line"></span><br><span class="line"> end</span><br><span class="line"> etc_tlbx &#x3D; get_absolute_file_path(&quot;toolbox_skeleton.start&quot;);</span><br><span class="line"></span><br><span class="line"> etc_tlbx &#x3D; getshortpathname(etc_tlbx);</span><br><span class="line"></span><br><span class="line"> root_tlbx &#x3D; strncpy( etc_tlbx, length(etc_tlbx)-length(&quot;\etc\&quot;) );</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F;Load functions library</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line"></span><br><span class="line"> mprintf(&quot;\tLoad macros\n&quot;);</span><br><span class="line"></span><br><span class="line"> pathmacros &#x3D; pathconvert( root_tlbx ) + &quot;macros&quot; + filesep();</span><br><span class="line"></span><br><span class="line"> toolbox_skeletonlib &#x3D; lib(pathmacros);</span><br><span class="line">&#x2F;&#x2F; load gateways and Java libraries</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line"></span><br><span class="line"> verboseMode &#x3D; ilib_verbose();</span><br><span class="line"></span><br><span class="line"> ilib_verbose(0);</span><br><span class="line"></span><br><span class="line"> mprintf(&quot;\tLoad gateways\n&quot;);</span><br><span class="line"></span><br><span class="line"> exec(pathconvert(root_tlbx+&quot;&#x2F;sci_gateway&#x2F;loader_gateway.sce&quot;,%f));</span><br><span class="line"></span><br><span class="line"> mprintf(&quot;\tLoad Java libraries\n&quot;);</span><br><span class="line"></span><br><span class="line"> exec(pathconvert(root_tlbx+&quot;&#x2F;src&#x2F;java&#x2F;loader.sce&quot;,%f));</span><br><span class="line"></span><br><span class="line"> ilib_verbose(verboseMode); </span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; load localization</span><br><span class="line"></span><br><span class="line"> addlocalizationdomain(TOOLBOX_NAME, root_tlbx + &quot;&#x2F;locales&quot;);</span><br><span class="line">&#x2F;&#x2F; Load and add help chapter</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line"></span><br><span class="line"> if or(getscilabmode() &#x3D;&#x3D; [&quot;NW&quot;;&quot;STD&quot;]) then</span><br><span class="line"></span><br><span class="line">  mprintf(&quot;\tLoad help\n&quot;);</span><br><span class="line"></span><br><span class="line">  path_addchapter &#x3D; pathconvert(root_tlbx+&quot;&#x2F;jar&quot;);</span><br><span class="line"></span><br><span class="line">  if ( isdir(path_addchapter) &lt;&gt; [] ) then</span><br><span class="line"></span><br><span class="line">   add_help_chapter(TOOLBOX_NAME, path_addchapter, %F);</span><br><span class="line"></span><br><span class="line">  end</span><br><span class="line"></span><br><span class="line"> end</span><br><span class="line">&#x2F;&#x2F; Load demos</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line"></span><br><span class="line"> if or(getscilabmode() &#x3D;&#x3D; [&quot;NW&quot;;&quot;STD&quot;]) then</span><br><span class="line"></span><br><span class="line">  mprintf(&quot;\tLoad demos\n&quot;);</span><br><span class="line"></span><br><span class="line">  pathdemos &#x3D; pathconvert(root_tlbx+&quot;&#x2F;demos&#x2F;toolbox_skeleton.dem.gateway.sce&quot;, %F, %T);</span><br><span class="line"></span><br><span class="line">  add_demo(TOOLBOX_TITLE, pathdemos);</span><br><span class="line"></span><br><span class="line"> end</span><br><span class="line">&#x2F;&#x2F; Load Preferences GUI</span><br><span class="line">&#x2F;&#x2F;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line"></span><br><span class="line"> if getscilabmode() &#x3D;&#x3D; &quot;STD&quot; then</span><br><span class="line"></span><br><span class="line">  addModulePreferences(TOOLBOX_TITLE, root_tlbx, etc_tlbx + &quot;toolbox_skeleton_preferences.xml&quot;);</span><br><span class="line"></span><br><span class="line"> end</span><br><span class="line">endfunction</span><br><span class="line"></span><br><span class="line">toolbox_skeletonlib &#x3D; startModule();</span><br><span class="line"></span><br><span class="line">clear startModule; &#x2F;&#x2F; remove startModule on stack</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br></pre></td></tr></table></figure><h1 id="功能包概述"><a href="#功能包概述" class="headerlink" title="功能包概述"></a>功能包概述</h1><h2 id="功能包结构"><a href="#功能包结构" class="headerlink" title="功能包结构"></a>功能包结构</h2><p>Scilab的软件架构是模块化结构，即每一类功能都类似于插件。现对源码中的层次架构进行说明 。SiROS</p><p>在最上面的文件层次中，如图所示，层级目录中有10个文件夹。每个文件夹都有具体的含义，因为本文档针对的是对功能包的挂载，所以在本报告中需要关注的是modules文件夹，其中共有77个子文件夹，每一个子文件夹代表了其实现的一种功能，即每个子文件夹都是基本上代表了一类功能包。</p><img src="/Users/joker/Documents/chen_blog/source/images/截屏2021-01-19 下午1.53.13.png" alt="截屏2021-01-19 下午1.53.13" style="zoom:50%;" /><p>现对功能组件包进行层级说明，以一般功能包为例。</p><p>Scilab 功能包构建层次说明</p><table><thead><tr><th>目录</th><th>内容</th><th>备注</th></tr></thead><tbody><tr><td>demos</td><td>模块的演示，显示模块的功能</td><td></td></tr><tr><td>etc</td><td>模块配置（初始化和配置）</td><td></td></tr><tr><td>example</td><td>示例（readme.txt）</td><td></td></tr><tr><td>help</td><td>功能包的帮助（SiROS帮助说明）</td><td></td></tr><tr><td>includes</td><td>可以在外部使用的头文件（其他功能包或第三方程序）</td><td></td></tr><tr><td>locales</td><td>本地化文件</td><td></td></tr><tr><td>macros</td><td>模块的宏脚本文件</td><td></td></tr><tr><td>sci_getway</td><td>链接外部程序到SiROS中</td><td>子目录应为(sci_getway/c/ sci_getway/fortran/…)</td></tr><tr><td>src</td><td>功能包的核心源码</td><td>子目录应为(src/c/ src/fortran/…包括一些本地.h头文件)</td></tr><tr><td>tests/unit_tests</td><td>单一测试文件</td><td></td></tr><tr><td>tests/nonreg_tests</td><td>非递归测试文件</td><td></td></tr><tr><td>tests/benchmarks</td><td>基准测试文件</td><td></td></tr></tbody></table><p>其中在一个功能包中，所必须的文件包括：</p><ol><li>changelog.txt</li><li>licence.txt</li><li>Makefile.am</li><li>Makefile.in (generated from Makefile.am by automake)</li><li>xxxx.vcproj</li><li>version.xml</li><li>readme.txt</li><li>etc/<module>.start</li><li>etc/<module>.quit</li><li>includes/gw_<module>.h</li><li>sci_gateway/xxx/gw_<module>.c</li><li>sci_gateway/<module>_gateway.xml</li></ol><p>可选项为：</p><p>locales/en_US/<module>.po</p><p>需要遵守的原则为：</p><p>如果它们是模块的native函数，则应在src / <language> /生成函数的.h头文件。如果可以从其他模块使用该函数，则应将其头文件放入includes /。</p><p>应该尽量避免使用extern字段。可以使用头文件（.h）以实现相同的功能。</p><p>当创建可以由fortran调用的C函数时，需要在函数名称前添加宏C2F，并需要引入 machine.h 头文件。</p><h2 id="功能包加载方式"><a href="#功能包加载方式" class="headerlink" title="功能包加载方式"></a>功能包加载方式</h2><p>功能包的加载方式，由于其是与源码一起编译生成相应的 dll/so 文件，所以其加载方式是在 Scilab 启动时，直接自动加载其对应的 dll/so 文件或者在需要时由 Scilab 决定其加载与否。所以我们需要关注的是其编译的过程而非加载过程。</p><p>下一小节将以 xcos 实现编译的 Makefile 文件为例，进行说明。</p><h2 id="功能包示例"><a href="#功能包示例" class="headerlink" title="功能包示例"></a>功能包示例</h2><p><strong>src/modules/xcos/Makefile.am</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br></pre></td><td class="code"><pre><span class="line">#&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line"></span><br><span class="line">\#### Target ######</span><br><span class="line"></span><br><span class="line">modulename&#x3D;xcos</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">\#### xcos : Conf files ####</span><br><span class="line"></span><br><span class="line">libscixcos_la_rootdir &#x3D; $(mydatadir)</span><br><span class="line"></span><br><span class="line">libscixcos_la_root_DATA &#x3D; license.txt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">\#### xcos : init scripts ####</span><br><span class="line"></span><br><span class="line">libscixcos_la_etcdir &#x3D; $(mydatadir)&#x2F;etc</span><br><span class="line"></span><br><span class="line">libscixcos_la_etc_DATA &#x3D; \</span><br><span class="line"></span><br><span class="line">​    etc&#x2F;xcos.quit \</span><br><span class="line"></span><br><span class="line">​    etc&#x2F;xcos.start \</span><br><span class="line"></span><br><span class="line">​    etc&#x2F;Xcos-style.xml \</span><br><span class="line"></span><br><span class="line">​    etc&#x2F;XcosFile.xsd \</span><br><span class="line"></span><br><span class="line">​    etc&#x2F;xcos.xml \</span><br><span class="line"></span><br><span class="line">​    etc&#x2F;XcosConfiguration.xsd \</span><br><span class="line"></span><br><span class="line">​    etc&#x2F;palettes.xml \</span><br><span class="line"></span><br><span class="line">​    etc&#x2F;PaletteConfiguration.xsd \</span><br><span class="line"></span><br><span class="line">​    etc&#x2F;Modelica.xsd \</span><br><span class="line"></span><br><span class="line">​    etc&#x2F;XConfiguration-xcos.xsl \</span><br><span class="line"></span><br><span class="line">​    etc&#x2F;XConfiguration-xcos.xml</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">XCOS_CXX_SOURCES &#x3D;  \</span><br><span class="line"></span><br><span class="line">  src&#x2F;jni&#x2F;Xcos.cpp \</span><br><span class="line"></span><br><span class="line">  src&#x2F;jni&#x2F;Xcos.hxx \</span><br><span class="line"></span><br><span class="line">  src&#x2F;jni&#x2F;XcosCellFactory.cpp \</span><br><span class="line"></span><br><span class="line">  src&#x2F;jni&#x2F;XcosCellFactory.hxx \</span><br><span class="line"></span><br><span class="line">  src&#x2F;jni&#x2F;Palette.cpp \</span><br><span class="line"></span><br><span class="line">  src&#x2F;jni&#x2F;Palette.hxx \</span><br><span class="line"></span><br><span class="line">  src&#x2F;jni&#x2F;Modelica.cpp \</span><br><span class="line"></span><br><span class="line">  src&#x2F;jni&#x2F;Modelica.hxx \</span><br><span class="line"></span><br><span class="line">  src&#x2F;jni&#x2F;JavaController_wrap.cxx \</span><br><span class="line"></span><br><span class="line">  src&#x2F;cpp&#x2F;xcosUtilities.cpp \</span><br><span class="line"></span><br><span class="line">  src&#x2F;cpp&#x2F;loadStatus.cpp \</span><br><span class="line"></span><br><span class="line">  src&#x2F;jni&#x2F;JavaXMIResource_wrap.cxx</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">GIWS_WRAPPERS &#x3D; \</span><br><span class="line"></span><br><span class="line">  src&#x2F;jni&#x2F;Xcos.giws.xml \</span><br><span class="line"></span><br><span class="line">  src&#x2F;jni&#x2F;XcosCellFactory.giws.xml \</span><br><span class="line"></span><br><span class="line">  src&#x2F;jni&#x2F;Palette.giws.xml \</span><br><span class="line"></span><br><span class="line">  src&#x2F;jni&#x2F;Modelica.giws.xml</span><br><span class="line"></span><br><span class="line">SWIG_WRAPPERS &#x3D; \</span><br><span class="line"></span><br><span class="line">  src&#x2F;jni&#x2F;JavaController.i \</span><br><span class="line"></span><br><span class="line">  src&#x2F;jni&#x2F;JavaXMIResource.i</span><br><span class="line"></span><br><span class="line">  </span><br><span class="line"></span><br><span class="line">\# FORCE directors due to a bug into swig</span><br><span class="line"></span><br><span class="line">SWIG_OPTIONS&#x3D;-c++ -directors</span><br><span class="line"></span><br><span class="line">BUILT_SOURCES&#x3D;</span><br><span class="line"></span><br><span class="line">if GIWS</span><br><span class="line"></span><br><span class="line">BUILT_SOURCES+&#x3D;giws</span><br><span class="line"></span><br><span class="line">endif</span><br><span class="line"></span><br><span class="line">if SWIG</span><br><span class="line"></span><br><span class="line">BUILT_SOURCES+&#x3D;swig</span><br><span class="line"></span><br><span class="line">endif</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">GATEWAY_C_SOURCES &#x3D;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">GATEWAY_CXX_SOURCES &#x3D; \</span><br><span class="line"></span><br><span class="line">​    sci_gateway&#x2F;cpp&#x2F;sci_Xcos.cpp \</span><br><span class="line"></span><br><span class="line">​    sci_gateway&#x2F;cpp&#x2F;sci_warnBlockByUID.cpp \</span><br><span class="line"></span><br><span class="line">​    sci_gateway&#x2F;cpp&#x2F;sci_closeXcosFromScilab.cpp \</span><br><span class="line"></span><br><span class="line">​    sci_gateway&#x2F;cpp&#x2F;sci_xcosCellCreated.cpp \</span><br><span class="line"></span><br><span class="line">​    sci_gateway&#x2F;cpp&#x2F;sci_xcosUpdateBlock.cpp \</span><br><span class="line"></span><br><span class="line">​    sci_gateway&#x2F;cpp&#x2F;sci_xcosDiagramToScilab.cpp \</span><br><span class="line"></span><br><span class="line">​    sci_gateway&#x2F;cpp&#x2F;sci_xcosPalLoad.cpp \</span><br><span class="line"></span><br><span class="line">​    sci_gateway&#x2F;cpp&#x2F;sci_xcosPalCategoryAdd.cpp \</span><br><span class="line"></span><br><span class="line">​    sci_gateway&#x2F;cpp&#x2F;sci_xcosPalDelete.cpp \</span><br><span class="line"></span><br><span class="line">​    sci_gateway&#x2F;cpp&#x2F;sci_xcosPalMove.cpp \</span><br><span class="line"></span><br><span class="line">​    sci_gateway&#x2F;cpp&#x2F;sci_xcosPalEnable.cpp \</span><br><span class="line"></span><br><span class="line">​    sci_gateway&#x2F;cpp&#x2F;sci_xcosPalDisable.cpp \</span><br><span class="line"></span><br><span class="line">​    sci_gateway&#x2F;cpp&#x2F;sci_xcosPalGenerateIcon.cpp \</span><br><span class="line"></span><br><span class="line">​    sci_gateway&#x2F;cpp&#x2F;sci_xcosPalGet.cpp \</span><br><span class="line"></span><br><span class="line">​    sci_gateway&#x2F;cpp&#x2F;sci_xcosConfigureXmlFile.cpp \</span><br><span class="line"></span><br><span class="line">​    sci_gateway&#x2F;cpp&#x2F;sci_xcosAddToolsMenu.cpp \</span><br><span class="line"></span><br><span class="line">​    sci_gateway&#x2F;cpp&#x2F;sci_loadXcos.cpp \</span><br><span class="line"></span><br><span class="line">​    sci_gateway&#x2F;cpp&#x2F;sci_xcosDiagramToSiROS.cpp \</span><br><span class="line"></span><br><span class="line">​    sci_gateway&#x2F;cpp&#x2F;sci_xcosSimulationStarted.cpp</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">libscixcos_la_CPPFLAGS &#x3D; \</span><br><span class="line"></span><br><span class="line">  -I$(srcdir)&#x2F;includes&#x2F; \</span><br><span class="line"></span><br><span class="line">  -I$(srcdir)&#x2F;src&#x2F;jni&#x2F; \</span><br><span class="line"></span><br><span class="line">  -I$(srcdir)&#x2F;src&#x2F;cpp&#x2F; \</span><br><span class="line"></span><br><span class="line">  -I$(srcdir)&#x2F;src&#x2F;c&#x2F; \</span><br><span class="line"></span><br><span class="line">  -I$(top_srcdir)&#x2F;modules&#x2F;scicos&#x2F;includes&#x2F; \</span><br><span class="line"></span><br><span class="line">  -I$(top_srcdir)&#x2F;modules&#x2F;scicos_blocks&#x2F;includes&#x2F; \</span><br><span class="line"></span><br><span class="line">  -I$(top_srcdir)&#x2F;modules&#x2F;scicos_blocks&#x2F;src&#x2F;jni&#x2F; \</span><br><span class="line"></span><br><span class="line">  -I$(top_srcdir)&#x2F;modules&#x2F;dynamic_link&#x2F;includes&#x2F; \</span><br><span class="line"></span><br><span class="line">  -I$(top_srcdir)&#x2F;modules&#x2F;string&#x2F;includes&#x2F; \</span><br><span class="line"></span><br><span class="line">  -I$(top_srcdir)&#x2F;modules&#x2F;ast&#x2F;includes&#x2F;ast&#x2F; \</span><br><span class="line"></span><br><span class="line">  -I$(top_srcdir)&#x2F;modules&#x2F;ast&#x2F;includes&#x2F;exps&#x2F; \</span><br><span class="line"></span><br><span class="line">  -I$(top_srcdir)&#x2F;modules&#x2F;ast&#x2F;includes&#x2F;operations&#x2F; \</span><br><span class="line"></span><br><span class="line">  -I$(top_srcdir)&#x2F;modules&#x2F;ast&#x2F;includes&#x2F;parse&#x2F; \</span><br><span class="line"></span><br><span class="line">  -I$(top_srcdir)&#x2F;modules&#x2F;ast&#x2F;includes&#x2F;symbol&#x2F; \</span><br><span class="line"></span><br><span class="line">  -I$(top_srcdir)&#x2F;modules&#x2F;ast&#x2F;includes&#x2F;system_env&#x2F; \</span><br><span class="line"></span><br><span class="line">  -I$(top_srcdir)&#x2F;modules&#x2F;ast&#x2F;includes&#x2F;types&#x2F; \</span><br><span class="line"></span><br><span class="line">  -I$(top_srcdir)&#x2F;modules&#x2F;ast&#x2F;includes&#x2F;analysis&#x2F; \</span><br><span class="line"></span><br><span class="line">  -I$(top_srcdir)&#x2F;modules&#x2F;threads&#x2F;includes&#x2F; \</span><br><span class="line"></span><br><span class="line">  -I$(top_srcdir)&#x2F;modules&#x2F;console&#x2F;includes&#x2F; \</span><br><span class="line"></span><br><span class="line">  -I$(top_srcdir)&#x2F;modules&#x2F;jvm&#x2F;includes&#x2F; \</span><br><span class="line"></span><br><span class="line">  -I$(top_srcdir)&#x2F;modules&#x2F;output_stream&#x2F;includes&#x2F; \</span><br><span class="line"></span><br><span class="line">  -I$(top_srcdir)&#x2F;modules&#x2F;commons&#x2F;src&#x2F;jni&#x2F; \</span><br><span class="line"></span><br><span class="line">  -I$(top_srcdir)&#x2F;modules&#x2F;localization&#x2F;includes&#x2F; \</span><br><span class="line"></span><br><span class="line">  -I$(top_srcdir)&#x2F;modules&#x2F;fileio&#x2F;includes&#x2F; \</span><br><span class="line"></span><br><span class="line">  -I$(top_srcdir)&#x2F;modules&#x2F;api_scilab&#x2F;includes&#x2F; \</span><br><span class="line"></span><br><span class="line">  $(XML_FLAGS) \</span><br><span class="line"></span><br><span class="line">  $(JAVA_JNI_INCLUDE) \</span><br><span class="line"></span><br><span class="line">  $(AM_CPPFLAGS)</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">\# Without the xcos module</span><br><span class="line"></span><br><span class="line">libscixcos_disable_la_CPPFLAGS &#x3D; \</span><br><span class="line"></span><br><span class="line">  -I$(srcdir)&#x2F;includes&#x2F; \</span><br><span class="line"></span><br><span class="line">  -I$(top_srcdir)&#x2F;modules&#x2F;ast&#x2F;includes&#x2F;ast&#x2F; \</span><br><span class="line"></span><br><span class="line">  -I$(top_srcdir)&#x2F;modules&#x2F;ast&#x2F;includes&#x2F;exps&#x2F; \</span><br><span class="line"></span><br><span class="line">  -I$(top_srcdir)&#x2F;modules&#x2F;ast&#x2F;includes&#x2F;operations&#x2F; \</span><br><span class="line"></span><br><span class="line">  -I$(top_srcdir)&#x2F;modules&#x2F;ast&#x2F;includes&#x2F;parse&#x2F; \</span><br><span class="line"></span><br><span class="line">  -I$(top_srcdir)&#x2F;modules&#x2F;ast&#x2F;includes&#x2F;symbol&#x2F; \</span><br><span class="line"></span><br><span class="line">  -I$(top_srcdir)&#x2F;modules&#x2F;ast&#x2F;includes&#x2F;system_env&#x2F; \</span><br><span class="line"></span><br><span class="line">  -I$(top_srcdir)&#x2F;modules&#x2F;ast&#x2F;includes&#x2F;types&#x2F; \</span><br><span class="line"></span><br><span class="line">  -I$(top_srcdir)&#x2F;modules&#x2F;localization&#x2F;includes&#x2F; \</span><br><span class="line"></span><br><span class="line">  -I$(top_srcdir)&#x2F;modules&#x2F;output_stream&#x2F;includes \</span><br><span class="line"></span><br><span class="line">  $(XML_FLAGS) \</span><br><span class="line"></span><br><span class="line">  $(AM_CPPFLAGS)</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">XCOS_DISABLE_C_SOURCES &#x3D; src&#x2F;noxcos&#x2F;noxcos.c</span><br><span class="line"></span><br><span class="line">libscixcos_disable_la_SOURCES &#x3D; $(XCOS_DISABLE_C_SOURCES)</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">pkglib_LTLIBRARIES &#x3D; libscixcos-disable.la</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">if XCOS</span><br><span class="line"></span><br><span class="line">if GUI</span><br><span class="line"></span><br><span class="line">noinst_LTLIBRARIES &#x3D; libscixcos-algo.la</span><br><span class="line"></span><br><span class="line">pkglib_LTLIBRARIES +&#x3D; libscixcos.la</span><br><span class="line"></span><br><span class="line">endif</span><br><span class="line"></span><br><span class="line">endif</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">libscixcos_algo_la_SOURCES &#x3D; $(XCOS_C_SOURCES) $(XCOS_CXX_SOURCES)</span><br><span class="line"></span><br><span class="line">libscixcos_la_SOURCES &#x3D; $(GATEWAY_C_SOURCES) $(GATEWAY_CXX_SOURCES)</span><br><span class="line"></span><br><span class="line">libscixcos_algo_la_CPPFLAGS &#x3D; $(libscixcos_la_CPPFLAGS)</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">\# For the code check (splint)</span><br><span class="line"></span><br><span class="line">CHECK_SRC&#x3D; $(XCOS_C_SOURCES) $(GATEWAY_C_SOURCES)</span><br><span class="line"></span><br><span class="line">INCLUDE_FLAGS &#x3D; $(libscixcos_la_CPPFLAGS)</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">libscixcos_la_LIBADD &#x3D; libscixcos-algo.la \</span><br><span class="line"></span><br><span class="line">​       $(top_builddir)&#x2F;modules&#x2F;scicos&#x2F;libsciscicos.la \</span><br><span class="line"></span><br><span class="line">​       $(top_builddir)&#x2F;modules&#x2F;commons&#x2F;libscicommons.la \</span><br><span class="line"></span><br><span class="line">​       $(top_builddir)&#x2F;modules&#x2F;jvm&#x2F;libscijvm.la \</span><br><span class="line"></span><br><span class="line">​       $(LIBXML_LIBS)</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">\#### xcos : gateway declaration ####</span><br><span class="line"></span><br><span class="line">libscixcos_la_sci_gatewaydir &#x3D; $(mydatadir)&#x2F;sci_gateway</span><br><span class="line"></span><br><span class="line">libscixcos_la_sci_gateway_DATA &#x3D; sci_gateway&#x2F;xcos_gateway.xml</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">\#### xcos : images files ####</span><br><span class="line"></span><br><span class="line">\#</span><br><span class="line"></span><br><span class="line">\# Generated with:</span><br><span class="line"></span><br><span class="line">\#  $ find images&#x2F; -type f -printf &quot;%p \\\\\n&quot; -name *.svg -or -name *.gif -or -name *.jpg -or -name *.png -not -name *_pal.jpg -not -name gui |sort |awk &#39;$0 !~ &#x2F;gui&#x2F;&#123;print;&#125;&#39; |awk &#39;$0 !~ &#x2F;_pal.jpg&#x2F;&#123;print;&#125;&#39;</span><br><span class="line"></span><br><span class="line">libscixcos_la_imagesdir&#x3D;$(mydatadir)&#x2F;</span><br><span class="line"></span><br><span class="line">nobase_libscixcos_la_images_DATA &#x3D; \</span><br><span class="line"></span><br><span class="line">images&#x2F;blocks&#x2F;3DSCOPE.svg \</span><br><span class="line"></span><br><span class="line">images&#x2F;blocks&#x2F;ANDBLK.svg \</span><br><span class="line"></span><br><span class="line">images&#x2F;icons&#x2F;16x16&#x2F;actions&#x2F;align-horizontal-center.png \</span><br><span class="line"></span><br><span class="line">images&#x2F;icons&#x2F;16x16&#x2F;actions&#x2F;align-horizontal-left.png \</span><br><span class="line"></span><br><span class="line">images&#x2F;palettes&#x2F;ABS_VALUE.png \</span><br><span class="line"></span><br><span class="line">images&#x2F;palettes&#x2F;AFFICH_m.png \</span><br><span class="line"></span><br><span class="line">...##省略一些重复的图片信息</span><br><span class="line"></span><br><span class="line">if XCOS</span><br><span class="line"></span><br><span class="line">if GUI</span><br><span class="line"></span><br><span class="line">USEANT&#x3D;1</span><br><span class="line"></span><br><span class="line">endif</span><br><span class="line"></span><br><span class="line">endif</span><br><span class="line"></span><br><span class="line">include $(top_srcdir)&#x2F;Makefile.incl.am</span><br><span class="line"></span><br><span class="line">\#&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br></pre></td></tr></table></figure><p>另外在添加或移除功能包时，亦需要修改</p><p>src/modules/Makefile.am</p><p>src/Makefile.am</p><p>src/configure.ac</p><p>这三项文件，然后在源码所在目录下的终端执行以下命令：</p><p><strong>[user@localhost src] autoconf</strong></p><p>此命令根据configure.ac生成configure文件</p><p><strong>[user@localhost src] automake</strong></p><p>此命令根据Makefile.am生成Makefile.in文件</p><p><strong>[user@localhost src] ./configure &amp;&amp; make &amp;&amp; make install</strong></p><p>此命令对源码进行重新编译。</p><h1 id="功能包与工具箱对比"><a href="#功能包与工具箱对比" class="headerlink" title="功能包与工具箱对比"></a>功能包与工具箱对比</h1><p>工具箱加载时，运行主构建器文件（builder.sce），由主构建器文件调用个文件夹中的构建器文件（builder_**.sce），分别生成各加载文件，等待各加载文件构建成功后，再加载主加载文件，将工具箱加载到Scilab平台。</p><p>Scilab工具箱的层次结构十分严谨，src文件夹内的函数需要与sci_gateway文件夹内的网关函数紧密对应；各文件夹内的builder文件（builder*.sce）需要与各自文件夹内的文件对应，尤其是src中的c文件和sci_gateway中的c文件。</p><p>实际上，功能包和工具箱的结构类似，实现的本质上是一样的。这是都是因为基于Scilab是一个模块化架构的软件。也就是说，功能包和工具箱是可以相互转化的。功能包可以按照工具箱的加载方式进行加载。工具箱也可以根据功能包的方式实现与Scilab内联，即成为Scilab基本的一部分。</p>]]></content>
      
      
      <categories>
          
          <category> Scilab </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Scilab 仿真云化改造 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Scilab 云化改造(2)_XCOS 模块结构</title>
      <link href="2018/03/25/Xcos%E6%A8%A1%E5%9D%97%E7%BB%93%E6%9E%84/"/>
      <url>2018/03/25/Xcos%E6%A8%A1%E5%9D%97%E7%BB%93%E6%9E%84/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p>Scilab 在 Windows 下是以 bin 文件夹中 WScilex.exe 开始的，然后在进行初始化配置（包括选择模式、绘制界面、绑定相关按钮功能）后，调用相关的动态库（dll文件/由编译过程生成）完成所有的任务和功能。在 Linux 下运行亦相仿，启动文件为 bin 文件夹目录下的名为 scilab 的二进制可执行文件脚本，调用的动态库文件为.so文件</p><a id="more"></a><h2 id="XCOS-模块结构"><a href="#XCOS-模块结构" class="headerlink" title="XCOS 模块结构"></a>XCOS 模块结构</h2><p>按照 Scilab 官网上的一篇文章（<a href="https://wiki.scilab.org/ScilabWithinEclipse%EF%BC%89%E6%8F%90%E4%BE%9B%E7%9A%84%E6%AD%A5%E9%AA%A4%E8%BF%9B%E8%A1%8C%E5%90%8E%E7%BB%AD%E8%B0%83%E8%AF%95%E3%80%82%E4%B8%80%E5%85%B1%E5%AF%BC%E5%85%A5%E4%BA%8623%E4%B8%AA%E6%A8%A1%E5%9D%97%EF%BC%8C%E5%85%B6%E4%B8%AD">https://wiki.scilab.org/ScilabWithinEclipse）提供的步骤进行后续调试。一共导入了23个模块，其中</a> Xcos 需要 Scilab 中 10 个模块的依赖，分别是 <code>commons</code>、<code>localization</code>、<code>graph</code>、<code>types</code>、<code>gui</code>、<code>action_bindings</code>、<code>core</code>、<code>helptools</code>、<code>javasci</code> 及 <code>xcos</code></p><p>Scilab 中 Xcos 的依赖模块架构模型进行分解</p><img src="/Users/joker/Documents/chen_blog/source/images/截屏2021-01-19 下午1.18.11.png" alt="截屏2021-01-19 下午1.18.11" style="zoom:50%;" /><p>Scilab 中 Xcos 的依赖块有 10 个</p><p>**Model **类下有5个，分别是：</p><ol><li><code>commons</code>：软件运行过程时的一些相关[工具、接口]；</li><li><code>localization</code>：软件默认初始化相关参数；</li><li><code>graph</code>：软件运行过程中所生成，应用的图像，图片，图形；</li><li><code>types</code>：软件运行过程中的一些变量数据[enum，boolean，list，string等]；</li><li><code>xcos</code>：Xcos运行过程中的关键包，包含Xcos相关方法，接口，默认参数等。</li></ol><p>**View **类下只有1个：</p><p><code>gui</code>：实现人机交互的应用界面。 </p><p>**Controller **类下有4个，分别是：</p><ol><li><p><code>action_bindings</code>：为执行动作提供反馈或者接口；</p></li><li><p><code>core</code>：实现控制台指令和控制命令；</p></li><li><p><code>helptools</code>：链接到帮助文档，在线帮助，说明文档；</p></li><li><p><code>javasci</code>：调用Scilab，Sciab的相关操作及接口。</p></li></ol><p>xcos 模块是 Xcos 运行过程中的关键模块，Xcos 运行过程中的操作是由 xcos 模块的 java 源代码编译后实现的。通过把 Scilab 的 java 源代码导入到 eclipse，在 eclipse 的 package presentation 上选择用 Hierarchical 显示，即层次关系显示，就可以清楚地看到 xcos 模块的 java 源代码是由 14 个不同的子模块组成的。</p><p>这 14 个子模块是由 11 个大类及 3 个java文件组成的，分别是：<code>actions</code>、<code>block</code>、<code>configuration</code>、<code>graph</code>、<code>io</code>、<code>link</code>、<code>modelica</code>、<code>palette</code>、<code>port</code>、<code>preferences</code>、<code>utils</code>、<code>ViewPortTab.java</code>、<code>Xcos.java</code>、<code>XcosTab.java</code>。每个大类下又有若干个 <code>java</code> 文件。</p><p>xcos 模块的结构图如图所示：</p><img src="/Users/joker/Documents/chen_blog/source/images/clip_image004.png" alt="img" style="zoom:100%;" /><p>xcos 模块详细层次目录及各 java 文件的说明如下：</p><ul><li>Java 平台核心模块存放目录<ul><li>actions Xcos 菜单动作<ul><li>dialog 对话信息管理<ul><li>DebugLevelDialog.java 调试等级对话日志信息</li><li>SetContextDialog.java 设置关联数据对话日志信息</li><li>SetupDialog.java 仿真设置对话日志信息</li></ul></li></ul></li></ul></li></ul><p>​       1.1.2—AboutXcosAction.java Xcos的关于组件的生成及显示动作</p><p>​             —CloseAction.java 使用管理权限行使关闭组件动作指令</p><p>​             —CompileAction.java xcos的编译动作指令</p><p>​             —DebugLevelAction.java 调试等级指令动作响应</p><p>​             —DiagramBackgroundAction.java 图表背景颜色设置动作</p><p>​            —DumpAction.java 组件倾斜动作</p><p>—EditFromatAction.java 组件参数设置动作</p><p>—ExportAction.java 结果导出动作(当前组件仿真)</p><p>—ExportAllAction.java 结果导出动作(所有组件仿真)</p><p>—ExternalAction.java 引入外部组件动作</p><p>—FitDiagramToViewAction.java 组件大小调整动作响应</p><p>—InitModelicaAction.java 初始化Modelica编译动作指令</p><p>—NewDiagramAction.java 新建图表动作</p><p>—NormalViewAction.java 还原正常视图动作</p><p>—OpenAction.java 打开组件动作指令</p><p>—OpenInSciAction.java 在sci编辑器里打开sci文件</p><p>—PrintAction.java 打印动作</p><p>—QuitAction.java 关闭xcos动作指令</p><p>—RecentFileAction.java 最近处理过的文件显示</p><p>—SaveAction.java 保存当前仿真框图至默认目录</p><p>—SaveAsAction.java 将当前仿真框图另存至用户目录</p><p>—SetContextAction.java 关联数据到模块</p><p>—SetupAction.java 模块仿真设置</p><p>—ShowHideShadowAction.java 显示隐藏组件动作</p><p>—SimulationNotRunningAction.java 仿真运行标记动作</p><p>—StartAction.java 开始仿真指令</p><p>—StopAction0.java 停止仿真指令</p><p>—ViewDiagramBrowserAction.java 切换图表浏览窗口视图动作</p><p>—ViewGridAction.java 切换网格视图动作</p><p>—ViewViewportAction.java 视图窗口视图动作</p><p>—XcosDemonstrationsAction.java Xcos组件应用示范动作</p><p>—XcosDocumentationAction.java Xcos组件说明文档动作</p><p>1.2—block Xcos组件</p><p>​       1.2.1—action 模块动作</p><p>​          1.2.1.1—alignement 模块对齐</p><p>​              1.2.1.1.1—AlignBlockAction.java 模块对齐动作</p><p>​                  —AlignBlockActionBottom.java 模块底部对齐</p><p>​                  —AlignBlockActionCenter.java 模块中心对齐</p><p>​                  —AlignBlockActionLeft.java 模块左对齐</p><p>​                  —AlignBlockActionMiddle.java 模块居中对齐</p><p>​                  —AlignBlockActionRight.java 模块右对齐</p><p>​                  —AlignBlockActionTop.java 模块顶部对齐</p><p>​          1.2.1.2—BlockDocumentationAction.java 组件功能说明帮助</p><p>文档动作</p><p>​             —BlockParamtersAction.java 组件参数设置动作</p><p>​             —BorderColorAction.java 组件边界颜色变化动作</p><p>​             —CodeGenerationAction.java 代码自动生成动作</p><p>​             —FilledColorAction.java 填充组件颜色动作</p><p>​             —FlipAction.java 组件翻转动作</p><p>​             —MirrorAction.java 组件镜像翻转动作</p><p>​             —RegionToSuperblockAction.java 组件合并动作</p><p>​             —RotateAction.java 组件旋转动作</p><p>​             —ShowParentAction.java 显示父组件动作</p><p>​             —SuperblockMaskCreateAction.java 集合组件显示生成</p><p>​             —SuperblockMaskCustomizeAction.java 定制化集合组</p><p>件显示</p><p>​             —SuperblockMaskRemoveAction.java 移除集合组件显示</p><p>​             —SuperblockSelectedAction.java 集合组件被选中动作响应</p><p>​             —ViewDetailsAction.java 细节视图动作</p><p>​      1.2.2—io 数据输入输出</p><p>​          1.2.2.1—ContextUpdate.java 关联数据更新</p><p>​             —EventInBlock.java 组件输入事件</p><p>​             —EventOutBlock.java 组件输出事件</p><p>​             —ExplicitInBlock.java 组件显式输入</p><p>​             —ExplicitOutBlock.java 组件显式输出</p><p>​             —ImplicitInBlock.java 组件隐式输入</p><p>​             —ImplicitOutBlock.java 组件隐式输出</p><p>​       1.2.3—listener 监听</p><p>​          1.2.3.1—ProPortLabelingListener.java 分支标签端口监听</p><p>​             —SumPortLabelingListener.java 汇总标签端口监听</p><p>​       1.2.4—positionning 定位</p><p>​          1.2.4.1—BigSom.java 在实例实现SumPortLabelingListener</p><p>的连接(通过BIGSOM_f函数)</p><p>​             —GroundBlock.java  构造新的单输入组件</p><p>​             —Product.java 在实例实现与ProPortLabelingListener </p><p>的连接(通过PRODUCT_f函数)</p><p>​             —RoundBlock.java 实现一个有输入的循环组件</p><p>​             —Summation.java 在实例实现SumPortLabelingListener</p><p>的连接(通过SUMMATION函数) </p><p>​             —VoltageSensorBlock.java  构造新的多输入组件</p><p>​       1.2.5—AfficheBlock.java 实现一个可显示的组件</p><p>​         —BasicBlock.java 实现一个基本组件</p><p>​         —BlockFactory.java 组件的工厂设计模式</p><p>​         —SplitBlock.java 用于组件之间的相互连接</p><p>​         —SuperBlock.java 操作集合组件属性的方法</p><p>​         —TextBlock.java 用于注释的文本组件</p><p>   1.3—configuration 配置</p><p>​       1.3.1—model 模块</p><p>​          1.3.1.1—DocumentType.java 解析xml文件</p><p>​             —ObjectFactory.java 对象工厂化</p><p>​             —SettingType.java 修改或添加新的xml文件</p><p>​       1.3.2—utils 功能</p><p>​          1.3.2.1—ConfigurationConstants.java 常规配置类</p><p>​       1.3.3—ConfigurationManager.java    配置管理器类</p><p>1.4—graph 图表</p><p>​       1.4.1—swing Swing处理</p><p>​          1.4.1.1—handler</p><p>​               1.4.1.1.1—ConnectionHandler.java 用于处理多点链接的</p><p>连接处理程序</p><p>​                    —ConnectPreview.java 用于处理连接预览的程序</p><p>​                    —GraphHandler.java 用于处理双击图片的程序</p><p>​                    —SelectionCellsHandler.java 清除子处理程序释放内存</p><p>​          1.4.1.2—GraphComponent.java 实现Xcos与mxGraphComponent</p><p>的接口</p><p>​       1.4.2—CompilationEngineStatus.java 控制当前Scicos引擎的状态</p><p>​         —DiagramComparator.java 将所有图表按照距离根图表的距</p><p>离进行排序</p><p>​         —PaletteDiagram.java 组件图设置</p><p>​         —ScicosParameters.java Scicos参数设置</p><p>​         —SuperBlockDiagram.java 集合组件图表设置</p><p>​         —XcosDiagram.java Xcos图表及参数设置</p><p>   1.5—io 输入输出</p><p>​       1.5.1—codec 自动编码</p><p>​          1.5.1.1—BasicBlockCodec.java 基本组件代码</p><p>​             —BasicLinkCodec.java 基本链接代码</p><p>​             —BasicPortCodec.java 端口连接代码</p><p>​             —OrientationCodec.java 连接方向代码</p><p>​             —XcosCodec.java 注册Xcos使用的所有软件包，用于</p><p>序列化/反序列化</p><p>​             —XcosDiagramCodec.java 将例程与XcosDiagram连接</p><p>​             — XcosObjectCodec.java Xcos实例化对象代码      </p><p>​       1.5.2—scicos Scicos方法</p><p>​          1.5.2.1—AbstractElement.java 为一些抽象类提供方法</p><p>​             —BasicBlockInfo.java 将基本对象个性化</p><p>​        —BlockElement.java 在Scicos和Xcos之间执行块转换</p><p>​             —BlockGraphicElement.java 将对象进行包装保护(Graphic化)</p><p>​             —BlockModelElement.java 将对象进行包装保护(Model化)</p><p>​             —BlockPartsElement.java 为组件对象提供公共方法</p><p>​             —DiagramElement.java 在Scicos和Xcos之间执行图表</p><p>转换</p><p>​             —Element.java 为所有读/写scicos元素的元素对象提供</p><p>方法</p><p>​             —Handler.java 处理Xcos相关的属性</p><p>​             —InputPortElement.java 处理Scicos与Xcos之间输入</p><p>端口的转化</p><p>​             —LableElement.java 处理Scicos与Xcos之间属性标签</p><p>的转化</p><p>​             —LinkElement.java 处理Scicos与Xcos之间连接的转化</p><p>​             —OutputPortElement.java 处理Scicos与Xcos之间输出</p><p>端口的转化</p><p>​             —ScicosFormatException.java 用于处理Scicos元素默</p><p>认的抛出异常</p><p>​               —ScicosParametersElement.java 用于处理Scicos元素</p><p>默认的仿真参数</p><p>​             —ScilabDirectHandler.java 处理Scilab数据的直接访问</p><p>​       1.5.3—spec 规范</p><p>​          1.5.3.1—ContentEntry.java 用于处理特殊压缩文件内容</p><p>​             —DictionaryEntry.java 用于处理特殊压缩文件的地址</p><p>和文件清单</p><p>​             —Entry.java 从Xcos包中加载/储存ZipEntry文件</p><p>​             —XcosPackage.java 用于处理标准.zip格式的Xcos文件</p><p>​       1.5.4—XcosFileType.java 所有可被Xcos识别的文件类型</p><p>   1.6—link 连接</p><p>​       1.6.1—actions 动作</p><p>​          1.6.1.1—StyleAction.java 转换连接类型的基础类</p><p>​             —StyleHorizontalAction.java 设置水平连接动作</p><p>​        —StyleStraightAction.java 设置横向连接动作</p><p>​             —StyleVerticalAction.java 设置垂直连接动作</p><p>​             —TextAction.java 为连接设置文本说明</p><p>​       1.6.2—commandcontrol  指令控制</p><p>​          1.6.2.1—CommandControlLink.java 为控制端口和指令端口提</p><p>供连接</p><p>​       1.6.3—explicit 显式连接</p><p>​          1.6.3.1—ExplicitLink.java 为显式连接的输入输出提供连接</p><p>​       1.6.4—implicit 隐式连接</p><p>​          1.6.4.1—ImplicitLink.java 为隐式连接的输入输出提供连接</p><p>​       1.6.5—BasicLink.java 连接基础类</p><p>​          —LinkPortMap.java 枚举用于从ids获取链接和端口类</p><p>   1.7—modelica  Modelica语法规则</p><p>​       1.7.1—listener 监听类</p><p>​          1.7.1.1—FixDerivativesAction.java 规定当前模块的衍生类</p><p>​            —FixStatesAction.java 规定当前模块的动作状态</p><p>​             —SolveAction.java 模型求解</p><p>​             —StatisticsUpdater.java 当前模块在表事件改变时更</p><p>新统计信息</p><p>​       1.7.2—model 模块</p><p>​          1.7.2.1—Info.java 规定所有参数,状态,输入和输出的数据类</p><p>型和大小</p><p>​             —Model.java Modelica建模树状图</p><p>​             —ModelicaValue.java Modelica指定字符串</p><p>​             —ObjectFactory.java 对象工厂化公用方法</p><p>​             —Output.java 以Modelica的规范输出</p><p>​             —Struct.java 内容结构的部分树状化(并非节点分支)</p><p>​             —Terminal.java 结构化树的终端节点(总是节点分支)</p><p>​       1.7.3—view 视图</p><p>​          1.7.3.1—LableWithValue.java 具有坐标轴和标题的面板</p><p>​             —MainPanel.java Modelica模型的主要视图的初始化</p><p>​       1.7.4—Modelica.java 模型操作的主要类</p><p>​         —ModelicaController.java 用接口封装的模型控制器</p><p>​         —ModelicaMessages.java 包含当前包及子包的本地信息</p><p>​         —ModelStatistics.java 模型的函数统计</p><p>​         —TerminalAccessor.java 访问Terminal列表模型</p><p>​         —TerminalTableModel.java 建立Terminal列表模型</p><p>   1.8—palette 组件盘</p><p>​       1.8.1—actions 动作</p><p>​          1.8.1.1—ClosePalettesAction.java 关闭组件盘</p><p>​             —LoadAsPalAction.java 加载组件盘</p><p>​             —NewPaletteAction.java 新建组件盘</p><p>​             —ViewPaletteBrowserAction.java 组件盘浏览器视图动作，</p><p>管理组件盘选项卡</p><p>​       1.8.2—listener 监听</p><p>​          1.8.2.1—PaletteBlockMouseListener.java 组件盘组件的鼠标</p><p>操作监听</p><p>—PaletteManagerMouseListener.java 组件盘管理器的鼠标操作监听</p><p>​             —PaletteManagerTreeSelectionListener.java 监听组件盘组件</p><p>树管理被选中</p><p>​             —PaletteTreeTransferHandler.java 处理组件盘组件树</p><p>​       1.8.3—model 模块</p><p>​          1.8.3.1—Category.java 可包含子组件的父组件集合</p><p>​             —Custom.java 组件盘加载自定义组件图形</p><p>​             —ObjectFactory.java 对象工厂化实例通用方法</p><p>​             —Palette.java 在主视图中组件盘的显示</p><p>​             —PaletteBlock.java在主视图中组件(函数化)的图形显示</p><p>​             —PaletteNode.java 组件盘标记语言的Java类化</p><p>​             —PreLoaded.java 通过连接组件信息文件在组件盘中进</p><p>行预加载组件</p><p>​             —VariablePath.java 实时计算所需文件的绝对路径</p><p>​       1.8.4—view 视图</p><p>​          1.8.4.1—ModifiedFlowLayout.java 改良后的流动布局类(可在</p><p>JScrollPane中运行)</p><p>​             —PaletteBlockView.java 组件视图</p><p>​             —PaletteComponent.java 自定义默认组件</p><p>​             —PaletteConfiguratorListView.java 组件列表布局配置</p><p>​             —PaletteManagerPanel.java 组件盘窗口视图</p><p>​             —PaletteManagerView.java 实现组件的默认视图</p><p>​             —PaletteTreeModel.java 将组件节点绑定在处理启用标                                       志的特定模型上。</p><p>​             —PaletteView.java 实现组件块的视图</p><p>​       1.8.5—Palette.java 组件类</p><p>​         —PaletteBlockCtrl.java 用于在工作区上加载或放置组件</p><p>​         —PaletteManager.java 组件管理的主类</p><p>​         —PreLoadedElement.java 连接到预加载的组件时对组件进行解码</p><p>​         —StyleElement.java 对放置在工作区的组件进行解码</p><p>   1.9—port 端口</p><p>​       1.9.1—command 指令</p><p>​          1.9.1.1—CommandPort.java 用于将指令传达到受控的组件</p><p>​       1.9.2—control 控制</p><p>​          1.9.2.1—ControlPort.java 用于控制受控的组件</p><p>​       1.9.3—input 输入</p><p>​          1.9.3.1—ExplicitInputPort.java 显式输入端口公用方法</p><p>​             —ImplicitInputPort.java 隐式输入端口公用方法</p><p>​             —InputPort.java 输入端口(用于连接组件与内连函数)</p><p>​       1.9.4—output 输出</p><p>​          1.9.4.1— ExplicitOutputPort.java 显式输出端口公用方法</p><p>​             —ImplicitOutputPort.java 隐式输出端口公用方法</p><p>​             —OutputPort.java 输出端口(用于连接组件与内连函数)</p><p>​       1.9.5—BasicPort.java 端口的公共基础类</p><p>​         —Orientation.java 端口默认方向</p><p>​         —PortCheck.java 依据规则检查端口连接是否正确</p><p>   1.10—preferences 综合</p><p>​       1.10.1—XcosConfiguration.java Xcos的相关配置类</p><p>​          —XcosKeyMap.java Xcos的所有类的集合</p><p>​          —XcosOptions.java Xcos选项设置</p><p>   1.11—utils 功能</p><p>​       1.11.1—BlockChange.java 改变组件块</p><p>​          —BlockPositioning.java 组件块端口放置帮助类</p><p>​          —FileUtils.java 包含文件的一般处理</p><p>​          —PaletteComponent.java 默认组件盘样式</p><p>​          —XcosConstants.java 包含所有应用的默认常量</p><p>​          —XcosDelegates.java 包含Xcos所有引用的其他模块的函数</p><p>​          —XcosDialogs.java Xcos标准的对话框样式</p><p>​          —XcosEvent.java 所有Xcos的事件</p><p>​          —XcosMessages.java 所有Xcos要用到的本地化信息</p><p>   1.12—ViewPortTab.java Xcos的视图操作</p><p>   1.13—Xcos.java Xcos的入口类</p><p>   1.14—XcosTab.java Xcos的界面设计与事件动作绑定</p>]]></content>
      
      
      <categories>
          
          <category> Scilab </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Scilab 仿真云化改造 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Scilab 云化改造(1)_Scilab/Xcos概述</title>
      <link href="2018/02/15/ScilabXcos%20%E7%BC%96%E8%AF%91%E5%AE%89%E8%A3%85%E5%8F%8A%E8%BF%90%E8%A1%8C%E5%88%86%E6%9E%90/"/>
      <url>2018/02/15/ScilabXcos%20%E7%BC%96%E8%AF%91%E5%AE%89%E8%A3%85%E5%8F%8A%E8%BF%90%E8%A1%8C%E5%88%86%E6%9E%90/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><h2 id="Scilab-Xcos-概述"><a href="#Scilab-Xcos-概述" class="headerlink" title="Scilab/Xcos 概述"></a>Scilab/Xcos 概述</h2><p>Scilab 是由法国国家信息、自动化研究院的科学家们开发的“开放源码”软件。Scilab作为一种科学工程计算软件，其数据类型丰富，可以很方便地实现各种矩阵运算与图形显示，能应用于科学计算、数学建模、信号处理、决策优化、线性、非线性控制等各个方面。</p><p>Scilab 最初叫做 Basile，是作为 Meta2 项目的一部分在 INRIA 开发的。更名为 Scilab 以后，其开发由来自 INRIA Metalau和ENPC的研究人员组成的研究小组继续进行。2004年起，Scilab 的开发由一个国际性组织负责。</p><p>目前有两大类的通用科学软件：进行符号运算的计算机代数系统；进行数值计算和专门为科学应用设计的通用数值系统。自由开源软件 Scilab 就属于第二类。</p><p>Scilab 是一种动态确定对象类型的解释性语言。Scilab 能以二进制的形式在 Unix/Linux 工作站、Windows和Mac OS X等主要平台上运行。用户可以通过源代码编译 Scilab，该过程非常直观。</p><p>Scilab 可以作为一种脚本语言进行算法测试或者数值计算，同时，它也是一种编程语言，标准的Scilab库包含大约2000多种Scilab函数。Scilab语法简单，而且相关专用函数和运算符的使用使得矩阵的计算变得更为简单。因此，Scilab的程序相当紧凑，一般比用C、C++、Java等语言编写的程序短小。</p><p>Scilab主要用于科学计算，极大地方便了线性代数、数值积分、最优化等数值计算。扩展Scilab环境也比较简单，可以利用静态或动态连接库从外部向Scilab中引入新的功能函数，也可以使用Scilab结构体定义新的数据类型，或者为新的数据类型加载标准操作。Scilab的官方网站上有大量的工具箱来扩展它的一些特殊功能。</p><p>Scilab同时提供了大量的可视化函数，如二维图形、三维图形、等高线、参数化绘图和动画等，由这些函数生成的图形能以gif、postscript等多种格式输出。</p><p>Scilab还提供可以满足不同工程与科学需要的工具箱，例如Xcos图形化建模仿真，信号处理工具箱，图与网络工具箱等。</p><h2 id="Xcos-介绍"><a href="#Xcos-介绍" class="headerlink" title="Xcos 介绍"></a>Xcos 介绍</h2><p>Xcos 是 Scilab 的核心组件,是一个图形动态系统建模器和模拟器，在 INRIA，巴黎 Rocquencourt 中心的 Metalau 项目中开发。使用 Xcos，用户可以创建框图来模拟混合动力系统的动态过程，并将模型编译为可执行代码。Xcos可用于信号处理，系统控制，以及研究物理和生物系统。在新的扩展中，允许使用 Modelica 语言生成基于组件的电气和液压电路的模型。</p><h2 id="Scilab-架构概述及模块组成"><a href="#Scilab-架构概述及模块组成" class="headerlink" title="Scilab 架构概述及模块组成"></a>Scilab 架构概述及模块组成</h2><p>Scilab 的模块可分成三大类：核心模块、扩展工具模块、可选模块。本节详细介绍三大类的模块，及各分类的模块的架构层次。对每个模块的详细功能进行了解释说明。</p><p>Scilab 的核心架构思想是每一个功能或者任务都可以分为一个个单独模块（module）。每个模块都执行各自对应的功能和任务，相互尽量不影响，以使得  模块之间相互的耦合性降到最低。在 VS2012 的调试环境中，就可以很清楚的看出 Scilab 是由 142 个模块(其中包括在 Windows下运行所需要的五个单独模块WScilex(程序入口函数)，Dumpexts(运行win7及以上版本的支持模块)，windows_tools(运行dos窗口模式的支持工具模块)，GetWindowsVersion(获取Windows版本信息并配置调试环境模块)，Scilex(Windows下无GUI模式控制模块))组成。</p><p>另外，Scilab 的底层开发语言是 Fortran 语言，早期的图形界面是用tcl/tk语言开发的，再后来为了便于开发，模块中还包括了很多调用Fortran语言代码的C++链接模块（例如core和core_f，Scilab的核心处理代码core_f是由Fortran语言写的，但是为了开发方便也写了调用这些Fortran代码的C++链接代码文件core），此外还有支持之前的tclsci模块（从Scilab源码的支持文件prerequirements-scilab中获取）。以及一些相关模块的管理/引入/缺失警告/单元测试项目模块（例如，graphic模块就包含了graphic_export(图表输出项目)，nographic_export(图表输出缺失警告项目)/graphic_JAVA_tests(图表输出Java单元测试)等），这些相似模块可以合成一类。</p><p>总结以上，Scilab中实际可分析的模块有73个。</p><p>在阅读源码和相关文献资料后，我们将这73个模块分成了<strong>核心模块</strong>，<strong>扩展工具模块</strong>，<strong>可选模块</strong>三大类。</p><img src="/Users/joker/Documents/chen_blog/source/images/Xnip2021-01-19_12-48-29.jpg" alt="Xnip2021-01-19_12-48-29" style="zoom:70%;" /><h3 id="核心模块"><a href="#核心模块" class="headerlink" title="核心模块"></a>核心模块</h3><p><strong>核心模块</strong>是包含 Scilab 所必需核心源码模块的，删减后会导致 Scilab 不能进行正常工作的，一共有56个</p><p>然后，根据其实现的结果，又分为两大类: 功能类和算法类。</p><h4 id="功能类"><a href="#功能类" class="headerlink" title="功能类"></a>功能类</h4><p>在功能类中根据其职能又细分为两小类: 控制类和工具类。</p><h5 id="控制类"><a href="#控制类" class="headerlink" title="控制类"></a>控制类</h5><p>主要是实现Scilab相关底层与计算机进行交互的功能：</p><ul><li>console</li><li>core</li><li>dynamic_link    </li><li>io</li><li>libjvm </li><li>localization</li><li>mpi</li><li>parallel </li><li>preferences  </li><li>scilab_windows </li></ul><h5 id="工具类"><a href="#工具类" class="headerlink" title="工具类"></a>工具类</h5><p>主要包括了 Scilab 一般操作的相关命令集合：</p><ul><li><p>action_binding  </p><p>事件绑定封装，调用工具</p></li><li><p>api-scilab  </p><p>Scilab 调用的常用接口 api</p></li><li><p>boolean    </p></li><li><p>call_scilab </p></li><li><p>data_structures</p></li><li><p>differential_equations  </p></li><li><p>external_objects </p></li><li><p>fileio  </p></li><li><p>functions  </p></li><li><p>grphic  </p></li><li><p>gui  </p></li><li><p>hdf5  </p></li><li><p>history_brower </p></li><li><p>integer  </p></li><li><p>intersci  </p></li><li><p>Javasci  </p></li><li><p>out_stream   </p></li><li><p>parameters </p></li><li><p>renderer  </p></li><li><p>scinotes  </p></li><li><p>SetupAltas  </p></li><li><p>spreadsheet  </p></li><li><p>string </p></li><li><p>double   </p></li><li><p>time   </p></li><li><p>types  </p></li><li><p>xml </p></li></ul><h4 id="算法类"><a href="#算法类" class="headerlink" title="算法类"></a>算法类</h4><p>基本功能算法和高级复杂算法。</p><h5 id="基本算法类"><a href="#基本算法类" class="headerlink" title="基本算法类"></a>基本算法类</h5><p>包括简单的科学计算方法：</p><ul><li>cacsd   </li><li>dcd_f   </li><li>eispack_f   </li><li>elementary_functions</li><li>interpolation  </li><li>linear_algebra  </li><li>linpack_f  </li><li>polynomialse </li><li>randlib  </li><li>slatec_f  </li><li>slicot_f   </li><li>statistics </li></ul><h5 id="高级复杂算法"><a href="#高级复杂算法" class="headerlink" title="高级复杂算法"></a>高级复杂算法</h5><p>提供了很多优秀的复杂计算方法(可提高计算性能)：</p><ul><li><p>arnoldi  </p></li><li><p>optimization  </p></li><li><p>singnal_processing  </p></li><li><p>special_functions </p></li><li><p>symbolic  </p></li><li><p>umfpack  </p></li><li><p>sparse</p><h1 id="Scilab-架构分析"><a href="#Scilab-架构分析" class="headerlink" title="Scilab 架构分析"></a>Scilab 架构分析</h1></li></ul><p>根据体系结构分层分析，Scilab 的依赖项关系图有如下生成：</p><img src="/Users/joker/Documents/chen_blog/source/images/截屏2021-01-19 下午12.57.23.png" alt="截屏2021-01-19 下午12.57.23" style="zoom:70%;" /><p>由图可知，一共有 157 个节点，1518 个链接，完整的显示了 Scilab 个项目之间的依赖关系。其中，箭头指向性为一个文件被另一个文件所依赖。</p><p>根据官网提供的一些资料支持，我们将上图简化，生成了下图的简易版依赖关系视图：</p><img src="/Users/joker/Documents/chen_blog/source/images/截屏2021-01-19 下午12.58.20.png" alt="截屏2021-01-19 下午12.58.20" style="zoom:70%;" /><p>图中共有 23 个节点，88 个链接。此图比较清晰完整的显示了 Scilab 主要模块之间的依赖关系。</p>]]></content>
      
      
      <categories>
          
          <category> Scilab </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Scilab 仿真云化改造 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
